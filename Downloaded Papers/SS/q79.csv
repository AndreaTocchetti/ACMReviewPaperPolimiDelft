Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
0,"Olivia M. Brown, B. Dillman","Proceedings of the Robust Artificial Intelligence System Assurance (RAISA) Workshop 2022",2022,"","","","",1,"2022-07-13 09:31:23","","","","",,,,,0,0.00,0,2,1,"The Robust Artificial Intelligence System Assurance (RAISA) workshop will focus on research, development and application of robust artificial intelligence (AI) and machine learning (ML) systems. Rather than studying robustness with respect to particular ML algorithms, our approach will be to explore robustness assurance at the system architecture level, during both development and deployment, and within the human-machine teaming context. While the research community is converging on robust solutions for individual AI models in specific scenarios, the problem of evaluating and assuring the robustness of an AI system across its entire life cycle is much more complex. Moreover, the operational context in which AI systems are deployed necessitates consideration of robustness and its relation to principles of fairness, privacy, and explainability.","",""
20,"L. McCoy, Sujay Nagaraj, F. Morgado, V. Harish, Sunit Das, L. Celi","What do medical students actually need to know about artificial intelligence?",2020,"","","","",2,"2022-07-13 09:31:23","","10.1038/s41746-020-0294-7","","",,,,,20,10.00,3,6,2,"","",""
427,"D. Ting, L. Pasquale, L. Peng, J. P. Campbell, Aaron Y. Lee, R. Raman, G. Tan, L. Schmetterer, P. Keane, T. Wong","Artificial intelligence and deep learning in ophthalmology",2018,"","","","",3,"2022-07-13 09:31:23","","10.1136/bjophthalmol-2018-313173","","",,,,,427,106.75,43,10,4,"Artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest in recent years. DL has been widely adopted in image recognition, speech recognition and natural language processing, but is only beginning to impact on healthcare. In ophthalmology, DL has been applied to fundus photographs, optical coherence tomography and visual fields, achieving robust classification performance in the detection of diabetic retinopathy and retinopathy of prematurity, the glaucoma-like disc, macular oedema and age-related macular degeneration. DL in ocular imaging may be used in conjunction with telemedicine as a possible solution to screen, diagnose and monitor major eye diseases for patients in primary care and community settings. Nonetheless, there are also potential challenges with DL application in ophthalmology, including clinical and technical challenges, explainability of the algorithm results, medicolegal issues, and physician and patient acceptance of the AI ‘black-box’ algorithms. DL could potentially revolutionise how ophthalmology is practised in the future. This review provides a summary of the state-of-the-art DL systems described for ophthalmic applications, potential challenges in clinical deployment and the path forward.","",""
0,"Hatma Suryotrisongko, Y. Musashi, A. Tsuneda, K. Sugitani","Robust Botnet DGA Detection: Blending XAI and OSINT for Cyber Threat Intelligence Sharing",2022,"","","","",4,"2022-07-13 09:31:23","","10.1109/ACCESS.2022.3162588","","",,,,,0,0.00,0,4,1,"We investigated 12 years DNS query logs of our campus network and identified phenomena of malicious botnet domain generation algorithm (DGA) traffic. DGA-based botnets are difficult to detect using cyber threat intelligence (CTI) systems based on blocklists. Artificial intelligence (AI)/machine learning (ML)-based CTI systems are required. This study (1) proposed a model to detect DGA-based traffic based on statistical features with datasets comprising 55 DGA families, (2) discussed how CTI can be expanded with computable CTI paradigm, and (3) described how to improve the explainability of the model outputs by blending explainable AI (XAI) and open-source intelligence (OSINT) for trust problems, an antidote for skepticism to the shared models and preventing automation bias. We define the XAI-OSINT blending as aggregations of OSINT for AI/ML model outcome validation. Experimental results show the effectiveness of our models (96.3% accuracy). Our random forest model provides better robustness against three state-of-the-art DGA adversarial attacks (CharBot, DeepDGA, MaskDGA) compared with character-based deep learning models (Endgame, CMU, NYU, MIT). We demonstrate the sharing mechanism and confirm that the XAI-OSINT blending improves trust for CTI sharing as evidence to validate our proposed computable CTI paradigm to assist security analysts in security operations centers using an automated, explainable OSINT approach (for second opinion). Therefore, the computable CTI reduces manual intervention in critical cybersecurity decision-making.","",""
0,"Aditya Saini, Ranjitha Prasad","Select Wisely and Explain: Active Learning and Probabilistic Local Post-hoc Explainability",2021,"","","","",5,"2022-07-13 09:31:23","","","","",,,,,0,0.00,0,2,1,"Albeit the tremendous performance improvements in designing complex artificial intelligence (AI) systems in data-intensive do-mains, the black-box nature of these systems leads to the lack of trustworthiness. Post-hoc interpretability methods explain the prediction of a black-box ML model for a single instance, and such explanations are being leveraged by domain experts to diagnose the underlying biases of these models. Despite their efficacy in providing valuable insights, existing approaches fail to deliver consistent and reliable explanations. In this paper, we propose an active learning-based technique called UnRAvEL (Uncertainty driven Robust Active Learning Based Locally Faithful Explanations), which consists of a novel acquisition function that is locally faithful and uses uncertainty-driven sampling based on the posterior distribution on the probabilistic locality using Gaussian process regression (GPR). We present a theoretical analysis of UnRAvEL by treating it as a local optimizer and analyzing its regret in terms of instantaneous regrets over a global optimizer. We demonstrate the efficacy of the local samples generated by UnRAvEL by incorporating different kernels such as the Matern and linear kernels in GPR. Through a series of experiments, we show that UnRAvEL outperforms the baselines with respect to stability and local fidelity on several real-world models and datasets. We show that UnRAvEL is an efficient surrogate dataset generator by deriving importance scores on this surrogate dataset using sparse linear models. We also showcase the sample efficiency and flexibility of the developed framework on the Imagenet dataset using a pre-trained ResNet model.","",""
13,"R. Shafik, A. Wheeldon, A. Yakovlev","Explainability and Dependability Analysis of Learning Automata based AI Hardware",2020,"","","","",6,"2022-07-13 09:31:23","","10.1109/iolts50870.2020.9159725","","",,,,,13,6.50,4,3,2,"Explainability remains the holy grail in designing the next-generation pervasive artificial intelligence (AI) systems. Current neural network based AI design methods do not naturally lend themselves to reasoning for a decision making process from the input data. A primary reason for this is the overwhelming arithmetic complexity.Built on the foundations of propositional logic and game theory, the principles of learning automata are increasingly gaining momentum for AI hardware design. The lean logic based processing has been demonstrated with significant advantages of energy efficiency and performance. The hierarchical logic underpinning can also potentially provide opportunities for by-design explainable and dependable AI hardware. In this paper, we study explainability and dependability using reachability analysis in two simulation environments. Firstly, we use a behavioral SystemC model to analyze the different state transitions. Secondly, we carry out illustrative fault injection campaigns in a low-level SystemC environment to study how reachability is affected in the presence of hardware stuck-at 1 faults. Our analysis provides the first insights into explainable decision models and demonstrates dependability advantages of learning automata driven AI hardware design.","",""
6,"Kaveri A. Thakoor, Sharath C. Koorathota, D. Hood, P. Sajda","Robust and Interpretable Convolutional Neural Networks to Detect Glaucoma in Optical Coherence Tomography Images",2020,"","","","",7,"2022-07-13 09:31:23","","10.1109/tbme.2020.3043215","","",,,,,6,3.00,2,4,2,"Recent studies suggest that deep learning systems can now achieve performance on par with medical experts in diagnosis of disease. A prime example is in the field of ophthalmology, where convolutional neural networks (CNNs) have been used to detect retinal and ocular diseases. However, this type of artificial intelligence (AI) has yet to be adopted clinically due to questions regarding robustness of the algorithms to datasets collected at new clinical sites and a lack of explainability of AI-based predictions, especially relative to those of human expert counterparts. In this work, we develop CNN architectures that demonstrate robust detection of glaucoma in optical coherence tomography (OCT) images and test with concept activation vectors (TCAVs) to infer what image concepts CNNs use to generate predictions. Furthermore, we compare TCAV results to eye fixations of clinicians, to identify common decision-making features used by both AI and human experts. We find that employing fine-tuned transfer learning and CNN ensemble learning create end-to-end deep learning models with superior robustness compared to previously reported hybrid deep-learning/machine-learning models, and TCAV/eye-fixation comparison suggests the importance of three OCT report sub-images that are consistent with areas of interest fixated upon by OCT experts to detect glaucoma. The pipeline described here for evaluating CNN robustness and validating interpretable image concepts used by CNNs with eye movements of experts has the potential to help standardize the acceptance of new AI tools for use in the clinic.","",""
0,"Dongfang Li, Dongfang Li, Baotian Hu, Qingcai Chen, Tujie Xu, Jingcong Tao, Yunan Zhang","Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction",2021,"","","","",8,"2022-07-13 09:31:23","","10.1609/aaai.v36i10.21342","","",,,,,0,0.00,0,7,1,"Recent works have shown explainability and robustness are two crucial ingredients of trustworthy and reliable text classification. However, previous works usually address one of two aspects: i) how to extract accurate rationales for explainability while being beneficial to prediction; ii) how to make the predictive model robust to different types of adversarial attacks. Intuitively, a model that produces helpful explanations should be more robust against adversarial attacks, because we cannot trust the model that outputs explanations but changes its prediction under small perturbations. To this end, we propose a joint classification and rationale extraction model named AT-BMC. It includes two key mechanisms: mixed Adversarial Training (AT) is designed to use various perturbations in discrete and embedding space to improve the model’s robustness, and Boundary Match Constraint (BMC) helps to locate rationales more precisely with the guidance of boundary information. Performances on benchmark datasets demonstrate that the proposed AT-BMC outperforms baselines on both classification and rationale extraction by a large margin. Robustness analysis shows that the proposed AT-BMC decreases the attack success rate effectively by up to 69%. The results indicate that there are connections between robust models and better explanations.","",""
1,"V. Venugopal, R. Takhar, S. Gupta, A. Saboo, V. Mahajan","Clinical Explainability Failure (CEF) & Explainability Failure Ratio (EFR): changing the way we validate classification algorithms?",2020,"","","","",9,"2022-07-13 09:31:23","","10.1101/2020.08.12.20169607","","",,,,,1,0.50,0,5,2,"Adoption of Artificial Intelligence (AI) algorithms into the clinical realm will depend on their inherent trustworthiness, which is built not only by robust validation studies but is also deeply linked to the explainability and interpretability of the algorithms. Most validation studies for medical imaging AI report performance of algorithms on study level labels and lay little emphasis on measuring the accuracy of explanations generated by these algorithms in the form of heat maps or bounding boxes, especially in true positive cases. We propose a new metric, Explainability Failure Ratio (EFR), derived from Clinical Explainability Failure (CEF) to address this gap in AI evaluation. We define an Explainability Failure as a case where the classification generated by an AI algorithm matches with study level ground truth but the explanation output generated by the algorithm is inadequate to explain the algorithms output. We measured EFR for two algorithms that automatically detect consolidation on chest X rays to determine the applicability of the metric and observed a lower EFR for the model that had lower sensitivity for identifying consolidation on chest X rays, implying that trustworthiness of a model should be determined not only by routine statistical metrics but also by novel clinically oriented models.","",""
0,"V. Venugopal, R. Takhar, Salil Gupta, V. Mahajan","Clinical Explainability Failure (CEF) & Explainability Failure Ratio (EFR) – Changing the Way We Validate Classification Algorithms",2022,"","","","",10,"2022-07-13 09:31:23","","10.1007/s10916-022-01806-2","","",,,,,0,0.00,0,4,1,"","",""
0,"Véronique M. Gomes, P. Melo-Pinto","Towards robust Machine Learning models for grape ripeness assessment",2021,"","","","",11,"2022-07-13 09:31:23","","10.1109/JCSSE53117.2021.9493822","","",,,,,0,0.00,0,2,1,"Artificial intelligence methods need to be more transparent for wider acceptance by the industry. In particular deep neural networks (DNN) are not explainable, due to the complex processes the input undergo. The present work addresses model explainability for wine grapes quality assessment through 1D-CNN, using regression activation maps (RAM) to show the contribution score of each wavelength for the prediction of sugar content. This way we identify the relevant regions related to this enological parameter. The results obtained indicate that the proposed approach can successfully highlight important spectral regions related to sugars absorption, improving the current state of the art, and opening way to dimensionality reduction methods and further model interpretation.","",""
51,"A. Garcez, L. Lamb","Neurosymbolic AI: The 3rd Wave",2020,"","","","",12,"2022-07-13 09:31:23","","","","",,,,,51,25.50,26,2,2,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.","",""
26,"Giuseppe Futia, Antonio Vetrò","On the Integration of Knowledge Graphs into Deep Learning Models for a More Comprehensible AI - Three Challenges for Future Research",2020,"","","","",13,"2022-07-13 09:31:23","","10.3390/info11020122","","",,,,,26,13.00,13,2,2,"Deep learning models contributed to reaching unprecedented results in prediction and classification tasks of Artificial Intelligence (AI) systems. However, alongside this notable progress, they do not provide human-understandable insights on how a specific result was achieved. In contexts where the impact of AI on human life is relevant (e.g., recruitment tools, medical diagnoses, etc.), explainability is not only a desirable property, but it is -or, in some cases, it will be soon-a legal requirement. Most of the available approaches to implement eXplainable Artificial Intelligence (XAI) focus on technical solutions usable only by experts able to manipulate the recursive mathematical functions in deep learning algorithms. A complementary approach is represented by symbolic AI, where symbols are elements of a lingua franca between humans and deep learning. In this context, Knowledge Graphs (KGs) and their underlying semantic technologies are the modern implementation of symbolic AI—while being less flexible and robust to noise compared to deep learning models, KGs are natively developed to be explainable. In this paper, we review the main XAI approaches existing in the literature, underlying their strengths and limitations, and we propose neural-symbolic integration as a cornerstone to design an AI which is closer to non-insiders comprehension. Within such a general direction, we identify three specific challenges for future research—knowledge matching, cross-disciplinary explanations and interactive explanations.","",""
2,"Melissa D. McCradden","When is accuracy off-target?",2021,"","","","",14,"2022-07-13 09:31:23","","10.1038/s41398-021-01479-4","","",,,,,2,2.00,2,1,1,"","",""
2,"Ayoub El Qadi, Natalia Díaz Rodríguez, M. Trocan, Thomas Frossard","Explaining Credit Risk Scoring through Feature Contribution Alignment with Expert Risk Analysts",2021,"","","","",15,"2022-07-13 09:31:23","","","","",,,,,2,2.00,1,4,1,"Credit assessments activities are essential for financial institutions and allow the global economy to grow. Building robust, solid and accurate models that estimate the probability of a default of a company is mandatory for credit insurance companies, specially when it comes to bridging the trade finance gap. Automating the risk assessment process will allow credit risk experts to reduce their workload and focus on the critical and complex cases, as well as to improve the loan approval process by reducing the time to process the application. The recent developments in Artificial Intelligence are offering new powerful opportunities. However, most AI techniques are labelled as blackbox models due to their lack of explainability. For both users and regulators, in order to deploy such technologies at scale, being able to understand the model logic is a must to grant accurate and ethical decision making. In this study, we focus on companies credit scoring and we benchmark different machine learning models. The aim is to build a model to predict whether a company will experience financial problems in a given time horizon. We address the black box problem using eXplainable Artificial Techniques –in particular, post-hoc explanations using SHapley Additive exPlanations. We bring light by providing an expertaligned feature relevance score highlighting the disagreement between a credit risk expert and a model feature attribution explanation in order to better quantify the convergence towards a better human-aligned decision making.","",""
1,"S. Lo, Yiqiao Yin","A Novel Interaction-based Methodology Towards Explainable AI with Better Understanding of Pneumonia Chest X-ray Images",2021,"","","","",16,"2022-07-13 09:31:23","","10.3390/a14110337","","",,,,,1,1.00,1,2,1,"  In the field of eXplainable AI (XAI), robust “black-box” algorithms such as Convolutional Neural Networks (CNNs) are known for making high prediction performance. However, the ability to explain and interpret these algorithms still require innovation in the understanding of influential and, more importantly, ex-plainable features that directly or indirectly impact the performance of predictivity. A number of methods existing in literature focus on visualization techniques but the concepts of explainability and interpretability still require rigorous definition. In view of the above needs, this paper proposes an interaction-based methodology – Influence Score (I-score) – to screen out the noisy and non-informative variables in the images hence it nourishes an environment with explainable and interpretable features that are directly associated to feature predictiv-ity. We apply the proposed method on a real world application in Pneumonia Chest X-ray Image data set and produced state-of-the-art results. We demonstrate how to apply the proposed approach for more general big data problems by improving the explainability and in-terpretability without sacrificing the prediction performance. The contribution of this paper opens a novel angle that moves the community closer to the future pipelines of XAI problems.","",""
0,"Vidhi Pitroda, M. Fouda, Z. Fadlullah","An Explainable AI Model for Interpretable Lung Disease Classification",2021,"","","","",17,"2022-07-13 09:31:23","","10.1109/IoTaIS53735.2021.9628573","","",,,,,0,0.00,0,3,1,"In this paper, we develop a framework for lung disease identification from chest X-ray images by differentiating the novel coronavirus disease (COVID-19) or other disease-induced lung opacity samples from normal cases. We perform image processing tasks, segmentation, and train a customized Convolutional Neural Network (CNN) that obtains reasonable performance in terms of classification accuracy. To address the black-box nature of this complex classification model, which emerged as a key barrier to applying such Artificial Intelligence (AI)-based methods for automating medical decisions raising skepticism among clinicians, we address the need to quantitatively interpret the performance of our adopted approach using a Layer-wise Relevance Propagation (LRP)-based method. We also used a pixel flipping-based, robust performance metric to evaluate the explainability of our adopted LRP method and compare its performance with other explainable methods, such as Local Interpretable Model Agnostic Explanation (LIME), Guided Backpropagation (GB), and Deep Taylor Decomposition (DTD).","",""
0,"E. Blasch, Haoran Li, Zhihao Ma, Yang Weng","The Powerful Use of AI in the Energy Sector: Intelligent Forecasting",2021,"","","","",18,"2022-07-13 09:31:23","","","","",,,,,0,0.00,0,4,1,"Artificial Intelligence (AI) techniques continue to broaden across governmental and public sectors, such as power and energy which serve as critical infrastructures for most societal operations. However, due to the requirements of reliability, accountability, and explainability, it is risky to directly apply AI-based methods to power systems because society cannot afford cascading failures and large-scale blackouts, which easily cost billions of dollars. To meet society requirements, this paper proposes a methodology to develop, deploy, and evaluate AI systems in the energy sector by: (1) understanding the power system measurements with physics, (2) designing AI algorithms to forecast the need, (3) developing robust and accountable AI methods, and (4) creating reliable measures to evaluate the performance of the AI model. The goal is to provide a high level of confidence to energy utility users. For illustration purposes, the paper uses power system event forecasting (PEF) as an example, which carefully analyzes synchrophasor patterns measured by the Phasor Measurement Units (PMUs). Such a physical understanding leads to a data-driven framework that reduces the dimensionality with physics and forecasts the event with high credibility. Specifically, for dimensionality reduction, machine learning arranges physical information from different dimensions, resulting inefficient information extraction. For event forecasting, the supervised learning model fuses the results of different models to increase the confidence. Finally, comprehensive experiments demonstrate the high accuracy, efficiency, and reliability as compared to other state-of-the-art machine learning methods.","",""
0,"F. Ventura, S. Greco, D. Apiletti, T. Cerquitelli","Explaining the Deep Natural Language Processing by Mining Textual Interpretable Features",2021,"","","","",19,"2022-07-13 09:31:23","","","","",,,,,0,0.00,0,4,1,"Despite the high accuracy offered by state-of-the-art deep natural-language models (e.g. LSTM, BERT), their application in reallife settings is still widely limited, as they behave like a black-box to the end-user. Hence, explainability is rapidly becoming a fundamental requirement of future-generation data-driven systems based on deep-learning approaches. Several attempts to fulfill the existing gap between accuracy and interpretability have been done. However, robust and specialized xAI (Explainable Artificial Intelligence) solutions tailored to deep natural-language models are still missing. We propose a new framework, named T-EBAnO, which provides innovative prediction-local and class-based model-global explanation strategies tailored to black-box deep natural-language models. Given a deep NLP model and the textual input data, T-EBAnO provides an objective, human-readable, domain-specific assessment of the reasons behind the automatic decision-making process. Specifically, the framework extracts sets of interpretable features mining the inner knowledge of the model. Then, it quantifies the influence of each feature during the prediction process by exploiting the novel normalized Perturbation Influence Relation index at the local level and the novel Global Absolute Influence and Global Relative Influence indexes at the global level. The effectiveness and the quality of the local and global explanations obtained with T-EBAnO are proved on (i) a sentiment analysis task performed by a fine-tuned BERT model, and (ii) a toxic comment classification task performed by an LSTM model.","",""
0,"A. Sarkar","QKSA: Quantum Knowledge Seeking Agent",2021,"","","","",20,"2022-07-13 09:31:23","","","","",,,,,0,0.00,0,1,1,"In this article we present the motivation and the core thesis towards the implementation of a Quantum Knowledge Seeking Agent (QKSA). QKSA is a general reinforcement learning agent that can be used to model classical and quantum dynamics. It merges ideas from universal artificial general intelligence, constructor theory and genetic programming to build a robust and general framework for testing the capabilities of the agent in a variety of environments. It takes the artificial life (or, animat) path to artificial general intelligence where a population of intelligent agents are instantiated to explore valid ways of modelling the perceptions. The multiplicity and survivability of the agents are defined by the fitness, with respect to the explainability and predictability, of a resource-bounded computational model of the environment. This general learning approach is then employed to model the physics of an environment based on subjective observer states of the agents. A specific case of quantum process tomography as a general modelling principle is presented. The various background ideas and a baseline formalism are discussed in this article which sets the groundwork for the implementations of the QKSA that are currently in active development.","",""
0,"A. Sarkar","J ul 2 02 1 QKSA : Quantum Knowledge Seeking Agent motivation , core thesis and baseline framework",2021,"","","","",21,"2022-07-13 09:31:23","","","","",,,,,0,0.00,0,1,1,"In this article we present the motivation and the core thesis towards the implementation of a Quantum Knowledge Seeking Agent (QKSA). QKSA is a general reinforcement learning agent that can be used to model classical and quantum dynamics. It merges ideas from universal artificial general intelligence, constructor theory and genetic programming to build a robust and general framework for testing the capabilities of the agent in a variety of environments. It takes the artificial life (or, animat) path to artificial general intelligence where a population of intelligent agents are instantiated to explore valid ways of modeling the perceptions. The multiplicity and survivability of the agents are defined by the fitness, with respect to the explainability and predictability, of a resource-bounded computational model of the environment. This general learning approach is then employed to model the physics of an environment based on subjective observer states of the agents. A specific case of quantum process tomography as a general modeling principle is presented. The various background ideas and a baseline formalism is discussed in this article which sets the groundwork for the implementations of the QKSA that are currently in active development. Section 2 presents a historic overview of the motivation behind this research In Section 3 we survey some general reinforcement learning models and bio-inspired computing techniques that forms a baseline for the design of the QKSA. Section 4 presents an overview of field of quantum artificial agents and the observer based operational theory that the QKSA aims to learn. In Section 5 and 6 we presents the salient features and a formal definition of our model. In Section 7 we present the task of quantum process tomography (QPT) as a general task to test our framework. In Section 8 we conclude the discussion with suggestive future directions for implementing the QKSA.","",""
0,"A. Leventi-Peetz, T. Östreich","Deep Learning Reproducibility and Explainable AI (XAI)",2022,"","","","",22,"2022-07-13 09:31:23","","","","",,,,,0,0.00,0,2,1,"The nondeterminism of Deep Learning (DL) training algorithms and its influence on the explainability of neural network (NN) models are investigated in this work with the help of image classification examples. To discuss the issue, two convolutional neural networks (CNN) have been trained and their results compared. The comparison serves the exploration of the feasibility of creating deterministic, robust DL models and deterministic explainable artificial intelligence (XAI) in practice. Successes and limitation of all here carried out efforts are described in detail. The source code of the attained deterministic models has been listed in this work. Reproducibility is indexed as a development-phase-component of the Model Governance Framework, proposed by the EU within their excellence in AI approach. Furthermore, reproducibility is a requirement for establishing causality for the interpretation of model results and building of trust towards the overwhelming expansion of AI systems applications. Problems that have to be solved on the way to reproducibility and ways to deal with some of them, are examined in this work.","",""
0,"F. Ventura, Salvatore Greco, D. Apiletti, T. Cerquitelli","Trusting deep learning natural-language models via local and global explanations",2022,"","","","",23,"2022-07-13 09:31:23","","10.1007/s10115-022-01690-9","","",,,,,0,0.00,0,4,1,"","",""
0,"Gaur Loveleen, Bhandari Mohan, Bhadwal Singh Shikhar, Jhanjhi Nz, Mohammad Shorfuzzaman, Mehedi Masud","Explanation-driven HCI Model to Examine the Mini-Mental State for Alzheimer’s Disease",2022,"","","","",24,"2022-07-13 09:31:23","","10.1145/3527174","","",,,,,0,0.00,0,6,1,"Directing research on Alzheimer’s towards only early prediction and accuracy cannot be considered a feasible approach towards tackling a ubiquitous degenerative disease today. Applying deep learning (DL), Explainable artificial intelligence(XAI) and advancing towards the human-computer interface(HCI) model can be a leap forward in medical research. This research aims to propose a robust explainable HCI model using shapley additive explanation (SHAP), local interpretable model-agnostic explanations (LIME) and DL algorithms. The use of DL algorithms: logistic regression(80.87%), support vector machine (85.8%), k-nearest neighbour(87.24%), multilayer perceptron(91.94%), decision tree(100%) and explainability can help exploring untapped avenues for research in medical sciences that can mould the future of HCI models. The outcomes of the proposed model depict higher prediction accuracy bringing efficient computer interface in decision making, and suggests a high level of relevance in the field of medical and clinical research.","",""
0,"Junhee Lee, Hyeonseong Cho, Yun Jang Pyun, Suk‐Ju Kang, H. Nam","Heatmap Assisted Accuracy Score Evaluation Method for Machine-Centric Explainable Deep Neural Networks",2022,"","","","",25,"2022-07-13 09:31:23","","10.1109/access.2022.3184453","","",,,,,0,0.00,0,5,1,"There have existed many studies about the explainable artificial intelligence (XAI) that explains the logic behind the complex deep neural network called a black box. At the same time, researchers have tried to evaluate the explainability performance of various XAIs. However, most previous evaluation methods are human-centric, that is, subjective, where they rely on how much the results of explanation are similar to what people’s decision is based on rather than what features actually affect the decision in the model. Their XAI selections are also dependent of datasets. Furthermore, they are focusing only on the output variation of a target class. On the other hand, this paper proposes a robust heatmap assisted accuracy score (HAAS) scheme over datasets that helps selecting machine-centric explanation algorithms to show what actually leads to the decision of a given classification network. The proposed method modifies the input image with the heatmap scores obtained by a given explanation algorithm and then puts the resultant heatmap assisted (HA) images into the network to estimate the accuracy change. The resultant metric (<inline-formula> <tex-math notation=""LaTeX"">$HAAS$ </tex-math></inline-formula>) is computed as a ratio of accuracies of the given network over HA and original images. The proposed evaluation scheme is verified in the image classification models of LeNet-5 for MNIST and VGG-16 for CIFAR-10, STL-10, and ILSVRC2012 over totally 11 XAI algorithms of saliency map, deconvolution, and 9 layer-wise relevance propagation (LRP) configurations. Consequently, for LRP1 and LRP3, MINST showed largest <inline-formula> <tex-math notation=""LaTeX"">$HAAS$ </tex-math></inline-formula> values of 1.0088 and 1.0079, CIFAR-10 achieved 1.1160 and 1.1254, STL-10 had 1.0906 and 1.0918, and ILSVRC2012 got 1.3207 and 1.3469. While LRP1 consists of <inline-formula> <tex-math notation=""LaTeX"">$\epsilon $ </tex-math></inline-formula>-rules for input, convolutional, and fully-connected layers, LRP3 adopts a bounded-rule for an input layer and the same <inline-formula> <tex-math notation=""LaTeX"">$\epsilon $ </tex-math></inline-formula>-rules for other layers as LRP1. The consistency of evaluation results of HAAS and AOPC has been compared by means of Kullback-Leibler divergence, ensuring that HAAS is the more robust evaluation method than AOPC independently of datasets since HAAS has much lower average divergence of 0.0251 than AOPC of 0.3048. In addition, the validity of the proposed HAAS scheme is further investigated through the inverted HA test that employs inverted HA images made up with inverted heatmap scores and estimates the accuracy degradation caused by applying them to the network. The XAI algorithms with largest <inline-formula> <tex-math notation=""LaTeX"">$HAAS$ </tex-math></inline-formula> results experience biggest accuracy degradation in the inverted HA test.","",""
2,"A. Garcez, L. Lamb","A I ] 1 0 D ec 2 02 0 Neurosymbolic AI : The 3 rd Wave",2020,"","","","",26,"2022-07-13 09:31:23","","","","",,,,,2,1.00,1,2,2,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.","",""
0,"Tae-Hyun Chun, Yong Yu","Enhancing CHF Prediction of AECL Look-Up Table Along with Machine Learning",2020,"","","","",27,"2022-07-13 09:31:23","","","","",,,,,0,0.00,0,2,2,"A critical heat flux (CHF) is a key safety parameter. For the CHF prediction, artificial neural network has been also applied and showed good performances [1, 2]. However, it is hardly accepted in the nuclear community due to a drawback of ‘Explainability’. A machine learning, as a subset of the artificial intelligence, can play a supplementary role for a more robust domain knowledge-based model. AECL Look-up Table (LUT) is widely used for the CHF prediction in reactor thermal-hydraulic design and safety analyses [3]. This domain knowledge model can predict the CHF by two schemes such as DSM (Direct Substitute Method) and HBM (Heat Balance Method). The uncertainty is much large in the DSM relative to the HBM. But the DSM is practically used in the nuclear engineering since HBM requires iterations to reach the heat balance in the CHF prediction. The purpose of this study is to show a feasibility that a machine learning-aided CHF LUT model enhances considerably the accuracy of the CHF prediction.","",""
7,"J. McDermid, Yan Jia, Zoe Porter, I. Habli","Artificial intelligence explainability: the technical and ethical dimensions",2021,"","","","",28,"2022-07-13 09:31:23","","10.1098/rsta.2020.0363","","",,,,,7,7.00,2,4,1,"In recent years, several new technical methods have been developed to make AI-models more transparent and interpretable. These techniques are often referred to collectively as ‘AI explainability’ or ‘XAI’ methods. This paper presents an overview of XAI methods, and links them to stakeholder purposes for seeking an explanation. Because the underlying stakeholder purposes are broadly ethical in nature, we see this analysis as a contribution towards bringing together the technical and ethical dimensions of XAI. We emphasize that use of XAI methods must be linked to explanations of human decisions made during the development life cycle. Situated within that wider accountability framework, our analysis may offer a helpful starting point for designers, safety engineers, service providers and regulators who need to make practical judgements about which XAI methods to employ or to require. This article is part of the theme issue ‘Towards symbiotic autonomous systems’.","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",29,"2022-07-13 09:31:23","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
60,"A. Markus, J. Kors, P. Rijnbeek","The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and evaluation strategies",2020,"","","","",30,"2022-07-13 09:31:23","","10.1016/j.jbi.2020.103655","","",,,,,60,30.00,20,3,2,"","",""
22,"Giulia Vilone, L. Longo","Notions of explainability and evaluation approaches for explainable artificial intelligence",2021,"","","","",31,"2022-07-13 09:31:23","","10.1016/J.INFFUS.2021.05.009","","",,,,,22,22.00,11,2,1,"","",""
2,"Sunny Raj","Towards Robust Artificial Intelligence Systems",2020,"","","","",32,"2022-07-13 09:31:23","","","","",,,,,2,1.00,2,1,2,"Adoption of deep neural networks (DNNs) into safety-critical and high-assurance systems has been hindered by the inability of DNNs to handle adversarial and out-of-distribution input. State-ofthe-art DNNs misclassify adversarial input and give high confidence output for out-of-distribution input. We attempt to solve this problem by employing two approaches, first, by detecting adversarial input and, second, by developing a confidence metric that can indicate when a DNN system has reached its limits and is not performing to the desired specifications. The effectiveness of our method at detecting adversarial input is demonstrated against the popular DeepFool adversarial image generation method. On a benchmark of 50,000 randomly chosen ImageNet adversarial images generated for CaffeNet and GoogLeNet DNNs, our method can recover the correct label with 95.76% and 97.43% accuracy, respectively. The proposed attribution-based confidence (ABC) metric utilizes attributions used to explain DNN output to characterize whether an output corresponding to an input to the DNN can be trusted. The attribution based approach removes the need to store training or test data or to train an ensemble of models to obtain confidence scores. Hence, the ABC metric can be used when only the trained DNN is available during inference. We test the effectiveness of the ABC metric against both adversarial and out-of-distribution input. We experimental demonstrate that the ABC metric is high for ImageNet input and low for adversarial input generated by FGSM, PGD, DeepFool, CW, and adversarial patch methods. For a DNN trained on MNIST images, ABC metric is high for in-distribution MNIST input and low for out-of-distribution Fashion-MNIST and notMNIST input.","",""
120,"Hoang Nguyen, X. Bui","Predicting Blast-Induced Air Overpressure: A Robust Artificial Intelligence System Based on Artificial Neural Networks and Random Forest",2018,"","","","",33,"2022-07-13 09:31:23","","10.1007/s11053-018-9424-1","","",,,,,120,30.00,60,2,4,"","",""
131,"J. Amann, A. Blasimme, E. Vayena, D. Frey, V. Madai","Explainability for artificial intelligence in healthcare: a multidisciplinary perspective",2020,"","","","",34,"2022-07-13 09:31:23","","10.1186/s12911-020-01332-6","","",,,,,131,65.50,26,5,2,"","",""
0,"Bushra Rasheed, M. Usama, Asmara Safdar","Robust Artificial Intelligence Approach to Stabilize and Control Propeller Driven Hybrid UGV",2022,"","","","",35,"2022-07-13 09:31:23","","10.1109/ICAI55435.2022.9773375","","",,,,,0,0.00,0,3,1,"Hybrid Unmanned Ground Vehicle (HUGV) can drive on any terrain including walls and fly as well, using the multi directional thrust force of propellers. In the era of industrial revolution, hybrid UGVs need to be autonomous with intelligent decision making capabilities. During wall climbing of hybrid UGVs, stability is essential and depends on real time feedback from multiple sensors. To increase stability and control, it is proposed that PID control loops should be replaced by AI based algorithms that reduce the decision time and mathematical complexity. For autonomous movement in any terrain using the proposed model, intelligent UGVs can map and localize simultaneously.They can make intelligent decisions about mode of movement i.e. driving on ground or wall, steering on ground or wall, flying and maneuvering by using real time sensor readings. Integration of the proposed AI models with HUGV can be applied to many areas which are hard for humans to access, for instance; inspection of large structures, bio & nuclear hazard environments, planetary exploration & magnetic fields detection.","",""
81,"M. Coeckelbergh","Artificial Intelligence, Responsibility Attribution, and a Relational Justification of Explainability",2019,"","","","",36,"2022-07-13 09:31:23","","10.1007/s11948-019-00146-8","","",,,,,81,27.00,81,1,3,"","",""
81,"Thomas G. Dietterich","Steps Toward Robust Artificial Intelligence",2017,"","","","",37,"2022-07-13 09:31:23","","10.1609/aimag.v38i3.2756","","",,,,,81,16.20,81,1,5,"Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system’s models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world","",""
52,"Hamon Ronan, Junklewitz Henrik, S. Ignacio","Robustness and Explainability of Artificial Intelligence",2020,"","","","",38,"2022-07-13 09:31:23","","10.2760/57493","","",,,,,52,26.00,17,3,2,"","",""
19,"Thomas G. Dietterich","Robust artificial intelligence and robust human organizations",2018,"","","","",39,"2022-07-13 09:31:23","","10.1007/s11704-018-8900-4","","",,,,,19,4.75,19,1,4,"","",""
11,"Sheikh Rabiul Islam, W. Eberle, S. Ghafoor","Towards Quantification of Explainability in Explainable Artificial Intelligence Methods",2019,"","","","",40,"2022-07-13 09:31:23","","","","",,,,,11,3.67,4,3,3,"Artificial Intelligence (AI) has become an integral part of domains such as security, finance, healthcare, medicine, and criminal justice. Explaining the decisions of AI systems in human terms is a key challenge--due to the high complexity of the model, as well as the potential implications on human interests, rights, and lives . While Explainable AI is an emerging field of research, there is no consensus on the definition, quantification, and formalization of explainability. In fact, the quantification of explainability is an open challenge. In our previous work, we incorporated domain knowledge for better explainability, however, we were unable to quantify the extent of explainability. In this work, we (1) briefly analyze the definitions of explainability from the perspective of different disciplines (e.g., psychology, social science), properties of explanation, explanation methods, and human-friendly explanations; and (2) propose and formulate an approach to quantify the extent of explainability. Our experimental result suggests a reasonable and model-agnostic way to quantify explainability","",""
423,"Andreas Holzinger, G. Langs, H. Denk, K. Zatloukal, Heimo Müller","Causability and explainability of artificial intelligence in medicine",2019,"","","","",41,"2022-07-13 09:31:23","","10.1002/widm.1312","","",,,,,423,141.00,85,5,3,"Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black‐box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use‐case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system","",""
195,"A. London","Artificial Intelligence and Black-Box Medical Decisions: Accuracy versus Explainability.",2019,"","","","",42,"2022-07-13 09:31:23","","10.1002/hast.973","","",,,,,195,65.00,195,1,3,"Although decision-making algorithms are not new to medicine, the availability of vast stores of medical data, gains in computing power, and breakthroughs in machine learning are accelerating the pace of their development, expanding the range of questions they can address, and increasing their predictive power. In many cases, however, the most powerful machine learning techniques purchase diagnostic or predictive accuracy at the expense of our ability to access ""the knowledge within the machine."" Without an explanation in terms of reasons or a rationale for particular decisions in individual cases, some commentators regard ceding medical decision-making to black box systems as contravening the profound moral responsibilities of clinicians. I argue, however, that opaque decisions are more common in medicine than critics realize. Moreover, as Aristotle noted over two millennia ago, when our knowledge of causal systems is incomplete and precarious-as it often is in medicine-the ability to explain how results are produced can be less important than the ability to produce such results and empirically verify their accuracy.","",""
1,"O. Jenkins, D. Lopresti, M. Mitchell","Next Wave Artificial Intelligence: Robust, Explainable, Adaptable, Ethical, and Accountable",2020,"","","","",43,"2022-07-13 09:31:23","","","","",,,,,1,0.50,0,3,2,"The history of AI has included several ""waves"" of ideas. The first wave, from the mid-1950s to the 1980s, focused on logic and symbolic hand-encoded representations of knowledge, the foundations of so-called ""expert systems"". The second wave, starting in the 1990s, focused on statistics and machine learning, in which, instead of hand-programming rules for behavior, programmers constructed ""statistical learning algorithms"" that could be trained on large datasets. In the most recent wave research in AI has largely focused on deep (i.e., many-layered) neural networks, which are loosely inspired by the brain and trained by ""deep learning"" methods. However, while deep neural networks have led to many successes and new capabilities in computer vision, speech recognition, language processing, game-playing, and robotics, their potential for broad application remains limited by several factors.  A concerning limitation is that even the most successful of today's AI systems suffer from brittleness-they can fail in unexpected ways when faced with situations that differ sufficiently from ones they have been trained on. This lack of robustness also appears in the vulnerability of AI systems to adversarial attacks, in which an adversary can subtly manipulate data in a way to guarantee a specific wrong answer or action from an AI system. AI systems also can absorb biases-based on gender, race, or other factors-from their training data and further magnify these biases in their subsequent decision-making. Taken together, these various limitations have prevented AI systems such as automatic medical diagnosis or autonomous vehicles from being sufficiently trustworthy for wide deployment. The massive proliferation of AI across society will require radically new ideas to yield technology that will not sacrifice our productivity, our quality of life, or our values.","",""
47,"L. Faes, B. Geerts, Xiaoxuan Liu, L. Morgan, P. Watkinson, P. McCulloch","DECIDE-AI: new reporting guidelines to bridge the development-to-implementation gap in clinical artificial intelligence.",2021,"","","","",44,"2022-07-13 09:31:23","","10.1038/s41591-021-01229-5","","",,,,,47,47.00,8,6,1,"","",""
39,"R. Confalonieri, Ludovik Çoba, Benedikt Wagner, Tarek R. Besold","A historical perspective of explainable Artificial Intelligence",2020,"","","","",45,"2022-07-13 09:31:23","","10.1002/widm.1391","","",,,,,39,19.50,10,4,2,"Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the “how” and “why” of automated decision‐making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge‐based) expert systems. Since then, the definition, understanding, and implementation of explainability have been picked up in several lines of research work, namely, expert systems, machine learning, recommender systems, and in approaches to neural‐symbolic learning and reasoning, mostly happening during different periods of AI history. In this article, we present a historical perspective of Explainable Artificial Intelligence. We discuss how explainability was mainly conceived in the past, how it is understood in the present and, how it might be understood in the future. We conclude the article by proposing criteria for explanations that we believe will play a crucial role in the development of human‐understandable explainable systems.","",""
29,"P. Angelov, E. Soares, Richard Jiang, Nicholas I. Arnold, Peter M. Atkinson","Explainable artificial intelligence: an analytical review",2021,"","","","",46,"2022-07-13 09:31:23","","10.1002/widm.1424","","",,,,,29,29.00,6,5,1,"This paper provides a brief analytical review of the current state‐of‐the‐art in relation to the explainability of artificial intelligence in the context of recent advances in machine learning and deep learning. The paper starts with a brief historical introduction and a taxonomy, and formulates the main challenges in terms of explainability building on the recently formulated National Institute of Standards four principles of explainability. Recently published methods related to the topic are then critically reviewed and analyzed. Finally, future directions for research are suggested.","",""
16,"J. Korteling, G. V. D. Boer-Visschedijk, R. Blankendaal, R. Boonekamp, A. Eikelboom","Human- versus Artificial Intelligence",2021,"","","","",47,"2022-07-13 09:31:23","","10.3389/frai.2021.622364","","",,,,,16,16.00,3,5,1,"AI is one of the most debated subjects of today and there seems little common understanding concerning the differences and similarities of human intelligence and artificial intelligence. Discussions on many relevant topics, such as trustworthiness, explainability, and ethics are characterized by implicit anthropocentric and anthropomorphistic conceptions and, for instance, the pursuit of human-like intelligence as the golden standard for Artificial Intelligence. In order to provide more agreement and to substantiate possible future research objectives, this paper presents three notions on the similarities and differences between human- and artificial intelligence: 1) the fundamental constraints of human (and artificial) intelligence, 2) human intelligence as one of many possible forms of general intelligence, and 3) the high potential impact of multiple (integrated) forms of narrow-hybrid AI applications. For the time being, AI systems will have fundamentally different cognitive qualities and abilities than biological systems. For this reason, a most prominent issue is how we can use (and “collaborate” with) these systems as effectively as possible? For what tasks and under what conditions, decisions are safe to leave to AI and when is human judgment required? How can we capitalize on the specific strengths of human- and artificial intelligence? How to deploy AI systems effectively to complement and compensate for the inherent constraints of human cognition (and vice versa)? Should we pursue the development of AI “partners” with human (-level) intelligence or should we focus more at supplementing human limitations? In order to answer these questions, humans working with AI systems in the workplace or in policy making have to develop an adequate mental model of the underlying ‘psychological’ mechanisms of AI. So, in order to obtain well-functioning human-AI systems, Intelligence Awareness in humans should be addressed more vigorously. For this purpose a first framework for educational content is proposed.","",""
15,"Alexandros Vassiliades, Nick Bassiliades, T. Patkos","Argumentation and explainable artificial intelligence: a survey",2021,"","","","",48,"2022-07-13 09:31:23","","10.1017/S0269888921000011","","",,,,,15,15.00,5,3,1,"Abstract Argumentation and eXplainable Artificial Intelligence (XAI) are closely related, as in the recent years, Argumentation has been used for providing Explainability to AI. Argumentation can show step by step how an AI System reaches a decision; it can provide reasoning over uncertainty and can find solutions when conflicting information is faced. In this survey, we elaborate over the topics of Argumentation and XAI combined, by reviewing all the important methods and studies, as well as implementations that use Argumentation to provide Explainability in AI. More specifically, we show how Argumentation can enable Explainability for solving various types of problems in decision-making, justification of an opinion, and dialogues. Subsequently, we elaborate on how Argumentation can help in constructing explainable systems in various applications domains, such as in Medical Informatics, Law, the Semantic Web, Security, Robotics, and some general purpose systems. Finally, we present approaches that combine Machine Learning and Argumentation Theory, toward more interpretable predictive models.","",""
17,"Yi-Shan Lin, Wen-Chuan Lee, Z. B. Celik","What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors",2020,"","","","",49,"2022-07-13 09:31:23","","10.1145/3447548.3467213","","",,,,,17,8.50,6,3,2,"EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the input parts deemed important to arrive at a decision for a specific target. However, it remains challenging to quantify the correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for the systematic evaluation of explanations that an XAI method generates. We evaluate seven state-of-the-art model-free and model-specific post-hoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We found six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region. We made our code available at https://github.com/yslin013/evalxai.","",""
11,"A. Holzinger, M. Dehmer, F. Emmert‐Streib, N. Díaz-Rodríguez, R. Cucchiara, Isabelle Augenstein, J. Ser, W. Samek, I. Jurisica","Information fusion as an integrative cross-cutting enabler to achieve robust, explainable, and trustworthy medical artificial intelligence",2021,"","","","",50,"2022-07-13 09:31:23","","10.1016/j.inffus.2021.10.007","","",,,,,11,11.00,1,9,1,"","",""
14,"Sheikh Rabiul Islam, W. Eberle, S. Ghafoor, Mohiuddin Ahmed","Explainable Artificial Intelligence Approaches: A Survey",2021,"","","","",51,"2022-07-13 09:31:23","","","","",,,,,14,14.00,4,4,1,"The lack of explainability of a decision from an Artificial Intelligence (AI) based “black box” system/model, despite its superiority in many real-world applications, is a key stumbling block for adopting AI in many high stakes applications of different domain or industry. While many popular Explainable Artificial Intelligence (XAI) methods or approaches are available to facilitate a human-friendly explanation of the decision, each has its own merits and demerits, with a plethora of open challenges. We demonstrate popular XAI methods with a mutual case study/task (i.e., credit default prediction), analyze for competitive advantages from multiple perspectives (e.g., local, global), provide meaningful insight on quantifying explainability, and recommend paths towards responsible or human-centered AI using XAI as a medium. Practitioners can use this work as a catalog to understand, compare, and correlate competitive advantages of popular XAI methods. In addition, this survey elicits future research directions towards responsible or human-centric AI systems, which is crucial to adopt AI in high stakes applications.","",""
11,"Taehyun Ha, Sangwon Lee, Sangyeon Kim","Designing Explainability of an Artificial Intelligence System",2018,"","","","",52,"2022-07-13 09:31:23","","10.1145/3183654.3183683","","",,,,,11,2.75,4,3,4,"Explainability and accuracy of the machine learning algorithms usually laid on a trade-off relationship. Several algorithms such as deep-learning artificial neural networks have high accuracy but low explainability. Since there were only limited ways to access the learning and prediction processes in algorithms, researchers and users were not able to understand how the results were given to them. However, a recent project, explainable artificial intelligence (XAI) by DARPA, showed that AI systems can be highly explainable but also accurate. Several technical reports of XAI suggested ways of extracting explainable features and their positive effects on users; the results showed that explainability of AI was helpful to make users understand and trust the system. However, only a few studies have addressed why the explainability can bring positive effects to users. We suggest theoretical reasons from the attribution theory and anthropomorphism studies. Trough a review, we develop three hypotheses: (1) causal attribution is a human nature and thus a system which provides casual explanation on their process will affect users to attribute the result of system; (2) Based on the attribution results, users will perceive the system as human-like and which will be a motivation of anthropomorphism; (3) The system will be perceived by the users through the anthropomorphism. We provide a research framework for designing causal explainability of an AI system and discuss the expected results of the research.","",""
195,"Jessica Fjeld, Nele Achten, Hannah Hilligoss, Ádám Nagy, Madhulika Srikumar","Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI",2020,"","","","",53,"2022-07-13 09:31:23","","10.2139/ssrn.3518482","","",,,,,195,97.50,39,5,2,"The rapid spread of artificial intelligence (AI) systems has precipitated a rise in ethical and human rights-based frameworks intended to guide the development and use of these technologies. Despite the proliferation of these ""AI principles,"" there has been little scholarly focus on understanding these efforts either individually or as contextualized within an expanding universe of principles with discernible trends.    To that end, this white paper and its associated data visualization compare the contents of thirty-six prominent AI principles documents side-by-side. This effort uncovered a growing consensus around eight key thematic trends: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. Underlying this “normative core,” our analysis examined the forty-seven individual principles that make up the themes, detailing notable similarities and differences in interpretation found across the documents. In sharing these observations, it is our hope that policymakers, advocates, scholars, and others working to maximize the benefits and minimize the harms of AI will be better positioned to build on existing efforts and to push the fractured, global conversation on the future of AI toward consensus.","",""
462,"Stuart J. Russell, Dan Dewey, Max Tegmark","Research Priorities for Robust and Beneficial Artificial Intelligence",2015,"","","","",54,"2022-07-13 09:31:23","","10.1609/aimag.v36i4.2577","","",,,,,462,66.00,154,3,7,"Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.","",""
85,"Giulia Vilone, L. Longo","Explainable Artificial Intelligence: a Systematic Review",2020,"","","","",55,"2022-07-13 09:31:23","","","","",,,,,85,42.50,43,2,2,"Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.","",""
42,"Weisi Guo","Explainable Artificial Intelligence for 6G: Improving Trust between Human and Machine",2019,"","","","",56,"2022-07-13 09:31:23","","10.1109/MCOM.001.2000050","","",,,,,42,14.00,42,1,3,"As 5G mobile networks are bringing about global societal benefits, the design phase for 6G has started. Evolved 5G and 6G will need sophisticated AI to automate information delivery simultaneously for mass autonomy, human machine interfacing, and targeted healthcare. Trust will become increasingly critical for 6G as it manages a wide range of mission-critical services. As we migrate from traditional mathematical model-dependent optimization to data-dependent deep learning, the insight and trust we have in our optimization modules decrease. This loss of model explainability means we are vulnerable to malicious data, poor neural network design, and the loss of trust from stakeholders and the general public -- all with a range of legal implications. In this review, we outline the core methods of explainable artificial intelligence (XAI) in a wireless network setting, including public and legal motivations, definitions of explainability, performance vs. explainability trade-offs, and XAI algorithms. Our review is grounded in case studies for both wireless PHY and MAC layer optimization and provide the community with an important research area to embark upon.","",""
42,"Christian Meske, Enrico Bunde, Johannes Schneider, Martin Gersch","Explainable Artificial Intelligence: Objectives, Stakeholders, and Future Research Opportunities",2020,"","","","",57,"2022-07-13 09:31:23","","10.1080/10580530.2020.1849465","","",,,,,42,21.00,11,4,2,"ABSTRACT Artificial Intelligence (AI) has diffused into many areas of our private and professional life. In this research note, we describe exemplary risks of black-box AI, the consequent need for explainability, and previous research on Explainable AI (XAI) in information systems research. Moreover, we discuss the origin of the term XAI, generalized XAI objectives, and stakeholder groups, as well as quality criteria of personalized explanations. We conclude with an outlook to future research on XAI.","",""
5,"Ayodeji Oseni, Nour Moustafa, H. Janicke, Peng Liu, Z. Tari, A. Vasilakos","Security and Privacy for Artificial Intelligence: Opportunities and Challenges",2021,"","","","",58,"2022-07-13 09:31:23","","","","",,,,,5,5.00,1,6,1,"The increased adoption of Artificial Intelligence (AI) presents an opportunity to solve many socio-economic and environmental challenges; however, this cannot happen without securing AI-enabled technologies. In recent years, most AI models are vulnerable to advanced and sophisticated hacking techniques. This challenge has motivated concerted research efforts into adversarial AI, with the aim of developing robust machine and deep learning models that are resilient to different types of adversarial scenarios. In this paper, we present a holistic cyber security review that demonstrates adversarial attacks against AI applications, including aspects such as adversarial knowledge and capabilities, as well as existing methods for generating adversarial examples and existing cyber defence models. We explain mathematical AI models, especially new variants of reinforcement and federated learning, to demonstrate how attack vectors would exploit vulnerabilities of AI models. We also propose a systematic framework for demonstrating attack techniques against AI applications, and reviewed several cyber defences that would protect the AI applications against those attacks. We also highlight the importance of understanding the adversarial goals and their capabilities, especially the recent attacks against industry applications, to develop adaptive defences that assess to secure AI applications. Finally, we describe the main challenges and future research directions in the domain of security and privacy of AI technologies.","",""
1,"L. Machowski, T. Marwala","Nano Version Control and the Repo as the Next Data Structure in Computer Science and Artificial Intelligence",2021,"","","","",59,"2022-07-13 09:31:23","","10.1109/EE-RDS53766.2021.9708575","","",,,,,1,1.00,1,2,1,"A new data structure called the Nano Version Control (NanoVC) Repo emerges from its origins in the fundamental data structures of computer science as well as from Git. It is acknowledged as a first-class data structure with the added benefit that the software industry already knows how to reason with it because of their experience with using it to version control software. The NanoVC Repo shines a light on the value of nano-scale modelling and in-memory representation of history. An initial implementation shows promising results where it out-performs Git implementations by 2–3 orders of magnitude. A parallel for fairness, transparency and explainability is made between Git (as used for versioning software algorithms) and the NanoVC Repo, which can be used for data structures. Applications of the NanoVC Repo are in computer science, storage and databases, data modelling, distributed and cloud computing, data quality, event streaming, artificial intelligence, and agent-based simulation. The hope is that this data structure is the next major step in computer science and artificial intelligence applications.","",""
3,"T. Sing, J. Yang, S. Yu","Boosted Tree Ensembles for Artificial Intelligence Based Automated Valuation Models (AI-AVM)",2021,"","","","",60,"2022-07-13 09:31:23","","10.1007/s11146-021-09861-1","","",,,,,3,3.00,1,3,1,"","",""
2,"Iam Palatnik de Sousa, Marley Vellasco, E. C. Silva","Explainable Artificial Intelligence for Bias Detection in COVID CT-Scan Classifiers",2021,"","","","",61,"2022-07-13 09:31:23","","10.3390/s21165657","","",,,,,2,2.00,1,3,1,"Problem: An application of Explainable Artificial Intelligence Methods for COVID CT-Scan classifiers is presented. Motivation: It is possible that classifiers are using spurious artifacts in dataset images to achieve high performances, and such explainable techniques can help identify this issue. Aim: For this purpose, several approaches were used in tandem, in order to create a complete overview of the classificatios. Methodology: The techniques used included GradCAM, LIME, RISE, Squaregrid, and direct Gradient approaches (Vanilla, Smooth, Integrated). Main results: Among the deep neural networks architectures evaluated for this image classification task, VGG16 was shown to be most affected by biases towards spurious artifacts, while DenseNet was notably more robust against them. Further impacts: Results further show that small differences in validation accuracies can cause drastic changes in explanation heatmaps for DenseNet architectures, indicating that small changes in validation accuracy may have large impacts on the biases learned by the networks. Notably, it is important to notice that the strong performance metrics achieved by all these networks (Accuracy, F1 score, AUC all in the 80 to 90% range) could give users the erroneous impression that there is no bias. However, the analysis of the explanation heatmaps highlights the bias.","",""
2,"D. Rawat","Secure and trustworthy machine learning/artificial intelligence for multi-domain operations",2021,"","","","",62,"2022-07-13 09:31:23","","10.1117/12.2592860","","",,,,,2,2.00,2,1,1,"Machine Learning (ML) algorithms and Artificial Intelligence (AI) are now regarded as very useful for data-driven applications including resilient multi-domain operations. However, ML algorithms and AI systems can be controlled, dodged, biased, and misled through flawed learning models and input data, they need robust security features and trust. Furthermore, ML algorithms and AI systems add challenges when we have (unlabeled/labeled) sparse/small data or big data for training and evaluation. It is very important to design, evaluate and test ML algorithms and AI systems that produce reliable, robust, trustworthy, explainable, and fair/unbiased outcomes to make them acceptable and reliable in mission critical multi-domain operations. ML algorithms rely on data and work on the principle of ``Garbage In, Garbage Out,"" which means that if the input data to learning model is corrupted or compromised, the outcomes of the ML/AI would not be optimal, reliable and trustworthy.","",""
0,"Mir Riyanul Islam, Mobyen Uddin Ahmed, S. Begum","Local and Global Interpretability Using Mutual Information in Explainable Artificial Intelligence",2021,"","","","",63,"2022-07-13 09:31:23","","10.1109/ISCMI53840.2021.9654898","","",,,,,0,0.00,0,3,1,"Numerous studies have exploited the potential of Artificial Intelligence (AI) and Machine Learning (ML) models to develop intelligent systems in diverse domains for complex tasks, such as analysing data, extracting features, prediction, recommendation etc. However, presently these systems embrace acceptability issues from the end-users. The models deployed at the back of the systems mostly analyse the correlations or dependencies between the input and output to uncover the important characteristics of the input features, but they lack explainability and interpretability that causing the acceptability issues of intelligent systems and raising the research domain of eXplainable Artificial Intelligence (XAI). In this study, to overcome these shortcomings, a hybrid XAI approach is developed to explain an AI/ML model’s inference mechanism as well as the final outcome. The overall approach comprises of 1) a convolutional encoder that extracts deep features from the data and computes their relevancy with features extracted using domain knowledge, 2) a model for classifying data points using the features from autoencoder, and 3) a process of explaining the model’s working procedure and decisions using mutual information to provide global and local interpretability. To demonstrate and validate the proposed approach, experimentation was performed using an electroencephalography dataset from road safety to classify drivers’ in-vehicle mental workload. The outcome of the experiment was found to be promising that produced a Support Vector Machine classifier for mental workload with approximately 89% performance accuracy. Moreover, the proposed approach can also provide an explanation for the classifier model’s behaviour and decisions with the combined illustration of Shapely values and mutual information.","",""
353,"Erico Tjoa, Cuntai Guan","A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI",2019,"","","","",64,"2022-07-13 09:31:23","","10.1109/TNNLS.2020.3027314","","",,,,,353,117.67,177,2,3,"Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.","",""
0,"Xiaohong W. Gao, B. Braden","Artificial intelligence in endoscopy: The challenges and future directions",2021,"","","","",65,"2022-07-13 09:31:23","","10.37126/aige.v2.i4.117","","",,,,,0,0.00,0,2,1,"Artificial intelligence based approaches, in particular deep learning, have achieved state-of-the-art performance in medical fields with increasing number of software systems being approved by both Europe and United States. This paper reviews their applications to early detection of oesophageal cancers with a focus on their advantages and pitfalls. The paper concludes with future recommendations towards the development of a real-time, clinical implementable, interpretable and robust diagnosis support systems.","",""
26,"F. Emmert‐Streib, O. Yli-Harja, M. Dehmer","Explainable artificial intelligence and machine learning: A reality rooted perspective",2020,"","","","",66,"2022-07-13 09:31:23","","10.1002/widm.1368","","",,,,,26,13.00,9,3,2,"As a consequence of technological progress, nowadays, one is used to the availability of big data generated in nearly all fields of science. However, the analysis of such data possesses vast challenges. One of these challenges relates to the explainability of methods from artificial intelligence (AI) or machine learning. Currently, many of such methods are nontransparent with respect to their working mechanism and for this reason are called black box models, most notably deep learning methods. However, it has been realized that this constitutes severe problems for a number of fields including the health sciences and criminal justice and arguments have been brought forward in favor of an explainable AI (XAI). In this paper, we do not assume the usual perspective presenting XAI as it should be, but rather provide a discussion what XAI can be. The difference is that we do not present wishful thinking but reality grounded properties in relation to a scientific theory beyond physics.","",""
44,"Chiara Longoni, Luca Cian","Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect",2020,"","","","",67,"2022-07-13 09:31:23","","10.1177/0022242920957347","","",,,,,44,22.00,22,2,2,"Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).","",""
0,"R. Ochoa-Montiel, G. Olague, Juan Humberto Sossa Azuela","Towards explainable artificial intelligence for the leukemia subtype recognition",2021,"","","","",68,"2022-07-13 09:31:23","","10.1109/LA-CCI48322.2021.9769826","","",,,,,0,0.00,0,3,1,"In this work, we provide a solution to the leukemia subtype recognition problem. Most approaches used for solving a variety of pattern recognition problems have a drawback: in general, they lack explainability. In this paper, we provide a solution for facing this situation. We describe a model whose stages allow deriving knowledge for solving the leukemia subtype recognition problem, are intelligible for the user. Results show that multiclass recognition is achieved from the solutions obtained by the model through multiple runs.","",""
19,"M. Kuzlu, Umit Cali, Vinayak Sharma, Özgür Güler","Gaining Insight Into Solar Photovoltaic Power Generation Forecasting Utilizing Explainable Artificial Intelligence Tools",2020,"","","","",69,"2022-07-13 09:31:23","","10.1109/ACCESS.2020.3031477","","",,,,,19,9.50,5,4,2,"Over the last two decades, Artificial Intelligence (AI) approaches have been applied to various applications of the smart grid, such as demand response, predictive maintenance, and load forecasting. However, AI is still considered to be a “black-box” due to its lack of explainability and transparency, especially for something like solar photovoltaic (PV) forecasts that involves many parameters. Explainable Artificial Intelligence (XAI) has become an emerging research field in the smart grid domain since it addresses this gap and helps understand why the AI system made a forecast decision. This article presents several use cases of solar PV energy forecasting using XAI tools, such as LIME, SHAP, and ELI5, which can contribute to adopting XAI tools for smart grid applications. Understanding the inner workings of a prediction model based on AI can give insights into the application field. Such insight can provide improvements to the solar PV forecasting models and point out relevant parameters.","",""
0,"Keeley A. Crockett, Edwin Colyer, A. Latham","The Ethical Landscape of Data and Artificial Intelligence: Citizen Perspectives",2021,"","","","",70,"2022-07-13 09:31:23","","10.1109/SSCI50451.2021.9660153","","",,,,,0,0.00,0,3,1,"Globally, there is growing acknowledgement that those involved in the development and deployment of AI products and services should act responsibly and conduct their work within robust ethical frameworks. Many of the ethical guidelines now published highlight a requirement for citizens to have greater voice and involvement in this process and to hold actors to account regarding compliance and the impacts of their AI innovations. For citizens to participate in co-creation activities they need to be representative of the diverse communities of society and have an appropriate level of understanding of basic AI concepts. This paper presents the preliminary results of a longitudinal survey designed to capture citizen perspectives of the ethical landscape of data and AI. Forty participants were asked to participate in a survey and results were analyzed based on gender, age range and educational attainment. Results have shown that participant perception of AI, trust, bias and fairness is different but related to specific AI applications, and the context in which is applied. Citizens also are also very receptive to undertaking free courses/workshops on a wide range of AI concepts, ranging from family workshops to work-based training.","",""
0,"Lucas Mendes Lima, Victor Calebe Cavalcante, Mariana Guimarães de Sousa, Cláudio Afonso Fleury, D. Oliveira, Eduardo Noronha de Andrade Freitas","Artificial Intelligence in Support of Welfare Monitoring of Dairy Cattle: A Systematic Literature Review",2021,"","","","",71,"2022-07-13 09:31:23","","10.1109/CSCI54926.2021.00324","","",,,,,0,0.00,0,6,1,"Context: Although agribusiness corresponded to more than 20% of Brazil’s Gross Domestic Product (GDP), most livestock is under manual control and manual monitoring. Additionally, alternative technologies are either uncomfortable and stressful, or expensive. Now, despite the great scientific advances in the area, there is still a pressing need for an automated robust, inexpensive and (sub)optimal technology to monitor animal behavior in a cost-effective, contact-less and stress-free fashion. Overall, this niche can leverage the benefits of Deep Learning schemes.Objective: This review aims to provide a systematic overview of most current projects in the area of comfort monitoring dairy cattle, as well as their corresponding image recognition-based techniques and technologies.Methods: First, a systematic review planning was carried out, and objectives, research questions, search strings, among others, were defined. Subsequently,a broad survey was conducted to extract, analyze and compile the data, to generate a easy-to-read visual source of information (tables and graphics).Results: Information was extracted from the reviewed papers. Among this data collected from the papers are techniques utilized, target behaviors, cow bodyparts identified in visual computational, besides their paper source font, the publication date, and localization. For example, the papers present are mostly recent. China has had a larger number of relevant papers in the area. The back was the body region most analyzed by the papers and the behaviors most analyzed were body condition score, lameness, cow’s body position and feeding/drinking behavior. Among the methods used is RCNN Inception V3 with the best accuracy for cow’s back region.Conclusion: The aim of this work is to present some of the papers that are being carried out in the area of dairy cow behavior monitoring, using techniques of Artifical Intelligence. It is expected that the information collected and presented in the present systematic review paper contribute to the future researches and projects of the area and the application of new techniques.","",""
0,"E. Shchetinin, L. Sevastianov","Improving the Learning Power of Artificial Intelligence Using Multimodal Deep Learning",2021,"","","","",72,"2022-07-13 09:31:23","","10.1051/EPJCONF/202124801017","","",,,,,0,0.00,0,2,1,"Computer paralinguistic analysis is widely used in security systems, biometric research, call centers and banks. Paralinguistic models estimate different physical properties of voice, such as pitch, intensity, formants and harmonics to classify emotions. The main goal is to find such features that would be robust to outliers and will retain variety of human voice properties at the same time. Moreover, the model used must be able to estimate features on a time scale for an effective analysis of voice variability. In this paper a paralinguistic model based on Bidirectional Long Short-Term Memory (BLSTM) neural network is described, which was trained for vocal-based emotion recognition. The main advantage of this network architecture is that each module of the network consists of several interconnected layers, providing the ability to recognize flexible long-term dependencies in data, which is important in context of vocal analysis. We explain the architecture of a bidirectional neural network model, its main advantages over regular neural networks and compare experimental results of BLSTM network with other models.","",""
10,"D. Thakker, B. Mishra, A. Abdullatif, Suvodeep Mazumdar, Sydney Simpson","Explainable Artificial Intelligence for Developing Smart Cities Solutions",2020,"","","","",73,"2022-07-13 09:31:23","","10.3390/smartcities3040065","","",,,,,10,5.00,2,5,2,"Traditional Artificial Intelligence (AI) technologies used in developing smart cities solutions, Machine Learning (ML) and recently Deep Learning (DL), rely more on utilising best representative training datasets and features engineering and less on the available domain expertise. We argue that such an approach to solution development makes the outcome of solutions less explainable, i.e., it is often not possible to explain the results of the model. There is a growing concern among policymakers in cities with this lack of explainability of AI solutions, and this is considered a major hindrance in the wider acceptability and trust in such AI-based solutions. In this work, we survey the concept of ‘explainable deep learning’ as a subset of the ‘explainable AI’ problem and propose a new solution using Semantic Web technologies, demonstrated with a smart cities flood monitoring application in the context of a European Commission-funded project. Monitoring of gullies and drainage in crucial geographical areas susceptible to flooding issues is an important aspect of any flood monitoring solution. Typical solutions for this problem involve the use of cameras to capture images showing the affected areas in real-time with different objects such as leaves, plastic bottles etc., and building a DL-based classifier to detect such objects and classify blockages based on the presence and coverage of these objects in the images. In this work, we uniquely propose an Explainable AI solution using DL and Semantic Web technologies to build a hybrid classifier. In this hybrid classifier, the DL component detects object presence and coverage level and semantic rules designed with close consultation with experts carry out the classification. By using the expert knowledge in the flooding context, our hybrid classifier provides the flexibility on categorising the image using objects and their coverage relationships. The experimental results demonstrated with a real-world use case showed that this hybrid approach of image classification has on average 11% improvement (F-Measure) in image classification performance compared to DL-only classifier. It also has the distinct advantage of integrating experts’ knowledge on defining the decision-making rules to represent the complex circumstances and using such knowledge to explain the results.","",""
12,"C. Ho, Joseph Ali, K. Caals","Ensuring trustworthy use of artificial intelligence and big data analytics in health insurance",2020,"","","","",74,"2022-07-13 09:31:23","","10.2471/BLT.19.234732","","",,,,,12,6.00,4,3,2,"Abstract Technological advances in big data (large amounts of highly varied data from many different sources that may be processed rapidly), data sciences and artificial intelligence can improve health-system functions and promote personalized care and public good. However, these technologies will not replace the fundamental components of the health system, such as ethical leadership and governance, or avoid the need for a robust ethical and regulatory environment. In this paper, we discuss what a robust ethical and regulatory environment might look like for big data analytics in health insurance, and describe examples of safeguards and participatory mechanisms that should be established. First, a clear and effective data governance framework is critical. Legal standards need to be enacted and insurers should be encouraged and given incentives to adopt a human-centred approach in the design and use of big data analytics and artificial intelligence. Second, a clear and accountable process is necessary to explain what information can be used and how it can be used. Third, people whose data may be used should be empowered through their active involvement in determining how their personal data may be managed and governed. Fourth, insurers and governance bodies, including regulators and policy-makers, need to work together to ensure that the big data analytics based on artificial intelligence that are developed are transparent and accurate. Unless an enabling ethical environment is in place, the use of such analytics will likely contribute to the proliferation of unconnected data systems, worsen existing inequalities, and erode trustworthiness and trust.","",""
1660,"Alejandro Barredo Arrieta, Natalia Díaz Rodríguez, J. Ser, Adrien Bennetot, S. Tabik, A. Barbado, S. García, S. Gil-Lopez, D. Molina, Richard Benjamins, Raja Chatila, Francisco Herrera","Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI",2019,"","","","",75,"2022-07-13 09:31:23","","10.1016/j.inffus.2019.12.012","","",,,,,1660,553.33,166,12,3,"","",""
0,"Yusen Xie, Ting Sun, Xinglong Cui, Shuixin Deng, Lei Deng, Baohua Chen","Fast-robust book information extraction system for automated intelligence library",2021,"","","","",76,"2022-07-13 09:31:23","","10.1109/AIID51893.2021.9456499","","",,,,,0,0.00,0,6,1,"At present, in the large-scale book management scene, book sorting, daily maintenance and book retrieval are very common, but the book information is complicated and the efficiency of relying on manual management is extremely poor. Although there have been many self-service book systems based on optics or vision, they are mostly based on traditional computer vision algorithms such as boundary extraction. Due to the fact that there are more artificial experience thresholds, some shortcomings such as low detection accuracy, poor robustness, and inability to systematically deploy on a large scale, which lack of insufficient intelligence. Therefore, we proposed a book information extraction algorithm based on object detection and optical character recognition (OCR) that is suitable for multiple book information recognition, multiple book image angles and multiple book postures. It can be applied to scenes such as book sorting, bookshelf management and book retrieval. The system we designed includes the classification of book covers and back covers, the classification of books upright and inverted, the detection of book pages side and spine side, the recognition of book pricing. In terms of accuracy, the classification accuracy of the front cover and the back cover is 99.9%, the upright classification accuracy of book front covers is 98.8%, the back cover reaches 99.9%, the accuracy of book price recognition get 94.5%, and the book spine/page side detection mAP reaches 99.6%; in terms of detection speed, Yolov5 detection model was improved and the statistical-based pre-pruning strategy was adopted, support by our algorithm the system reaches 2.09 FPS in book price recognition, which improves the detection speed to meet actual needs.","",""
8,"Jun Zhu, Hang Su, Bo Zhang","Toward the third generation of artificial intelligence",2020,"","","","",77,"2022-07-13 09:31:23","","10.1360/ssi-2020-0204","","",,,,,8,4.00,3,3,2,"There have been two competing paradigms of artificial intelligence (AI) development since 1956, i.e., symbolism and connectionism (or subsymbolism). Both started at the same time, but symbolism had dominated AI development until the end of the 1980s. Connectionism began to develop in the 1990s and reached its climax at the beginning of this century, and it is likely to displace symbolism. Today, it seems that the two paradigms only simulate the human mind (or brain) in different ways and have their own advantages. True human intelligence cannot be achieved by relying on only one paradigm. Both are necessary to establish a new, explainable, and robust AI theory and method and develop safe, trustworthy, reliable, and extensible AI technology. To this end, it is imperative to combine the two paradigms, and the present article will illustrate this idea. For the sake of description, symbolism, connectionism, and the newly developed paradigm are termed as first-, second-, and third-generation AIs.","",""
257,"David Gunning, M. Stefik, Jaesik Choi, Timothy Miller, Simone Stumpf, Guang-Zhong Yang","XAI—Explainable artificial intelligence",2019,"","","","",78,"2022-07-13 09:31:23","","10.1126/scirobotics.aay7120","","",,,,,257,85.67,43,6,3,"Explainability is essential for users to effectively understand, trust, and manage powerful artificial intelligence applications. Explainability is essential for users to effectively understand, trust, and manage powerful artificial intelligence applications.","",""
5,"Cathy O'Neil, H. Gunn","Near-Term Artificial Intelligence and the Ethical Matrix",2020,"","","","",79,"2022-07-13 09:31:23","","10.1093/oso/9780190905033.003.0009","","",,,,,5,2.50,3,2,2,"This chapter takes up the issue of near-term artificial intelligence, or the algorithms that are already in place in a variety of public and private sectors, guiding decisions from advertising and to credit ratings to sentencing in the justice system. There is a pressing need to recognize and evaluate the ways that structural racism, sexism, classism, and ableism may be embedded in and amplified by these systems. The chapter proposes a framework for ethical analysis that can be used to facilitate more robust ethical reflection in AI development and implementation. It presents an ethical matrix that incorporates the language of data science as a tool that data scientists can build themselves in order to integrate ethical analysis into the design process, addressing the need for immediate analysis and accountability over the design and deployment of near-term AI.","",""
496,"Christopher J. Kelly, A. Karthikesalingam, Mustafa Suleyman, Greg Corrado, Dominic King","Key challenges for delivering clinical impact with artificial intelligence",2019,"","","","",80,"2022-07-13 09:31:23","","10.1186/s12916-019-1426-2","","",,,,,496,165.33,99,5,3,"","",""
0,"A. Paic","Policies for Artificial Intelligence in Science and Innovation",2020,"","","","",81,"2022-07-13 09:31:23","","10.22323/1.372.0045","","",,,,,0,0.00,0,1,2,"This contribution synthesizes the discussions of the special session on policies for Artificial Intelligence in Science and Innovation, organized by the OECD’s Directorate for Science, Technology and Innovation. The session was opened by Dr Judith Arrieta, Minister of the Foreign Service at the Chief of Staff’s Office of the Secretary of Foreign Affairs of Mexico, and the two panels included speakers from governments, industry and civil society from European countries, USA,Canada China and Australia. Participants discussed the disruptive nature of AI and the formidable challenges it poses. Most of the discussion focused under the umbrella title of ethics, but they span very different issues of human-centered values, fairness, transparency, explainability, and many more. Other challenges include employment, education, SME policy, enabling environment, access to data and computing technology. Responses by governments were also discussed with a particular focus on national strategies, whose main pillars are oriented toward knowledge creation through AI research, knowledge diffusion through linkages to the private sector, development of human capital which will underpin the development of the sector, and a strong values, ethical and regulatory framework to create the conditions for the development of trustworthy AI.  In a world of finite resources, discussants concluded that one cannot apply very stringent requirements to all AI decisions, and there is clearly a need to require more transparency, explainability and robustness from systems which have the greatest impact on human lives. Therefore an approach based on algorithmic impact assessment seems reasonable. Such an approach needs to be further developed and standardized.","",""
11,"Aditya Kuppa, N. Le-Khac","Black Box Attacks on Explainable Artificial Intelligence(XAI) methods in Cyber Security",2020,"","","","",82,"2022-07-13 09:31:23","","10.1109/IJCNN48605.2020.9206780","","",,,,,11,5.50,6,2,2,"Cybersecurity community is slowly leveraging Machine Learning (ML) to combat ever evolving threats. One of the biggest drivers for successful adoption of these models is how well domain experts and users are able to understand and trust their functionality. As these black-box models are being employed to make important predictions, the demand for transparency and explainability is increasing from the stakeholders.Explanations supporting the output of ML models are crucial in cyber security, where experts require far more information from the model than a simple binary output for their analysis. Recent approaches in the literature have focused on three different areas: (a) creating and improving explainability methods which help users better understand the internal workings of ML models and their outputs; (b) attacks on interpreters in white box setting; (c) defining the exact properties and metrics of the explanations generated by models. However, they have not covered, the security properties and threat models relevant to cybersecurity domain, and attacks on explainable models in black box settings.In this paper, we bridge this gap by proposing a taxonomy for Explainable Artificial Intelligence (XAI) methods, covering various security properties and threat models relevant to cyber security domain. We design a novel black box attack for analyzing the consistency, correctness and confidence security properties of gradient based XAI methods. We validate our proposed system on 3 security-relevant data-sets and models, and demonstrate that the method achieves attacker’s goal of misleading both the classifier and explanation report and, only explainability method without affecting the classifier output. Our evaluation of the proposed approach shows promising results and can help in designing secure and robust XAI methods.","",""
0,"R. Porcher","CORR Insights®: Does Artificial Intelligence Outperform Natural Intelligence in Interpretation of Musculoskeletal Radiological Studies? A Systematic Review.",2020,"","","","",83,"2022-07-13 09:31:23","","10.1097/CORR.0000000000001415","","",,,,,0,0.00,0,1,2,"Machine learning, and artificial intelligence more generally, are quickly growing areas of applied medical decisionmaking research. Compared with what are now considered moretraditional analytical approaches, such as statistical prediction models, machine learning is seen as providing unique advantages; in particular, it may improve healthcare delivery because it can learn from millions of digitized patient charts or images, and so provide robust, reproducible, and rapid decision-support tools [8, 14]. Artificial intelligence has already transformed many aspects of daily life outside health care; machine-learning algorithms allow us to translate large pieces of text into any language, recognize speech, drive a car, make a plane take off or land, or detect banking fraud. The advantages of machine learning include the ability to analyze enormous amounts of data, capture complex nonlinear relationships among these data, and consider a wide range of data. It can handle structured data, similar to other statistical prediction methods, but machine learning can also analyze free text and images, as well as high-frequency sampled data streams such as those produced by wearable devices. In this respect, no approach other than artificial intelligence and machine learning has enabled the analysis of such data so far. These approaches have begun to show promise in orthopaedic surgery, specifically. For example, one recent study used machine learning to predict whether patients would achieve clinically important improvements in validated outcome scores 2 years after joint arthroplasty [5, 10], which is important in light of the fact that even experienced surgeons’ abilities in this sort of prediction for patients undergoing TKA are no better than a coin toss [6]. However, despite the hype and hopes about artificial intelligence, more-nuanced opinions have emerged [1, 13]. Identifying associations among data does not prevent confounding, and this may prevent us from translating modifiable factors flagged by algorithms into real targets for interventions. Additionally, despite the underlying idea that the more data we have to train an algorithm, the more accurate they are, the greed for more data does not always translate into more-accurate predictions [1]. Predicting what will occur in 1, 5, or 10 years may be difficult because all past data, not just available data, do not contain sufficient information. This may explain why machine-learning algorithms have often outperformed human experts in imaging or diagnostics, where most information is present in the data analyzed [8]. The advantage over moreclassic statistical models for longer-term risk prediction modeling is likely less evident [2]. This CORR Insights is a commentary on the article “Does Artificial Intelligence Outperform Natural Intelligence in Interpretation of Musculoskeletal Radiological Studies? A Systematic Review” by Groot and colleagues available at: DOI: 10.1097/CORR.0000000000001360. The author certifies that he, or anymembers of his immediate family, has no commercial associations (eg, consultancies, stock ownership, equity interest, patent/licensing arrangements, etc) that might pose a conflict of interest in connection with the submitted article. All ICMJE Conflict of Interest Forms for authors and Clinical Orthopaedics and Related Research editors and board members are on file with the publication and can be viewed on request. The opinions expressed are those of the writer, and do not reflect the opinion or policy of CORR or the Association of Bone and Joint Surgeons. R. Porcher ✉, Centre d’Epidémiologie Clinique, Hôpital Hôtel-Dieu, 1 Parvis NotreDame Place Jean-Paul II, 75004 Paris, France, Email: raphael.porcher@aphp.fr R. Porcher, Université de Paris, CRESS UMR1153, INSERM, INRA, F-75004 Paris, France; Centre d’Epidémiologie Clinique, AP-HP, Hôtel-Dieu, F-75004 Paris, France","",""
0,"T. Sing, J. Yang, S. Yu","Decision Tree and Boosting Techniques in Artificial Intelligence Based Automated Valuation Models (AI-AVM)",2020,"","","","",84,"2022-07-13 09:31:23","","10.2139/ssrn.3605798","","",,,,,0,0.00,0,3,2,"This paper develops an artificial intelligence-based automated valuation model (AI-AVM) using the decision tree and the boosting techniques to predict residential property prices in Singapore. We use more than 300,000 property transaction data from Singapore’s private residential property market for the period from 1995 to 2017 for the training of the AI-AVM models. The two tree-based AI-AVM models show superior performance over the traditional multiple regression analysis (MRA) model when predicting the property prices. We also extend the application of the AI-AVM to more homogenous public housing prices in Singapore, and the predictive performance remains robust. The boosting AI-AVM models that allow for inter-dependence structure in the decision trees is the best model that explains more than 88% of the variance in both private and public housing prices and keep the prediction errors to less than 6% for HDB and 9% for the private market. When subject the AI-AVM to the out-of-sample forecasting using the 2017-2019 testing property sale samples, the prediction errors remain within a narrow range of between 5% and 9%.","",""
0,"Katanosh Morovat, B. Panda","A Survey of Artificial Intelligence in Cybersecurity",2020,"","","","",85,"2022-07-13 09:31:23","","10.1109/CSCI51800.2020.00026","","",,,,,0,0.00,0,2,2,"During the last decades, not only the number of cyberattacks have increased significantly, they have also become more sophisticated. Hence designing a cyber-resilient approach is of paramount importance. Traditional security methods are not adequate to prevent data breaches in case of cyberattacks. Cybercriminals have learned how to use new techniques and robust tools to hack, attack, and breach data. Fortunately, Artificial Intelligence (AI) technologies have been introduced into cyberspace to construct smart models for defending systems from attacks. Since AI technologies can rapidly evolve to address complex situations, they can be used as fundamental tools in the field of cybersecurity. Al-based techniques can provide efficient and powerful cyber defense tools to recognize malware attacks, network intrusions, phishing and spam emails, and data breaches, to name a few, and to alert security incidents when they occur. In this paper, we review the impact of AI in cybersecurity and summarize existing research in terms of benefits of AI in cybersecurity.","",""
68,"Ilia Stepin, J. M. Alonso, Alejandro Catalá, Martin Pereira-Fariña","A Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial Intelligence",2021,"","","","",86,"2022-07-13 09:31:23","","10.1109/ACCESS.2021.3051315","","",,,,,68,68.00,17,4,1,"A number of algorithms in the field of artificial intelligence offer poorly interpretable decisions. To disclose the reasoning behind such algorithms, their output can be explained by means of so-called evidence-based (or factual) explanations. Alternatively, contrastive and counterfactual explanations justify why the output of the algorithms is not any different and how it could be changed, respectively. It is of crucial importance to bridge the gap between theoretical approaches to contrastive and counterfactual explanation and the corresponding computational frameworks. In this work we conduct a systematic literature review which provides readers with a thorough and reproducible analysis of the interdisciplinary research field under study. We first examine theoretical foundations of contrastive and counterfactual accounts of explanation. Then, we report the state-of-the-art computational frameworks for contrastive and counterfactual explanation generation. In addition, we analyze how grounded such frameworks are on the insights from the inspected theoretical approaches. As a result, we highlight a variety of properties of the approaches under study and reveal a number of shortcomings thereof. Moreover, we define a taxonomy regarding both theoretical and practical approaches to contrastive and counterfactual explanation.","",""
61,"Markus Langer, Daniel Oster, Timo Speith, H. Hermanns, Lena Kästner, Eva Schmidt, Andreas Sesing, Kevin Baum","What Do We Want From Explainable Artificial Intelligence (XAI)? - A Stakeholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary XAI Research",2021,"","","","",87,"2022-07-13 09:31:23","","10.1016/j.artint.2021.103473","","",,,,,61,61.00,8,8,1,"","",""
24,"Maxime Sermesant, H. Delingette, H. Cochet, P. Jaïs, N. Ayache","Applications of artificial intelligence in cardiovascular imaging",2021,"","","","",88,"2022-07-13 09:31:23","","10.1038/s41569-021-00527-2","","",,,,,24,24.00,5,5,1,"","",""
19,"Basim Mahbooba, Mohan Timilsina, R. Sahal, M. Serrano","Explainable Artificial Intelligence (XAI) to Enhance Trust Management in Intrusion Detection Systems Using Decision Tree Model",2021,"","","","",89,"2022-07-13 09:31:23","","10.1155/2021/6634811","","",,,,,19,19.00,5,4,1,"Despite the growing popularity of machine learning models in the cyber-security applications (e.g., an intrusion detection system (IDS)), most of these models are perceived as a black-box. The eXplainable Artificial Intelligence (XAI) has become increasingly important to interpret the machine learning models to enhance trust management by allowing human experts to understand the underlying data evidence and causal reasoning. According to IDS, the critical role of trust management is to understand the impact of the malicious data to detect any intrusion in the system. The previous studies focused more on the accuracy of the various classification algorithms for trust in IDS. They do not often provide insights into their behavior and reasoning provided by the sophisticated algorithm. Therefore, in this paper, we have addressed XAI concept to enhance trust management by exploring the decision tree model in the area of IDS. We use simple decision tree algorithms that can be easily read and even resemble a human approach to decision-making by splitting the choice into many small subchoices for IDS. We experimented with this approach by extracting rules in a widely used KDD benchmark dataset. We also compared the accuracy of the decision tree approach with the other state-of-the-art algorithms.","",""
15,"R. Cadario, Chiara Longoni, Carey K. Morewedge","Understanding, explaining, and utilizing medical artificial intelligence.",2021,"","","","",90,"2022-07-13 09:31:23","","10.1038/S41562-021-01146-0","","",,,,,15,15.00,5,3,1,"","",""
19,"Ruhhee Tabbussum, A. Q. Dar","Performance evaluation of artificial intelligence paradigms—artificial neural networks, fuzzy logic, and adaptive neuro-fuzzy inference system for flood prediction",2021,"","","","",91,"2022-07-13 09:31:23","","10.1007/s11356-021-12410-1","","",,,,,19,19.00,10,2,1,"","",""
51,"Miriam C. Buiten","Towards Intelligent Regulation of Artificial Intelligence",2019,"","","","",92,"2022-07-13 09:31:23","","10.1017/err.2019.8","","",,,,,51,17.00,51,1,3,"Artificial intelligence (AI) is becoming a part of our daily lives at a fast pace, offering myriad benefits for society. At the same time, there is concern about the unpredictability and uncontrollability of AI. In response, legislators and scholars call for more transparency and explainability of AI. This article considers what it would mean to require transparency of AI. It advocates looking beyond the opaque concept of AI, focusing on the concrete risks and biases of its underlying technology: machine-learning algorithms. The article discusses the biases that algorithms may produce through the input data, the testing of the algorithm and the decision model. Any transparency requirement for algorithms should result in explanations of these biases that are both understandable for the prospective recipients, and technically feasible for producers. Before asking how much transparency the law should require from algorithms, we should therefore consider if the explanation that programmers could offer is useful in specific legal contexts.","",""
99,"R. Colling, Helen Pitman, K. Oien, N. Rajpoot, P. Macklin, D. Snead, Tony Sackville, C. Verrill","Artificial intelligence in digital pathology: a roadmap to routine use in clinical practice",2019,"","","","",93,"2022-07-13 09:31:23","","10.1002/path.5310","","",,,,,99,33.00,12,8,3,"The use of artificial intelligence will transform clinical practice over the next decade and the early impact of this will likely be the integration of image analysis and machine learning into routine histopathology. In the UK and around the world, a digital revolution is transforming the reporting practice of diagnostic histopathology and this has sparked a proliferation of image analysis software tools. While this is an exciting development that could discover novel predictive clinical information and potentially address international pathology workforce shortages, there is a clear need for a robust and evidence‐based framework in which to develop these new tools in a collaborative manner that meets regulatory approval. With these issues in mind, the NCRI Cellular Molecular Pathology (CM‐Path) initiative and the British In Vitro Diagnostics Association (BIVDA) have set out a roadmap to help academia, industry, and clinicians develop new software tools to the point of approved clinical use. © 2019 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.","",""
85,"A. Grzybowski, Piotr Brona, Gilbert Lim, P. Ruamviboonsuk, G. Tan, M. Abràmoff, D. Ting","Artificial intelligence for diabetic retinopathy screening: a review",2019,"","","","",94,"2022-07-13 09:31:23","","10.1038/s41433-019-0566-0","","",,,,,85,28.33,12,7,3,"","",""
73,"Valentina Bellemo, Gilbert Lim, T. Rim, G. Tan, C. Cheung, S. Sadda, M. He, A. Tufail, M. Lee, W. Hsu, D. Ting","Artificial Intelligence Screening for Diabetic Retinopathy: the Real-World Emerging Application",2019,"","","","",95,"2022-07-13 09:31:23","","10.1007/s11892-019-1189-3","","",,,,,73,24.33,7,11,3,"","",""
67,"Yonghui Shang, Hoang Nguyen, X. Bui, Quang-Hieu Tran, H. Moayedi","A Novel Artificial Intelligence Approach to Predict Blast-Induced Ground Vibration in Open-Pit Mines Based on the Firefly Algorithm and Artificial Neural Network",2019,"","","","",96,"2022-07-13 09:31:23","","10.1007/s11053-019-09503-7","","",,,,,67,22.33,13,5,3,"","",""
755,"Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Q. Song, K. Cao, Daliang Liu, Guisheng Wang, Qizhong Xu, Xisheng Fang, Shiqin Zhang, J. Xia, Jun Xia","Artificial Intelligence Distinguishes COVID-19 from Community Acquired Pneumonia on Chest CT",2020,"","","","",97,"2022-07-13 09:31:23","","10.1148/radiol.2020200905","","",,,,,755,377.50,76,18,2,"Background Coronavirus disease has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performances. Materials and Methods In this retrospective and multi-center study, a deep learning model, COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT exams for the detection of COVID-19. Community acquired pneumonia (CAP) and other non-pneumonia CT exams were included to test the robustness of the model. The datasets were collected from 6 hospitals between August 2016 and February 2020. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity and specificity. Results The collected dataset consisted of 4356 chest CT exams from 3,322 patients. The average age is 49±15 years and there were slightly more male patients than female (1838 vs 1484; p-value=0.29). The per-exam sensitivity and specificity for detecting COVID-19 in the independent test set was 114 of 127 (90% [95% CI: 83%, 94%]) and 294 of 307 (96% [95% CI: 93%, 98%]), respectively, with an AUC of 0.96 (p-value<0.001). The per-exam sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175) and 92% (239 of 259), respectively, with an AUC of 0.95 (95% CI: 0.93, 0.97). Conclusions A deep learning model can accurately detect COVID-19 and differentiate it from community acquired pneumonia and other lung diseases.","",""
32,"Jun-Ho Huh, Yeong-Seok Seo","Understanding Edge Computing: Engineering Evolution With Artificial Intelligence",2019,"","","","",98,"2022-07-13 09:31:23","","10.1109/ACCESS.2019.2945338","","",,,,,32,10.67,16,2,3,"The key to the explosion of the Internet of Things and the ability to collect, analyze, and provide big data in the cloud is edge computing, which is a new computing paradigm in which data is processed from edges. Edge Computing has been attracting attention as one of the top 10 strategic technology trends in the past two years and has innovative potential. It provides shorter response times, lower bandwidth costs, and more robust data safety and privacy protection than cloud computing. In particular, artificial intelligence technologies are rapidly incorporating edge computing. In this paper, we introduce the concepts, backgrounds, and pros and cons of edge computing, explain how it operates and its structure hierarchically with artificial intelligence concepts, list examples of its applications in various fields, and finally suggest some improvements and discuss the challenges of its application in three representative technological fields. We intend to clarify various analyses and opinions regarding edge computing and artificial intelligence.","",""
32,"Matt Taddy","The Technological Elements of Artificial Intelligence",2018,"","","","",99,"2022-07-13 09:31:23","","10.3386/W24301","","",,,,,32,8.00,32,1,4,"We have seen in the past decade a sharp increase in the extent that companies use data to optimize their businesses. Variously called the `Big Data' or `Data Science' revolution, this has been characterized by massive amounts of data, including unstructured and nontraditional data like text and images, and the use of fast and flexible Machine Learning (ML) algorithms in analysis. With recent improvements in Deep Neural Networks (DNNs) and related methods, application of high-performance ML algorithms has become more automatic and robust to different data scenarios. That has led to the rapid rise of an Artificial Intelligence (AI) that works by combining many ML algorithms together – each targeting a straightforward prediction task – to solve complex problems. We will define a framework for thinking about the ingredients of this new ML-driven AI. Having an understanding of the pieces that make up these systems and how they fit together is important for those who will be building businesses around this technology. Those studying the economics of AI can use these definitions to remove ambiguity from the conversation on AI's projected productivity impacts and data requirements. Finally, this framework should help clarify the role for AI in the practice of modern business analytics and economic measurement.","",""
7,"Mir Riyanul Islam, Mobyen Uddin Ahmed, Shaibal Barua, S. Begum","A Systematic Review of Explainable Artificial Intelligence in Terms of Different Application Domains and Tasks",2022,"","","","",100,"2022-07-13 09:31:23","","10.3390/app12031353","","",,,,,7,7.00,2,4,1,"Artificial intelligence (AI) and machine learning (ML) have recently been radically improved and are now being employed in almost every application domain to develop automated or semi-automated systems. To facilitate greater human acceptability of these systems, explainable artificial intelligence (XAI) has experienced significant growth over the last couple of years with the development of highly accurate models but with a paucity of explainability and interpretability. The literature shows evidence from numerous studies on the philosophy and methodologies of XAI. Nonetheless, there is an evident scarcity of secondary studies in connection with the application domains and tasks, let alone review studies following prescribed guidelines, that can enable researchers’ understanding of the current trends in XAI, which could lead to future research for domain- and application-specific method development. Therefore, this paper presents a systematic literature review (SLR) on the recent developments of XAI methods and evaluation metrics concerning different application domains and tasks. This study considers 137 articles published in recent years and identified through the prominent bibliographic databases. This systematic synthesis of research articles resulted in several analytical findings: XAI methods are mostly developed for safety-critical domains worldwide, deep learning and ensemble models are being exploited more than other types of AI/ML models, visual explanations are more acceptable to end-users and robust evaluation metrics are being developed to assess the quality of explanations. Research studies have been performed on the addition of explanations to widely used AI/ML models for expert users. However, more attention is required to generate explanations for general users from sensitive domains such as finance and the judicial system.","",""
24,"P. Giudici, E. Raffinetti","Shapley-Lorenz eXplainable Artificial Intelligence",2020,"","","","",101,"2022-07-13 09:31:23","","10.1016/j.eswa.2020.114104","","",,,,,24,12.00,12,2,2,"","",""
0,"","A Novel Approach to Adopt Explainable Artificial Intelligence in X-ray Image Classification",2022,"","","","",102,"2022-07-13 09:31:23","","10.33140/amlai.03.01.01","","",,,,,0,0.00,0,0,1,"Robust “Blackbox” algorithms such as Convolutional Neural Networks (CNNs) are known for making high prediction performance. However, the ability to explain and interpret these algorithms still require innovation in the understanding of influential and, more importantly, explainable features that directly or indirectly impact the performance of predictivity. In view of the above needs, this study proposes an interaction- based methodology – Influence Score (I-score) – to screen out the noisy and non-informative variables in the images hence it nourishes an environment with explainable and interpretable features that are directly associated to feature predictivity. We apply the proposed method on a real-world application in Pneumonia Chest X-ray Image data set and produced state- of-the-art results. We demonstrate how to apply the proposed approach for more general big data problems by improving the explain ability and interpretability without sacrificing the prediction performance. The contribution of this paper opens a novel angle that moves the community closer to the future pipelines of XAI problems.","",""
0,"Nina de Lacy, Michael J. Ramshaw, J. Kutz","Integrated Evolutionary Learning: An Artificial Intelligence Approach to Joint Learning of Features and Hyperparameters for Optimized, Explainable Machine Learning",2022,"","","","",103,"2022-07-13 09:31:23","","10.3389/frai.2022.832530","","",,,,,0,0.00,0,3,1,"Artificial intelligence and machine learning techniques have proved fertile methods for attacking difficult problems in medicine and public health. These techniques have garnered strong interest for the analysis of the large, multi-domain open science datasets that are increasingly available in health research. Discovery science in large datasets is challenging given the unconstrained nature of the learning environment where there may be a large number of potential predictors and appropriate ranges for model hyperparameters are unknown. As well, it is likely that explainability is at a premium in order to engage in future hypothesis generation or analysis. Here, we present a novel method that addresses these challenges by exploiting evolutionary algorithms to optimize machine learning discovery science while exploring a large solution space and minimizing bias. We demonstrate that our approach, called integrated evolutionary learning (IEL), provides an automated, adaptive method for jointly learning features and hyperparameters while furnishing explainable models where the original features used to make predictions may be obtained even with artificial neural networks. In IEL the machine learning algorithm of choice is nested inside an evolutionary algorithm which selects features and hyperparameters over generations on the basis of an information function to converge on an optimal solution. We apply IEL to three gold standard machine learning algorithms in challenging, heterogenous biobehavioral data: deep learning with artificial neural networks, decision tree-based techniques and baseline linear models. Using our novel IEL approach, artificial neural networks achieved ≥ 95% accuracy, sensitivity and specificity and 45–73% R2 in classification and substantial gains over default settings. IEL may be applied to a wide range of less- or unconstrained discovery science problems where the practitioner wishes to jointly learn features and hyperparameters in an adaptive, principled manner within the same algorithmic process. This approach offers significant flexibility, enlarges the solution space and mitigates bias that may arise from manual or semi-manual hyperparameter tuning and feature selection and presents the opportunity to select the inner machine learning algorithm based on the results of optimized learning for the problem at hand.","",""
0,"G. Antoniou, Emmanuel Papadakis, George Baryannis","Mental Health Diagnosis: A Case for Explainable Artificial Intelligence",2022,"","","","",104,"2022-07-13 09:31:23","","10.1142/s0218213022410032","","",,,,,0,0.00,0,3,1,"Mental illnesses are becoming increasingly prevalent, in turn leading to an increased interest in exploring artificial intelligence (AI) solutions to facilitate and enhance healthcare processes ranging from diagnosis to monitoring and treatment. In contrast to application areas where black box systems may be acceptable, explainability in healthcare applications is essential, especially in the case of diagnosing complex and sensitive mental health issues. In this paper, we first summarize recent developments in AI research for mental health, followed by an overview of approaches to explainable AI and their potential benefits in healthcare settings. We then present a recent case study of applying explainable AI for ADHD diagnosis which is used as a basis to identify challenges in realizing explainable AI solutions for mental health diagnosis and potential future research directions to address these challenges.","",""
0,"Jurgita Černevičienė, Audrius Kabašinskas","Review of Multi-Criteria Decision-Making Methods in Finance Using Explainable Artificial Intelligence",2022,"","","","",105,"2022-07-13 09:31:23","","10.3389/frai.2022.827584","","",,,,,0,0.00,0,2,1,"The influence of Artificial Intelligence is growing, as is the need to make it as explainable as possible. Explainability is one of the main obstacles that AI faces today on the way to more practical implementation. In practise, companies need to use models that balance interpretability and accuracy to make more effective decisions, especially in the field of finance. The main advantages of the multi-criteria decision-making principle (MCDM) in financial decision-making are the ability to structure complex evaluation tasks that allow for well-founded financial decisions, the application of quantitative and qualitative criteria in the analysis process, the possibility of transparency of evaluation and the introduction of improved, universal and practical academic methods to the financial decision-making process. This article presents a review and classification of multi-criteria decision-making methods that help to achieve the goal of forthcoming research: to create artificial intelligence-based methods that are explainable, transparent, and interpretable for most investment decision-makers.","",""
14,"Shipher Wu, Chun-Min Chang, Guan-Shuo Mai, D. Rubenstein, Chen-Ming Yang, Yu-Ting Huang, Hsu-Hong Lin, Li-Cheng Shih, Sheng-Wei Chen, S. Shen","Artificial intelligence reveals environmental constraints on colour diversity in insects",2019,"","","","",106,"2022-07-13 09:31:23","","10.1038/s41467-019-12500-2","","",,,,,14,4.67,1,10,3,"","",""
1,"Jean-Pierre St Mart, E. L. Goh, Ignatius Liew, Z. Shah, Joydeep Sinha","Artificial intelligence in orthopaedics surgery: transforming technological innovation in patient care and surgical training",2022,"","","","",107,"2022-07-13 09:31:23","","10.1136/postgradmedj-2022-141596","","",,,,,1,1.00,0,5,1,"Artificial intelligence (AI) is an exciting field combining computer science with robust data sets to facilitate problem-solving. It has the potential to transform education, practice and delivery of healthcare especially in orthopaedics. This review article outlines some of the already used AI pathways as well as recent technological advances in orthopaedics. Additionally, this article further explains how potentially these two entities could be combined in the future to improve surgical education, training and ultimately patient care and outcomes.","",""
0,"Chris Yang","Explainable Artificial Intelligence for Predictive Modeling in Healthcare",2022,"","","","",108,"2022-07-13 09:31:23","","10.1007/s41666-022-00114-1","","",,,,,0,0.00,0,1,1,"","",""
0,"T. Hailu, G. Viajiprabhu, Abdulkerim Seid Endris, Nedumaran Arappali","Artificial Intelligence based Network Security System to Predict the Possible Threats in Healthcare Data",2022,"","","","",109,"2022-07-13 09:31:23","","10.1109/ICSCDS53736.2022.9760951","","",,,,,0,0.00,0,4,1,"The world is filled with exciting technologies and ideas; scientists build machines to avoid human intervention in completing work. It is highly challenging to complete the task without the Artificial Intelligence (AI) Technology intervention. With the technological development, certain processes or consultations are performed with the aid of doctors available around the world. In this scenario, it could be noticed that health care is one of the world's expected domains that require the most incredible attention in data security while performing data transfer. Nodes in the network are considered based on the weakest link to overcome the cyber attacker's issues. Besides building the software for data storage, a better mechanism has to be incorporated to provide security to the stored data. This process is a delicate task for every network engineer. This paper will explain such concepts related to health prediction and health care by building the most robust network security systems. Finally, the discussion would cross over human-looping systems, which act as one of the common problems that are affected mentally for a person. The proposed optimized neural network model is compared with the existing data summarization method. From the results it is observed that the proposed model achieves an accuracy of 98.89% which is 4.76% higher than the existing model.","",""
0,"Alexander Williams, C. S. Bangun","Artificial Intelligence System Framework in Improving The Competence of Indonesian Human Resources",2022,"","","","",110,"2022-07-13 09:31:23","","10.34306/ijcitsm.v2i1.91","","",,,,,0,0.00,0,2,1,"In this rapidly evolving period, notably the digital era, technology is critical. The globe is presently living in the technological era, sometimes known as the ""Industrial Revolution 4.0."" This state is characterized by the widespread use of digital machines and the internet, which has resulted in quick and substantial changes in many aspects of human existence, making it simpler for humans to perform numerous tasks. The digital transformation age is part of a more robust technology, which is a shift in how digital technology is applied to many elements of life in society. Artificial intelligence is a field of study that looks at ways to make computers behave like humans. In order to advance science, technology, and art in Indonesia, an artificial intelligence system framework is required. The goal of this research is to explain the Case-Based Reasoning (CBR) paradigm in the context of artificial intelligence development. This framework is intended to serve as a model for implementing intelligence systems in Indonesia.","",""
0,"N. Rafie, J. Jentzer, P. Noseworthy, A. Kashou","Mortality Prediction in Cardiac Intensive Care Unit Patients: A Systematic Review of Existing and Artificial Intelligence Augmented Approaches",2022,"","","","",111,"2022-07-13 09:31:23","","10.3389/frai.2022.876007","","",,,,,0,0.00,0,4,1,"The medical complexity and high acuity of patients in the cardiac intensive care unit make for a unique patient population with high morbidity and mortality. While there are many tools for predictions of mortality in other settings, there is a lack of robust mortality prediction tools for cardiac intensive care unit patients. The ongoing advances in artificial intelligence and machine learning also pose a potential asset to the advancement of mortality prediction. Artificial intelligence algorithms have been developed for application of electrocardiogram interpretation with promising accuracy and clinical application. Additionally, artificial intelligence algorithms applied to electrocardiogram interpretation have been developed to predict various variables such as structural heart disease, left ventricular systolic dysfunction, and atrial fibrillation. These variables can be used and applied to new mortality prediction models that are dynamic with the changes in the patient's clinical course and may lead to more accurate and reliable mortality prediction. The application of artificial intelligence to mortality prediction will fill the gaps left by current mortality prediction tools.","",""
0,"Sandro González-González, L. Serpa-Andrade","Development of a virtual assistant chatbot based on Artificial Intelligence to control and supervise a process of 4 tanks which are interconnected",2022,"","","","",112,"2022-07-13 09:31:23","","10.54941/ahfe1001464","","",,,,,0,0.00,0,2,1,"This article presents the gathering of works related to the usage of virtual assistants into the 4.0 industry in order to stablish the parameters and essential characteristics to define the creation of a ‘chatbot’ virtual assistant. This device should be applicable to a process of 4 tanks which are interconnected with a robust multivariable PID control with the aim of controlling and supervising this process using a mobile messaging application from a smartphone by sending key words in text messages which will be interpreted by the chatbot and this will be capable of acting depending on the message it receives; it can be either a consultation of the status of the process and the tanks which will be answered with a text message with the required information, or a command which will make it work starting or stopping the process. This system is proposed as a solution in the case of long-distance supervision and control during different processes. With this, an option to optimize the execution of actions such as security, speed, reliability of data, and resource maximization can be implemented, which leads to a better general performance of an industry","",""
148,"Jos'e Jim'enez-Luna, F. Grisoni, G. Schneider","Drug discovery with explainable artificial intelligence",2020,"","","","",113,"2022-07-13 09:31:23","","10.1038/s42256-020-00236-4","","",,,,,148,74.00,49,3,2,"","",""
2,"O. Ahmad, L. Lovat","Artificial intelligence for colorectal polyp detection: are we ready for prime time?",2019,"","","","",114,"2022-07-13 09:31:23","","10.21037/jmai.2019.09.02","","",,,,,2,0.67,1,2,3,"Colorectal cancer (CRC) is a leading cause of cancer-related mortality worldwide. Colonoscopy is protective against CRC through the detection and removal of neoplastic polyps. Unfortunately, the procedure is highly operator dependent with significant miss rates for polyps. Artificial intelligence (AI) and computer-aided detection software offers a promising solution by providing real-time assistance to highlight lesions that may otherwise be overlooked. Rapid advances have occurred in the field with recent prospective clinical trials demonstrating an improved adenoma detection rate (ADR) with AI assistance. Deployment in routine clinical practice is possible in the near future although further robust clinical trials are necessary and important practical challenges relating to real-world implementation must be addressed.","",""
132,"Shakir Mohamed, Marie-Thérèse Png, William S. Isaac","Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence",2020,"","","","",115,"2022-07-13 09:31:23","","10.1007/s13347-020-00405-8","","",,,,,132,66.00,44,3,2,"","",""
129,"Arun Das, P. Rad","Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey",2020,"","","","",116,"2022-07-13 09:31:23","","","","",,,,,129,64.50,65,2,2,"Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.","",""
0,"Rifat Uğurlutan, Murat Ayaz","An Artificial Intelligence Solution For Reduce Complications at Tissue Expansion",2019,"","","","",117,"2022-07-13 09:31:23","","10.1109/TIPTEKNO.2019.8894998","","",,,,,0,0.00,0,2,3,"In this study, a robust artificial intelligence system which aims to minimize the common complications in the application of balloon plasty method which has been used for more than half a century and its application is explained. The system was tested as a prototype on new zealand rabbits for twenty days and 1 of the 6 subjects was excluded from the study due to a lack of literature and 5 of them were successful. In general, the structure of the system, working algorithm and data from the subjects are summarized.","",""
86,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu","Artificial-Intelligence-Enabled Intelligent 6G Networks",2019,"","","","",118,"2022-07-13 09:31:23","","10.1109/MNET.011.2000195","","",,,,,86,28.67,14,6,3,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","",""
90,"R. Shafin, Lingjia Liu, V. Chandrasekhar, Hao Chen, J. Reed, Jianzhong Zhang","Artificial Intelligence-Enabled Cellular Networks: A Critical Path to Beyond-5G and 6G",2019,"","","","",119,"2022-07-13 09:31:23","","10.1109/MWC.001.1900323","","",,,,,90,30.00,15,6,3,"Mobile network operators (MNOs) are in the process of overlaying their conventional macro cellular networks with shorter range cells such as outdoor pico cells. The resultant increase in network complexity creates substantial overhead in terms of operating expenses, time, and labor for their planning and management. Artificial intelligence (AI) offers the potential for MNOs to operate their networks in a more organic and cost-efficient manner. We argue that deploying AI in fifth generation (5G) and beyond will require surmounting significant technical barriers in terms of robustness, performance, and complexity. We outline future research directions, identify top five challenges, and present a possible roadmap to realize the vision of AI-enabled cellular networks for Beyond- 5G and sixth generation (6G) networks.","",""
0,"D. Ting, L. Peng, A. Varadarajan, Pearse Keane FRCOphth, P. Burlina, M. Chiang, L. Schmetterer, L. Pasquale, N. Bressler, D. Webster, M. Abràmoff, T. Y. Wong","Artificial Intelligence, Machine Learning and Deep Learning in Ophthalmology: Current Clinical Relevance",2019,"","","","",120,"2022-07-13 09:31:23","","","","",,,,,0,0.00,0,12,3,"With the advent of computer graphic processing units, improvement in mathematical models and availability of big data, artificial intelligence (AI) using machine learning (ML) and deep learning (DL) techniques have achieved robust performance for potential application across many industries, including social-media, the internet of things, the automotive industry and healthcare. DL systems provide capability in image, speech and motion recognition as well as in natural language processing. In medicine, most of the progress of AI, ML and DL systems has been demonstrated in image-centric specialties such as radiology, dermatology and pathology. There is increasing interest in AI in ophthalmology. New studies, including pre-registered prospective clinical trials, have shown DL systems are effective in detecting diabetic retinopathy (DR), glaucoma, age-related macular degeneration, retinopathy of prematurity, refractive error and in identifying cardiovascular risk factors and diseases, using image based data such as fundus photographs and optical coherence tomography. Additionally, the application of ML to Humphrey visual fields may be useful in detecting glaucoma progression. There are fewer studies that incorporate clinical data in AL algorithms and no prospective studies to demonstrate that AI algorithms can predict the development of eye disease. This article describes the current global eye disease burden, clinical unmet needs and selected common ophthalmic conditions of public health importance for which AI and DL systems may be applicable. Technical and clinical aspects to build a DL system to address those gaps, and the potential challenges for clinical adoption are discussed. AI, ML and DL likely will play a crucial role in clinical ophthalmology practice, with implications for screening, diagnosis and follow up of the major causes of vision impairment, in the setting of the ageing population globally. Introduction With the advent of graphic processing units (GPUs), advances in mathematical models, the availability of big datasets and low cost sensors, artificial intelligence (AI) using machine learning (ML) techniques initially and deep learning (DL) techniques subsequently, has sparked tremendous interest in many industries. These include application of AI in social-media, the internet of things, finance and banking, the automotive industry and healthcare. AI systems can be designed not only for image, speech and motion recognition, but also in natural language processing. In medicine, the most robust AI algorithms have been demonstrated in image-centric specialties, including radiology, dermatology, pathology and increasingly so in ophthalmology. For example, Lakhani et al demonstrated excellent performance in detecting pulmonary tuberculosis from chest radiographs, while Esteva et al was able to differentiate malignant melanoma from benign lesions on skin photographs. In ophthalmology, there have been two major areas in which AI and new DL systems have been applied. First, AI systems have been shown in new studies, including preregistered prospective clinical trials, to accurately detect diabetic retinopathy (DR), 13 glaucoma, age-related macular degeneration (AMD), retinopathy of prematurity (ROP), and refractive error, from digital fundus photographs. A range of cardiovascular risk factors have also been accurately predicted from fundus photographs. Second, several retinal conditions [e.g., neovascular AMD, earlier stages of AMD, and diabetic macular edema (DME)] has also be detected accurately using optical coherence tomography (OCT). There are relatively fewer AI studies using other data, such as studies which show good performance in detecting glaucoma progression from serial Humphrey visual fields (HVFs). However, there are fewer studies that incorporate clinical and imaging data in AL algorithms, and no prospective studies to demonstrate that AI algorithms can predict the development of eye diseases over time. Furthermore, the implementation and adoption of AI into routine clinical care remains extremely challenging. These remain significant goals of AI research in ophthalmology This article describes basic concepts of AI, ML and DL and how such systems might address some of the global burdens created by common eye conditions. Furthermore, the technical and clinical aspects of developing and validating an AI/DL system, potential challenges and future directions are also discussed in this article. Artificial Intelligence, Machine Learning and Deep Learning AI was conceptualized in 1956, after a workshop at Dartmouth College (Figure 1). In the workshop, many AI groups showed promising results in computer learning of checkers strategies, solving word problems in algebra and proving logical theorems. These tasks involved mostly pattern recognition and computational learning. All AI systems were designed to execute and maximise its chance of ‘winning’ within a constructed environment. The term ‘machine learning’ (ML) was subsequently coined by Arthur Samuel in 1959 and stated that “the computer should have the ability to learn using various statistical techniques, without being explicitly programmed”. Using ML, the algorithm can learn and make predictions based on the data that has been fed into the training phase, using either a supervised or unsupervised approach. ML has been widely adopted in applications such as computer vision and predictive analytics using complex mathematical models. In supervised learning, the computer is trained with labelled examples, also known as ground truth, whereas for unsupervised learning, no labelling is required for the algorithm to find its own structure in the input. The majority of AI application in biomedical research uses supervised learning. DL utilizes multiple processing layers to learn representation of data with multiple levels of abstraction. Although some forms of deep neural networks have already been investigated in the past, the advent of graphic processing units (GPU) with improved processing power, larger annotated datasets, and other factors, have recently boosted its diagnostic performance in many domains. Using learning approaches such as backpropagation, a ML or DL system is able to discover intricate structure in large data sets, then changing its internal parameters that are used to compute the representation in each layer from the previous one. These approaches permit the use of regional samples to allow the network to learn to detect biomarkers; furthermore these approaches use complete images, and associate the entire image with a diagnostic output, thereby eliminating the use of “hand-engineered” image features. Given the much improved performance, DL has been widely adopted in image recognition, speech recognition and natural language processing. General Approach in Building a Robust AI system This section explains some common terminologies, software framework, network architectures, datasets selection, assistive vs. autonomous AI system, consideration factors to ensure the robustness of these algorithms (Table 1). In order to build a robust DL system, it is important to have 2 main components – the ‘brain’ (technical networks – Convolutional Neural Network (CNN) and the ‘dictionary’ (the datasets). 1. What is a CNN? A CNN is a deep neural network consisting of a cascade of processing layers that resemble the biological processes of the animal visual cortex. It transforms the input volume into an output volume via a differentiable function. Inspired by Hubel and Weisel, each neuron in the visual cortex will respond to the stimulus that is specific to a region within an image, similar to how the brain neuron would respond to the visual stimuli, that will activate a particular region of the visual space, known as the receptive field. These receptive fields are tiled together to cover the entire visual field. Two classes of cells are found in this region – simple vs complex cells. The simple cells active when they detect edge-like patterns, while the more complex cells activate when they have a larger receptive field and are invariant to the position of the pattern. Broadly, the CNN can be divided into the input, hidden (also known as featureextraction layers) and output layers (Figure 2A). The hidden layers usually consist of convolutional, pooling, fully connected and normalization layers, and the number of hidden layers will differ for different CNNs. The input layer specifies the width, height and the number of channels (usually 3 channels – red, green and blue). The convolutional layer is the core building block of a CNN, transforming the input data by applying a set of filters (also known as kernels) that acts as the feature detectors. The filter will slide over the input image to produce a feature map (as the output). A CNN learns the values of these filters weights on its own during the training process, although the specific parameters such as number of filters, filter size, network architecture still need to be set prior to that. Additional operations called activations (for example ReLU or Rectified Linear Unit) are used after every convolution operation. For pooling, the aim is to reduce the dimensionality of each feature map and make it somewhat spatially invariant, and retain the most important information. Pooling can be divided into different types: maximum, average and minimum. In the case of maximum pooling, the largest element from the rectified feature map will be taken (Figure 2B). The output from the convolutional and pooling layers represent the high-level features of the input image. The purpose of the fully connected layer is to use these high-level features to classify the input image into various classes based on the training dataset. Following which, backpropagation is conducted to compute the network weights and uses the gradient descent to update all filters and parameter values to minimize the output error. T","",""
43,"Zeynep Akata, D. Balliet, M. de Rijke, F. Dignum, V. Dignum, Gusz Eiben, Antske Fokkens, D. Grossi, K. Hindriks, H. Hoos, Hayley Hung, C. Jonker, Christof Monz, M. Neerincx, Frans Oliehoek, H. Prakken, S. Schlobach, Linda Christina van der Gaag, F. van Harmelen, Herke van Hoof, Birna van Riemsdijk, A. van Wynsberghe, R. Verbrugge, B. Verheij, P. Vossen, M. Welling","A Research Agenda for Hybrid Intelligence: Augmenting Human Intellect With Collaborative, Adaptive, Responsible, and Explainable Artificial Intelligence",2020,"","","","",121,"2022-07-13 09:31:23","","10.1109/MC.2020.2996587","","",,,,,43,21.50,4,26,2,"We define hybrid intelligence (HI) as the combination of human and machine intelligence, augmenting human intellect and capabilities instead of replacing them and achieving goals that were unreachable by either humans or machines. HI is an important new research focus for artificial intelligence, and we set a research agenda for HI by formulating four challenges.","",""
34,"T. H. Aldhyani, M. Al-Yaari, Hasan Alkahtani, Mashael S. Maashi","Water Quality Prediction Using Artificial Intelligence Algorithms",2020,"","","","",122,"2022-07-13 09:31:23","","10.1155/2020/6659314","","",,,,,34,17.00,9,4,2,"During the last years, water quality has been threatened by various pollutants. Therefore, modeling and predicting water quality have become very important in controlling water pollution. In this work, advanced artificial intelligence (AI) algorithms are developed to predict water quality index (WQI) and water quality classification (WQC). For the WQI prediction, artificial neural network models, namely nonlinear autoregressive neural network (NARNET) and long short-term memory (LSTM) deep learning algorithm, have been developed. In addition, three machine learning algorithms, namely, support vector machine (SVM), K-nearest neighbor (K-NN), and Naive Bayes, have been used for the WQC forecasting. The used dataset has 7 significant parameters, and the developed models were evaluated based on some statistical parameters. The results revealed that the proposed models can accurately predict WQI and classify the water quality according to superior robustness. Prediction results demonstrated that the NARNET model performed slightly better than the LSTM for the prediction of the WQI values and the SVM algorithm has achieved the highest accuracy (97.01%) for the WQC prediction. Furthermore, the NARNET and LSTM models have achieved similar accuracy for the testing phase with a slight difference in the regression coefficient (RNARNET = 96.17% and RLSTM = 94.21%). This kind of promising research can contribute significantly to water management.","",""
7,"A. Rawal, J. Mccoy, D. Rawat, Brian M. Sadler, R. Amant","Recent Advances in Trustworthy Explainable Artificial Intelligence: Status, Challenges and Perspectives",2021,"","","","",123,"2022-07-13 09:31:23","","10.36227/techrxiv.17054396.v1","","",,,,,7,7.00,1,5,1,"This is a survey paper on Explainable Artificial Intelligence (XAI).","",""
0,"S. Sadeghi, M. Amiri, Farzaneh Mansoori Mooseloo","Artificial Intelligence and Its Application in Optimization under Uncertainty",2021,"","","","",124,"2022-07-13 09:31:23","","10.5772/intechopen.98628","","",,,,,0,0.00,0,3,1,"Nowadays, the increase in data acquisition and availability and complexity around optimization make it imperative to jointly use artificial intelligence (AI) and optimization for devising data-driven and intelligent decision support systems (DSS). A DSS can be successful if large amounts of interactive data proceed fast and robustly and extract useful information and knowledge to help decision-making. In this context, the data-driven approach has gained prominence due to its provision of insights for decision-making and easy implementation. The data-driven approach can discover various database patterns without relying on prior knowledge while also handling flexible objectives and multiple scenarios. This chapter reviews recent advances in data-driven optimization, highlighting the promise of data-driven optimization that integrates mathematical programming and machine learning (ML) for decision-making under uncertainty and identifies potential research opportunities. This chapter provides guidelines and implications for researchers, managers, and practitioners in operations research who want to advance their decision-making capabilities under uncertainty concerning data-driven optimization. Then, a comprehensive review and classification of the relevant publications on the data-driven stochastic program, data-driven robust optimization, and data-driven chance-constrained are presented. This chapter also identifies fertile avenues for future research that focus on deep-data-driven optimization, deep data-driven models, as well as online learning-based data-driven optimization. Perspectives on reinforcement learning (RL)-based data-driven optimization and deep RL for solving NP-hard problems are discussed. We investigate the application of data-driven optimization in different case studies to demonstrate improvements in operational performance over conventional optimization methodology. Finally, some managerial implications and some future directions are provided.","",""
103,"F. Schwendicke, W. Samek, J. Krois","Artificial Intelligence in Dentistry: Chances and Challenges",2020,"","","","",125,"2022-07-13 09:31:23","","10.1177/0022034520915714","","",,,,,103,51.50,34,3,2,"The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.","",""
89,"S. Lauritsen, Mads Kristensen, Mathias Vassard Olsen, Morten Skaarup Larsen, K. M. Lauritsen, Marianne Johansson Jørgensen, Jeppe Lange, B. Thiesson","Explainable artificial intelligence model to predict acute critical illness from electronic health records",2019,"","","","",126,"2022-07-13 09:31:23","","10.1038/s41467-020-17431-x","","",,,,,89,29.67,11,8,3,"","",""
0,"Joseph Noussa-Yao, D. Heudes, P. Degoulet","Using Artificial Intelligence and Big Data-Based Documents to Optimize Medical Coding",2019,"","","","",127,"2022-07-13 09:31:23","","10.5772/INTECHOPEN.85749","","",,,,,0,0.00,0,3,3,"Clinical information systems (CISs) in some hospitals streamline the data management from data warehouses. These warehouses contain heterogeneous information from all medical specialties that offer patient care services. It is increasingly difficult to manage large volumes of data in a specific clinical context such as quality coding of medical services. The document-based not only SQL (NoSQL) model can provide an accessible, extensive, and robust coding data management framework while maintaining certain flexibility. This paper focuses on the design and implementation of a big data-coding warehouse, and it also defines the rules to convert a conceptual model of coding into a document-oriented logical model. Using that model, we implemented and analyzed a big data-coding warehouse via the MongoDB database and evaluated it using data research monoand multicriteria and then calculated the precision of our model.","",""
61,"N. Mirchi, Vincent Bissonnette, R. Yilmaz, N. Ledwos, A. Winkler-Schwartz, R. Del Maestro","The Virtual Operative Assistant: An explainable artificial intelligence tool for simulation-based training in surgery and medicine",2020,"","","","",128,"2022-07-13 09:31:23","","10.1371/journal.pone.0229596","","",,,,,61,30.50,10,6,2,"Simulation-based training is increasingly being used for assessment and training of psychomotor skills involved in medicine. The application of artificial intelligence and machine learning technologies has provided new methodologies to utilize large amounts of data for educational purposes. A significant criticism of the use of artificial intelligence in education has been a lack of transparency in the algorithms’ decision-making processes. This study aims to 1) introduce a new framework using explainable artificial intelligence for simulation-based training in surgery, and 2) validate the framework by creating the Virtual Operative Assistant, an automated educational feedback platform. Twenty-eight skilled participants (14 staff neurosurgeons, 4 fellows, 10 PGY 4–6 residents) and 22 novice participants (10 PGY 1–3 residents, 12 medical students) took part in this study. Participants performed a virtual reality subpial brain tumor resection task on the NeuroVR simulator using a simulated ultrasonic aspirator and bipolar. Metrics of performance were developed, and leave-one-out cross validation was employed to train and validate a support vector machine in Matlab. The classifier was combined with a unique educational system to build the Virtual Operative Assistant which provides users with automated feedback on their metric performance with regards to expert proficiency performance benchmarks. The Virtual Operative Assistant successfully classified skilled and novice participants using 4 metrics with an accuracy, specificity and sensitivity of 92, 82 and 100%, respectively. A 2-step feedback system was developed to provide participants with an immediate visual representation of their standing related to expert proficiency performance benchmarks. The educational system outlined establishes a basis for the potential role of integrating artificial intelligence and virtual reality simulation into surgical educational teaching. The potential of linking expertise classification, objective feedback based on proficiency benchmarks, and instructor input creates a novel educational tool by integrating these three components into a formative educational paradigm.","",""
61,"S. N. Payrovnaziri, Zhaoyi Chen, Pablo A Rengifo-Moreno, Tim Miller, J. Bian, Jonathan H. Chen, Xiuwen Liu, Zhe He","Explainable artificial intelligence models using real-world electronic health record data: a systematic scoping review",2020,"","","","",129,"2022-07-13 09:31:23","","10.1093/jamia/ocaa053","","",,,,,61,30.50,8,8,2,"OBJECTIVE To conduct a systematic scoping review of explainable artificial intelligence (XAI) models that use real-world electronic health record data, categorize these techniques according to different biomedical applications, identify gaps of current studies, and suggest future research directions.   MATERIALS AND METHODS We searched MEDLINE, IEEE Xplore, and the Association for Computing Machinery (ACM) Digital Library to identify relevant papers published between January 1, 2009 and May 1, 2019. We summarized these studies based on the year of publication, prediction tasks, machine learning algorithm, dataset(s) used to build the models, the scope, category, and evaluation of the XAI methods. We further assessed the reproducibility of the studies in terms of the availability of data and code and discussed open issues and challenges.   RESULTS Forty-two articles were included in this review. We reported the research trend and most-studied diseases. We grouped XAI methods into 5 categories: knowledge distillation and rule extraction (N = 13), intrinsically interpretable models (N = 9), data dimensionality reduction (N = 8), attention mechanism (N = 7), and feature interaction and importance (N = 5).   DISCUSSION XAI evaluation is an open issue that requires a deeper focus in the case of medical applications. We also discuss the importance of reproducibility of research work in this field, as well as the challenges and opportunities of XAI from 2 medical professionals' point of view.   CONCLUSION Based on our review, we found that XAI evaluation in medicine has not been adequately and formally practiced. Reproducibility remains a critical concern. Ample opportunities exist to advance XAI research in medicine.","",""
25,"P. Phillips, Carina A. Hahn, Peter C. Fontana, David A. Broniatowski, Mark A. Przybocki","Four Principles of Explainable Artificial Intelligence",2020,"","","","",130,"2022-07-13 09:31:23","","10.6028/nist.ir.8312-draft","","",,,,,25,12.50,5,5,2,"We introduce four principles for explainable artificial intelligence (AI) that comprise fundamental properties for explainable AI systems. We propose that explainable AI systems deliver accompanying evidence or reasons for outcomes and processes; provide explanations that are understandable to individual users; provide explanations that correctly reflect the system’s process for generating the output; and that a system only operates under conditions for which it was designed and when it reaches sufficient confidence in its output. We have termed these four principles as explanation, meaningful, explanation accuracy, and knowledge limits, respectively. Through significant stakeholder engagement, these four principles were developed to encompass the multidisciplinary nature of explainable AI, including the fields of computer science, engineering, and psychology. Because one-sizefits-all explanations do not exist, different users will require different types of explanations. We present five categories of explanation and summarize theories of explainable AI. We give an overview of the algorithms in the field that cover the major classes of explainable algorithms. As a baseline comparison, we assess how well explanations provided by people follow our four principles. This assessment provides insights to the challenges of designing explainable AI systems.","",""
41,"Alexander Campolo, K. Crawford","Enchanted Determinism: Power without Responsibility in Artificial Intelligence",2020,"","","","",131,"2022-07-13 09:31:23","","10.17351/ests2020.277","","",,,,,41,20.50,21,2,2,"Deep learning techniques are growing in popularity within the field of artificial intelligence (AI). These approaches identify patterns in large scale datasets, and make classifications and predictions, which have been celebrated as more accurate than those of humans. But for a number of reasons, including nonlinear path from inputs to outputs, there is a dearth of theory that can explain why deep learning techniques work so well at pattern detection and prediction. Claims about “superhuman” accuracy and insight, paired with the inability to fully explain how these results are produced, form a discourse about AI that we call enchanted determinism . To analyze enchanted determinism, we situate it within a broader epistemological diagnosis of modernity: Max Weber’s theory of disenchantment. Deep learning occupies an ambiguous position in this framework. On one hand, it represents a complex form of technological calculation and prediction, phenomena Weber associated with disenchantment. On the other hand, both deep learning experts and observers deploy enchanted, magical discourses to describe these systems’ uninterpretable mechanisms and counter-intuitive behavior. The combination of predictive accuracy and mysterious or unexplainable properties results in myth-making about deep learning’s transcendent, superhuman capacities, especially when it is applied in social settings. We analyze how discourses of magical deep learning produce techno-optimism, drawing on case studies from game-playing, adversarial examples, and attempts to infer sexual orientation from facial images. Enchantment shields the creators of these systems from accountability while its deterministic, calculative power intensifies social processes of classification and control.","",""
42,"M. Nassar, K. Salah, M. H. Rehman, D. Svetinovic","Blockchain for explainable and trustworthy artificial intelligence",2019,"","","","",132,"2022-07-13 09:31:23","","10.1002/widm.1340","","",,,,,42,14.00,11,4,3,"The increasing computational power and proliferation of big data are now empowering Artificial Intelligence (AI) to achieve massive adoption and applicability in many fields. The lack of explanation when it comes to the decisions made by today's AI algorithms is a major drawback in critical decision‐making systems. For example, deep learning does not offer control or reasoning over its internal processes or outputs. More importantly, current black‐box AI implementations are subject to bias and adversarial attacks that may poison the learning or the inference processes. Explainable AI (XAI) is a new trend of AI algorithms that provide explanations of their AI decisions. In this paper, we propose a framework for achieving a more trustworthy and XAI by leveraging features of blockchain, smart contracts, trusted oracles, and decentralized storage. We specify a framework for complex AI systems in which the decision outcomes are reached based on decentralized consensuses of multiple AI and XAI predictors. The paper discusses how our proposed framework can be utilized in key application areas with practical use cases.","",""
43,"M. González-Rivero, Oscar Beijbom, A. Rodriguez-Ramirez, D. Bryant, A. Ganase, Y. González-Marrero, A. Herrera-Reveles, E. Kennedy, Catherine J. S. Kim, S. Lopez-Marcano, Kathryn Markey, B. Neal, K. Osborne, C. Reyes-Nivia, E. Sampayo, Kristin Stolberg, Abbie Taylor, J. Vercelloni, Mathew Wyatt, O. Hoegh‐Guldberg","Monitoring of Coral Reefs Using Artificial Intelligence: A Feasible and Cost-Effective Approach",2020,"","","","",133,"2022-07-13 09:31:23","","10.3390/rs12030489","","",,,,,43,21.50,4,20,2,"Ecosystem monitoring is central to effective management, where rapid reporting is essential to provide timely advice. While digital imagery has greatly improved the speed of underwater data collection for monitoring benthic communities, image analysis remains a bottleneck in reporting observations. In recent years, a rapid evolution of artificial intelligence in image recognition has been evident in its broad applications in modern society, offering new opportunities for increasing the capabilities of coral reef monitoring. Here, we evaluated the performance of Deep Learning Convolutional Neural Networks for automated image analysis, using a global coral reef monitoring dataset. The study demonstrates the advantages of automated image analysis for coral reef monitoring in terms of error and repeatability of benthic abundance estimations, as well as cost and benefit. We found unbiased and high agreement between expert and automated observations (97%). Repeated surveys and comparisons against existing monitoring programs also show that automated estimation of benthic composition is equally robust in detecting change and ensuring the continuity of existing monitoring data. Using this automated approach, data analysis and reporting can be accelerated by at least 200x and at a fraction of the cost (1%). Combining commonly used underwater imagery in monitoring with automated image annotation can dramatically improve how we measure and monitor coral reefs worldwide, particularly in terms of allocating limited resources, rapid reporting and data integration within and across management areas.","",""
41,"M. Y. Chia, Yuk Feng Huang, C. Koo, K. F. Fung","Recent Advances in Evapotranspiration Estimation Using Artificial Intelligence Approaches with a Focus on Hybridization Techniques—A Review",2020,"","","","",134,"2022-07-13 09:31:23","","10.3390/agronomy10010101","","",,,,,41,20.50,10,4,2,"Difficulties are faced when formulating hydrological processes, including that of evapotranspiration (ET). Conventional empirical methods for formulating these possess some shortcomings. The artificial intelligence approach emerges as the best possible solution to map the relationships between climatic parameters and ET, even with limited knowledge of the interactions between variables. This review presents the state-of-the-art application of artificial intelligence models in ET estimation, along with different types and sources of data. This paper discovers the most significant climatic parameters for different climate patterns. The characteristics of the basic artificial intelligence models are also explored in this review. To overcome the pitfalls of the individual models, hybrid models which use techniques such as data fusion and ensemble modeling, data decomposition as well as remote sensing-based hybridization, are introduced. In particular, the principles and applications of the hybridization techniques, as well as their combinations with basic models, are explained. The review covers most of the related and excellent papers published from 2011 to 2019 to keep its relevancy in terms of time frame and field of study. Guidelines for the future prospects of ET estimation in research are advocated. It is anticipated that such work could contribute to the development of agriculture-based economy.","",""
5,"N. Amoroso, Domenico Pomarico, A. Fanizzi, V. Didonna, F. Giotta, D. La Forgia, A. Latorre, A. Monaco, Ester Pantaleo, N. Petruzzellis, P. Tamborra, A. Zito, V. Lorusso, R. Bellotti, R. Massafra","A Roadmap towards Breast Cancer Therapies Supported by Explainable Artificial Intelligence",2021,"","","","",135,"2022-07-13 09:31:23","","10.3390/APP11114881","","",,,,,5,5.00,1,15,1,"In recent years personalized medicine reached an increasing importance, especially in the design of oncological therapies. In particular, the development of patients’ profiling strategies suggests the possibility of promising rewards. In this work, we present an explainable artificial intelligence (XAI) framework based on an adaptive dimensional reduction which (i) outlines the most important clinical features for oncological patients’ profiling and (ii), based on these features, determines the profile, i.e., the cluster a patient belongs to. For these purposes, we collected a cohort of 267 breast cancer patients. The adopted dimensional reduction method determines the relevant subspace where distances among patients are used by a hierarchical clustering procedure to identify the corresponding optimal categories. Our results demonstrate how the molecular subtype is the most important feature for clustering. Then, we assessed the robustness of current therapies and guidelines; our findings show a striking correspondence between available patients’ profiles determined in an unsupervised way and either molecular subtypes or therapies chosen according to guidelines, which guarantees the interpretability characterizing explainable approaches to machine learning techniques. Accordingly, our work suggests the possibility to design data-driven therapies to emphasize the differences observed among the patients.","",""
4,"Karim Lekadira, Richard Osuala, C. Gallin, Noussair Lazrak, Kaisar Kushibar, G. Tsakou, Susanna Auss'o, Leonor Cerd'a Alberich, K. Marias, Manolis Tskinakis, S. Colantonio, Nickolas Papanikolaou, Zohaib Salahuddin, H. Woodruff, P. Lambin, L. Mart'i-Bonmat'i","FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging",2021,"","","","",136,"2022-07-13 09:31:23","","","","",,,,,4,4.00,0,16,1,"The recent advancements in artificial intelligence (AI) combined with the extensive amount of data generated by today’s clinical systems, has led to the development of imaging AI solutions across the whole value chain of medical imaging, including image reconstruction, medical image segmentation, image-based diagnosis and treatment planning. Notwithstanding the successes and future potential of AI in medical imaging, many stakeholders are concerned of the potential risks and ethical implications of imaging AI solutions, which are perceived as complex, opaque, and difficult to comprehend, utilise, and trust in critical clinical applications. Despite these concerns and risks, there are currently no concrete guidelines and best practices for guiding future AI developments in medical imaging towards increased trust, safety and adoption. To bridge this gap, this paper introduces a careful selection of guiding principles drawn from the accumulated experiences, consensus, and best practices from five large European projects on AI in Health Imaging. These guiding principles are named FUTURE-AI and its building blocks consist of (i) Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness and (vi) Explainability. In a step-by-step approach, these guidelines are further translated into a framework of concrete recommendations for specifying, developing, evaluating, and deploying technically, clinically and ethically trustworthy AI solutions into clinical practice.","",""
4,"José Daniel López-Cabrera, R. Orozco-Morales, Jorge Armando Portal-Díaz, Orlando Lovelle-Enríquez, M. Pérez-Díaz","Current limitations to identify covid-19 using artificial intelligence with chest x-ray imaging (part ii). The shortcut learning problem",2021,"","","","",137,"2022-07-13 09:31:23","","10.1007/s12553-021-00609-8","","",,,,,4,4.00,1,5,1,"","",""
3,"Amy Papadopoulos, J. Salinas, Cindy Crump","Computational modeling approaches to characterize risk and achieve safe, effective, and trusted designs in the development of artificial intelligence and autonomous closed-loop medical systems",2021,"","","","",138,"2022-07-13 09:31:23","","10.1117/12.2586101","","",,,,,3,3.00,1,3,1,"While software using artificial intelligence and machine learning (AI/ML) is pervasive in many areas of society today, the use of these technologies to diagnose and treat medical conditions is limited due to a number of challenges associated with the trustworthiness of the results. This may include the inability to fully explain how an algorithm works inherent to the black-box nature of the system. Additionally, AI/ML may create a potential for bias and artifacts that cannot be validated due to the same limitations. In a medical application, the lack of transparency in how the system operates may lead to a loss of trust by users. Bayesian approaches that use computational modeling to quantify the level of uncertainty in a given result may provide a path towards improved confidence and use. In this paper, evidence from studies in a range of medical applications is presented and discussed, showing how Bayesian approaches can help to foster trust. A retrospective study using a publicly available dataset explored the feasibility of creating predictive models for early intervention in a Type 1 diabetes population. Creating the perfect model was not the goal of the exercise, rather the study aimed to demonstrate how Bayesian methods could be used to identify areas of uncertainty during model development. Feature selection was based on analytical assessment of various patterns found in the data. Models were trained, validated, and tested, generating uncertainty estimates. A two-feature Gaussian Naïve Bayes (GNB) model, using the previous five minutes and ten minutes of blood glucose values, showed similar results for predictive accuracy as a threefeature model that included average change over the preceding 30 minutes. The two-feature model was selected because it allowed for a more easily understood visualization of uncertainty. The 2-feature GNB achieved an AUC = .94. The model showed good sensitivity for exceeding the < 180 mg/dl limit, obtaining threshold prediction = 89.8% and normal range prediction = 90.8%. The sensitivity was lower for the < 70 mg/dl limit, attaining a sensitivity = 77.5%. Posterior probabilities showed differing levels of uncertainty in the prediction of high and low out-of-range conditions. The model demonstrated the feasibility of providing robust parameter estimates. Bayesian machine learning approaches to model uncertainty may improve the transparency, explainability, and applicability of AI/ML in medical treatment, realizing the promise to improve patient safety and outcomes.","",""
3,"T. Kaur, Anirudra Diwakar, Kirandeep, Pranav Mirpuri, M. Tripathi, P. Chandra, T. Gandhi","Artificial Intelligence in Epilepsy",2021,"","","","",139,"2022-07-13 09:31:23","","10.4103/0028-3886.317233","","",,,,,3,3.00,0,7,1,"Background: The study of seizure patterns in electroencephalography (EEG) requires several years of intensive training. In addition, inadequate training and human error may lead to misinterpretation and incorrect diagnosis. Artificial intelligence (AI)-based automated seizure detection systems hold an exciting potential to create paradigms for proper diagnosis and interpretation. AI holds the promise to transform healthcare into a system where machines and humans can work together to provide an accurate, timely diagnosis, and treatment to the patients. Objective: This article presents a brief overview of research on the use of AI systems for pattern recognition in EEG for clinical diagnosis. Material and Methods: The article begins with the need for understanding nonstationary signals such as EEG and simplifying their complexity for accurate pattern recognition in medical diagnosis. It also explains the core concepts of AI, machine learning (ML), and deep learning (DL) methods. Results and Conclusions: In this present context of epilepsy diagnosis, AI may work in two ways; first by creating visual representations (e.g., color-coded paradigms), which allow persons with limited training to make a diagnosis. The second is by directly explaining a complete automated analysis, which of course requires more complex paradigms than the previous one. We also clarify that AI is not about replacing doctors and strongly emphasize the need for domain knowledge in building robust AI models that can work in real-time scenarios rendering good detection accuracy in a minimum amount of time.","",""
1,"S. Greenstein","Preserving the rule of law in the era of artificial intelligence (AI)",2021,"","","","",140,"2022-07-13 09:31:23","","10.1007/S10506-021-09294-4","","",,,,,1,1.00,1,1,1,"","",""
1,"Vitor Bento, Manoela Kohler, Pedro Diaz, L. Mendoza, Marco Aurélio Cavalcanti Pacheco","Improving deep learning performance by using Explainable Artificial Intelligence (XAI) approaches",2021,"","","","",141,"2022-07-13 09:31:23","","10.1007/s44163-021-00008-y","","",,,,,1,1.00,0,5,1,"","",""
1,"R. Joshi, Neeraj Kumar","Artificial Intelligence for Autonomous Molecular Design: A Perspective",2021,"","","","",142,"2022-07-13 09:31:23","","10.3390/molecules26226761","","",,,,,1,1.00,1,2,1,"Domain-aware artificial intelligence has been increasingly adopted in recent years to expedite molecular design in various applications, including drug design and discovery. Recent advances in areas such as physics-informed machine learning and reasoning, software engineering, high-end hardware development, and computing infrastructures are providing opportunities to build scalable and explainable AI molecular discovery systems. This could improve a design hypothesis through feedback analysis, data integration that can provide a basis for the introduction of end-to-end automation for compound discovery and optimization, and enable more intelligent searches of chemical space. Several state-of-the-art ML architectures are predominantly and independently used for predicting the properties of small molecules, their high throughput synthesis, and screening, iteratively identifying and optimizing lead therapeutic candidates. However, such deep learning and ML approaches also raise considerable conceptual, technical, scalability, and end-to-end error quantification challenges, as well as skepticism about the current AI hype to build automated tools. To this end, synergistically and intelligently using these individual components along with robust quantum physics-based molecular representation and data generation tools in a closed-loop holds enormous promise for accelerated therapeutic design to critically analyze the opportunities and challenges for their more widespread application. This article aims to identify the most recent technology and breakthrough achieved by each of the components and discusses how such autonomous AI and ML workflows can be integrated to radically accelerate the protein target or disease model-based probe design that can be iteratively validated experimentally. Taken together, this could significantly reduce the timeline for end-to-end therapeutic discovery and optimization upon the arrival of any novel zoonotic transmission event. Our article serves as a guide for medicinal, computational chemistry and biology, analytical chemistry, and the ML community to practice autonomous molecular design in precision medicine and drug discovery.","",""
2,"J. Arslan, K. Benke","Artificial Intelligence and Telehealth may Provide Early Warning of Epidemics",2021,"","","","",143,"2022-07-13 09:31:23","","10.3389/frai.2021.556848","","",,,,,2,2.00,1,2,1,"The COVID-19 pandemic produced a very sudden and serious impact on public health around the world, greatly adding to the burden of overloaded professionals and national medical systems. Recent medical research has demonstrated the value of using online systems to predict emerging spatial distributions of transmittable diseases. Concerned internet users often resort to online sources in an effort to explain their medical symptoms. This raises the prospect that incidence of COVID-19 may be tracked online by search queries and social media posts analyzed by advanced methods in data science, such as Artificial Intelligence. Online queries can provide early warning of an impending epidemic, which is valuable information needed to support planning timely interventions. Identification of the location of clusters geographically helps to support containment measures by providing information for decision-making and modeling.","",""
2,"P. W. Grimm, Maura R. Grossman, G. Cormack","Artificial Intelligence as Evidence",2021,"","","","",144,"2022-07-13 09:31:23","","","","",,,,,2,2.00,1,3,1,"This article explores issues that govern the admissibility of Artificial Intelligence (“AI”) applications in civil and criminal cases, from the perspective of a federal trial judge and two computer scientists, one of whom also is an experienced attorney. It provides a detailed yet intelligible discussion of what AI is and how it works, a history of its development, and a description of the wide variety of functions that it is designed to accomplish, stressing that AI applications are ubiquitous, both in the private and public sectors. Applications today include: health care, education, employment-related decision-making, finance, law enforcement, and the legal profession. The article underscores the importance of determining the validity of an AI application (i.e., how accurately the AI measures, classifies, or predicts what it is designed to), as well as its reliability (i.e., the consistency with which the AI produces accurate results when applied to the same or substantially similar circumstances), in deciding whether it should be admitted into evidence in civil and criminal cases. The article further discusses factors that can affect the validity and reliability of AI evidence, including bias of various types, “function creep,” lack of transparency and explainability, and the sufficiency of the objective testing of AI applications before they are released for public use. The article next provides an in-depth discussion of the evidentiary principles that govern whether AI evidence should be admitted in court cases, a topic which, at present, is not the subject of comprehensive analysis in decisional law. The focus of this discussion is on providing a step-by-step analysis of the most important issues, and the factors that affect decisions on whether to admit AI evidence. Finally, the article concludes with a discussion of practical suggestions intended to assist lawyers and judges as they are called upon to introduce, object to, or decide on whether to admit AI evidence. 1 Hon. Paul W. Grimm is a United States District Judge for the District of Maryland, and an adjunct professor at both the University of Maryland Carey School of Law and the University of Baltimore School of Law. Maura R. Grossman, J.D., Ph.D., is a Research Professor, and Gordon V. Cormack, Ph.D., is a Professor, in the David R. Cheriton School of Computer Science at the University of Waterloo. Professor Grossman is also an affiliate faculty member at the Vector Institute for Artificial Intelligence. Her work is funded, in part, by the National Sciences and Engineering Council of Canada (“NESERC”). The opinions expressed in this article are the authors’ own, and do not necessarily reflect the views of the institutions or organizations with which they are affiliated. NORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY 10 INTRODUCTION .............................................................................................................. 10 I. WHAT IS “ARTIFICIAL INTELLIGENCE”? .................................................................... 14 II. WHY AI HAS COME TO THE FOREFRONT TODAY ...................................................... 17 III. THE AI TECHNOLOGY LANDSCAPE .......................................................................... 24 IV. USES OF AI IN BUSINESS AND LAW TODAY .............................................................. 32 V. ISSUES RAISED BY THE USE OF AI IN BUSINESS AND LAW TODAY ............................ 41 A. Bias ............................................................................................................... 42 B. Lack of Robust Testing for Validity and Reliability ....................................... 48 C. Failure to Monitor for Function Creep ......................................................... 51 D. Failure to Ensure Data Privacy and Data Protection .................................. 53 E. Lack of Transparency and Explainabilty ....................................................... 60 F. Lack of Accountability ................................................................................... 65 G. Lack of Resilience ......................................................................................... 72 VI. ESTABLISHING VALIDITY AND RELIABILITY ........................................................... 79 A. Testimony, Expert Testimony, or Technology? .............................................. 79 B. Benchmarks and Goodhart’s Law ................................................................. 82 VII. EVIDENTIARY PRINCIPLES THAT SHOULD BE CONSIDERED IN EVALUATING THE ADMISSIBILITY OF AI EVIDENCE IN CIVIL AND CRIMINAL TRIALS .................... 84 A. Adequacy of the Federal Rules of Evidence in Addressing the Admissibility of AI Evidence ......................................................................... 84 B. Relevance ...................................................................................................... 86 C. Authentication of AI Evidence ....................................................................... 90 D. Usefulness of the Daubert Factors in Determining Whether to Admit AI Evidence ....................................................................................................... 95 E. Practice Pointers for Lawyers and Judges .................................................... 97 CONCLUSION ............................................................................................................... 105","",""
2,"Zahraa Bassyouni, I. Elhajj","Augmented Reality Meets Artificial Intelligence in Robotics: A Systematic Review",2021,"","","","",145,"2022-07-13 09:31:23","","10.3389/frobt.2021.724798","","",,,,,2,2.00,1,2,1,"Recently, advancements in computational machinery have facilitated the integration of artificial intelligence (AI) to almost every field and industry. This fast-paced development in AI and sensing technologies have stirred an evolution in the realm of robotics. Concurrently, augmented reality (AR) applications are providing solutions to a myriad of robotics applications, such as demystifying robot motion intent and supporting intuitive control and feedback. In this paper, research papers combining the potentials of AI and AR in robotics over the last decade are presented and systematically reviewed. Four sources for data collection were utilized: Google Scholar, Scopus database, the International Conference on Robotics and Automation 2020 proceedings, and the references and citations of all identified papers. A total of 29 papers were analyzed from two perspectives: a theme-based perspective showcasing the relation between AR and AI, and an application-based analysis highlighting how the robotics application was affected. These two sections are further categorized based on the type of robotics platform and the type of robotics application, respectively. We analyze the work done and highlight some of the prevailing limitations hindering the field. Results also explain how AR and AI can be combined to solve the model-mismatch paradigm by creating a closed feedback loop between the user and the robot. This forms a solid base for increasing the efficiency of the robotic application and enhancing the user’s situational awareness, safety, and acceptance of AI robots. Our findings affirm the promising future for robust integration of AR and AI in numerous robotic applications.","",""
1,"Pingping Sun, Lingang Gu","Fuzzy knowledge graph system for artificial intelligence-based smart education",2021,"","","","",146,"2022-07-13 09:31:23","","10.3233/JIFS-189332","","",,,,,1,1.00,1,2,1,"Fuzzy knowledge graph system is a semantic network that reveals the relationships between entities, and a tool or methodology that can formally describe things in the real world and their relationships. Smart education is an educational concept or model that uses advanced information technology to build a smart environment, integrates theory and practice to build an educational framework for information age, and provides paths to practice it. Artificial intelligence (AI) is a comprehensive discipline developed by the interpenetration of computer science, cybernetics, information theory, linguistics, neurophysiology and other disciplines, which is a direction for the development of information technology in the future. On the basis of summarizing and analyzing of previous research works, this paper expounded the research status and significance of AI technology, elaborated the development background, current status and future challenges of the construction and application of fuzzy knowledge graph system for smart education, introduced the methods and principles of data acquisition methods and digitalized apprenticeship, realized the process design, information extraction, entity recognition and relationship mining of smart education, constructed a systematic framework for fuzzy knowledge graph, and analyzed the high-quality resources sharing and personalized service of AI-assisted smart education, discussed automatic knowledge acquisition and fusion of fuzzy knowledge graph, performed co-occurrence relationship analysis, and finally conducted application case analysis. The results show that the smart education knowledge graph for AI-assisted smart education can integrate teaching experience and domain knowledge of discipline experts, enhance explainable and robust machine intelligence for AI-assisted smart education, and provide data-driven and knowledge-driven information processing methods; it can also discover the analysis hotspots and main content of research objects through clustering of high-frequency topic words, reveal the corresponding research structure in depth, and then systematically explore its research dimensions, subject background and theoretical basis.","",""
1,"Ke Zhang, Peidong Xu, Tianlu Gao, Jun Zhang","A Trustworthy Framework of Artificial Intelligence for Power Grid Dispatching Systems",2021,"","","","",147,"2022-07-13 09:31:23","","10.1109/DTPI52967.2021.9540198","","",,,,,1,1.00,0,4,1,"With the widespread application of artificial intelligence (AI) technologies in power systems, the properties of lack of reliability and transparency for AI technologies have revealed gradually. Here, how to build a trustworthy-AI framework based on the power system is the focus. Due to the multidimensional and heterogeneous information of power grid data, the heterogeneous graph attention network (HGAT) model of power grid dispatching is established, and the corresponding explainer (HGAT-Explainer) for the model of power equipment faults is proposed to provide more favorable support for the trustworthy-AI systems.","",""
2,"Kamelia Moh’d Khier Al Momani, Abdulnasr Ibrahim Nour, N. Jamaludin, W. Z. W. Zanani Wan Abdullah","Fourth Industrial Revolution, Artificial Intelligence, Intellectual Capital, and COVID-19 Pandemic",2021,"","","","",148,"2022-07-13 09:31:23","","10.1007/978-3-030-72080-3_5","","",,,,,2,2.00,1,4,1,"","",""
15,"Hubert Baniecki, Wojciech Kretowicz, Piotr Piatyszek, J. Wiśniewski, P. Biecek","dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python",2020,"","","","",149,"2022-07-13 09:31:23","","","","",,,,,15,7.50,3,5,2,"The increasing amount of available data, computing power, and the constant pursuit for higher performance results in the growing complexity of predictive models. Their black-box nature leads to opaqueness debt phenomenon inflicting increased risks of discrimination, lack of reproducibility, and deflated performance due to data drift. To manage these risks, good MLOps practices ask for better validation of model performance and fairness, higher explainability, and continuous monitoring. The necessity of deeper model transparency appears not only from scientific and social domains, but also emerging laws and regulations on artificial intelligence. To facilitate the development of responsible machine learning models, we showcase dalex, a Python package which implements the model-agnostic interface for interactive model exploration. It adopts the design crafted through the development of various tools for responsible machine learning; thus, it aims at the unification of the existing solutions. This library's source code and documentation are available under open license at this https URL","",""
44,"A. Vellido","Societal Issues Concerning the Application of Artificial Intelligence in Medicine",2018,"","","","",150,"2022-07-13 09:31:23","","10.1159/000492428","","",,,,,44,11.00,44,1,4,"Background: Medicine is becoming an increasingly data-centred discipline and, beyond classical statistical approaches, artificial intelligence (AI) and, in particular, machine learning (ML) are attracting much interest for the analysis of medical data. It has been argued that AI is experiencing a fast process of commodification. This characterization correctly reflects the current process of industrialization of AI and its reach into society. Therefore, societal issues related to the use of AI and ML should not be ignored any longer and certainly not in the medical domain. These societal issues may take many forms, but they all entail the design of models from a human-centred perspective, incorporating human-relevant requirements and constraints. In this brief paper, we discuss a number of specific issues affecting the use of AI and ML in medicine, such as fairness, privacy and anonymity, explainability and interpretability, but also some broader societal issues, such as ethics and legislation. We reckon that all of these are relevant aspects to consider in order to achieve the objective of fostering acceptance of AI- and ML-based technologies, as well as to comply with an evolving legislation concerning the impact of digital technologies on ethically and privacy sensitive matters. Our specific goal here is to reflect on how all these topics affect medical applications of AI and ML. This paper includes some of the contents of the “2nd Meeting of Science and Dialysis: Artificial Intelligence,” organized in the Bellvitge University Hospital, Barcelona, Spain. Summary and Key Messages: AI and ML are attracting much interest from the medical community as key approaches to knowledge extraction from data. These approaches are increasingly colonizing ambits of social impact, such as medicine and healthcare. Issues of social relevance with an impact on medicine and healthcare include (although they are not limited to) fairness, explainability, privacy, ethics and legislation.","",""
442,"M. Ridley","Explainable Artificial Intelligence (XAI)",2022,"","","","",151,"2022-07-13 09:31:23","","10.6017/ital.v41i2.14683","","",,,,,442,442.00,442,1,1,"The field of explainable artificial intelligence (XAI) advances techniques, processes, and strategies that provide explanations for the predictions, recommendations, and decisions of opaque and complex machine learning systems. Increasingly academic libraries are providing library users with systems, services, and collections created and delivered by machine learning. Academic libraries should adopt XAI as a tool set to verify and validate these resources, and advocate for public policy regarding XAI that serves libraries, the academy, and the public interest.","",""
29,"Konstantin D. Pandl, Scott Thiebes, Manuel Schmidt-Kraepelin, A. Sunyaev","On the Convergence of Artificial Intelligence and Distributed Ledger Technology: A Scoping Review and Future Research Agenda",2020,"","","","",152,"2022-07-13 09:31:23","","10.1109/ACCESS.2020.2981447","","",,,,,29,14.50,7,4,2,"Developments in artificial intelligence (AI) and distributed ledger technology (DLT) currently lead to lively debates in academia and practice. AI processes data to perform tasks that were previously thought possible only for humans. DLT has the potential to create consensus over data among a group of participants in untrustworthy environments. In recent research, both technologies are used in similar and even the same systems. This can lead to a convergence of AI and DLT, which in the past, has paved the way for major innovations of other information technologies. Previous work highlights several potential benefits of a convergence of AI and DLT but only provides a limited theoretical framework to describe upcoming real-world integration cases of both technologies. In this research, we review and synthesize extant research on integrating AI with DLT and vice versa to rigorously develop a future research agenda on the convergence of both technologies. In terms of integrating AI with DLT, we identified research opportunities in the areas of secure DLT, automated referee and governance, and privacy-preserving personalization. With regard to integrating DLT with AI, we identified future research opportunities in the areas of decentralized computing for AI, secure data sharing and marketplaces, explainable AI, and coordinating devices. In doing so, this research provides a four-fold contribution. First, it is not constrained to blockchain but instead investigates the broader phenomenon of DLT. Second, it considers the reciprocal nature of a convergence of AI and DLT. Third, it bridges the gap between theory and practice by helping researchers active in AI or DLT to overcome current limitations in their field, and practitioners to develop systems along with the convergence of both technologies. Fourth, it demonstrates the feasibility of applying the convergence concept to research on AI and DLT.","",""
0,"Jie Wang, Xiangyuan Zheng, Qingdong He","Artificial Intelligence Applied to Extreme Value Prediction of Non-Gaussian Processes with Bandwidth Effect and Non-monotonicity",2021,"","","","",153,"2022-07-13 09:31:23","","10.1109/ICAICA52286.2021.9498204","","",,,,,0,0.00,0,3,1,"Extreme value prediction of a short-term non-Gaussian random process like ocean waves has been a tough issue for decades. In the 1990’s Winterstein proposed a cubic Hermite transformation using skewness and kurtosis, which has been widely applied in many areas for its accuracy and robustness. However, this approach is valid for monotonic transformation and narrow-banded processes. When the bandwidth of a random process is wide, no reasonable methods are available for acquiring the extreme value. This paper therefore applies the artificial neural network and genetic algorithm to do the extreme value prediction, without seeking rigorous mathematical derivations. Not only skewness and kurtosis are used, the spectral moments up to 4th-order reflecting bandwidth effects are also adopted. The results of many random case studies show that the artificial intelligence method is more accurate than the Hermite method in most of situations, especially for non-monotonic transformations. Besides, the artificial intelligence method has a wider application range.","",""
0,"S. Stanišić, M. Perišić, G. Jovanović, D. Maletic, D. Vudragovic, Anja Vranic, A. Stojić","What Information on Volatile Organic Compounds Can Be Obtained from the Data of a Single Measurement Site Through the Use of Artificial Intelligence?",2021,"","","","",154,"2022-07-13 09:31:23","","10.1007/978-3-030-72711-6_12","","",,,,,0,0.00,0,7,1,"","",""
0,"Victoria Tucci, J. Saary, Thomas E. Doyle","Factors influencing trust in medical artificial intelligence for healthcare professionals: a narrative review",2021,"","","","",155,"2022-07-13 09:31:23","","10.21037/jmai-21-25","","",,,,,0,0.00,0,3,1,"Objective: We performed a comprehensive review of the literature to better understand the trust dynamics between medical artificial intelligence (AI) and healthcare expert end-users. We explored the factors that influence trust in these technologies and how they compare to established concepts of trust in the engineering discipline. By identifying the qualitatively and quantitatively assessed factors that influence trust in medical AI, we gain insight into understanding how autonomous systems can be optimized during the development phase to improve decision-making support and clinician-machine teaming. This facilitates an enhanced understanding of the qualities that healthcare professional users seek in AI to consider it trustworthy. We also highlight key considerations for promoting on-going improvement of trust in autonomous medical systems to support the adoption of medical technologies into practice. Background: decision support systems introduces challenges and barriers to adoption and implementation into clinical practice. Methods: We searched databases including, Ovid MEDLINE, Ovid EMBASE, Clarivate Web of Science, and Google Scholar, as well as gray literature, for publications from 2000 to July 15, 2021, that reported features of AI-based diagnostic and clinical decision support systems that contribute to enhanced end-user trust. Papers discussing implications and applications of medical AI in clinical practice were also recorded. Results were based on the quantity of papers that discussed each trust concept, either quantitatively or qualitatively, using frequency of concept commentary as a proxy for importance of a respective concept. Conclusions: Explainability, transparency, interpretability, usability, and education are among the key identified factors thought to influence a healthcare professionals’ trust in medical AI and enhance clinician-machine teaming in critical decision-making healthcare environments. We also identified the need to better evaluate and incorporate other critical factors to promote trust by consulting medical professionals when developing AI systems for clinical decision-making and diagnostic support.","",""
0,"Sai Prasanth Kadiyala, Wai Lok Woo","Flood Prediction and Analysis on the Relevance of Features using Explainable Artificial Intelligence",2021,"","","","",156,"2022-07-13 09:31:23","","10.1145/3516529.3516530","","",,,,,0,0.00,0,2,1,"This paper presents flood prediction models for the state of Kerala in India by analyzing the monthly rainfall data and applying machine learning algorithms including Logistic Regression, K-Nearest Neighbors, Decision Trees, Random Forests, and Support Vector Machine. Although these models have shown high accuracy prediction of the occurrence of flood in a particular year, they do not quantitatively and qualitatively explain the prediction decision. This paper shows how the background features are learned that contributed to the prediction decision and further extended to explain the models with the development of explainable artificial intelligence modules such as SHAP and LIME. The obtained results have confirmed the validity of the findings uncovered by the explainer modules basing on the historical flood monthly rainfall data in Kerala","",""
0,"R. Sheh","Explainable Artificial Intelligence Requirements for Safe, Intelligent Robots",2021,"","","","",157,"2022-07-13 09:31:23","","10.1109/ISR50024.2021.9419498","","",,,,,0,0.00,0,1,1,"While requirements for robot performance to perform a task are generally well understood, the requirements around the explanatory capabilities of these systems are often at best an afterthought. This results in a dangerous situation where neither users nor experts can predict situations where the robot will or will not work, nor understand what causes failures and unexpected behaviour. In this paper, we discuss and survey the field of Explainable Artificial Intelligence, as it relates to the generation of requirements for the development of safe, intelligent robots. We then present a categorisation of explanatory capabilities and requirements that aims to help users and developers alike to ensure an appropriate match between the types of explanations that a given application requires, and the capabilities of various underlying AI techniques.","",""
0,"I. Mohammed","ARTIFICIAL INTELLIGENCE: THE KEY TO SELF-DRIVING IDENTITY GOVERNANCE",2021,"","","","",158,"2022-07-13 09:31:23","","","","",,,,,0,0.00,0,1,1,"The main goal of this article is to examine how artificial intelligence is playing an increasingly important role in advancing identity governance. The speed of global digital change is accelerating, and businesses are under increasing pressure to guarantee that their technology can keep up with the times. Today's headlines often include stories about high-profile data leakage, which have a significant impact on a company's image and financial health [1]. The moment has come for businesses to invest in AIpowered identity security to remain abreast of security and compliance risks. Information technology, a shifting workforce, and an onslaught of compliance regulations have brought an unprecedented number of users, points of access, apps, and data, to the point that IT departments are struggling to stay up [1]. A human-centric approach to identity security can only scale so far, and with it comes inaccuracy in risk identification. Security requirements are becoming more complicated, decentralized, and integrated with business operations, owing to new methods of working enabled by cloud technology [2]. This implies that robust identity governance and access control are more critical than ever. In this paper, we examine the current status of the information technology environment and explain how artificial intelligence will help companies address their present issues relating to identity governance.","",""
0,"Abdulsadek Hassan","The Usage of Artificial Intelligence in Digital Marketing: A Review",2021,"","","","",159,"2022-07-13 09:31:23","","10.1007/978-3-030-72080-3_20","","",,,,,0,0.00,0,1,1,"","",""
0,"Yalin Dong","Application of artificial intelligence in clothing intelligence manufacturing",2021,"","","","",160,"2022-07-13 09:31:23","","10.1109/cisai54367.2021.00172","","",,,,,0,0.00,0,1,1,"At present, the rapid development of science and technology in China not only provides solid scientific and technological support for the development of various intelligent technology products, but also opens up a new road for intelligent clothing manufacturing. Many aspects of artificial intelligence, such as industrial chain and computer application, can make a contribution to clothing intelligent manufacturing. This paper first introduces the relevant information of artificial intelligence, then explains the specific application direction of artificial intelligence in clothing intelligence manufacturing, and finally looks forward to its application prospects.","",""
0,"Abdulraqeb Alhammadi, Ayman A. El-Saleh, Ibraheem Shayea","MOS Prediction for Mobile Broadband Networks Using Bayesian Artificial Intelligence",2021,"","","","",161,"2022-07-13 09:31:23","","10.1109/ICAICST53116.2021.9497834","","",,,,,0,0.00,0,3,1,"Mobile broadband (MBB) networks are growing fast with supporting high-speed internet access. Fifth-generation networks promise an enhanced MBB that offers a high-speed data rate and video streaming with ultra-low latency. Thus, monitoring the level quality of these services supported by network providers becomes essential. Mobile network operators continuously optimize their network performance to provide a better quality of service and quality of experience. Moreover, artificial intelligence has been used considerably in optimizations to efficiently meet the requirements of future mobile networks. In this paper, we propose a Bayesian network model to predict the minimum opinion score (MOS), which contributes to evaluating the network performance of video streaming services. The proposed model depends on several input data, namely, bite rate, stalling load, and round-trip time. The predicted MOS depends on prior probability distributions to generate posterior probabilities. The predicted MOS depends on these input data. Results demonstrate that the proposed model achieves a high prediction accuracy of 86%, with a mean square error of 0.34. The proposed model also has a robust performance design through various testing methods.","",""
24,"I. Cohen","Informed Consent and Medical Artificial Intelligence: What to Tell the Patient?",2020,"","","","",162,"2022-07-13 09:31:23","","10.2139/ssrn.3529576","","",,,,,24,12.00,24,1,2,"Imagine you are a patient who has been diagnosed with prostate cancer. The two main approaches to treating it in the United States are active surveillance versus the surgical option of radical prostatectomy. Your physician recommends the surgical option, and spends considerable time explaining the steps in the surgery, the benefits of (among other things) eliminating the tumor and the risks of (among other things) erectile dysfunction and urinary incontinence after the surgery. What your physician does not tell you is that she has arrived at her recommendation of prostatectomy over active surveillance based on the analysis of an Artificial Intelligence (AI)/Machine Learning (ML) system, which recommended this treatment plan based on analysis of your age, tumor size, and other personal characteristics found in your electronic health record. Has the doctor secured informed consent from a legal perspective? From an ethical perspective? If the doctor actually chose to “overrule” the AI system, and the doctor fails to tell you that, has she violated your legal or ethical right to informed consent? If you were to find out that the AI/ML system was used to make recommendations on your care and no one told you, how would you feel? Well, come to think of it, do you know whether an AI/ML system was used the last time you saw a physician?    This Article, part of a Symposium in the Georgetown Law Journal, is the first to examine in depth how medical AI/ML interfaces with our concept of informed consent. Part I provides a brief primer on medical Artificial Intelligence and Machine Learning. Part II sets out the core and penumbra of U.S. informed consent law and then seeks to determine to what extent AI/ML involvement in a patient’s health should be disclosed under the current doctrine. Part III examines whether the current doctrine “has it right,” examining more openly empirical and normative approaches to the question.    To forefront my conclusions: while there is some play in the joints, my best reading of the existing legal doctrine is that in general, liability will not lie for failing to inform patients about the use of medical AI/ML to help formulate treatment recommendations. There are a few situations where the doctrine may be more capacious, which I try to draw out (such as when patients inquire, when the medical AI/ML is more opaque, when it is given an outsized role in the final decision-making, or when the AI/ML is used to reduce costs rather than improve patient health), though extending it even here is not certain. I also offer some thoughts on the question: if there is room in the doctrine (either via common law or legislative action), what would it be desirable for the doctrine to look like when it comes to medical AI/ML? I also briefly touch on the question of how the doctrine of informed consent should interact with concerns about biased training data for AI/ML.","",""
21,"Chuan Zhang, Yeong-Luh Ueng, Christoph Studer, A. Burg","Artificial Intelligence for 5G and Beyond 5G: Implementations, Algorithms, and Optimizations",2020,"","","","",163,"2022-07-13 09:31:23","","10.1109/JETCAS.2020.3000103","","",,,,,21,10.50,5,4,2,"The communication industry is rapidly advancing towards 5G and beyond 5G (B5G) wireless technologies in order to fulfill the ever-growing needs for higher data rates and improved quality-of-service (QoS). Emerging applications require wireless connectivity with tremendously increased data rates, substantially reduced latency, and growing support for a large number of devices. These requirements pose new challenges that can no longer be efficiently addressed by conventional approaches. Artificial intelligence (AI) is considered as one of the most promising solutions to improve the performance and robustness of 5G and B5G systems, fueled by the massive amount of data generated in 5G and B5G networks and the availability of powerful data processing fabrics. As a consequence, a plethora of research on AI-based communication technologies has emerged recently, promising higher data rates and improved QoS with affordable implementation overhead. In this overview paper, we summarize the state-of-the-art of AI-based 5G and B5G techniques on the algorithm, implementation, and optimization levels. We shed light on the advantages and limitations of AI-based solutions, and we provide a summary of emerging techniques and open research problems.","",""
0,"","Neural Network Training Using Genetic Algorithms Series In Machine Perception And Artificial Intelligence",2021,"","","","",164,"2022-07-13 09:31:23","","","","",,,,,0,0.00,0,0,1,"Knowledge-Based Intelligent Information and Engineering Systems 2Nature-inspired Methods in Chemometrics: Genetic Algorithms and Artificial Neural NetworksParallel Implementations of Backpropagation Neural Networks on TransputersEvolutionary Algorithms and Neural NetworksTraining Neural Networks Using Hybrids with Genetic AlgorithmsNeural Network Training Using Genetic AlgorithmsGene Expression ProgrammingTraining a Neural Network with a Genetic AlgorithmMethods and Applications of Artificial IntelligenceClassification and Learning Using Genetic AlgorithmsPractical Computer Vision Applications Using Deep Learning with CNNsIntelligent Hybrid SystemsNeurogenetic LearningAdvances in Neural Networks ISNN 2007Hybrid Intelligent SystemsEncyclopedia of Computer Science and TechnologyMachine LearningUsing a Genetic Algorithm in Training an Artificial Neural Network to Implement the XOR FunctionGenetic and Evolutionary Computation — GECCO 2004Handbook of Fuzzy ComputationNeural Network Data Analysis Using SimulnetTMArtificial Neural Nets and Genetic AlgorithmsApplied Soft Computing Technologies: The Challenge of ComplexityGenetic Algorithm for Artificial Neural Network Training for the Purpose of Automated Part RecognitionNEURAL NETWORKS, FUZZY LOGIC AND GENETIC ALGORITHMAutomatic Generation of Neural Network Architecture Using Evolutionary ComputationPGANETThe Sixth International Symposium on Neural Networks (ISNN 2009)Evolutionary Machine Learning TechniquesModeling Decisions for Artificial IntelligenceEmpirical Studies on the Utility of Genetic Algorithms for Training and Designing of Neural NetworksTraining Neural Networks Using Genetic AlgorithmsTraining feedforward neural networks using genetic algorithmsMetaheuristic Procedures for Training Neural NetworksArtificial Neural Nets and Genetic AlgorithmsNature-Inspired Computing: Concepts, Methodologies, Tools, and ApplicationsApplications of Evolutionary ComputingArtificial Intelligence and CreativityArtificial Neural Nets and Genetic AlgorithmsDeep Learning Using Genetic Algorithms Creativity is one of the least understood aspects of intelligence and is often seen as `intuitive' and not susceptible to rational enquiry. Recently, however, there has been a resurgence of interest in the area, principally in artificial intelligence and cognitive science, but also in psychology, philosophy, computer science, logic, mathematics, sociology, and architecture and design. This volume brings this work together and provides an overview of this rapidly developing field. It addresses a range of issues. Can computers be creative? Can they help us to understand human creativity? How can artificial intelligence (AI) enhance human creativity? How, in particular, can it contribute to the `sciences of the artificial', such as design? Does the new wave of AI (connectionism, geneticism and artificial life) offer more promise in these areas than classical, symbol-handling AI? What would the implications be for AI and cognitive science if computers could not be creative? These issues are explored in five interrelated parts, each of which is introducted and explained by a leading figure in the field. Prologue (Margaret Boden) Part I: Foundational Issues (Terry Dartnall) Part II: Creativity and Cognition (Graeme S. Halford and Robert Levinson) Part III: Creativity and Connectionism (Chris Thornton) Part IV: Creativity and Design (John Gero) Part V: Human Creativity Enhancement (Ernest Edmonds) Epilogue (Douglas Hofstadter) For researchers in AI, cognitive science, computer science, philosophy, psychology, mathematics, logic, sociology, and architecture and design; and anyone interested in the rapidly growing field of artificial intelligence and creativity.From the contents: Neural networks – theory and applications: NNs (= neural networks) classifier on continuous data domains– quantum associative memory – a new class of neuron-like discrete filters to image processing – modular NNs for improving generalisation properties – presynaptic inhibition modelling for image processing application – NN recognition system for a curvature primal sketch – NN based nonlinear temporalspatial noise rejection system – relaxation rate for improving Hopfield network – Oja's NN and influence of the learning gain on its dynamics Genetic algorithms – theory and applications: transposition: a biological-inspired mechanism to use with GAs (= genetic algorithms) – GA for decision tree induction – optimising decision classifications using GAs – scheduling tasks with intertask communication onto multiprocessors by GAs – design of robust networks with GA – effect of degenerate coding on GAs – multiple traffic signal control using a GA – evolving musical harmonisation – niched-penalty approach for constraint handling in GAs – GA with dynamic population size – GA with dynamic niche clustering for multimodal function optimisation Soft computing and uncertainty: self-adaptation of evolutionary constructed decision trees by information spreading – evolutionary programming of near optimal NNsArtificial neural networks and genetic algorithms both are areas of research","",""
19,"Adarsh Ghosh, D. Kandasamy","Interpretable Artificial Intelligence: Why and When.",2020,"","","","",165,"2022-07-13 09:31:23","","10.2214/ajr.19.22145","","",,,,,19,9.50,10,2,2,"OBJECTIVE. The purpose of this article is to discuss the problem of interpretability of artificial intelligence (AI) and highlight the need for continuing scientific discovery using AI algorithms to deal with medical big data. CONCLUSION. A plethora of AI algorithms are currently being used in medical research, but the opacity of these algorithms makes their clinical implementation a dilemma. Clinical decision making cannot be assigned to something that we do not understand. Therefore, AI research should not be limited to reporting accuracy and sensitivity but, rather, should try to explain the underlying reasons for the predictions, in an attempt to enrich biologic understanding and knowledge.","",""
36,"William R. Frey, D. Patton, M. Gaskell, K. McGregor","Artificial Intelligence and Inclusion: Formerly Gang-Involved Youth as Domain Experts for Analyzing Unstructured Twitter Data",2018,"","","","",166,"2022-07-13 09:31:23","","10.1177/0894439318788314","","",,,,,36,9.00,9,4,4,"Mining social media data for studying the human condition has created new and unique challenges. When analyzing social media data from marginalized communities, algorithms lack the ability to accurately interpret off-line context, which may lead to dangerous assumptions about and implications for marginalized communities. To combat this challenge, we hired formerly gang-involved young people as domain experts for contextualizing social media data in order to create inclusive, community-informed algorithms. Utilizing data from the Gang Intervention and Computer Science Project—a comprehensive analysis of Twitter data from gang-involved youth in Chicago—we describe the process of involving formerly gang-involved young people in developing a new part-of-speech tagger and content classifier for a prototype natural language processing system that detects aggression and loss in Twitter data. We argue that involving young people as domain experts leads to more robust understandings of context, including localized language, culture, and events. These insights could change how data scientists approach the development of corpora and algorithms that affect people in marginalized communities and who to involve in that process. We offer a contextually driven interdisciplinary approach between social work and data science that integrates domain insights into the training of qualitative annotators and the production of algorithms for positive social impact.","",""
318,"David Gunning","DARPA's explainable artificial intelligence (XAI) program",2019,"","","","",167,"2022-07-13 09:31:23","","10.1145/3301275.3308446","","",,,,,318,106.00,318,1,3,"The DARPA's Explainable Artificial Intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. This talk will summarize the XAI program and present highlights from these Phase 1 evaluations.","",""
167,"Max Tegmark","Life 3.0: Being Human in the Age of Artificial Intelligence",2017,"","","","",168,"2022-07-13 09:31:23","","","","",,,,,167,33.40,167,1,5,"New York Times Best Seller How will Artificial Intelligence affect crime, war, justice, jobs, society and our very sense of being human? The rise of AI has the potential to transform our future more than any other technologyand theres nobody better qualified or situated to explore that future than Max Tegmark, an MIT professor whos helped mainstream research on how to keep AI beneficial. How can we grow our prosperity through automation without leaving people lacking income or purpose? What career advice should we give todays kids? How can we make future AI systems more robust, so that they do what we want without crashing, malfunctioning or getting hacked? Should we fear an arms race in lethal autonomous weapons? Will machines eventually outsmart us at all tasks, replacing humans on the job market and perhaps altogether? Will AI help life flourish like never before or give us more power than we can handle? What sort of future do you want? This book empowers you to join what may be the most important conversation of our time. It doesnt shy away from the full range of viewpoints or from the most controversial issuesfrom superintelligence to meaning, consciousness and the ultimate physical limits on life in the cosmos.","",""
1,"Ossama H. Embarak","Explainable Artificial Intelligence for Services Exchange in Smart Cities",2021,"","","","",169,"2022-07-13 09:31:23","","10.1201/9781003172772-2","","",,,,,1,1.00,1,1,1,"","",""
1,"Tomasz Rutkowski","Explainable Artificial Intelligence Based on Neuro-Fuzzy Modeling with Applications in Finance",2021,"","","","",170,"2022-07-13 09:31:23","","10.1007/978-3-030-75521-8","","",,,,,1,1.00,1,1,1,"","",""
2,"Yuanyuan Hu, Rafael Ferreira Mello, Dragan Gacseviac","Automatic analysis of cognitive presence in online discussions: An approach using deep learning and explainable artificial intelligence",2021,"","","","",171,"2022-07-13 09:31:23","","10.1016/j.caeai.2021.100037","","",,,,,2,2.00,1,3,1,"","",""
1,"Andrea Torcianti, S. Matzka","Explainable Artificial Intelligence for Predictive Maintenance Applications using a Local Surrogate Model",2021,"","","","",172,"2022-07-13 09:31:23","","10.1109/AI4I51902.2021.00029","","",,,,,1,1.00,1,2,1,"This paper provides an explanatory interface using Local Interpretable Model-agnostic Explanations (LIME) for a predictive maintenance dataset. The explanations are evaluated and the explanatory quality of the model is compared to two previous explainable models for the same dataset.","",""
1,"Anna Visvizi","Artificial Intelligence (AI): Explaining, Querying, Demystifying",2021,"","","","",173,"2022-07-13 09:31:23","","10.1007/978-3-030-88972-2_2","","",,,,,1,1.00,1,1,1,"","",""
30,"L. Longo, R. Goebel, F. Lécué, Peter Kieseberg, Andreas Holzinger","Explainable Artificial Intelligence: Concepts, Applications, Research Challenges and Visions",2020,"","","","",174,"2022-07-13 09:31:23","","10.1007/978-3-030-57321-8_1","","",,,,,30,15.00,6,5,2,"","",""
0,"E. Cambouropoulos, Maximos A. Kaliakatsos-Papakostas","Cognitive Musicology and Artificial Intelligence: Harmonic Analysis, Learning, and Generation",2021,"","","","",175,"2022-07-13 09:31:23","","10.1007/978-3-030-72116-9_10","","",,,,,0,0.00,0,2,1,"","",""
0,"Ana Carolina Borges Monteiro, R. França, Rangel Arthur, Y. Iano","An Overview of Explainable Artificial Intelligence (XAI) from a Modern Perspective",2021,"","","","",176,"2022-07-13 09:31:23","","10.1201/9781003172772-1","","",,,,,0,0.00,0,4,1,"","",""
0,"C. Yong","Classification of Kinematic Data Using Explainable Artificial Intelligence (XAI) for Smart Motion",2021,"","","","",177,"2022-07-13 09:31:23","","10.1201/9781003172772-10","","",,,,,0,0.00,0,1,1,"","",""
0,"V. Ahuja","Explainable Artificial Intelligence: Guardian for Cancer Care",2021,"","","","",178,"2022-07-13 09:31:23","","10.1201/9781003172772-5","","",,,,,0,0.00,0,1,1,"","",""
13,"Meicheng Yang, Chengyu Liu, Xingyao Wang, Yuwen Li, Hongxiang Gao, Xing Liu, Jianqing Li","An Explainable Artificial Intelligence Predictor for Early Detection of Sepsis",2020,"","","","",179,"2022-07-13 09:31:23","","10.1097/CCM.0000000000004550","","",,,,,13,6.50,2,7,2,"Supplemental Digital Content is available in the text. Objectives: Early detection of sepsis is critical in clinical practice since each hour of delayed treatment has been associated with an increase in mortality due to irreversible organ damage. This study aimed to develop an explainable artificial intelligence model for early predicting sepsis by analyzing the electronic health record data from ICU provided by the PhysioNet/Computing in Cardiology Challenge 2019. Design: Retrospective observational study. Setting: We developed our model on the shared ICUs publicly data and verified on the full hidden populations for challenge scoring. Patients: Public database included 40,336 patients’ electronic health records sourced from Beth Israel Deaconess Medical Center (hospital system A) and Emory University Hospital (hospital system B). A total of 24,819 patients from hospital systems A, B, and C (an unidentified hospital system) were sequestered as full hidden test sets. Interventions: None. Measurements and Main Results: A total of 168 features were extracted on hourly basis. Explainable artificial intelligence sepsis predictor model was trained to predict sepsis in real time. Impact of each feature on hourly sepsis prediction was explored in-depth to show the interpretability. The algorithm demonstrated the final clinical utility score of 0.364 in this challenge when tested on the full hidden test sets, and the scores on three separate test sets were 0.430, 0.422, and –0.048, respectively. Conclusions: Explainable artificial intelligence sepsis predictor model achieves superior performance for predicting sepsis risk in a real-time way and provides interpretable information for understanding sepsis risk in ICU.","",""
20,"H. Anysz, Ł. Brzozowski, Wojciech Kretowicz, P. Narloch","Feature Importance of Stabilised Rammed Earth Components Affecting the Compressive Strength Calculated with Explainable Artificial Intelligence Tools",2020,"","","","",180,"2022-07-13 09:31:23","","10.3390/ma13102317","","",,,,,20,10.00,5,4,2,"Cement-stabilized rammed earth (CSRE) is a sustainable construction material. The use of it allows for economizing on the cost of a structure. These two properties of CSRE are based on the fact that the soil used for the rammed mixture is usually dug close to the construction site, so it has random characteristics. That is the reason for the lack of widely accepted prescriptions for CSRE mixture, which could ascertain high enough compressive strength. Therefore, assessing which components of CSRE have the highest impact on its compressive strength becomes an important issue. There are three machine learning regression tools, i.e., artificial neural networks, decision tree, and random forest, used for predicting the compressive strength based on the relative content of CSRE composites (clay, silt, sand, gravel, cement, and water content). The database consisted of 434 samples of CSRE, which were prepared and crushed for testing purposes. Relatively low prediction errors of aforementioned models allowed for the use of explainable artificial intelligence tools (drop-out loss, mean squared error reduction, accumulated local effect) to rank the influence of the ingredients on the dependent variable—the compressive strength. Consistent results from all above-mentioned methods are discussed and compared to some statistical analysis of selected features. This innovative approach, helpful in designing the construction material is a solid base for reliable conclusions.","",""
1549,"Amina Adadi, M. Berrada","Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)",2018,"","","","",181,"2022-07-13 09:31:23","","10.1109/ACCESS.2018.2870052","","",,,,,1549,387.25,775,2,4,"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.","",""
11,"I. Linkov, S. Galaitsi, Benjamin D. Trump, J. Keisler, A. Kott","Cybertrust: From Explainable to Actionable and Interpretable Artificial Intelligence",2020,"","","","",182,"2022-07-13 09:31:23","","10.1109/MC.2020.2993623","","",,,,,11,5.50,2,5,2,"We argue that artificial intelligence (AI) systems should be designed with features that build trust by bringing decision-analytic perspectives into AI. Actionable and interpretable AI will incorporate explicit quantifications and visualizations of user confidence in AI recommendations.","",""
10,"Régis Pierrard, J. Poli, C. Hudelot","Learning Fuzzy Relations and Properties for Explainable Artificial Intelligence",2018,"","","","",183,"2022-07-13 09:31:23","","10.1109/FUZZ-IEEE.2018.8491538","","",,,,,10,2.50,3,3,4,"The goal of explainable artificial intelligence is to solve problems in a way that humans can understand how it does it. However, few approaches have been proposed so far and some of them lay more emphasis on interpretability than on explainability. In this paper, we propose an approach that is based on learning fuzzy relations and fuzzy properties. We extract frequent relations from a dataset to generate an explained decision. Our approach can deal with different problems, such as classification or annotation. A model was built to perform explained classification on a toy dataset that we generated. It managed to correctly classify examples while providing convincing explanations. A few areas for improvement have been spotted, such as the need to filter relations and properties before or while learning them in order to avoid useless computations.","",""
7,"R. Y. Goh, L. Lee, H. Seow, Kathiresan Gopal","Hybrid Harmony Search–Artificial Intelligence Models in Credit Scoring",2020,"","","","",184,"2022-07-13 09:31:23","","10.3390/e22090989","","",,,,,7,3.50,2,4,2,"Credit scoring is an important tool used by financial institutions to correctly identify defaulters and non-defaulters. Support Vector Machines (SVM) and Random Forest (RF) are the Artificial Intelligence techniques that have been attracting interest due to their flexibility to account for various data patterns. Both are black-box models which are sensitive to hyperparameter settings. Feature selection can be performed on SVM to enable explanation with the reduced features, whereas feature importance computed by RF can be used for model explanation. The benefits of accuracy and interpretation allow for significant improvement in the area of credit risk and credit scoring. This paper proposes the use of Harmony Search (HS), to form a hybrid HS-SVM to perform feature selection and hyperparameter tuning simultaneously, and a hybrid HS-RF to tune the hyperparameters. A Modified HS (MHS) is also proposed with the main objective to achieve comparable results as the standard HS with a shorter computational time. MHS consists of four main modifications in the standard HS: (i) Elitism selection during memory consideration instead of random selection, (ii) dynamic exploration and exploitation operators in place of the original static operators, (iii) a self-adjusted bandwidth operator, and (iv) inclusion of additional termination criteria to reach faster convergence. Along with parallel computing, MHS effectively reduces the computational time of the proposed hybrid models. The proposed hybrid models are compared with standard statistical models across three different datasets commonly used in credit scoring studies. The computational results show that MHS-RF is most robust in terms of model performance, model explainability and computational time.","",""
4,"S. Abe, I. Oda","Real‐time pharyngeal cancer detection utilizing artificial intelligence: Journey from the proof of concept to the clinical use",2020,"","","","",185,"2022-07-13 09:31:23","","10.1111/den.13833","","",,,,,4,2.00,2,2,2,"Early detection of pharyngeal cancer contributes to providing excellent long-term outcomes and preserving pharyngeal function. Thus, an efficient detection system for superficial pharyngeal cancer (SPC) is necessary worldwide, especially in Asia. However, early detection proves challenging because SPC is typically flat with subtle color changes. Although image enhanced endoscopy, such as narrow band imaging (NBI), substantially helps us detect SPC, it is still difficult even for experienced endoscopists. Based on this background, artificial intelligence (AI) technologyhasbeen recently utilized for the earlydetection ofpharyngeal cancer.Mascharak et al.first reported imageprocessing andbasic machine learning techniques to automate the assessment of oropharyngeal cancer using white light endoscopy (WLE) and NBI. However, the sensitivity and specificitywere unsatisfactory, even using NBI, perhaps owing to a sample size of only 30 patients. Later, Tamashiro et al. developed an AI system using 5403 still images of pharyngeal cancer. In this study, as expected, the AI system took only 28 seconds to analyze 1912 validation still images consisting of 928 and 984 with or without pharyngeal cancers, respectively. The sensitivity and the specificity per image analysis were 79.7% and were 57.1%, respectively. This AI system demonstrated sensitive pharyngeal cancer detection. However, validation using still images would be subject to bias because still images are usually taken under good conditions (e.g., adequate angle, adequate distance, and in focus). Although the AI system for still image diagnosis can be utilized for doublechecking after screening endoscopy, it is unsuitable for real-world endoscopic diagnosis. In this issue of Digestive Endoscopy, Kono et al. present the diagnostic performance of their AI system for pharyngeal cancer detection using an independent validation dataset of 25 videos of pharyngeal cancer and 36 videos of nonpharyngeal cancer. The sensitivity, specificity, and accuracy for detecting cancer were 92%, 47%, and 66%, respectively. This study highlighted that the AI system allowed for sensitive and real-time endoscopic pharyngeal cancer detection. The validation with video images is more challenging than that with still images because the images inevitably include poor-quality features (e.g., defocus, light reflection, mucus, or saliva), which were excluded in Tamashiro et al. Nevertheless, the video-based validation provides a more realistic and practical assessment of the diagnostic performance of the AI system. The concept of this study is highly valuable because real-time pharyngeal cancer detection is more supportive and educational than still image assessment, particularly for inexperienced endoscopists. Moreover, the concept could play a role in quality assurance. As shown above, the AI system was a promising tool for pharyngeal cancer detection. However, we want to discuss areas of further improvement of the AI system because it appears to be at an investigational stage that requires adaptation for the real-world endoscopic detection. First, further robust data of its sensitivity should be accumulated. In terms of imaging modality, the sensitivity of the AI system appears to be higher for NBI thanWLE, as it is in human diagnosis. Although the sensitivity was reported to be relatively favorable, the sample size was small for its use in real clinical practice. It is desirable to further train the AI system with more pharyngeal cancer images, primarily SPC. Close collaboration with otolaryngologists is considered necessary to increase the sample size as they still manage most SPC. Also, given the low prevalence of pharyngeal cancer, it is not realistic to establish the training in a single center, even if endoscopists focus on high-risk populations. A specific multicenter database such as the Japan Endoscopy Database (JED) will help collect enough samples of rare diseases and establish a better trained AI system. Secondly, more importantly, specificity should also be improved. In both studies, the specificity was unsatisfactory, as the authors pointed out. Consequently, the positive predictive value was low. The positive predictive value of the AI system will become further lower in real clinical practice given the low prevalence of pharyngeal cancer and the proportion of pharyngeal cancer and non-neoplastic lesions. It means that endoscopists will face excessive caution introduced by the AI system during an endoscopy examination. The main reason to explain the low specificity was insufficient training of noncancerous lesions, due to having included only images of pharyngeal cancer in the training set. However, a higher specificity will not be achieved by simply educating with non-","",""
4,"Tingting Wu, Yunwei Dong, Zhiwei Dong, Aziz Singa, Xiong Chen, Yu Zhang","Testing Artificial Intelligence System Towards Safety and Robustness: State of the Art",2020,"","","","",186,"2022-07-13 09:31:23","","","","",,,,,4,2.00,1,6,2,"With the increasing development of machine learning, conventional embedded systems cannot meet the requirement of current academic researches and industrial applications. Artificial Intelligence System (AIS) based on machine learning has been widely used in various safety-critical systems, such as machine vision, autonomous vehicles, collision avoidance system. Different from conventional embedded systems, AIS generates and updates control strategies through learning algorithms which make the control behaviors nondeterministic and bring about the test oracle problem in AIS testing procedure. There have been various testing approaches for AIS to guarantee the safety and robustness. However, few researches explain how to conduct AIS testing with a complete workflow systematically. This paper provides a comprehensive survey of existing testing techniques to detect the erroneous behaviors of AIS, and sums up the involved key steps and testing components in terms of test coverage criterion, test data generation, testing approach and common dataset. This literature review aims at organizing a standardized workflow and leading to a practicable insight and research trend towards AIS testing.","",""
3,"M. Nouri, Parveen Sihag, F. Salmasi, O. Kisi","Energy Loss in Skimming Flow over Cascade Spillways: Comparison of Artificial Intelligence-Based and Regression Methods",2020,"","","","",187,"2022-07-13 09:31:23","","10.3390/APP10196903","","",,,,,3,1.50,1,4,2,"In this study, the energy dissipation of cascade spillways was studied by conducting a series of laboratory experiments. Five spillways slope angles (α) (10°, 20°, 30°, 40°, and 50°), various step numbers (N) ranging from 4 to 75, and a wide range of discharges (Q), were considered. Some data-based models were developed to explain the relationships between hydraulic parameters. Multiple linear and nonlinear regression-based equations were developed based on dimensional analysis theory to compute energy dissipation over cascade spillways. For testing the robustness of developed data-based models, M5P, stochastic M5P, and random forest (RF) were used as new artificial intelligence (AI)-based techniques. To relate the input and output variables of energy dissipation, AI-based and regression approaches were developed. It was found that the formulation based on the stochastic M5P approach in solving energy dissipation problems over cascade spillways is more successful than the other regression and AI-based methods. Sensitivity analysis suggests that spillway slope in degrees (α) is the most influential input variable in predicting the relative energy dissipation (%) of the spillway in comparison to other input variables.","",""
16,"","Knowledge Graphs for eXplainable Artificial Intelligence: Foundations, Applications and Challenges",2020,"","","","",188,"2022-07-13 09:31:23","","","","",,,,,16,8.00,0,0,2,"","",""
1,"T. Sethi, R. Awasthi","Use of artificial intelligence based models for learning better policy for maternal and child health",2020,"","","","",189,"2022-07-13 09:31:23","","10.1093/eurpub/ckaa165.291","","",,,,,1,0.50,1,2,2,"  More than 640,000 babies died of sepsis before they reach the age of one month in India in 2016. Despite a large number of government schemes aimed at reducing this rate, this number still remains high because of the complexity and interplay of factors involved. Finding an optimum policy and solutions to this problem needs learning from data. We integrated diverse sources of data and applied Bayesian Artificial Intelligence methods for learning to mitigate sepsis and adverse pregnancy outcomes in India. In this project, we created models that combine the robustness of ensemble averaged Baeysian Networks with decision learning and impact evaluation by using simulations and counterfactual reasoning respectively. We will demonstrate the process of learning these models and how these led us to infer the pivotal role of Water, Sanitation and Hygiene for reducing Adverse Pregnancy Outcome and neonatal sepsis in the population studied. We will also demonstrate the creation of explainable AI models for complex public health challenges and their deployment with wiseR, our in-house, open source platform for doing end-to-end Bayesian Decision Network learning.","",""
1,"Adam Norton, Amy Saretsky, H. Yanco","Developing Metrics and Evaluation Methods for Assessing Artificial Intelligence Enabled Robots in Manufacturing",2020,"","","","",190,"2022-07-13 09:31:23","","","","",,,,,1,0.50,0,3,2,"Evaluating the capabilities of a robotic system for manufacturing can include metrics related to performance, efficiency, and productivity. Measures for traditional industrial automation typically address operations that rely on strict repetition that does not allow for much variation. The inclusion of artificial intelligence (AI) in robotic systems can allow for greater aptitude in maintaining capability in the presence of variation, such as local changes in environmental characteristics or global changes in task execution parameters. New evaluation methods and metrics are needed to allow these advanced capabilities to be appropriately measured. This paper discusses evaluating the robustness, adaptability, generalizability, and versatility of AI-enabled robotic manufacturing systems. The considerations for conducting evaluations of these capabilities are reviewed, including implications for robots that learn and those that are designed to be explainable. Recommendations are made for advancing the development of metrics and evaluation methods that highlight the capabilities afforded by AI. A prototype framework is presented to guide the design of evaluations and classification of metrics.","",""
1,"D. Dubois, H. Prade","A Glance at Causality Theories for Artificial Intelligence",2020,"","","","",191,"2022-07-13 09:31:23","","10.1007/978-3-030-06164-7_9","","",,,,,1,0.50,1,2,2,"","",""
0,"F. Schwendicke, W. Samek, J. Krois","Artificial Intelligence in Dental Diagnostics: Chances and challenges",2020,"","","","",192,"2022-07-13 09:31:23","","","","",,,,,0,0.00,0,3,2,"The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. is a using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning and conduct, e.g. image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, a, predictive, preventive and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to (1) limited data availability, accessibility, structure and comprehensiveness, (2) lacking methodological rigor and standards in their development, (3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, e.g. by improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Last, trustworthiness into and generalizability of dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.","",""
0,"T. D. Raheni, P. Thirumoorthi","Stochastic Artificial Intelligence: Review Article",2020,"","","","",193,"2022-07-13 09:31:23","","10.5772/INTECHOPEN.90003","","",,,,,0,0.00,0,2,2,"Artificial intelligence (AI) is a region of computer techniques that deals with the design of intelligent machines that respond like humans. It has the skill to operate as a machine and simulate various human intelligent algorithms according to the user’s choice. It has the ability to solve problems, act like humans, and perceive information. In the current scenario, intelligent techniques minimize human effort especially in industrial fields. Human beings create machines through these intelligent techniques and perform various processes in different fields. Artificial intelligence deals with real-time insights where decisions are made by connecting the data to various resources. To solve real-time problems, powerful machine learning-based techniques such as artificial intelligence, neural networks, fuzzy logic, genetic algorithms, and particle swarm optimization have been used in recent years. This chapter explains artificial neural network-based adaptive linear neuron networks, back-propagation networks, and radial basis networks.","",""
0,"Wei Yan","IEEE Transactions on Artificial Intelligence",2020,"","","","",194,"2022-07-13 09:31:23","","10.1109/tfuzz.2020.2987029","","",,,,,0,0.00,0,1,2,"The IEEE Transactions on Artificial Intelligence (TAI) is a multidisciplinary journal publishing papers on theories and methodologies of Artificial Intelligence. Applications of Artificial Intelligence are also considered. Topics covered by IEEE TAI include, but not limited to, Agent-based Systems, Augmented Intelligence, Autonomic Computing, Constraint Systems, Explainable AI, Knowledge-Based Systems, Learning Theories, Planning, Reasoning, Search, Natural Language Processing, and Applications. Technical papers addressing contemporary topics in AI such as Ethics and Social Implications are welcomed.","",""
0,"Charlie T. Veal, Marshall Lindsay, S. Kovaleski, Derek T. Anderson, Stanton R. Price","Evolutionary Algorithm Driven Explainable Adversarial Artificial Intelligence",2020,"","","","",195,"2022-07-13 09:31:23","","10.1109/SSCI47803.2020.9308361","","",,,,,0,0.00,0,5,2,"It is well-known that machine learning algorithms can be susceptible to undesirable effects when exposed to conditions that are not expressed adequately in the training dataset. This leads to a growing interest throughout many communities; where do algorithms and trained models break? Recently, methods such as generative adversarial neural networks and variational autoencoders were proposed to create adversarial examples that challenge algorithms. This results in artificial intelligence having higher false detections or completely losing recognition. The problem is that existing solutions, are for the most part, black boxes. Current gaps include how do we better control and understand adversarial algorithms. Herein, we propose the concept of an adversarial modifier set as an understandable and controlled way to generate adversarial examples. This is achieved by exploiting the improved evolution-constructed algorithm to identify ideal features that a victim algorithm values in imagery. These features are combined to realize a tuple library that preserves spatial relations. Last, a set of algorithmically controlled modifiers that generate the imagery are found by examining the content of the false imagery. Preliminary results are encouraging and demonstrate that this approach has benefits in both generating explainable adversarial examples, as well as shedding some insight into victim algorithm decision making.","",""
19,"B. Verheij","Artificial intelligence as law",2020,"","","","",196,"2022-07-13 09:31:23","","10.1007/s10506-020-09266-0","","",,,,,19,9.50,19,1,2,"","",""
6,"J. New","Why the United States Needs a National Artificial Intelligence Strategy and What It Should Look Like",2018,"","","","",197,"2022-07-13 09:31:23","","","","",,,,,6,1.50,6,1,4,"The United States is the global leader in developing and using artificial intelligence (AI), but it may not be for long. Succeeding in AI requires more than just having leading companies make investments. It requires a healthy ecoystem of AI companies, robust AI inputs—including skills, research, and data—and organizations that are motivated and free to use AI. And that requires the federal government to support the development and adoption of AI. Many other countries, including China, France, and the United Kingdom, are developing significant initiatives to gain global market share in AI. While the U.S. government has taken some steps, it lacks a comprehensive strategy to proactively spur the development and adoption of AI. This report explains why a national AI strategy is necessary to bolster U.S. competitiveness, strengthen national security, and maximize the societal benefits that the country could derive from AI. It then lays out six overarching goals and 40 specific recommendations for Congress and the administration to support AI development and adoption.","",""
128,"Kacper Sokol, Peter A. Flach","Explainability fact sheets: a framework for systematic assessment of explainable approaches",2019,"","","","",198,"2022-07-13 09:31:23","","10.1145/3351095.3372870","","",,,,,128,42.67,64,2,3,"Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.","",""
66,"Keping Yu, Zhiwei Guo, Yulian Shen, Wei Wang, Jerry Chun‐wei Lin, Takuro Sato","Secure Artificial Intelligence of Things for Implicit Group Recommendations",2021,"","","","",199,"2022-07-13 09:31:23","","10.1109/JIOT.2021.3079574","","",,,,,66,66.00,11,6,1,"The emergence of Artificial Intelligence of Things (AIoT) has provided novel insights for many social computing applications, such as group recommender systems. As the distances between people have been greatly shortened, there has been more general demand for the provision of personalized services aimed at groups instead of individuals. The existing methods for capturing group-level preference features from individuals have mostly been established via aggregation and face two challenges: 1) secure data management workflows are absent and 2) implicit preference feedback is ignored. To tackle these current difficulties, this article proposes secure AIoT for implicit group recommendations (SAIoT-GRs). For the hardware module, a secure Internet of Things structure is developed as the bottom support platform. For the software module, a collaborative Bayesian network model and noncooperative game are introduced as algorithms. This secure AIoT architecture is able to maximize the advantages of the two modules. In addition, a large number of experiments are carried out to evaluate the performance of SAIoT-GR in terms of efficiency and robustness.","",""
199,"Dong Wook Kim, H. Jang, K. Kim, Youngbin Shin, S. Park","Design Characteristics of Studies Reporting the Performance of Artificial Intelligence Algorithms for Diagnostic Analysis of Medical Images: Results from Recently Published Papers",2019,"","","","",200,"2022-07-13 09:31:23","","10.3348/kjr.2019.0025","","",,,,,199,66.33,40,5,3,"Objective To evaluate the design characteristics of studies that evaluated the performance of artificial intelligence (AI) algorithms for the diagnostic analysis of medical images. Materials and Methods PubMed MEDLINE and Embase databases were searched to identify original research articles published between January 1, 2018 and August 17, 2018 that investigated the performance of AI algorithms that analyze medical images to provide diagnostic decisions. Eligible articles were evaluated to determine 1) whether the study used external validation rather than internal validation, and in case of external validation, whether the data for validation were collected, 2) with diagnostic cohort design instead of diagnostic case-control design, 3) from multiple institutions, and 4) in a prospective manner. These are fundamental methodologic features recommended for clinical validation of AI performance in real-world practice. The studies that fulfilled the above criteria were identified. We classified the publishing journals into medical vs. non-medical journal groups. Then, the results were compared between medical and non-medical journals. Results Of 516 eligible published studies, only 6% (31 studies) performed external validation. None of the 31 studies adopted all three design features: diagnostic cohort design, the inclusion of multiple institutions, and prospective data collection for external validation. No significant difference was found between medical and non-medical journals. Conclusion Nearly all of the studies published in the study period that evaluated the performance of AI algorithms for diagnostic analysis of medical images were designed as proof-of-concept technical feasibility studies and did not have the design features that are recommended for robust validation of the real-world clinical performance of AI algorithms.","",""
