Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
1,"T. Welchowski, K. Maloney, R. Mitchell, M. Schmid","Techniques to Improve Ecological Interpretability of Black-Box Machine Learning Models",2021,"","","","",1,"2022-07-13 09:40:40","","10.1007/s13253-021-00479-7","","",,,,,1,1.00,0,4,1,"","",""
4,"Hala Abdelkader","Towards Robust Production Machine Learning Systems: Managing Dataset Shift",2020,"","","","",2,"2022-07-13 09:40:40","","10.1145/3324884.3415281","","",,,,,4,2.00,4,1,2,"The advances in machine learning (ML) have stimulated the integration of their capabilities into software systems. However, there is a tangible gap between software engineering and machine learning practices, that is delaying the progress of intelligent services development. Software organisations are devoting effort to adjust the software engineering processes and practices to facilitate the integration of machine learning models. Machine learning researchers as well are focusing on improving the interpretability of machine learning models to support overall system robustness. Our research focuses on bridging this gap through a methodology that evaluates the robustness of machine learning-enabled software engineering systems. In particular, this methodology will automate the evaluation of the robustness properties of software systems against dataset shift problems in ML. It will also feature a notification mechanism that facilitates the debugging of ML components.","",""
879,"Tyler Martin","Interpretable Machine Learning",2019,"","","","",3,"2022-07-13 09:40:40","","","","",,,,,879,293.00,879,1,3,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.","",""
3,"F. Biessmann, D. Refiano","Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated",2021,"","","","",4,"2022-07-13 09:40:40","","","","",,,,,3,3.00,2,2,1,"The field explainable artificial intelligence (XAI) has brought about an arsenal of methods to render Machine Learning (ML) predictions more interpretable. But how useful explanations provided by transparent ML methods are for humans remains difficult to assess. Here we investigate the quality of interpretable computer vision algorithms using techniques from psychophysics. In crowdsourced annotation tasks we study the impact of different interpretability approaches on annotation accuracy and task time. We compare these quality metrics with classical XAI, automated quality metrics. Our results demonstrate that psychophysical experiments allow for robust quality assessment of transparency in machine learning. Interestingly the quality metrics computed without humans in the loop did not provide a consistent ranking of interpretability methods nor were they representative for how useful an explanation was for humans. These findings highlight the potential of methods from classical psychophysics for modern machine learning applications. We hope that our results provide convincing arguments for evaluating interpretability in its natural habitat, human-ML interaction, if the goal is to obtain an authentic assessment of interpretability.","",""
2,"Artur Movsessian, D. Cava, D. Tcherniak","Interpretable machine learning in damage detection using Shapley Additive Explanations",2021,"","","","",5,"2022-07-13 09:40:40","","10.31224/osf.io/96yf5","","",,,,,2,2.00,1,3,1,"In recent years, Machine Learning (ML) techniques have gained popularity in Structural Health Monitoring (SHM). These have been particularly used for damage detection in a wide range of engineering applications such as wind turbine blades. The outcomes of previous research studies in this area have demonstrated the capabilities of ML for robust damage detection. However, the primary challenge facing ML in SHM is the lack of interpretability of the prediction models hindering the broader implementation of these techniques. For this purpose, this study integrates the novel Shapley Additive exPlanations (SHAP) method into a ML-based damage detection process as a tool for introducing interpretability and, thus, build evidence for reliable decision-making in SHM applications. The SHAP method is based on coalitional game theory and adds global and local interpretability to ML-based models by computing the marginal contribution of each feature. The contribution is used to understand the nature of damage indices (DIs). The applicability of the SHAP method is first demonstrated on a simple lumped mass-spring-damper system with simulated temperature variabilities. Later, the SHAP method has been evaluated on data from an in-operation V27 wind turbine with artificially introduced damage in one of its blades. The results show the relationship between the environmental and operational variabilities (EOVs) and their direct influence on the damage indices. This ultimately helps to understand the difference between false positives caused by EOVs and true positives resulting from damage in the structure.","",""
1,"P. Benner, A. Klawonn, M. Stoll","Topical Issue Scientific Machine Learning (1/2)",2021,"","","","",6,"2022-07-13 09:40:40","","10.1002/gamm.202100005","","",,,,,1,1.00,0,3,1,"Scientific Machine Learning is a rapidly evolving field of research that combines and further develops techniques of scientific computing and machine learning. Special emphasis is given to the scientific (physical, chemical, biological, etc.) interpretability of models learned from data and their usefulness for robust predictions. On the other hand, this young field also investigates the utilization of Machine Learning methods for improving numerical algorithms in Scientific Computing. The name Scientific Machine Learning has been coined at a Basic Research Needs Workshop of the US Department of Energy (DOE) in January, 2018. It resulted in a report [2] published in February, 2019; see also [1] for a short brochure on this topic. The present special issue of the GAMM Mitteilungen, which is the first of a two-part series, contains contributions on the topic of Scientific Machine Learning in the context of complex applications across the sciences and engineering. Research in this new exciting field needs to address challenges such as complex physics, uncertain parameters, and possibly limited data through the development of new methods that combine algorithms from computational science and engineering and from numerical analysis with state of the art techniques from machine learning. At the GAMM Annual Meeting 2019, the activity group Computational and Mathematical Methods in Data Science (CoMinDS) has been established. Meanwhile, it has become a meeting place for researchers interested in all aspects of data science. All three editors of this special issue are founding members of this activity group. Because of the rapid development both in the theoretical foundations and the applicability of Scientific Machine Learning techniques, it is time to highlight developments within the field in the hope that it will become an essential domain within the GAMM and topical issues like this will have a frequent occurrence within this journal. We are happy that eight teams of authors have accepted our invitation to report on recent research highlights in Scientific Machine Learning, and to point out the relevant literature as well as software. The four papers in this first part of the special issue are: • Stoll, Benner: Machine Learning for Material Characterization with an Application for Predicting Mechanical Properties. This work explores the use of machine learning techniques for material property prediction. Given the abundance of data available in industrial applications, machine learning methods can help finding patterns in the data and the authors focus on the case of the small punch test and tensile data for illustration purposes. • Beck, Kurz: A Perspective on Machine Modelling Learning Methods in Turbulence. Turbulence modelling remains a humongous challenge in the simulation and analysis of complex flows. The authors review the use of data-driven techniques to open up new ways for studying turbulence and focus on the challenges and opportunities that machine learning brings to this field. • Heinlein, Klawonn, Lanser, Weber: Combining Machine Learning and Domain Decomposition Methods for the Solution of Partial Differential Equations – A Review. Domain decomposition (DD) has been a workhorse of solving complex simulation tasks. The authors review the combination of machine learning approaches with state-of-the-art DD-schemes. Their focus is on the use of ML techniques to improve the computational effort of adaptive domain decomposition schemes and the use of novel ML methods for the discretization and solution of subdomain problems. • Budd, van Gennip, Latz: Classification and image processing with a semi-discrete scheme for fidelity forced Allen–Cahn on graphs. Learning based on graphs provides exciting possibilities for discovering and using additional structure in data. In this work, the authors illustrate the use of a PDE-based learning technique relying on the graph Allen-Cahn equation for the segmentation of images. The authors illustrate that computational and mathematical advances can lead to efficiency and accuracy gains. Peter Benner1,2 Axel Klawonn3,4 Martin Stoll5","",""
0,"James Dean, M. Scheffler, Thomas A. R. Purcell, S. Barabash, Rahul Bhowmik, T. Bazhirov","Interpretable Machine Learning for Materials Design",2021,"","","","",7,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,6,1,"Fueled by the widespread adoption of Machine Learning and the high-throughput screening of materials, the data-centric approach to materials design has asserted itself as a robust and powerful tool for the in-silico prediction of materials properties. When training models to predict material properties, researchers often face a difficult choice between a model’s interpretability or its performance. We study this trade-off by leveraging four different state-of-the-art Machine Learning techniques: XGBoost, SISSO, Roost, and TPOT for the prediction of structural and electronic properties of perovskites and 2D materials. We then assess the future outlook of the continued integration of Machine Learning into materials discovery, and identify key problems that will continue to challenge researchers as the size of the literature’s datasets and complexity of models increases. Finally, we offer several possible solutions to these challenges with a focus on retaining interpretability, and share our thoughts on magnifying the impact of Machine Learning on materials design.","",""
1,"Keyang Cheng, Ning Wang, Maozhen Li","Interpretability of Deep Learning: A Survey",2020,"","","","",8,"2022-07-13 09:40:40","","10.1007/978-3-030-70665-4_54","","",,,,,1,0.50,0,3,2,"","",""
2,"Weishen Pan, Changshui Zhang","The Definitions of Interpretability and Learning of Interpretable Models",2021,"","","","",9,"2022-07-13 09:40:40","","","","",,,,,2,2.00,1,2,1,"As machine learning algorithms getting adopted in an ever-increasing number of applications, interpretation has emerged as a crucial desideratum. In this paper, we propose a mathematical definition for the humaninterpretable model. In particular, we define interpretability between two information process systems. If a prediction model is interpretable by a human recognition system based on the above interpretability definition, the prediction model is defined as a completely human-interpretable model. We further design a practical framework to train a completely human-interpretable model by user interactions. Experiments on image datasets show the advantages of our proposed model in two aspects: 1) The completely human-interpretable model can provide an entire decisionmaking process that is human-understandable; 2) The completely humaninterpretable model is more robust against adversarial attacks.","",""
405,"David Alvarez-Melis, T. Jaakkola","Towards Robust Interpretability with Self-Explaining Neural Networks",2018,"","","","",10,"2022-07-13 09:40:40","","","","",,,,,405,101.25,203,2,4,"Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general – explicitness, faithfulness, and stability – and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.","",""
3,"Numair Sani, Jaron J. R. Lee, Razieh Nabi, I. Shpitser","A Semiparametric Approach to Interpretable Machine Learning",2020,"","","","",11,"2022-07-13 09:40:40","","","","",,,,,3,1.50,1,4,2,"Black box models in machine learning have demonstrated excellent predictive performance in complex problems and high-dimensional settings. However, their lack of transparency and interpretability restrict the applicability of such models in critical decision-making processes. In order to combat this shortcoming, we propose a novel approach to trading off interpretability and performance in prediction models using ideas from semiparametric statistics, allowing us to combine the interpretability of parametric regression models with performance of nonparametric methods. We achieve this by utilizing a two-piece model: the first piece is interpretable and parametric, to which a second, uninterpretable residual piece is added. The performance of the overall model is optimized using methods from the sufficient dimension reduction literature. Influence function based estimators are derived and shown to be doubly robust. This allows for use of approaches such as double Machine Learning in estimating our model parameters. We illustrate the utility of our approach via simulation studies and a data application based on predicting the length of stay in the intensive care unit among surgery patients.","",""
2,"S. Siltanen, Takanori Ide","Electrical Impedance Tomography, Enclosure Method and Machine Learning",2020,"","","","",12,"2022-07-13 09:40:40","","10.1109/MLSP49062.2020.9231717","","",,,,,2,1.00,1,2,2,"Electrical impedance tomography (EIT) is a non-destructive imaging method, where a physical body is probed with electric measurements at the boundary, and information about the internal conductivity is extracted from the data. The enclosure method of Ikehata [J. Inv. III-Posed Prob. 8(2000)] recovers the convex hull of an inclusion of unknown conductivity embedded in known background conductivity. Practical implementations of the enclosure method are based on least-squares (LS) fitting of lines to noise-robust values of the so-called indicator function. It is shown how a convolutional neural network instead of LS fitting improves the accuracy of the enclosure method significantly while retaining interpretability.","",""
138,"A. Garcez, M. Gori, L. Lamb, L. Serafini, Michael Spranger, S. Tran","Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",2019,"","","","",13,"2022-07-13 09:40:40","","","","",,,,,138,46.00,23,6,3,"Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.","",""
0,"Gabriel D. Patrón, D. León, Edwin Lopez, G. Hernández","An Interpretable Automated Machine Learning Credit Risk Model",2020,"","","","",14,"2022-07-13 09:40:40","","10.1007/978-3-030-61834-6_2","","",,,,,0,0.00,0,4,2,"","",""
0,"S. Shah","Addressing the interpretability problem for deep learning using many valued quantum logic",2020,"","","","",15,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,1,2,"Deep learning models are widely used for various industrial and scientific applications. Even though these models have achieved considerable success in recent years, there exists a lack of understanding of the rationale behind decisions made by such systems in the machine learning community. This problem of interpretability is further aggravated by the increasing complexity of such models. This paper utilizes concepts from machine learning, quantum computation and quantum field theory to demonstrate how a many valued quantum logic system naturally arises in a specific class of generative deep learning models called Convolutional Deep Belief Networks. It provides a robust theoretical framework for constructing deep learning models equipped with the interpretability of many valued quantum logic systems without compromising their computing efficiency.","",""
10,"Alan Rozet, I. Kronish, J. Schwartz, K. Davidson","Using Machine Learning to Derive Just-In-Time and Personalized Predictors of Stress: Observational Study Bridging the Gap Between Nomothetic and Ideographic Approaches",2019,"","","","",16,"2022-07-13 09:40:40","","10.2196/12910","","",,,,,10,3.33,3,4,3,"Background Investigations into person-specific predictors of stress have typically taken either a population-level nomothetic approach or an individualized ideographic approach. Nomothetic approaches can quickly identify predictors but can be hindered by the heterogeneity of these predictors across individuals and time. Ideographic approaches may result in more predictive models at the individual level but require a longer period of data collection to identify robust predictors. Objective Our objectives were to compare predictors of stress identified through nomothetic and ideographic models and to assess whether sequentially combining nomothetic and ideographic models could yield more accurate and actionable predictions of stress than relying on either model. At the same time, we sought to maintain the interpretability necessary to retrieve individual predictors of stress despite using nomothetic models. Methods Data collected in a 1-year observational study of 79 participants performing low levels of exercise were used. Physical activity was continuously and objectively monitored by actigraphy. Perceived stress was recorded by participants via daily ecological momentary assessments on a mobile app. Environmental variables including daylight time, temperature, and precipitation were retrieved from the public archives. Using these environmental, actigraphy, and mobile assessment data, we built machine learning models to predict individual stress ratings using linear, decision tree, and neural network techniques employing nomothetic and ideographic approaches. The accuracy of the approaches for predicting individual stress ratings was compared based on classification errors. Results Across the group of patients, an individual’s recent history of stress ratings was most heavily weighted in predicting a future stress rating in the nomothetic recurrent neural network model, whereas environmental factors such as temperature and daylight, as well as duration and frequency of bouts of exercise, were more heavily weighted in the ideographic models. The nomothetic recurrent neural network model was the highest performing nomothetic model and yielded 72% accuracy for an 80%/20% train/test split. Using the same 80/20 split, the ideographic models yielded 75% accuracy. However, restricting ideographic models to participants with more than 50 valid days in the training set, with the same 80/20 split, yielded 85% accuracy. Conclusions We conclude that for some applications, nomothetic models may be useful for yielding higher initial performance while still surfacing personalized predictors of stress, before switching to ideographic models upon sufficient data collection.","",""
7,"Haoran Li, Yang Weng, E. Farantatos, Mahendra Patel","A Hybrid Machine Learning Framework for Enhancing PMU-based Event Identification with Limited Labels",2019,"","","","",17,"2022-07-13 09:40:40","","10.1109/SGSMA.2019.8784550","","",,,,,7,2.33,2,4,3,"The energy industry is experiencing rapid and dramatic changes on both the generator side and the load side, necessitating faster, more accurate, and robust event detection methods for situational awareness. Growing installations of PMU devices that provide high resolution synchronized measurements combined with the advancement of artificial intelligence and big data analytics techniques have recently attracted the R&D community interest. Some supervised learning techniques have been proposed using PMU measurements, however, they are facing challenges in 1) limited interpretability, 2) biased learning models/results, and 3) insufficient labeled data for learning. To address these issues, we propose a machine learning-based framework for physically-meaningful interpretability, hybrid-learning method with indexes, and a flexible data-preparation approach. Specifically, a thoroughly designed feature selection method is proposed for discovering event signatures. Then, a hybrid machine learning process is constructed to reduce biases of different machine learners due to their diversified working mechanisms. Finally, we propose to utilize unlabeled data via semi-supervised learning and add strategical event data via active learning, e.g., simulations. The goal is to significantly improve the supervised learning results via computational efficient techniques. Extensive simulations are conducted using a commercial power system dynamics simulator and synthetic realistic transmission grid models. Significant improvements are observed via hybrid supervised learning methods, semi-supervised learning, and active learning.","",""
2,"Chris Emmery, Ákos Kádár, Travis J. Wiltshire, Andrew T. Hendrickson","Towards Replication in Computational Cognitive Modeling: a Machine Learning Perspective",2019,"","","","",18,"2022-07-13 09:40:40","","10.1007/S42113-019-00055-W","","",,,,,2,0.67,1,4,3,"","",""
1,"Peishuo Sun, Ying Wu, Chaoyi Yin, H. Jiang, Ying Xu, Huiyan Sun","Molecular Subtyping of Cancer Based on Distinguishing Co-Expression Modules and Machine Learning",2022,"","","","",19,"2022-07-13 09:40:40","","10.3389/fgene.2022.866005","","",,,,,1,1.00,0,6,1,"Molecular subtyping of cancer is recognized as a critical and challenging step towards individualized therapy. Most existing computational methods solve this problem via multi-classification of gene-expressions of cancer samples. Although these methods, especially deep learning, perform well in data classification, they usually require large amounts of data for model training and have limitations in interpretability. Besides, as cancer is a complex systemic disease, the phenotypic difference between cancer samples can hardly be fully understood by only analyzing single molecules, and differential expression-based molecular subtyping methods are reportedly not conserved. To address the above issues, we present here a new framework for molecular subtyping of cancer through identifying a robust specific co-expression module for each subtype of cancer, generating network features for each sample by perturbing correlation levels of specific edges, and then training a deep neural network for multi-class classification. When applied to breast cancer (BRCA) and stomach adenocarcinoma (STAD) molecular subtyping, it has superior classification performance over existing methods. In addition to improving classification performance, we consider the specific co-expressed modules selected for subtyping to be biologically meaningful, which potentially offers new insight for diagnostic biomarker design, mechanistic studies of cancer, and individualized treatment plan selection.","",""
1,"W. Monroe, T. Anthony, M. Tanik, F. Skidmore","Towards a Framework for Validating Machine Learning Results in Medical Imaging: Opening the black box",2019,"","","","",20,"2022-07-13 09:40:40","","10.1145/3332186.3332193","","",,,,,1,0.33,0,4,3,"In the medical imaging domain, non-linear warping has enabled pixel by pixel mapping of one image dataset to a reference dataset. This co-registration of data allows for robust, pixel-wise, statistical maps to be developed in the domain, leading to new insights regarding disease mechanisms [20]. Deep learning technologies have given way to some impressive discoveries. In some applications, deep learning algorithms have surpassed the abilities of human image readers to classify data. As long as endpoints are clearly defined, and the input data volume is large enough, deep learning networks can often converge and reach prediction, classification, and segmentation with success rates as high or higher than human operators [13]. However, machine learning, and deep learning algorithms are complex and interpretability is not always a straightforward byproduct of the classification performed. Visualization techniques have been developed to add a layer of interpretability. The work presented here compares a simplified machine learning workflow for medical imaging to a statistical map from a previous study to validate that the machine learning model used does indeed focus its attention on known important regions.","",""
0,"George J. Siedel, S. Vock, A. Morozov, Stefan Voss","Utilizing Class Separation Distance for the Evaluation of Corruption Robustness of Machine Learning Classifiers",2022,"","","","",21,"2022-07-13 09:40:40","","10.48550/arXiv.2206.13405","","",,,,,0,0.00,0,4,1,"Robustness is a fundamental pillar of Machine Learning (ML) classifiers, substantially determining their reliability. Methods for assessing classifier robustness are therefore essential. In this work, we address the challenge of evaluating corruption robustness in a way that allows comparability and interpretability on a given dataset. We propose a test data augmentation method that uses a robustness distance 𝜖 derived from the datasets minimal class separation distance. The resulting MSCR (mean statistical corruption robustness) metric allows a dataset-specific comparison of different classifiers with respect to their corruption robustness. The MSCR value is interpretable, as it represents the classifiers avoidable loss of accuracy due to statistical corruptions. On 2D and image data, we show that the metric reflects different levels of classifier robustness. Furthermore, we observe unexpected optima in classifiers robust accuracy through training and testing classifiers with different levels of noise. While researchers have frequently reported on a significant tradeoff on accuracy when training robust models, we strengthen the view that a tradeoff between accuracy and corruption robustness is not inherent. Our results indicate that robustness training through simple data augmentation can already slightly improve accuracy.","",""
0,"Yilin Ning, Siqi Li, M. Ong, F. Xie, B. Chakraborty, D. Ting, Nan Liu","A novel interpretable machine learning system to generate clinical risk scores: An application for predicting early mortality or unplanned readmission in a retrospective cohort study",2022,"","","","",22,"2022-07-13 09:40:40","","10.1371/journal.pdig.0000062","","",,,,,0,0.00,0,7,1,"Risk scores are widely used for clinical decision making and commonly generated from logistic regression models. Machine-learning-based methods may work well for identifying important predictors to create parsimonious scores, but such ‘black box’ variable selection limits interpretability, and variable importance evaluated from a single model can be biased. We propose a robust and interpretable variable selection approach using the recently developed Shapley variable importance cloud (ShapleyVIC) that accounts for variability in variable importance across models. Our approach evaluates and visualizes overall variable contributions for in-depth inference and transparent variable selection, and filters out non-significant contributors to simplify model building steps. We derive an ensemble variable ranking from variable contributions across models, which is easily integrated with an automated and modularized risk score generator, AutoScore, for convenient implementation. In a study of early death or unplanned readmission after hospital discharge, ShapleyVIC selected 6 variables from 41 candidates to create a well-performing risk score, which had similar performance to a 16-variable model from machine-learning-based ranking. Our work contributes to the recent emphasis on interpretability of prediction models for high-stakes decision making, providing a disciplined solution to detailed assessment of variable importance and transparent development of parsimonious clinical risk scores.","",""
0,"Daniel C. Anderson","Comment on gmd-2022-44 Anonymous Referee # 1 Referee comment on "" A Machine Learning Methodology for the Generation of a Parameterization of the Hydroxyl Radical : a Tool to Improve Computational-Efficiency in Chemistry Climate Models",2022,"","","","",23,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,1,1,"This paper describes the application of a gradient boosted regression tree machine learning approach to derive a parameterization for tropospheric OH based on CCM simulations. The approach is shown to reproduce simulated OH well under current conditions even for cases it has not been trained on, and it behaves acceptably, albeit with increasing errors, when applied to future conditions outside the standard training set. There is substantial novelty in the approach taken, and the results offer a degree of interpretability that is very interesting. The paper is generally well structured, clearly written, and appropriately illustrated. The authors have been thorough in evaluating their approach, and it is particularly good to see robust testing of input variable choice and hyperparameter value selection. the paper approach","",""
0,"Jing-Jing Liu, Jian-chao Liu","Permeability Predictions for Tight Sandstone Reservoir Using Explainable Machine Learning and Particle Swarm Optimization",2022,"","","","",24,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,2,1,"High-precision permeability prediction is of great significance to tight sandstone reservoirs. However, while considerable progress has recently been made in the machine learning based prediction of reservoir permeability, the generalization of this approach is limited by weak interpretability. Hence, an interpretable XGBoost model is proposed herein based on particle swarm optimization to predict the permeability of tight sandstone reservoirs with higher accuracy and robust interpretability. The porosity and permeability of 202 core plugs and 6 logging curves (namely, the gamma-ray (GR) curve, the acoustic curve (AC), the spontaneous potential (SP) curve, the caliper (CAL) curve, the deep lateral resistivity (RILD) curve, and eight lateral resistivity (RFOC) curve) are extracted along with three derived variables (i.e., the shale content, the AC slope, and the GR slope) as data sets. Based on the data preprocessing, global and local interpretations are performed according to the Shapley additive explanations (SHAP) analysis, and the redundant features in the data set are screened to identify the porosity, AC, CAL, and GR slope as the four most important features. The particle swarm optimization algorithm is then used to optimize the hyperparameters of the XGBoost model. The prediction results of the PSO-XGBoost model indicate a superior performance compared with that of the benchmark XGBoost model. In addition, the reliable application of the interpretable PSO-XGBoost model in the prediction of tight sandstone reservoir permeability is examined by comparing the results with those of two traditional mathematical regression models, five machine learning models, and three deep learning models. Thus, the interpretable PSO-XGBoost model is shown to have more advantages in permeability prediction along with the lowest root mean square error, thereby confirming the effectiveness and practicability of this method.","",""
0,"R. Shokri","Trusting Machine Learning: Privacy, Robustness, and Transparency Challenges",2019,"","","","",25,"2022-07-13 09:40:40","","10.1145/3335203.3335728","","",,,,,0,0.00,0,1,3,"Machine learning algorithms have shown an unprecedented predictive power for many complex learning tasks. As they are increasingly being deployed in large scale critical applications for processing various types of data, new questions related to their trustworthiness would arise. Can machine learning algorithms be trusted to have access to individuals’ sensitive data? Can they be robust against noisy or adversarially perturbed data? Can we reliably interpret their learning process, and explain their predictions? In this talk, I will go over the challenges of building trustworthy machine learning algorithms in centralized and distributed (federated) settings, and will discuss the inter-relation between privacy, robustness, and interpretability.","",""
1,"Antonia Gogoglou, C. B. Bruss, Keegan E. Hines","On the Interpretability and Evaluation of Graph Representation Learning",2019,"","","","",26,"2022-07-13 09:40:40","","","","",,,,,1,0.33,0,3,3,"With the rising interest in graph representation learning, a variety of approaches have been proposed to effectively capture a graph's properties. While these approaches have improved performance in graph machine learning tasks compared to traditional graph techniques, they are still perceived as techniques with limited insight into the information encoded in these representations. In this work, we explore methods to interpret node embeddings and propose the creation of a robust evaluation framework for comparing graph representation learning algorithms and hyperparameters. We test our methods on graphs with different properties and investigate the relationship between embedding training parameters and the ability of the produced embedding to recover the structure of the original graph in a downstream task.","",""
327,"Nicolas Papernot, P. Mcdaniel","Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning",2018,"","","","",27,"2022-07-13 09:40:40","","","","",,,,,327,81.75,164,2,4,"Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.","",""
2,"V. Ardulov, Victor R. Martinez, Krishna Somandepalli, S. Zheng, E. Salzman, C. Lord, S. Bishop, Shrikanth S. Narayanan","Robust diagnostic classification via Q-learning",2021,"","","","",28,"2022-07-13 09:40:40","","10.1038/s41598-021-90000-4","","",,,,,2,2.00,0,8,1,"","",""
87,"Nicholas Wagner, J. Rondinelli","Theory-Guided Machine Learning in Materials Science",2016,"","","","",29,"2022-07-13 09:40:40","","10.3389/fmats.2016.00028","","",,,,,87,14.50,44,2,6,"Materials scientists are increasingly adopting the use of machine learning tools to discover hidden trends in data and make predictions. Applying concepts from data science without foreknowledge of their limitations and the unique qualities of materials data, however, could lead to errant conclusions. The differences that exist between various kinds of experimental and calculated data require careful choices of data processing and machine learning methods. Here, we outline potential pitfalls involved in using machine learning without robust protocols. We address some problems of overfitting to training data using decision trees as an example, rational descriptor selection in the field of perovskites, and preserving physical interpretability in the application of dimensionality reducing techniques such as principal component analysis. We show how proceeding without the guidance of domain knowledge can lead to both quantitatively and qualitatively incorrect predictive models.","",""
8,"David Alvarez-Melis, Harmanpreet Kaur, Hal Daum'e, H. Wallach, Jennifer Wortman Vaughan","From Human Explanation to Model Interpretability: A Framework Based on Weight of Evidence",2021,"","","","",30,"2022-07-13 09:40:40","","","","",,,,,8,8.00,2,5,1,"We take inspiration from the study of human explanation to inform the design and evaluation of interpretability methods in machine learning. First, we survey the literature on human explanation in philosophy, cognitive science, and the social sciences, and propose a list of design principles for machine- generated explanations that are meaningful to humans. Using the concept of weight of evidence from information theory, we develop a method for generating explanations that adhere to these principles. We show that this method can be adapted to handle high-dimensional, multi-class settings, yielding a ﬂexible framework for generating explanations. We demon- strate that these explanations can be estimated accurately from ﬁnite samples and are robust to small perturbations of the inputs. We also evaluate our method through a qualitative user study with machine learning practitioners, where we observe that the resulting explanations are usable despite some participants struggling with background concepts like prior class probabilities. Finally, we conclude by surfacing design implications for interpretability tools in general.","",""
1,"W. Monroe, F. Skidmore, David G. Odaibo, M. Tanik","HihO: accelerating artificial intelligence interpretability for medical imaging in IoT applications using hierarchical occlusion",2020,"","","","",31,"2022-07-13 09:40:40","","10.1007/S00521-020-05379-4","","",,,,,1,0.50,0,4,2,"","",""
0,"Gabriel Deza, Adelin Travers, C. Rowat, Nicolas Papernot","Interpretability in Safety-Critical FinancialTrading Systems",2021,"","","","",32,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,4,1,"Sophisticated machine learning (ML) models to inform trading in the financial sector create problems of interpretability and risk management. Seemingly robust forecasting models may behave erroneously in out of distribution settings. In 2020, some of the world’s most sophisticated quant hedge funds suffered losses as their ML models were first underhedged, and then overcompensated. We implement a gradient-based approach for precisely stress-testing how a trading model’s forecasts can be manipulated, and their effects on downstream tasks at the trading execution level. We construct inputs – whether in changes to sentiment or market variables – that efficiently affect changes in the return distribution. In an industry-standard trading pipeline, we perturb model inputs for eight S&P 500 stocks. We find our approach discovers seemingly in-sample input settings that result in large negative shifts in return distributions. We provide the financial community with mechanisms to interpret ML forecasts in trading systems. For the security community, we provide a compelling application where studying ML robustness necessitates that one capture an end-to-end system’s performance rather than study a ML model in isolation. Indeed, we show in our evaluation that errors in the forecasting model’s predictions alone are not sufficient for trading decisions made based on these forecasts to yield a negative return.","",""
0,"David Alvarez-Melis, Harmanpreet Kaur, Hal Daum'e, H. Wallach, Jennifer Wortman Vaughan","A Human-Centered Interpretability Framework Based on Weight of Evidence",2021,"","","","",33,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,5,1,"In this paper, we take a human-centered approach to interpretable machine learning. First, drawing inspiration from the study of explanation in philosophy, cognitive science, and the social sciences, we propose a list of design principles for machinegenerated explanations that are meaningful to humans. Using the concept of weight of evidence from information theory, we develop a method for producing explanations that adhere to these principles. We show that this method can be adapted to handle high-dimensional, multi-class settings, yielding a flexible meta-algorithm for generating explanations. We demonstrate that these explanations can be estimated accurately from finite samples and are robust to small perturbations of the inputs. We also evaluate our method through a qualitative user study with machine learning practitioners, where we observe that the resulting explanations are usable despite some participants struggling with background concepts like prior class probabilities. Finally, we conclude by surfacing design implications for interpretability tools.","",""
5,"Naoya Takeishi, Alexandros Kalousis","Physics-Integrated Variational Autoencoders for Robust and Interpretable Generative Modeling",2021,"","","","",34,"2022-07-13 09:40:40","","","","",,,,,5,5.00,3,2,1,"Integrating physics models within machine learning models holds considerable promise toward learning robust models with improved interpretability and abilities to extrapolate. In this work, we focus on the integration of incomplete physics models into deep generative models. In particular, we introduce an architecture of variational autoencoders (VAEs) in which a part of the latent space is grounded by physics. A key technical challenge is to strike a balance between the incomplete physics and trainable components such as neural networks for ensuring that the physics part is used in a meaningful manner. To this end, we propose a regularized learning method that controls the effect of the trainable components and preserves the semantics of the physics-based latent variables as intended. We not only demonstrate generative performance improvements over a set of synthetic and real-world datasets, but we also show that we learn robust models that can consistently extrapolate beyond the training distribution in a meaningful manner. Moreover, we show that we can control the generative process in an interpretable manner.","",""
3,"Chi-Heng Lin, Mehdi Azabou, Eva L. Dyer","Making transport more robust and interpretable by moving data through a small number of anchor points",2020,"","","","",35,"2022-07-13 09:40:40","","","","",,,,,3,1.50,1,3,2,"Optimal transport (OT) is a widely used technique for distribution alignment, with applications throughout the machine learning, graphics, and vision communities. Without any additional structural assumptions on transport, however, OT can be fragile to outliers or noise, especially in high dimensions. Here, we introduce Latent Optimal Transport (LOT), a new approach for OT that simultaneously learns low-dimensional structure in data while leveraging this structure to solve the alignment task. The idea behind our approach is to learn two sets of ""anchors"" that constrain the flow of transport between a source and target distribution. In both theoretical and empirical studies, we show that LOT regularizes the rank of transport and makes it more robust to outliers and the sampling density. We show that by allowing the source and target to have different anchors, and using LOT to align the latent spaces between anchors, the resulting transport plan has better structural interpretability and highlights connections between both the individual data points and the local geometry of the datasets.","",""
1,"Nathan Justin, S. Aghaei, A. Gómez, P. Vayanos","Optimal Robust Classification Trees",2021,"","","","",36,"2022-07-13 09:40:40","","","","",,,,,1,1.00,0,4,1,"In many high-stakes domains, the data used to drive machine learning algorithms is noisy (due to e.g., the sensitive nature of the data being collected, limited resources available to validate the data, etc). This may cause a distribution shift to occur, where the distribution of the training data does not match the distribution of the testing data. In the presence of distribution shifts, any trained model can perform poorly in the testing phase. In this paper, motivated by the need for interpretability and robustness, we propose a mixed-integer optimization formulation and a tailored solution algorithm for learning optimal classification trees that are robust to adversarial perturbations in the data features. We evaluate the performance of our approach on numerous publicly available datasets, and compare the performance to a regularized, nonrobust optimal tree. We show an increase of up to 14.16% in worst-case accuracy and increase of up to 4.72% in averagecase accuracy across several data sets and distribution shifts from using our robust solution in comparison to the nonrobust solution.","",""
7,"Jacopo de Berardinis, A. Cangelosi, E. Coutinho","The multiple voices of musical emotions: source separation for improving music emotion recognition models and their interpretability",2020,"","","","",37,"2022-07-13 09:40:40","","","","",,,,,7,3.50,2,3,2,"Despite the manifold developments in music emotion recognition and related areas, estimating the emotional impact of music still poses many challenges. These are often associated to the complexity of the acoustic codes to emotion and the lack of large amounts of data with robust golden standards. In this paper, we propose a new computational model (EmoMucs) that considers the role of different musical voices in the prediction of the emotions induced by music. We combine source separation algorithms for breaking up music signals into independent song elements (vocals, bass, drums, other) and end-to-end state-of-the-art machine learning techniques for feature extraction and emotion modelling (valence and arousal regression). Through a series of computational experiments on a benchmark dataset using source-specialised models trained independently and different fusion strategies, we demonstrate that EmoMucs outperforms state-of-the-art approaches with the advantage of providing insights into the relative contribution of different musical elements to the emotions perceived by listeners.","",""
0,"Xiaohang Zhang, Ling Wu, Zhengren Li, Huayuan Liu","A Robust Method to Measure the Global Feature Importance of Complex Prediction Models",2021,"","","","",38,"2022-07-13 09:40:40","","10.1109/ACCESS.2021.3049412","","",,,,,0,0.00,0,4,1,"Because machine learning has been widely used in various domains, interpreting internal mechanisms and predictive results of models is crucial for further applications of complex machine learning models. However, the interpretability of complex machine learning models on biased data remains a difficult problem. When the important explanatory features of concerned data are highly influenced by contaminated distributions, particularly in risk-sensitive fields, such as self-driving vehicles and healthcare, it is crucial to provide a robust interpretation of complex models for users. The interpretation of complex models is often associated with analyzing model features by measuring feature importance. Therefore, this article proposes a novel method derived from high-dimensional model representation (HDMR) to measure feature importance. The proposed method can provide robust estimation when the input features follow contaminated distributions. Moreover, the method is model-agnostic, which can enhance its ability to compare different interpretations due to its generalizability. Experimental evaluations on artificial models and machine learning models show that the proposed method is more robust than the traditional method based on HDMR.","",""
0,"Evan M. Yu, Alan Q. Wang, Adrian V. Dalca, M. Sabuncu","KeypointMorph: Robust Multi-modal A ne Registration via Unsupervised Keypoint Detection",2021,"","","","",39,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,4,1,"Registration is a fundamental task in medical imaging, and recent machine learning methods have become the state-of-the-art. However, these approaches are often not interpretable, lack robustness to large misalignments, and do not incorporate symmetries of the problem. In this work, we propose KeypointMorph, an unsupervised end-to-end learningbased image registration framework that relies on automatically detecting corresponding keypoints. Our core insight is straightforward: matching keypoints between images can be used to obtain the optimal transformation via a di↵erentiable closed-form expression. We use this observation to drive the unsupervised learning of anatomically-consistent keypoints from images. This not only leads to substantially more robust registration but also yields better interpretability, since the keypoints reveal which parts of the image are driving the final alignment. Moreover, KeypointMorph can be designed to be equivariant under image translations and/or symmetric with respect to the input image ordering. We demonstrate the proposed framework in solving 3D a ne registration of multi-modal brain MRI scans. Remarkably, we show that this strategy leads to consistent keypoints, even across modalities. We demonstrate registration accuracy that surpasses current state-of-the-art methods, especially in the context of large displacements. Our code is available at URL1","",""
0,"Jingjing Lu, Shuangyan Yi, Yongsheng Liang, Wei Liu, Jiaoyan Zhao, Qiangqiang Shen","Robust Unsupervised Feature Selection Based on Sparse Reconstruction of Learned Clean Data",2021,"","","","",40,"2022-07-13 09:40:40","","10.1109/CCISP52774.2021.9639338","","",,,,,0,0.00,0,6,1,"Unsupervised feature selection is an essential task in machine learning. Current methods select representative features under slight noise interference. However, their performance is degraded when the data is grossly corrupted. To fundamentally improve the robustness, we propose a robust unsupervised feature selection method based on the sparse reconstruction of learned clean data(SRCD). In this method, we choose the clean data learned by the low-rank constrain as the reconstruction object. Using the noiseless data, SRCD is then capable of obtaining effective features. To further enhance the interpretability, the projection matrix in the reconstruction term is constrained to column sparse patterns for consistent feature selection. Experiment results based on the benchmark and noisy data confirm the effectiveness of the proposed method.","",""
1,"Yunchuan Liu, Lei Yang, Amir Ghasemkhani, H. Livani, Virgilio Centeno, Pin-Yu Chen, Junshan Zhang","Robust Event Classification Using Imperfect Real-world PMU Data",2021,"","","","",41,"2022-07-13 09:40:40","","10.1109/jiot.2022.3177686","","",,,,,1,1.00,0,7,1,"This paper studies robust event classification using imperfect real-world phasor measurement unit (PMU) data. By analyzing the real-world PMU data, we find it is challenging to directly use this dataset for event classifiers due to the low data quality observed in PMU measurements and event logs. To address these challenges, we develop a novel machine learning framework for training robust event classifiers, which consists of three main steps: data preprocessing, fine-grained event data extraction, and feature engineering. Specifically, the data preprocessing step addresses the data quality issues of PMU measurements (e.g., bad data and missing data); in the finegrained event data extraction step, a model-free event detection method is developed to accurately localize the events from the inaccurate event timestamps in the event logs; and the feature engineering step constructs the event features based on the patterns of different event types, in order to improve the performance and the interpretability of the event classifiers. Based on the proposed framework, we develop a workflow for event classification using the real-world PMU data streaming into the system in real time. Using the proposed framework, robust event classifiers can be efficiently trained based on many off-the-shelf lightweight machine learning models. Numerical experiments using the realworld dataset from the Western Interconnection of the U.S power transmission grid show that the event classifiers trained under the proposed framework can achieve high classification accuracy while being robust against low-quality data.","",""
13,"Michal Balazia, Petr Sojka","Learning robust features for gait recognition by Maximum Margin Criterion",2016,"","","","",42,"2022-07-13 09:40:40","","10.1109/ICPR.2016.7899750","","",,,,,13,2.17,7,2,6,"In the field of gait recognition from motion capture data, designing human-interpretable gait features is a common practice of many fellow researchers. To refrain from ad-hoc schemes and to find maximally discriminative features we may need to explore beyond the limits of human interpretability. This paper contributes to the state-of-the-art with a machine learning approach for extracting robust gait features directly from raw joint coordinates. The features are learned by a modification of Linear Discriminant Analysis with Maximum Margin Criterion so that the identities are maximally separated and, in combination with an appropriate classifier, used for gait recognition. Experiments on the CMU MoCap database show that this method outperforms eight other relevant methods in terms of the distribution of biometric templates in respective feature spaces expressed in four class separability coefficients. Additional experiments indicate that this method is a leading concept for rank-based classifier systems.","",""
0,"Nahim Adnan, Maryam Zand, T. Huang, Jianhua Ruan","Construction and Evaluation of Robust Interpretation Models for Breast Cancer Metastasis Prediction",2021,"","","","",43,"2022-07-13 09:40:40","","10.1109/TCBB.2021.3120673","","",,,,,0,0.00,0,4,1,"Interpretability of machine learning (ML) models represents the extent to which a model’s decision-making process can be understood by model developers and/or end users. Transcriptomics-based cancer prognosis models, for example, while achieving good accuracy, are usually hard to interpret, due to the high-dimensional feature space and the complexity of models. As interpretability is critical for the transparency and fairness of ML models, several algorithms have been proposed to improve the interpretability of arbitrary classifiers. However, evaluation of these algorithms often requires substantial domain knowledge. Here, we propose a breast cancer metastasis prediction model using a very small number of biologically interpretable features, and a simple yet novel model interpretation approach that can provide personalized interpretations. In addition, we contributed, to the best of our knowledge, the first method to quantitatively compare different interpretation algorithms. Experimental results show that our model not only achieved competitive prediction accuracy, but also higher inter-classifier interpretation consistency than state-of-the-art interpretation methods. Importantly, our interpretation results can improve the generalizability of the prediction models. Overall, this work provides several novel ideas to construct and evaluate interpretable ML models that can be valuable to both the cancer machine learning community and related application domains.","",""
1,"Zhe Xu","Robust Inference and Verification of Temporal Logic Classifier-in-the-loop Systems",2020,"","","","",44,"2022-07-13 09:40:40","","","","",,,,,1,0.50,1,1,2,"Autonomous systems embedded with machine learning modules often rely on deep neural networks for classifying different objects of interest in the environment or different actions or strategies to take for the system. Due to the non-linearity and high-dimensionality of deep neural networks, the interpretability of the autonomous systems is compromised. Besides, the machine learning methods in autonomous systems are mostly data-intensive and lack commonsense knowledge and reasoning that are natural to humans. In this paper, we propose the framework of temporal logic classifier-in-the-loop systems. The temporal logic classifiers can output different actions to take for an autonomous system based on the environment, such that the behavior of the autonomous system can satisfy a given temporal logic specification. Our approach is robust and provably-correct, as we can prove that the behavior of the autonomous system can satisfy a given temporal logic specification in the presence of (bounded) disturbances.","",""
1,"Huijun Wu, Chen Wang, R. Nock, Wei Wang, Jie Yin, Kai Lu, Liming Zhu","SMINT: Toward Interpretable and Robust Model Sharing for Deep Neural Networks",2020,"","","","",45,"2022-07-13 09:40:40","","10.1145/3381833","","",,,,,1,0.50,0,7,2,"Sharing a pre-trained machine learning model, particularly a deep neural network via prediction APIs, is becoming a common practice on machine learning as a service (MLaaS) platforms nowadays. Although deep neural networks (DNN) have shown remarkable successes in many tasks, they are also criticized for the lack of interpretability and transparency. Interpreting a shared DNN model faces two additional challenges compared with interpreting a general model. (1) Limited training data can be disclosed to users. (2) The internal structure of the models may not be available. These two challenges impede the application of most existing interpretability approaches, such as saliency maps or influence functions, for DNN models. Case-based reasoning methods have been used for interpreting decisions; however, how to select and organize the data points under the constraints of shared DNN models is not discussed. Moreover, simply providing cases as explanations may not be sufficient for supporting instance level interpretability. Meanwhile, existing interpretation methods for DNN models generally lack the means to evaluate the reliability of the interpretation. In this article, we propose a framework named Shared Model INTerpreter (SMINT) to address the above limitations. We propose a new data structure called a boundary graph to organize training points to mimic the predictions of DNN models. We integrate local features, such as saliency maps and interpretable input masks, into the data structure to help users to infer the model decision boundaries. We show that the boundary graph is able to address the reliability issues in many local interpretation methods. We further design an algorithm named hidden-layer aware p-test to measure the reliability of the interpretations. Our experiments show that SMINT is able to achieve above 99% fidelity to corresponding DNN models on both MNIST and ImageNet by sharing only a tiny fraction of training data to make these models interpretable. The human pilot study demonstrates that SMINT provides better interpretability compared with existing methods. Moreover, we demonstrate that SMINT is able to assist model tuning for better performance on different user data.","",""
0,"Jian Jiang","MIIDL: a Python package for microbial biomarkers identification powered by interpretable deep learning",2021,"","","","",46,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,1,1,"Summary: Detecting microbial biomarkers used to predict disease phenotypes and clinical outcomes is crucial for disease early-stage screening and diagnosis. Most methods for biomarker identification are linear-based, which is very limited as biological processes are rarely fully linear. The introduction of machine learning to this field tends to bring a promising solution. However, identifying microbial biomarkers in an interpretable, datadriven and robust manner remains challenging. We present MIIDL, a Python package for the identification of microbial biomarkers based on interpretable deep learning. MIIDL innovatively applies convolutional neural networks, a variety of interpretability algorithms and plenty of pre-processing methods to provide a one-stop and robust pipeline for microbial biomarkers identification from high-dimensional and sparse data sets. Availability: Source code is available on GitHub (https://github.com/chunribu/miidl/) under the MIT license. MIIDL is operating system independent and can be installed directly via pip or conda. Contact: chunribu@mail.sdu.edu.cn","",""
1,"Johannes Haug, Klaus Broelemann, Gjergji Kasneci","Dynamic Model Tree for Interpretable Data Stream Learning",2022,"","","","",47,"2022-07-13 09:40:40","","10.48550/arXiv.2203.16181","","",,,,,1,1.00,0,3,1,"—Data streams are ubiquitous in modern business and society. In practice, data streams may evolve over time and cannot be stored indeﬁnitely. Effective and transparent machine learning on data streams is thus often challenging. Hoeffding Trees have emerged as a state-of-the art for online predictive modelling. They are easy to train and provide meaningful convergence guarantees under a stationary process. Yet, at the same time, Hoeffding Trees often require heuristic and costly extensions to adjust to distributional change, which may considerably impair their interpretability. In this work, we revisit Model Trees for machine learning in evolving data streams. Model Trees are able to maintain more ﬂexible and locally robust representations of the active data concept, making them a natural ﬁt for data stream applications. Our novel framework, called Dynamic Model Tree, satisﬁes desirable consistency and minimality properties. In experiments with synthetic and real-world tabular streaming data sets, we show that the proposed framework can drastically reduce the number of splits required by existing incremental decision trees. At the same time, our framework often outperforms state- of-the-art models in terms of predictive quality – especially when concept drift is involved. Dynamic Model Trees are thus a powerful online learning framework that contributes to more lightweight and interpretable machine learning in data streams.","",""
0,"Justin Talbot, Daniel Ting","Statistical Schema Learning with Occam's Razor",2022,"","","","",48,"2022-07-13 09:40:40","","10.1145/3514221.3526174","","",,,,,0,0.00,0,2,1,"A judiciously normalized database schema can increase data interpretability, reduce data size, and improve data integrity. However, real world data sets are often stored or shared in a denormalized state. We examine the problem of automatically creating a good schema for a denormalized table, approaching it as an unsupervised machine learning problem which must learn an optimal schema from the data. This differs from past rule-based approaches that focus on normalization into a canonical form. We define a principled schema optimization criterion, based on Occam's razor, that is robust to noise and extensible---allowing users to easily specify desirable properties of the resulting schema. We develop an efficient learning algorithm for this criterion and empirically demonstrate that it is 3 to 100 times faster than previous work and produces higher quality schemas with 1/5th the errors.","",""
158,"G. Cui, M. Wong, H. Lui","Machine Learning for Direct Marketing Response Models: Bayesian Networks with Evolutionary Programming",2006,"","","","",49,"2022-07-13 09:40:40","","10.1287/mnsc.1060.0514","","",,,,,158,9.88,53,3,16,"Machine learning methods are powerful tools for data mining with large noisy databases and give researchers the opportunity to gain new insights into consumer behavior and to improve the performance of marketing operations. To model consumer responses to direct marketing, this study proposes Bayesian networks learned by evolutionary programming. Using a large direct marketing data set, we tested the endogeneity bias in the recency, frequency, monetary value (RFM) variables using the control function approach; compared the results of Bayesian networks with those of neural networks, classification and regression tree (CART), and latent class regression; and applied a tenfold cross-validation. The results suggest that Bayesian networks have distinct advantages over the other methods in accuracy of prediction, transparency of procedures, interpretability of results, and explanatory insight. Our findings lend strong support to Bayesian networks as a robust tool for modeling consumer response and other marketing problems and for assisting management decision making.","",""
3,"A. Preece, Daniel Harborne, R. Raghavendra, Richard J. Tomsett, Dave Braines","Provisioning Robust and Interpretable AI/ML-Based Service Bundles",2018,"","","","",50,"2022-07-13 09:40:40","","10.1109/MILCOM.2018.8599838","","",,,,,3,0.75,1,5,4,"Coalition operations environments are characterised by the need to share intelligence, surveillance and reconnaissance services. Increasingly, such services are based on artificial intelligence (AI)and machine learning (ML)technologies. Two key issues in the exploitation of AI/ML services are robustness and interpretability. Employing a diverse portfolio of services can make a system robust to ‘unknown unknowns’. Interpretability - the need for services to offer explanation facilities to engender user trust - can be addressed by a variety of methods to generate either transparent or post hoc explanations according to users' requirements. This paper shows how a service-provisioning framework for coalition operations can be extended to address specific requirements for robustness and interpretability, allowing automatic selection of service bundles for intelligence, surveillance and reconnaissance tasks. The approach is demonstrated in a case study on traffic monitoring featuring a diverse set of AI/ML services based on deep neural networks and heuristic reasoning approaches.","",""
6,"Mina Lee, Tatsunori B. Hashimoto, Percy Liang","Learning Autocomplete Systems as a Communication Game",2019,"","","","",51,"2022-07-13 09:40:40","","","","",,,,,6,2.00,2,3,3,"We study textual autocomplete---the task of predicting a full sentence from a partial sentence---as a human-machine communication game. Specifically, we consider three competing goals for effective communication: use as few tokens as possible (efficiency), transmit sentences faithfully (accuracy), and be learnable to humans (interpretability). We propose an unsupervised approach which tackles all three desiderata by constraining the communication scheme to keywords extracted from a source sentence for interpretability and optimizing the efficiency-accuracy tradeoff. Our experiments show that this approach results in an autocomplete system that is 52% more accurate at a given efficiency level compared to baselines, is robust to user variations, and saves time by nearly 50% compared to typing full sentences.","",""
2,"Xin Qiu, Yuanjia Wang","Composite interaction tree for simultaneous learning of optimal individualized treatment rules and subgroups",2019,"","","","",52,"2022-07-13 09:40:40","","10.1002/sim.8105","","",,,,,2,0.67,1,2,3,"Treatment response heterogeneity has long been observed in patients affected by chronic diseases. Administering an individualized treatment rule (ITR) offers an opportunity to tailor treatment strategies according to patient‐specific characteristics. Overly complex machine learning methods for estimating ITRs may produce treatment rules that have higher benefit but lack transparency and interpretability. In clinical practices, it is desirable to derive a simple and interpretable ITR while maintaining certain optimality that leads to improved benefit in subgroups of patients, if not on the overall sample. In this work, we propose a tree‐based robust learning method to estimate optimal piecewise linear ITRs and identify subgroups of patients with a large benefit. We achieve these goals by simultaneously identifying qualitative and quantitative interactions through a tree model, referred to as the composite interaction tree (CITree). We show that it has improved performance compared to existing methods on both overall sample and subgroups via extensive simulation studies. Lastly, we fit CITree to Research Evaluating the Value of Augmenting Medication with Psychotherapy trial for treating patients with major depressive disorders, where we identified both qualitative and quantitative interactions and subgroups of patients with a large benefit.","",""
51,"A. Garcez, L. Lamb","Neurosymbolic AI: The 3rd Wave",2020,"","","","",53,"2022-07-13 09:40:40","","","","",,,,,51,25.50,26,2,2,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.","",""
19,"Mohsen Hajiloo, H. Rabiee, Mahdi Anooshahpour","Fuzzy support vector machine: an efficient rule-based classification technique for microarrays",2013,"","","","",54,"2022-07-13 09:40:40","","10.1186/1471-2105-14-S13-S4","","",,,,,19,2.11,6,3,9,"","",""
2,"Marc T. Ratkovic, D. Tingley","Estimation and Inference on Nonlinear and Heterogeneous Effects∗",2021,"","","","",55,"2022-07-13 09:40:40","","","","",,,,,2,2.00,1,2,1,"Multiple regression has been the go-to method for data analysis for generations of scholars due to its transparency, interpretability, and desirable theoretical properties. However, the method’s simplicity precludes the discovery of complex heterogeneities in the data. We introduce the Method of Direct Estimation and Inference (MDEI) that embraces these potential complexities, is interpretable, has desirable theoretical guarantees, and, unlike some existing methods, returns appropriate uncertainty estimates. The proposed method uses a machine learning regression methodology to estimate the observation-level effect of a treatment variable. Importantly, we introduce a robust approach to uncertainty estimates. We provide simulation evidence and an application illustrating the performance of the method.","",""
2,"K. Yan, Adam P. Harrison","Interpretable Medical Image Classification with Self-Supervised Anatomical Embedding and Prior Knowledge",2021,"","","","",56,"2022-07-13 09:40:40","","","","",,,,,2,2.00,1,2,1,"In medical image analysis tasks, it is important to make machine learning models focus on correct anatomical locations, so as to improve interpretability and robustness of the model. We adopt a latest algorithm called self-supervised anatomical embedding (SAM) to locate point of interest (POI) on computed tomography (CT) scans. SAM can detect arbitrary POI with only one labeled sample needed. Then, we can extract targeted features from the POIs to train a simple prediction model guided by clinical prior knowledge. This approach mimics the practice of human radiologists, thus is interpretable, controllable, and robust. We illustrate our approach on the application of CT contrast phase classification and it outperforms an existing deep learning based method trained on the whole image.","",""
0,"M. Andel, F. Masri, J. Kléma, Z. Krejcík, M. Belickova","Sparse omics-network regularization to increase interpretability and performance of linear classification models",2015,"","","","",57,"2022-07-13 09:40:40","","10.1109/BIBM.2015.7359754","","",,,,,0,0.00,0,5,7,"Current high-throughput technologies lead to the boost of omics data with thousands of features measured in parallel. The phenotype specific markers are learned from the data to better understand the disease mechanism and to build predictive models. However, the learning is prone to overfitting, caused by a small sample size and large feature space dimension. Consequently, resulting models are inaccurate and difficult to interpret due to the complex nature of omics processes. In this paper, we propose a methodology for learning simple yet biologically meaningful linear classification models. A linear support vector machine is trained; the learning is regularized by prior knowledge. Regularization parameters enable the expert to operatively adjust the interpretation of the models and their conformity with recent domain research while maintaining their accuracy. We performed robust experiments showing empirical validity of our methodology. In the study related to myelodysplastic syndrome we demonstrate the performance and interpretation of disease classification models. These models are consistent with recent progress in myelodysplastic syndrome research.","",""
1,"Burim Ramosaj","Interpretable Machines: Constructing Valid Prediction Intervals with Random Forests",2021,"","","","",58,"2022-07-13 09:40:40","","","","",,,,,1,1.00,1,1,1,"An important issue when using Machine Learning algorithms in recent research is the lack of interpretability. Although these algorithms provide accurate point predictions for various learning problems, uncertainty estimates connected with point predictions are rather sparse. A contribution to this gap for the Random Forest Regression Learner is presented here. Based on its Out-of-Bag procedure, several parametric and nonparametric prediction intervals are provided for Random Forest point predictions and theoretical guarantees for its correct coverage probability is delivered. In a second part, a thorough investigation through MonteCarlo simulation is conducted evaluating the performance of the proposed methods from three aspects: (i) Analyzing the correct coverage rate of the proposed prediction intervals, (ii) Inspecting interval width and (iii) Verifying the competitiveness of the proposed intervals with existing methods. The simulation yields that the proposed prediction intervals are robust towards non-normal residual distributions and are competitive by providing correct coverage rates and comparably narrow interval lengths, even for comparably small samples.","",""
1,"Sumanta Basu, D. Matteson","A Survey of Estimation Methods for Sparse High-dimensional Time Series Models",2021,"","","","",59,"2022-07-13 09:40:40","","","","",,,,,1,1.00,1,2,1,"High-dimensional time series datasets are becoming increasingly common in many areas of biological and social sciences. Some important applications include gene regulatory network reconstruction using time course gene expression data, brain connectivity analysis from neuroimaging data, structural analysis of a large panel of macroeconomic indicators, and studying linkages among financial firms for more robust financial regulation. These applications have led to renewed interest in developing principled statistical methods and theory for estimating large time series models given only a relatively small number of temporally dependent samples. Sparse modeling approaches have gained popularity over the last two decades in statistics and machine learning for their interpretability and predictive accuracy. Although there is a rich literature on several sparsity inducing methods when samples are independent, research on the statistical properties of these methods for estimating time series models is still in progress. We survey some recent advances in this area, focusing on empirically successful lasso based estimation methods for two canonical multivariate time series models stochastic regression and vector autoregression. We discuss key technical challenges arising in high-dimensional time series analysis and outline several interesting research directions.","",""
0,"Vu Pham","Sparse optimization models with robust sketching and applications",2016,"","","","",60,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,1,6,"Author(s): Pham, Vu | Advisor(s): El Ghaoui, Laurent | Abstract: Sparse machine learning has recently emerged as powerful tool to obtain models of high-dimensional data with high degree of interpretability, at low computational cost. The approach has been successfully used in many areas, such as signal and image processing. In sparse learning classification, for example, the prediction accuracy or some other classical measure of performance is not the sole concern: we also wish to be able to better understand which few features are relevant as markers for classification. Furthermore, many of sparse learning tasks in practice, including cross-validation, parameter search, or leave-one-out analysis, involve multiple instances of similar problems, each instance sharing a large part of learning data with the others. In this thesis, we introduce a robust framework for solving these multiple sparse regressions in the form of square-root LASSO problems, based on a sketch of the learning data that uses low-rank approximations. Our approach allows a dramatic reduction in computational effort, while not sacrificing—sometimes even improving—the statistical performance.We present our technique by first studying sparse optimization with applications in different domain of interests, from text analytics to system design, and then developing theories for robust solutions for sparse regression in multi-instance setting. We also provide comparisons with other heuristics to obtain sparse models in various applications. In more detail, our central contributions from this thesis include:- Identifying key tasks in domains of interests under real-world setting,- Suggesting models that are suitable for these tasks along the axes of computational complexity and model understandability,- Exploiting problem structures when working with multiple instances to robustly improve computation while maintaining high learning performance, and- Proposing applications of our robust solutions in high-dimensional setting.","",""
0,"Neha Karadkar","“Spam Mail Classification Using SVM and Genetic Algorithm”",2021,"","","","",61,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,1,1,"Feature selection is a problem of global combinatorial optimization in machine learning in which subsets of relevant features are selected to realize robust learning models. The inclusion of irrelevant and redundant features in the dataset can result in poor predictions and high computational overhead. Thus, selecting relevant feature subsets can help reduce the computational cost of feature measurement, speed up learning process and improve model interpretability. SVM classifier has proven inefficient in its inability to produce accurate classification results in the face of large e-mail dataset while it also consumes a lot of computational resources. In this study, a Genetic AlgorithmSupport Vector Machine (GA-SVM) feature selection technique is developed to optimize the SVM classification parameters, the prediction accuracy and computation time. Spam assassin dataset was used to validate the performance of the proposed system. The hybrid GA-SVM showed remarkable improvements over SVM in terms of classification accuracy and computation time. KeywordsEmail classification, Feature Selection, Spam mail detection, Support Vector Machine.","",""
42,"Udo Schlegel, Hiba Arnout, Mennatallah El-Assady, D. Oelke, D. Keim","Towards A Rigorous Evaluation Of XAI Methods On Time Series",2019,"","","","",62,"2022-07-13 09:40:40","","10.1109/ICCVW.2019.00516","","",,,,,42,14.00,8,5,3,"Explainable Artificial Intelligence (XAI) methods are typically deployed to explain and debug black-box machine learning models. However, most proposed XAI methods are black-boxes themselves and designed for images. Thus, they rely on visual interpretability to evaluate and prove explanations. In this work, we apply XAI methods previously used in the image and text-domain on time series. We present a methodology to test and evaluate various XAI methods on time series by introducing new verification techniques to incorporate the temporal dimension. We further conduct preliminary experiments to assess the quality of selected XAI method explanations with various verification methods on a range of datasets and inspecting quality metrics on it. We demonstrate that in our initial experiments, SHAP works robust for all models, but others like DeepLIFT, LRP, and Saliency Maps work better with specific architectures.","",""
8,"Laura Rieger, L. K. Hansen","A simple defense against adversarial attacks on heatmap explanations",2020,"","","","",63,"2022-07-13 09:40:40","","","","",,,,,8,4.00,4,2,2,"With machine learning models being used for more sensitive applications, we rely on interpretability methods to prove that no discriminating attributes were used for classification. A potential concern is the so-called ""fair-washing"" - manipulating a model such that the features used in reality are hidden and more innocuous features are shown to be important instead.  In our work we present an effective defence against such adversarial attacks on neural networks. By a simple aggregation of multiple explanation methods, the network becomes robust against manipulation. This holds even when the attacker has exact knowledge of the model weights and the explanation methods used.","",""
7,"M. Singh, Nupur Kumari, Puneet Mangla, Abhishek Sinha, V. Balasubramanian, Balaji Krishnamurthy","Attributional Robustness Training Using Input-Gradient Spatial Alignment",2019,"","","","",64,"2022-07-13 09:40:40","","10.1007/978-3-030-58583-9_31","","",,,,,7,2.33,1,6,3,"","",""
18,"F. Horn, R. Pack, Michael Rieger","The autofeat Python Library for Automatic Feature Engineering and Selection",2019,"","","","",65,"2022-07-13 09:40:40","","10.1007/978-3-030-43823-4_10","","",,,,,18,6.00,6,3,3,"","",""
2,"A. Garcez, L. Lamb","A I ] 1 0 D ec 2 02 0 Neurosymbolic AI : The 3 rd Wave",2020,"","","","",66,"2022-07-13 09:40:40","","","","",,,,,2,1.00,1,2,2,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.","",""
2,"Jing Zhang, Bo Chen, Lingxi Zhang, Xirui Ke, Haipeng Ding","Neural-Symbolic Reasoning on Knowledge Graphs",2020,"","","","",67,"2022-07-13 09:40:40","","","","",,,,,2,1.00,0,5,2,"Knowledge graph reasoning is the fundamental component to support machine learning applications such as information extraction, information retrieval and recommendation. Since knowledge graph can be viewed as the discrete symbolic representations of knowledge, reasoning on knowledge graphs can naturally leverage the symbolic techniques. However, symbolic reasoning is intolerant of the ambiguous and noisy data. On the contrary, the recent advances of deep learning promote neural reasoning on knowledge graphs, which is robust to the ambiguous and noisy data, but lacks interpretability compared to symbolic reasoning. Considering the advantages and disadvantages of both methodologies, recent efforts have been made on combining the two reasoning methods. In this survey, we take a thorough look at the development of the symbolic reasoning, neural reasoning and the neural-symbolic reasoning on knowledge graphs. We survey two specific reasoning tasks, knowledge graph completion and question answering on knowledge graphs, and explain them in a unified reasoning framework. We also briefly discuss the future directions for knowledge graph reasoning.","",""
5,"Handong Zhao, Zhengming Ding, Ming Shao, Y. Fu","Part-Level Regularized Semi-Nonnegative Coding for Semi-Supervised Learning",2015,"","","","",68,"2022-07-13 09:40:40","","10.1109/ICDM.2015.23","","",,,,,5,0.71,1,4,7,"Graph-based semi-supervised learning method has been influential in the data mining and machine learning fields. The key is to construct an effective graph to capture the intrinsic data structure, which further benefits for propagating the unlabeled data over the graph. The existing methods have shown the effectiveness of a graph regularization term on measuring the similarities among samples, which further uncovers the data structure. However, all the existing graph-based methods are on the sample-level, i.e. calculate the similarity based on sample-level representation coefficients, inevitably overlooking the underlying part-level structure within sample. Inspired by the strong interpretability of Non-negative Matrix Factorization (NMF) method, we design a more robust and discriminative graph, by integrating low-rank factorization and graph regularizer into a unified framework. Specifically, a novel low-rank factorization through Semi-Non-negative Matrix Factorization (SNMF) is proposed to extract the semantically part-level representation. Moreover, instead of incorporating a graph regularization on sample-level, we propose a sparse graph regularization term built on the decomposed part-level representation. This practice results in a more accurate measurement among samples, generating a more discriminative graph for semi-supervised learning. As a non-trivial contribution, we also provide an optimization solution to the proposed method. Comprehensive experimental evaluations show that our proposed method is able to achieve superior performance compared with the state-of-the-art semi-supervised classification baselines in both transductive and inductive scenarios.","",""
1,"Ruichen Xu, Y. Pang, Zhibing Hu","Sensitivity analysis of external conditions based on the MARS-Sobol method: case study of Tai Lake, China",2020,"","","","",69,"2022-07-13 09:40:40","","10.2166/ws.2020.359","","",,,,,1,0.50,0,3,2,"  This study utilized the ECO Lab model calculation samples of Tai Lake, in combination with robust analysis and the GCV test, to promote a faster intelligent application of machine learning and evaluate the MARS machine learning method. The results revealed that this technique can be better trained with small-scale samples, as indicated by the R2 values of the water quality test results, which were all >0.995. In combination with the Sobol sensitivity analysis method, the contribution degree of the parameterized external conditions as well as the relationship with the water quality were examined, which indicated that TP and TN are primarily related to the external input water quality and flow, while Chl-a is related to inflow (36.42%), TP (26.65%), wind speed (25.89%), temperature (8.38%), thus demonstrating that the governance of Chl-a is more difficult. In general, the accuracy and interpretability of MARS machine learning are more in line with the actual situation, and the use of the Sobol method can save computer calculation time. The results of this research can provide a certain scientific basis for future intelligent management of lake environments.","",""
1,"Quanxue Li, Wentao Dai, Jixiang Liu, Qingqing Sang, Yixue Li, Yuanyuan Li","Gene dysregulation analysis builds a mechanistic signature for prognosis and therapeutic benefit in colorectal cancer",2020,"","","","",70,"2022-07-13 09:40:40","","10.1093/jmcb/mjaa041","","",,,,,1,0.50,0,6,2,"Abstract The implementation of cancer precision medicine requires biomarkers or signatures for predicting prognosis and therapeutic benefits. Most of current efforts in this field are paying much more attention to predictive accuracy than to molecular mechanistic interpretability. Mechanism-driven strategy has recently emerged, aiming to build signatures with both predictive power and explanatory power. Driven by this strategy, we developed a robust gene dysregulation analysis framework with machine learning algorithms, which is capable of exploring gene dysregulations underlying carcinogenesis from high-dimensional data with cooperativity and synergy between regulators and several other transcriptional regulation rules taken into consideration. We then applied the framework to a colorectal cancer (CRC) cohort from The Cancer Genome Atlas. The identified CRC-related dysregulations significantly covered known carcinogenic processes and exhibited good prognostic effect. By choosing dysregulations with greedy strategy, we built a four-dysregulation (4-DysReg) signature, which has the capability of predicting prognosis and adjuvant chemotherapy benefit. 4-DysReg has the potential to explain carcinogenesis in terms of dysfunctional transcriptional regulation. These results demonstrate that our gene dysregulation analysis framework could be used to develop predictive signature with mechanistic interpretability for cancer precision medicine, and furthermore, elucidate the mechanisms of carcinogenesis.","",""
0,"H. P. Oliveira, Jaime S. Cardoso","BCCT.core Model Enhancement With Interpretability and Lateral Information",2010,"","","","",71,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,2,12,"Breast Cancer Conservative Treatment (BCCT) is considered the gold standard of breast cancer treatment. The heterogeneity of the aesthetic result and the limited reproducibility of the subjective evaluation motivated the research towards objective methods, such as, the recent computer system named BCCT.core, based on machine learning techniques, namely support vector machines (SVMs). In the current work we investigate the accuracy of different interpretable methods against the model currently deployed in the BCCT.core software and the improvement of the model by introducing lateral information extracted from patients images. Experimental results show only a marginal improvement in the performance of the new models, suggesting that is essential to use more robust models, such as 3D approaches.","",""
8,"M. Singh, Nupur Kumari, Puneet Mangla, Abhishek Sinha, V. Balasubramanian, Balaji Krishnamurthy","On the Benefits of Attributional Robustness",2019,"","","","",72,"2022-07-13 09:40:40","","","","",,,,,8,2.67,1,6,3,"Interpretability is an emerging area of research in trustworthy machine learning. Safe deployment of machine learning system mandates that the prediction and its explanation be reliable and robust. Recently, it was shown that one could craft perturbations that produce perceptually indistinguishable inputs having the same prediction, yet very different interpretations. We tackle the problem of attributional robustness (i.e. models having robust explanations) by maximizing the alignment between the input image and its saliency map using soft-margin triplet loss. We propose a robust attribution training methodology that beats the state-of-the-art attributional robustness measure by a margin of approximately 6-18% on several standard datasets, ie. SVHN, CIFAR-10 and GTSRB. We further show the utility of the proposed robust model in the domain of weakly supervised object localization and segmentation. Our proposed robust model also achieves a new state-of-the-art object localization accuracy on the CUB-200 dataset.","",""
9,"Beibit Abdikenov, Zangir Iklassov, Askhat Sharipov, Shahid Hussain, P. Jamwal","Analytics of Heterogeneous Breast Cancer Data Using Neuroevolution",2019,"","","","",73,"2022-07-13 09:40:40","","10.1109/ACCESS.2019.2897078","","",,,,,9,3.00,2,5,3,"Breast cancer prognostic modeling is difficult since it is governed by many diverse factors. Given the low median survival and large scale breast cancer data, which comes from high throughput technology, the accurate and reliable prognosis of breast cancer is becoming increasingly difficult. While accurate and timely prognosis may save many patients from going through painful and expensive treatments, it may also help oncologists in managing the disease more efficiently and effectively. Data analytics augmented by machine-learning algorithms have been proposed in past for breast cancer prognosis; and however, most of these could not perform well owing to the heterogeneous nature of available data and model interpretability related issues. A robust prognostic modeling approach is proposed here whereby a Pareto optimal set of deep neural networks (DNNs) exhibiting equally good performance metrics is obtained. The set of DNNs is initialized and their hyperparameters are optimized using the evolutionary algorithm, NSGAIII. The final DNN model is selected from the Pareto optimal set of many DNNs using a fuzzy inferencing approach. Contrary to using DNNs as the black box, the proposed scheme allows understanding how various performance metrics (such as accuracy, sensitivity, F1, and so on) change with changes in hyper-parameters. This enhanced interpretability can be further used to improve or modify the behavior of DNNs. The heterogeneous breast cancer database requires preprocessing for better interpretation of categorical variables in order to improve prognosis from classifiers. Furthermore, we propose to use a neural network-based entity-embedding method for categorical features with high cardinality. This approach can provide a vector representation of categorical features in multidimensional space with enhanced interpretability. It is shown with evidence that DNNs optimized using evolutionary algorithms exhibit improved performance over other classifiers mentioned in this paper.","",""
8,"J. Pfau, Albert T. Young, Maria L. Wei, Michael J. Keiser","Global Saliency: Aggregating Saliency Maps to Assess Dataset Artefact Bias",2019,"","","","",74,"2022-07-13 09:40:40","","","","",,,,,8,2.67,2,4,3,"In high-stakes applications of machine learning models, interpretability methods provide guarantees that models are right for the right reasons. In medical imaging, saliency maps have become the standard tool for determining whether a neural model has learned relevant robust features, rather than artefactual noise. However, saliency maps are limited to local model explanation because they interpret predictions on an image-by-image basis. We propose aggregating saliency globally, using semantic segmentation masks, to provide quantitative measures of model bias across a dataset. To evaluate global saliency methods, we propose two metrics for quantifying the validity of saliency explanations. We apply the global saliency method to skin lesion diagnosis to determine the effect of artefacts, such as ink, on model bias.","",""
20,"Andrew Cattle, Xiaojuan Ma","Recognizing Humour using Word Associations and Humour Anchor Extraction",2018,"","","","",75,"2022-07-13 09:40:40","","","","",,,,,20,5.00,10,2,4,"This paper attempts to marry the interpretability of statistical machine learning approaches with the more robust models of joke structure and joke semantics capable of being learned by neural models. Specifically, we explore the use of semantic relatedness features based on word associations, rather than the more common Word2Vec similarity, on a binary humour identification task and identify several factors that make word associations a better fit for humour. We also explore the effects of using joke structure, in the form of humour anchors (Yang et al., 2015), for improving the performance of semantic features and show that, while an intriguing idea, humour anchors contain several pitfalls that can hurt performance.","",""
3,"Selim Ickin, Jawwad Ahmed, A. Johnsson, Jorgen Gustafsson","On Network Performance Indicators for Network Promoter Score Estimation",2019,"","","","",76,"2022-07-13 09:40:40","","10.1109/QoMEX.2019.8743206","","",,,,,3,1.00,1,4,3,"Estimation of user perceived quality of offered services, from massive number of Key Performance Indicator (KPI)’s that are measured in diverse components, has been a necessity for mobile network operators. The goal is first to have a good estimator for poor Quality of Experience (QoE), which can potentially be achieved with machine learning, and then pinpoint the features that are contributing to the poor performance. There is often a tradeoff between accuracy and interpretability of models. In this paper, we address this tradeoff by first developing a robust but complex teacher machine learning model to map the subjective Net Promoter Score (NPS) values computed from the user quality feedback to the underlying subset of KPI metrics. Next, we develop a rather interpretable student model supervised by the pre-trained teacher model. Eventually the compact student decision tree model learns to mimic the behavior of the teacher model with an at least 10 % improved accuracy in testset as compared to conventional way of directly training using the decision tree model. In the last step, we extract the rules and important influential features of the distilled student model.","",""
12,"E. Marchiori, C. Jimenez, Mikkel West-Nielsen, N. Heegaard","Robust SVM-Based Biomarker Selection with Noisy Mass Spectrometric Proteomic Data",2006,"","","","",77,"2022-07-13 09:40:40","","10.1007/11732242_8","","",,,,,12,0.75,3,4,16,"","",""
13,"S. Saralajew, Lars Holdijk, Maike Rees, T. Villmann","Prototype-based Neural Network Layers: Incorporating Vector Quantization",2018,"","","","",78,"2022-07-13 09:40:40","","","","",,,,,13,3.25,3,4,4,"Neural networks currently dominate the machine learning community and they do so for good reasons. Their accuracy on complex tasks such as image classification is unrivaled at the moment and with recent improvements they are reasonably easy to train. Nevertheless, neural networks are lacking robustness and interpretability. Prototype-based vector quantization methods on the other hand are known for being robust and interpretable. For this reason, we propose techniques and strategies to merge both approaches. This contribution will particularly highlight the similarities between them and outline how to construct a prototype-based classification layer for multilayer networks. Additionally, we provide an alternative, prototype-based, approach to the classical convolution operation. Numerical results are not part of this report, instead the focus lays on establishing a strong theoretical framework. By publishing our framework and the respective theoretical considerations and justifications before finalizing our numerical experiments we hope to jump-start the incorporation of prototype-based learning in neural networks and vice versa.","",""
44,"Fagbola Temitayo, Olabiyisi Stephen Adigun Abimbola","Hybrid GA-SVM for Efficient Feature Selection in E-mail Classification",2012,"","","","",79,"2022-07-13 09:40:40","","","","",,,,,44,4.40,22,2,10,"Feature selection is a problem of global combinator ial optimization in machine learning in which subse ts of relevant features are selected to realize robust le arning models. The inclusion of irrelevant and redu ndant features in the dataset can result in poor predicti ons and high computational overhead. Thus, selectin g relevant feature subsets can help reduce the comput ational cost of feature measurement, speed up learn ing process and improve model interpretability. SVM classifier has proven inefficient in its inability to produce accurate classification results in the face of larg e e-mail dataset while it also consumes a lot of computational resources. In this study, a Genetic A lgorithm-Support Vector Machine (GA-SVM) feature selection technique is developed to optimize the SV M classification parameters, the prediction accurac y and computation time. Spam assassin dataset was used to validate the performance of the proposed syste m. The hybrid GA-SVM showed remarkable improvements over SVM in terms of classification accuracy and computation time.","",""
5,"Tyler Folkman, Rey Furner, D. Pearson","GenERes: A Genealogical Entity Resolution System",2018,"","","","",80,"2022-07-13 09:40:40","","10.1109/ICDMW.2018.00079","","",,,,,5,1.25,2,3,4,"Entity resolution is the problem of identifying and linking different manifestations of the same real-world object. This is an important step for many databases to ensure a clean version of the data and to leverage the information from multiple views of the same entity. At Ancestry, we have many manifestations of the same person in our databases. For example, the same person may be found in multiple family trees, or a person may have multiple types of records which refer to him or her, such as birth, marriage, or death records. The ability to resolve entities helps us unlock powerful genealogical discoveries for our users. To resolve these entities, we have developed a robust, scalable machine learning method which works across many different types of genealogical content as well as time and place. We find substantial improvements over a previous rule-based system and demonstrate how a machine-learning-related approach can also allow for interpretability. While we focus our example within the realm of genealogical data and historical records, we provide a model architecture and some learnings which should be applicable to many entity resolution domains.","",""
6,"Alwin Yaoxian Zhang, Sean Shao Wei Lam, Nan Liu, Yan Pang, L. Chan, Phua Hwee Tang","Development of a Radiology Decision Support System for the Classification of MRI Brain Scans",2018,"","","","",81,"2022-07-13 09:40:40","","10.1109/BDCAT.2018.00021","","",,,,,6,1.50,1,6,4,"Previous studies revealed that the ordering of Magnetic resonance imaging (MRI) brain scans following American College of Radiology (ACR) guidelines showed a higher percentage of brain abnormalities compared to scans that do not. As the process of manually labelling patient orders obtained from a local tertiary hospital in accordance to ACR guidelines is intensive and time consuming, this study aims to develop predictive machine learning models; Logistic Regression (LR), Support Vector Machine (SVM), Random Forest (RF) and XGBoost (XGB), to automate the classification process through text mining methods and derive insights that are useful for future clinical decision-making and resource optimization. Using 1,924 observations as the labelled training data, RF and XGB were found to be the best performing robust models with ROC values of 0.9459 and 0.9508 respectively on the validation set (481 observations). Further exploration into the interpretability of black-box algorithms using the model agnostic LIME (Local Interpretable Model-Agnostic Explanations) framework was used to generate further insights for decisions made using a separate XGB model with respect to individual patients. The LIME framework is a significant first step towards the development of a comprehensive decision support system for patient-level decisions in the ordering of MRI scans.","",""
29,"Goce Ristanoski, Wei Liu, J. Bailey","Time Series Forecasting Using Distribution Enhanced Linear Regression",2013,"","","","",82,"2022-07-13 09:40:40","","10.1007/978-3-642-37453-1_40","","",,,,,29,3.22,10,3,9,"","",""
3,"Si-Yao Fu, Guosheng Yang, Z. Hou","Image category learning and classification via optimal linear combination of multiple partially matching kernels",2010,"","","","",83,"2022-07-13 09:40:40","","10.1007/s00500-009-0436-y","","",,,,,3,0.25,1,3,12,"","",""
18,"M. Strickert, B. Hammer, T. Villmann, Michael Biehl","Regularization and improved interpretation of linear data mappings and adaptive distance measures",2013,"","","","",84,"2022-07-13 09:40:40","","10.1109/CIDM.2013.6597211","","",,,,,18,2.00,5,4,9,"Linear data transformations are essential operations in many machine learning algorithms, helping to make such models more flexible or to emphasize certain data directions. In particular for high dimensional data sets linear transformations are not necessarily uniquely determined, though, and alternative parameterizations exist which do not change the mapping of the training data. Thus, regularization is required to make the model robust to noise and more interpretable for the user. In this contribution, we characterize the group of transformations which leave a linear mapping invariant for a given finite data set, and we discuss the consequences on the interpretability of the models. We propose an intuitive regularization mechanism to avoid problems in under-determined configurations, and we test the approach in two machine learning models.","",""
8,"Óscar García Hinde, V. Gómez-Verdejo, M. Martínez-Ramón, C. Casanova-Mateo, J. Sanz-Justo, S. Jiménez-Fernández, S. Salcedo-Sanz","Feature selection in solar radiation prediction using bootstrapped SVRs",2016,"","","","",85,"2022-07-13 09:40:40","","10.1109/CEC.2016.7744250","","",,,,,8,1.33,1,7,6,"During the past years solar radiation prediction has become increasingly relevant among the scientific community and Machine Learning techniques have proven to be a useful tool to automatically learn an accurate prediction model. In this paper, we move one step further and try to gain interpretability during the learning process by introducing a novel feature selection approach. Our method trains a set of bootstrapped SVR classifiers to detect those features that are informative for the prediction task. This way we obtain a more robust set of selected features compared to other selection methods. This allows us to detect in a multivariate fashion not only the features needed to solve the prediction task, but also those that are informative for the problem at hand. The application of this algorithm to a Weather Research and Forecasting model, and its comparison to some state of the art tools, shows the advantages of the proposed method both in terms of resistance to overfitting, selection consistency and interpretability, while at the same time improving performance in terms of prediction accuracy.","",""
6,"Tianxiang Gao, Sigurdur Olofsson, Songtao Lu","Minimum-volume-regularized weighted symmetric nonnegative matrix factorization for clustering",2016,"","","","",86,"2022-07-13 09:40:40","","10.1109/GlobalSIP.2016.7905841","","",,,,,6,1.00,2,3,6,"In recent years, nonnegative matrix factorization (NMF) attracts much attention in machine learning and signal processing fields due to its interpretability of data in a low dimensional subspace. For clustering problems, symmetric nonnegative matrix factorization (SNMF) as an extension of NMF factorizes the similarity matrix of data points directly and outperforms NMF when dealing with nonlinear data structure. However, the clustering results of SNMF is very sensitive to noisy data. In this paper, we propose a minimum-volume-regularized weighted SNMF (MV-WSNMF) based on the relationship between robust NMF and SNMF. The proposed MV-WSNMF can approximate the similarity matrices flexibly such that the resulting performance is more robust against noise. A computationally efficient algorithm is also proposed with convergence guarantee. The numerical simulation results show the improvement of the proposed algorithm with respective to clustering accuracy in comparison with the state-of-the-art algorithms.","",""
6,"Benjamin Ulfenborg, K. Klinga-Levan, B. Olsson","Classification of Tumor Samples from Expression Data Using Decision Trunks",2013,"","","","",87,"2022-07-13 09:40:40","","10.4137/CIN.S10356","","",,,,,6,0.67,2,3,9,"We present a novel machine learning approach for the classification of cancer samples using expression data. We refer to the method as “decision trunks,” since it is loosely based on decision trees, but contains several modifications designed to achieve an algorithm that: (1) produces smaller and more easily interpretable classifiers than decision trees; (2) is more robust in varying application scenarios; and (3) achieves higher classification accuracy. The decision trunk algorithm has been implemented and tested on 26 classification tasks, covering a wide range of cancer forms, experimental methods, and classification scenarios. This comprehensive evaluation indicates that the proposed algorithm performs at least as well as the current state of the art algorithms in terms of accuracy, while producing classifiers that include on average only 2–3 markers. We suggest that the resulting decision trunks have clear advantages over other classifiers due to their transparency, interpretability, and their correspondence with human decision-making and clinical testing practices.","",""
14,"G. Landrum, J. Penzotti, S. Putta","Feature-map vectors: a new class of informative descriptors for computational drug discovery",2007,"","","","",88,"2022-07-13 09:40:40","","10.1007/s10822-006-9085-8","","",,,,,14,0.93,5,3,15,"","",""
1,"Nicolas Labroche, C. Marsala","Optimization of a fuzzy decision trees forest with artificial ant based clustering",2010,"","","","",89,"2022-07-13 09:40:40","","10.1109/SOCPAR.2010.5686103","","",,,,,1,0.08,1,2,12,"In the recent years, forests of decision trees have seen an increasing interest from the Machine Learning community since they allow to aggregate the decisions from a set of decision trees into one robust answer. However, this approach suffers from two well-known limits: first, their performances depend on the number of trees and thus finding the right size and how to aggregate decisions could be very difficult and second, large forests loose the interpretability capacity of a single decision tree. In this paper, we propose a new approach in which decisions trees from a forest are clustered to simplify the overall decision process while maintaining a large amount of decision trees and to facilitate the interpretation of the results. The preliminary results that are presented in this paper show the effectiveness of our approach.","",""
9,"V. Gavrishchaka","BOOSTING-BASED FRAMEWORK FOR PORTFOLIO STRATEGY DISCOVERY AND OPTIMIZATION",2006,"","","","",90,"2022-07-13 09:40:40","","10.1142/S1793005706000506","","",,,,,9,0.56,9,1,16,"Increasing availability of the multi-scale market data exposes limitations of the existing quantitative models such as low accuracy of the simplified analytical and statistical frameworks as well as insufficient interpretability and stability of the best machine learning algorithms. Boosting was recently proposed as a simple and robust framework for intelligent combination of the clarity and stability of the analytical and parsimonious statistical models with the accuracy of the adaptive data-driven models. Encouraging results of the boosting application to symbolic volatility forecasting have also been reported. However, accurate forecasting does not always warrant optimal decision making that leads to acceptable performance of the portfolio strategy. In this work, a boosting-based framework for a direct trading strategy and portfolio optimization is introduced. Due to inherent adaptive control of the parameter space dimensionality, this technique can work with very large pools of base strategies and financial instruments that are usually prohibitive for other portfolio optimization frameworks. Unlike existing approaches, this framework can be effectively used for the coupled optimization of the portfolio capital/asset allocation and dynamic trading strategies. Generated portfolios of trading strategies not only exhibit stable and robust performance but also remain interpretable. Encouraging preliminary results based on real market data are presented and discussed.","",""
242,"Pantelis Linardatos, Vasilis Papastefanopoulos, S. Kotsiantis","Explainable AI: A Review of Machine Learning Interpretability Methods",2020,"","","","",91,"2022-07-13 09:40:40","","10.3390/e23010018","","",,,,,242,121.00,81,3,2,"Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.","",""
404,"D. V. Carvalho, E. M. Pereira, Jaime S. Cardoso","Machine Learning Interpretability: A Survey on Methods and Metrics",2019,"","","","",92,"2022-07-13 09:40:40","","10.3390/ELECTRONICS8080832","","",,,,,404,134.67,135,3,3,"Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.","",""
162,"Harsha Nori, Samuel Jenkins, Paul Koch, R. Caruana","InterpretML: A Unified Framework for Machine Learning Interpretability",2019,"","","","",93,"2022-07-13 09:40:40","","","","",,,,,162,54.00,41,4,3,"InterpretML is an open-source Python package which exposes machine learning interpretability algorithms to practitioners and researchers. InterpretML exposes two types of interpretability - glassbox models, which are machine learning models designed for interpretability (ex: linear models, rule lists, generalized additive models), and blackbox explainability techniques for explaining existing systems (ex: Partial Dependence, LIME). The package enables practitioners to easily compare interpretability algorithms by exposing multiple methods under a unified API, and by having a built-in, extensible visualization platform. InterpretML also includes the first implementation of the Explainable Boosting Machine, a powerful, interpretable, glassbox model that can be as accurate as many blackbox models. The MIT licensed source code can be downloaded from github.com/microsoft/interpret.","",""
2,"Yousef Sheikhi Garjan, Mehdi Ghaneezabadi","Machine Learning Interpretability Application to Optimize Well Completion in Montney",2020,"","","","",94,"2022-07-13 09:40:40","","10.2118/200019-ms","","",,,,,2,1.00,1,2,2,"Recently machine learning has being extensively deployed for oil and gas industry for improving result and expedite process. However, the black box models do not explain their prediction which considered as a barrier to adopt machine learning. This paper is about optimizing hydraulic fracture with machine learning methods and making informative decision with interpreting machine learning model. The solution can show that it could save over million dollars per well and improve well performance significantly. Interestingly, the machine leaning explainability approach was utilized to explain and measure the reason behind of why some wells are performing better than other and vice versa.Hydraulic fracturing modeling and optimization in tight oil and unconventional reservoir requires substantial geological modeling, fracture design, post-fracture production simulation with excessive sensitivity analysis due to complexity and uncertainty in the nature of data. These types of studies are computationally and monetarily expensive. Furthermore, digital oil technology has facilitated the process of data gathering enabled operators to have access to huge amount of data. Common approaches are no longer suitable to handle this pile of data but machine learning methods could be successfully utilized for this purpose.In this paper, a variety types of advanced machine learning methods including linear regression, Random forest, Gradient Boost, XGBoost, Bagging, ExtraTrees and neural network were employed to optimize well completion in Montney formation. The objective was to create a robust predictive model capturing all the effective operational well parameters (features) capable of optimizing the first 12 months cumulative of equivalent well production.Special Individual Conditional Expectation (ICE) plots and Partial Dependency plots(PDP) were used to depict how HF completion features influence the prediction of a machine learning model. Furthermore, a novel approach was employed to explain the model prediction of an existing well by computing the contribution of each feature to the prediction.Over 1838 hydraulically fractured (HF) wells producing from 2008 till 2019 in Montney formation have been considered for this analysis. The outcome of Explanatory Data Analysis (EDA) revealed that well production performance has not been improved despite of continues enhancement of hydraulic fracture parameters such as proppant injected volume, length of stimulated horizontal wells, and number of stages per well in the course of two years. This finding raises the concern of whether operators are properly optimizing completion design. After comparing all machine learning methods, Random Forest method was chosen as the most appropriate and accurate method to proceed for further analysis. ICE and PDP plots helped to understand the impacts of different fracturing features on production for individual well in addition to define optimum operation features on Montney Formation. Furthermore, quantifying of each feature’s impact on individual well production and linking it to an economic model, we were able to demonstrate potential profit and loss for each well. The model suggests that some wells could have achieved over $1 million extra profit during the first 12-months of production.In this study, not only a reliable predictive data-driven model has been built for hydraulically-fractured wells in Montney formation, but also a comprehensive workflow of sensitivity and explainatability analysis has been introduced to obtain an optimized fit-to-purpose well completion design.","",""
37,"Radwa El Shawi, Youssef Mohamed, M. Al-mallah, S. Sakr","Interpretability in HealthCare A Comparative Study of Local Machine Learning Interpretability Techniques",2019,"","","","",95,"2022-07-13 09:40:40","","10.1109/CBMS.2019.00065","","",,,,,37,12.33,9,4,3,"Although complex machine learning models (e.g., Random Forest, Neural Networks) are commonly outperforming the traditional simple interpretable models (e.g., Linear Regression, Decision Tree), in the healthcare domain, clinicians find it hard to understand and trust these complex models due to the lack of intuition and explanation of their predictions. With the new General Data Protection Regulation (GDPR), the importance for plausibility and verifiability of the predictions made by machine learning models has become essential. To tackle this challenge, recently, several machine learning interpretability techniques have been developed and introduced. In general, the main aim of these interpretability techniques is to shed light and provide insights into the predictions process of the machine learning models and explain how the model predictions have resulted. However, in practice, assessing the quality of the explanations provided by the various interpretability techniques is still questionable. In this paper, we present a comprehensive experimental evaluation of three recent and popular local model agnostic interpretability techniques, namely, LIME, SHAP and Anchors on different types of real-world healthcare data. Our experimental evaluation covers different aspects for its comparison including identity, stability, separability, similarity, execution time and bias detection. The results of our experiments show that LIME achieves the lowest performance for the identity metric and the highest performance for the separability metric across all datasets included in this study. On average, SHAP has the smallest average time to output explanation across all datasets included in this study. For detecting the bias, SHAP enables the participants to better detect the bias.","",""
1,"Tim G. J. Rudner, H. Toner","Key Concepts in AI Safety: Interpretability in Machine Learning",2021,"","","","",96,"2022-07-13 09:40:40","","10.51593/20190042","","",,,,,1,1.00,1,2,1,"This paper is the third installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces interpretability as a means to enable assurance in modern machine learning systems.","",""
3,"A. Heinlein, A. Klawonn, M. Lanser, J. Weber","Combining Machine Learning and Adaptive Coarse Spaces---A Hybrid Approach for Robust FETI-DP Methods in Three Dimensions",2020,"","","","",97,"2022-07-13 09:40:40","","10.1137/20m1344913","","",,,,,3,1.50,1,4,2,"The hybrid ML-FETI-DP algorithm combines the advantages of adaptive coarse spaces in domain decomposition methods and certain supervised machine learning techniques. Adaptive coarse spaces ensure robustness of highly scalable domain decomposition solvers, even for highly heterogeneous coefficient distributions with arbitrary coefficient jumps. However, their construction requires the setup and solution of local generalized eigenvalue problems, which is typically computationally expensive. The idea of ML-FETI-DP is to interpret the coefficient distribution as image data and predict whether an eigenvalue problem has to be solved or can be neglected while still maintaining robustness of the adaptive FETI-DP method. For this purpose, neural networks are used as image classifiers. In the present work, the ML-FETI-DP algorithm is extended to three dimensions, which requires both a complex data preprocessing procedure to construct consistent input data for the neural network as well as a representative training and validation data set to ensure generalization properties of the machine learning model. Numerical experiments for stationary diffusion and linear elasticity problems with realistic coefficient distributions show that a large number of eigenvalue problems can be saved; in the best case of the numerical results presented here, 97% of the eigenvalue problems can be avoided to be set up and solved.","",""
26,"W. Gou, Chu-wen Ling, Yan He, Zengliang Jiang, Yuanqing Fu, Fengzhe Xu, Z. Miao, Ting-yu Sun, Jie-sheng Lin, Hui-lian Zhu, Hongwei Zhou, Yu-ming Chen, Ju-Sheng Zheng","Interpretable Machine Learning Framework Reveals Robust Gut Microbiome Features Associated With Type 2 Diabetes",2020,"","","","",98,"2022-07-13 09:40:40","","10.2337/dc20-1536","","",,,,,26,13.00,3,13,2,"OBJECTIVE To identify the core gut microbial features associated with type 2 diabetes risk and potential demographic, adiposity, and dietary factors associated with these features. RESEARCH DESIGN AND METHODS We used an interpretable machine learning framework to identify the type 2 diabetes–related gut microbiome features in the cross-sectional analyses of three Chinese cohorts: one discovery cohort (n = 1,832, 270 cases of type 2 diabetes) and two validation cohorts (cohort 1: n = 203, 48 cases; cohort 2: n = 7,009, 608 cases). We constructed a microbiome risk score (MRS) with the identified features. We examined the prospective association of the MRS with glucose increment in 249 participants without type 2 diabetes and assessed the correlation between the MRS and host blood metabolites (n = 1,016). We transferred human fecal samples with different MRS levels to germ-free mice to confirm the MRS–type 2 diabetes relationship. We then examined the prospective association of demographic, adiposity, and dietary factors with the MRS (n = 1,832). RESULTS The MRS (including 14 microbial features) consistently associated with type 2 diabetes, with risk ratio for per 1-unit change in MRS 1.28 (95% CI 1.23–1.33), 1.23 (1.13–1.34), and 1.12 (1.06–1.18) across three cohorts. The MRS was positively associated with future glucose increment (P < 0.05) and was correlated with a variety of gut microbiota–derived blood metabolites. Animal study further confirmed the MRS–type 2 diabetes relationship. Body fat distribution was found to be a key factor modulating the gut microbiome–type 2 diabetes relationship. CONCLUSIONS Our results reveal a core set of gut microbiome features associated with type 2 diabetes risk and future glucose increment.","",""
34,"Muhammad Abdullah Hanif, Faiq Khalid, Rachmad Vidya Wicaksana Putra, Semeen Rehman, M. Shafique","Robust Machine Learning Systems: Reliability and Security for Deep Neural Networks",2018,"","","","",99,"2022-07-13 09:40:40","","10.1109/IOLTS.2018.8474192","","",,,,,34,8.50,7,5,4,"Machine learning is commonly being used in almost all the areas that involve advanced data analytics and intelligent control. From applications like Natural Language Processing (NLP) to autonomous driving are based upon machine learning algorithms. An increasing trend is observed in the use of Deep Neural Networks (DNNs) for such applications. While the slight inaccuracy in applications like NLP does not have any severe consequences, it is not the same for other safety-critical applications, like autonomous driving and smart healthcare, where a small error can lead to catastrophic effects. Apart from high-accuracy DNN algorithms, there is a significant need for robust machine learning systems and hardware architectures that can generate reliable and trustworthy results in the presence of hardware-level faults while also preserving security and privacy. This paper provides an overview of the challenges being faced in ensuring reliable and secure execution of DNNs. To address the challenges, we present several techniques for analyzing and mitigating the reliability and security threats in machine learning systems.","",""
189,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Auto-sklearn: Efficient and Robust Automated Machine Learning",2019,"","","","",100,"2022-07-13 09:40:40","","10.1007/978-3-030-05318-5_6","","",,,,,189,63.00,32,6,3,"","",""
6,"D. Raimondi, J. Simm, A. Arany, P. Fariselli, I. Cleynen, Y. Moreau","An interpretable low-complexity machine learning framework for robust exome-based in-silico diagnosis of Crohn’s disease patients",2020,"","","","",101,"2022-07-13 09:40:40","","10.1093/nargab/lqaa011","","",,,,,6,3.00,1,6,2,"Abstract Whole exome sequencing (WES) data are allowing researchers to pinpoint the causes of many Mendelian disorders. In time, sequencing data will be crucial to solve the genome interpretation puzzle, which aims at uncovering the genotype-to-phenotype relationship, but for the moment many conceptual and technical problems need to be addressed. In particular, very few attempts at the in-silico diagnosis of oligo-to-polygenic disorders have been made so far, due to the complexity of the challenge, the relative scarcity of the data and issues such as batch effects and data heterogeneity, which are confounder factors for machine learning (ML) methods. Here, we propose a method for the exome-based in-silico diagnosis of Crohn’s disease (CD) patients which addresses many of the current methodological issues. First, we devise a rational ML-friendly feature representation for WES data based on the gene mutational burden concept, which is suitable for small sample sizes datasets. Second, we propose a Neural Network (NN) with parameter tying and heavy regularization, in order to limit its complexity and thus the risk of over-fitting. We trained and tested our NN on 3 CD case-controls datasets, comparing the performance with the participants of previous CAGI challenges. We show that, notwithstanding the limited NN complexity, it outperforms the previous approaches. Moreover, we interpret the NN predictions by analyzing the learned patterns at the variant and gene level and investigating the decision process leading to each prediction.","",""
932,"Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael A. Specter, Lalana Kagal","Explaining Explanations: An Overview of Interpretability of Machine Learning",2018,"","","","",102,"2022-07-13 09:40:40","","10.1109/DSAA.2018.00018","","",,,,,932,233.00,155,6,4,"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.","",""
62,"Philipp Schmidt, F. Biessmann","Quantifying Interpretability and Trust in Machine Learning Systems",2019,"","","","",103,"2022-07-13 09:40:40","","","","",,,,,62,20.67,31,2,3,"Decisions by Machine Learning (ML) models have become ubiquitous. Trusting these decisions requires understanding how algorithms take them. Hence interpretability methods for ML are an active focus of research. A central problem in this context is that both the quality of interpretability methods as well as trust in ML predictions are difficult to measure. Yet evaluations, comparisons and improvements of trust and interpretability require quantifiable measures. Here we propose a quantitative measure for the quality of interpretability methods. Based on that we derive a quantitative measure of trust in ML decisions. Building on previous work we propose to measure intuitive understanding of algorithmic decisions using the information transfer rate at which humans replicate ML model predictions. We provide empirical evidence from crowdsourcing experiments that the proposed metric robustly differentiates interpretability methods. The proposed metric also demonstrates the value of interpretability for ML assisted human decision making: in our experiments providing explanations more than doubled productivity in annotation tasks. However unbiased human judgement is critical for doctors, judges, policy makers and others. Here we derive a trust metric that identifies when human decisions are overly biased towards ML predictions. Our results complement existing qualitative work on trust and interpretability by quantifiable measures that can serve as objectives for further improving methods in this field of research.","",""
0,"Qiming Wu","A robust audio-based symbol recognition system using machine learning techniques",2020,"","","","",104,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,1,2,"This research investigates the creation of an audio-shape recognition system that is able to interpret a user’s drawn audio shapes—fundamental shapes, digits and/or letters— on a given surface such as a table-top using a generic stylus such as the back of a pen. The system aims to make use of one, two or three Piezo microphones, as required, to capture the sound of the audio gestures, and a combination of the Mel-Frequency Cepstral Coefficients (MFCC) feature descriptor and Support Vector Machines (SVMs) to recognise audio shapes. The novelty of the system is in the use of piezo microphones which are low cost, light-weight and portable, and the main investigation is around determining whether these microphones are able to provide sufficiently rich information to recognise the audio shapes mentioned in such a framework.","",""
0,"M. Barandas, Duarte Folgado, Ricardo Santos, Raquel Simão, Hugo Gamboa","Uncertainty-Based Rejection in Machine Learning: Implications for Model Development and Interpretability",2022,"","","","",105,"2022-07-13 09:40:40","","10.3390/electronics11030396","","",,,,,0,0.00,0,5,1,"Uncertainty is present in every single prediction of Machine Learning (ML) models. Uncertainty Quantification (UQ) is arguably relevant, in particular for safety-critical applications. Prior research focused on the development of methods to quantify uncertainty; however, less attention has been given to how to leverage the knowledge of uncertainty in the process of model development. This work focused on applying UQ into practice, closing the gap of its utility in the ML pipeline and giving insights into how UQ is used to improve model development and its interpretability. We identified three main research questions: (1) How can UQ contribute to choosing the most suitable model for a given classification task? (2) Can UQ be used to combine different models in a principled manner? (3) Can visualization techniques improve UQ’s interpretability? These questions are answered by applying several methods to quantify uncertainty in both a simulated dataset and a real-world dataset of Human Activity Recognition (HAR). Our results showed that uncertainty quantification can increase model robustness and interpretability.","",""
2,"J. Sarkar, Cory Peterson","Operational Workload Impact on Robust Solid-State Storage Analyzed with Interpretable Machine Learning",2019,"","","","",106,"2022-07-13 09:40:40","","10.1109/IRPS.2019.8720510","","",,,,,2,0.67,1,2,3,"Solid-state storage technology is finding increasing adoption in enterprise and data center environments due to their high reliability and reducing cost. With high performance solid-state storage devices (SSDs) internally designed as distributed resilient systems, their operational behavior under materially different workloads is described in this research. Application of interpretable machine learning on internal parametric data of SSDs enables insights on workloads' interaction with the resilient system design. After prior research demonstrated significantly different accelerated workload stress, the analysis on resilience of the SSDs under random vs. pseudo-sequential workloads emphasize the efficacy and importance of their distributed resilience schemes. As such, these results provide causational insights on the mechanism of differential stress of the workloads impacting the resilience design principles. Moreover, the results elucidate guidelines strongly relevant from design robustness perspective for research on novel SSD architectures such as the proposed Open Channel SSD, towards deployment in hyperscale and virtualization environments.","",""
0,"Jianbo Chen","Towards Interpretability and Robustness of Machine Learning Models",2019,"","","","",107,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,1,3,"Author(s): Chen, Jianbo | Advisor(s): Jordan, Michael I; Wainwright, Martin J | Abstract: Modern machine learning models can be difficult to probe and understand after they have been trained. This is a major problem for the field, with consequences for trustworthiness, diagnostics, debugging, robustness, and a range of other engineering and human interaction issues surrounding the deployment of a model. Another problem of modern machine learning models is their vulnerability to small adversarial perturbations to the input, which incurs a security risk when they are applied to critical areas.In this thesis, we develop systematic and efficient tools for interpreting machine learning models and evaluating their adversarial robustness. Part I focuses on model interpretation. We derive an efficient feature scoring method by exploiting the graph structure in data. We also develop a learning-based method under an information-based framework. As an attempt to leverage prior knowledge about what constitutes a satisfying interpretation in a given domain, we propose a systematic approach to exploiting syntactic constituency structure by leveraging a parse tree for interpretation of models in the setting of linguistic data. Part II focuses on the evaluation of adversarial robustness. We first propose a probabilistic framework for generating adversarial examples on discrete data, and develop two algorithms to implement it. We also introduce a novel attack method in the setting where the attacker has access to model decisions alone. We investigate the robustness of various machine learning models and existing defense mechanisms under the proposed attack method. In Part III, we build a connection between the two fields by developing a method for detecting adversarial examples via tools in model interpretation.","",""
0,"Joseph Giorgio, W. Jagust, S. Baker, S. Landau, P. Tiňo, Z. Kourtzi","A robust and interpretable machine learning approach using multimodal biological data to predict future pathological tau accumulation",2022,"","","","",108,"2022-07-13 09:40:40","","10.1038/s41467-022-28795-7","","",,,,,0,0.00,0,6,1,"","",""
0,"J. Sarkar, Cory Peterson","Enabling Prognostics of Robust Design with Interpretable Machine Learning",2019,"","","","",109,"2022-07-13 09:40:40","","10.1109/IEDM19573.2019.8993481","","",,,,,0,0.00,0,2,3,"Design of robust systems needs to fully account for reliability physics, operational stresses and interactions thereof - while accommodating range of stresses from qualification to field. This research demonstrates the method of empirically analyzing system-internal parametric data of Solid-State Storage devices (SSD) with Machine Learning (ML). ML is shown to be a necessary, effective and novel means of proactively assessing and interpreting prognostics of the resilient system design. The methodologies and results also bear strong relevance to assessment of current and future designs for evolving usage models and new application areas.","",""
0,"Pongpisit Thanasutives, Takeshi Morita, M. Numao, Ken-ichi Fukui","Noise-aware Physics-informed Machine Learning for Robust PDE Discovery",2022,"","","","",110,"2022-07-13 09:40:40","","10.48550/arXiv.2206.12901","","",,,,,0,0.00,0,4,1,"—This work is concerned with discovering the gov- erning partial differential equation (PDE) of a physical system. Existing methods have demonstrated the PDE identiﬁcation from ﬁnite observations but failed to maintain satisfying performance against noisy data, partly owing to suboptimal estimated deriva- tives and found PDE coefﬁcients. We address the issues by introducing a noise-aware physics-informed machine learning (nPIML) framework to discover the governing PDE from data following arbitrary distributions. Our proposals are twofold. First, we propose a couple of neural networks, namely solver and preselector, which yield an interpretable neural representation of the hidden physical constraint. After they are jointly trained, the solver network approximates potential candidates, e.g., partial derivatives, which are then fed to the sparse regression algorithm that initially unveils the most likely parsimonious PDE, decided according to the information criterion. Second, we propose the denoising physics-informed neural networks (dPINNs), based on Discrete Fourier Transform (DFT), to deliver a set of the optimal ﬁnetuned PDE coefﬁcients respecting the noise-reduced variables. The denoising PINNs’ structures are compartmentalized into forefront projection networks and a PINN, by which the formerly learned solver initializes. Our extensive experiments on ﬁve canonical PDEs afﬁrm that the proposed framework presents a robust and interpretable approach for PDE discovery, applicable to a wide range of systems, possibly complicated by noise. and model while suppressing paved towards the applications of interpretable artiﬁcial intelligence (AI) to enhance the understandability of physical sciences.","",""
439,"Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin","Model-Agnostic Interpretability of Machine Learning",2016,"","","","",111,"2022-07-13 09:40:40","","","","",,,,,439,73.17,146,3,6,"Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.","",""
4,"Kenji Suzuki, M. Reyes, T. Syeda-Mahmood","Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support: Second International Workshop, iMIMIC 2019, and 9th International Workshop, ML-CDS 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 17, 2019, Proceedings",2019,"","","","",112,"2022-07-13 09:40:40","","10.1007/978-3-030-33850-3","","",,,,,4,1.33,1,3,3,"","",""
14,"Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, D. Dou","Interpretable Deep Learning: Interpretations, Interpretability, Trustworthiness, and Beyond",2021,"","","","",113,"2022-07-13 09:40:40","","","","",,,,,14,14.00,2,8,1,"Deep neural networks have been well-known for their superb performance in handling various machine learning and artificial intelligence tasks. However, due to their over-parameterized black-box nature, it is often difficult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal the ways that deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Specifically, we introduce and clarify two basic concepts— interpretations and interpretability—that people usually get confused. First of all, to address the research efforts in interpretations, we elaborate the design of several recent interpretation algorithms, from different perspectives, through proposing a new taxonomy. Then, to understand the results of interpretation, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the existing work in evaluating models’ interpretability using “trustworthy” interpretation algorithms. Finally, we review and discuss the connections between deep models’ interpretations and other factors, such as adversarial robustness and data augmentations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches.","",""
6,"S. Kalinin, K. Roccapriore, S. Cho, D. Milliron, R. Vasudevan, M. Ziatdinov, J. Hachtel","Separating Physically Distinct Mechanisms in Complex Infrared Plasmonic Nanostructures via Machine Learning Enhanced Electron Energy Loss Spectroscopy",2020,"","","","",114,"2022-07-13 09:40:40","","10.1002/adom.202001808","","",,,,,6,3.00,1,7,2,"Electron energy loss spectroscopy (EELS) enables direct exploration of plasmonic phenomena at the nanometer level. To isolate individual plasmon modes, linear unmixing methods can be used to separate different physical mechanisms, but in larger and more complex systems the interpretability of the components becomes uncertain. Here, infrared plasmonic resonances in self‐assembled heterogeneous monolayer films of doped‐semiconductor nanoparticles are examined beyond linear unmixing techniques, and both supervised and unsupervised machine‐learning‐based analyses of hyperspectral EELS datasets are demonstrated. In the supervised approach, a human operator labels a small number of pixels in the hyperspectral dataset corresponding to features of interest which are then propagated across the entire dataset. In the unsupervised approach, non‐linear autoencoders are used to create a highly‐reduced latent‐space representation of the dataset, within which insight into the relevant physics can be gleaned from straightforward distance metrics that do not depend on operator input and bias. The advantage of these approaches is that the labeling separates physical mechanisms without altering the data, enabling robust analyses of the influence of heterogeneities in mesoscale complex systems.","",""
7,"E. Rozos, P. Dimitriadis, V. Bellos","Machine Learning in Assessing the Performance of Hydrological Models",2021,"","","","",115,"2022-07-13 09:40:40","","10.3390/hydrology9010005","","",,,,,7,7.00,2,3,1,"Machine learning has been employed successfully as a tool virtually in every scientific and technological field. In hydrology, machine learning models first appeared as simple feed-forward networks that were used for short-term forecasting, and have evolved into complex models that can take into account even the static features of catchments, imitating the hydrological experience. Recent studies have found machine learning models to be robust and efficient, frequently outperforming the standard hydrological models (both conceptual and physically based). However, and despite some recent efforts, the results of the machine learning models require significant effort to interpret and derive inferences. Furthermore, all successful applications of machine learning in hydrology are based on networks of fairly complex topology that require significant computational power and CPU time to train. For these reasons, the value of the standard hydrological models remains indisputable. In this study, we suggest employing machine learning models not as a substitute for hydrological models, but as an independent tool to assess their performance. We argue that this approach can help to unveil the anomalies in catchment data that do not fit in the employed hydrological model structure or configuration, and to deal with them without compromising the understanding of the underlying physical processes.","",""
6,"Matthew Norton, Akiko Takeda, Alexander Mafusalov","Optimistic Robust Optimization With Applications To Machine Learning",2017,"","","","",116,"2022-07-13 09:40:40","","","","",,,,,6,1.20,2,3,5,"Robust Optimization has traditionally taken a pessimistic, or worst-case viewpoint of uncertainty which is motivated by a desire to find sets of optimal policies that maintain feasibility under a variety of operating conditions. In this paper, we explore an optimistic, or best-case view of uncertainty and show that it can be a fruitful approach. We show that these techniques can be used to address a wide variety of problems. First, we apply our methods in the context of robust linear programming, providing a method for reducing conservatism in intuitive ways that encode economically realistic modeling assumptions. Second, we look at problems in machine learning and find that this approach is strongly connected to the existing literature. Specifically, we provide a new interpretation for popular sparsity inducing non-convex regularization schemes. Additionally, we show that successful approaches for dealing with outliers and noise can be interpreted as optimistic robust optimization problems. Although many of the problems resulting from our approach are non-convex, we find that DCA or DCA-like optimization approaches can be intuitive and efficient.","",""
5,"Francesco Regazzoni, D. Chapelle, P. Moireau","Combining data assimilation and machine learning to build data‐driven models for unknown long time dynamics—Applications in cardiovascular modeling",2021,"","","","",117,"2022-07-13 09:40:40","","10.1002/cnm.3471","","",,,,,5,5.00,2,3,1,"We propose a method to discover differential equations describing the long‐term dynamics of phenomena featuring a multiscale behavior in time, starting from measurements taken at the fast‐scale. Our methodology is based on a synergetic combination of data assimilation (DA), used to estimate the parameters associated with the known fast‐scale dynamics, and machine learning (ML), used to infer the laws underlying the slow‐scale dynamics. Specifically, by exploiting the scale separation between the fast and the slow dynamics, we propose a decoupling of time scales that allows to drastically lower the computational burden. Then, we propose a ML algorithm that learns a parametric mathematical model from a collection of time series coming from the phenomenon to be modeled. Moreover, we study the interpretability of the data‐driven models obtained within the black‐box learning framework proposed in this paper. In particular, we show that every model can be rewritten in infinitely many different equivalent ways, thus making intrinsically ill‐posed the problem of learning a parametric differential equation starting from time series. Hence, we propose a strategy that allows to select a unique representative model in each equivalence class, thus enhancing the interpretability of the results. We demonstrate the effectiveness and noise‐robustness of the proposed methods through several test cases, in which we reconstruct several differential models starting from time series generated through the models themselves. Finally, we show the results obtained for a test case in the cardiovascular modeling context, which sheds light on a promising field of application of the proposed methods.","",""
4,"Alexandra Renouard, A. Maggi, M. Grunberg, C. Doubre, C. Hibert","Toward False Event Detection and Quarry Blast versus Earthquake Discrimination in an Operational Setting Using Semiautomated Machine Learning",2021,"","","","",118,"2022-07-13 09:40:40","","10.1785/0220200305","","",,,,,4,4.00,1,5,1,"  Small-magnitude earthquakes shed light on the spatial and magnitude distribution of natural seismicity, as well as its rate and occurrence, especially in stable continental regions where natural seismicity remains difficult to explain under slow strain-rate conditions. However, capturing them in catalogs is strongly hindered by signal-to-noise ratio issues, resulting in high rates of false and man-made events also being detected. Accurate and robust discrimination of these events is critical for optimally detecting small earthquakes. This requires uncovering recurrent salient features that can rapidly distinguish first false events from real events, then earthquakes from man-made events (mainly quarry blasts), despite high signal variability and noise content. In this study, we combined the complementary strengths of human and interpretable rule-based machine-learning algorithms for solving this classification problem. We used human expert knowledge to co-create two reliable machine-learning classifiers through human-assisted selection of classification features and review of events with uncertain classifier predictions. The two classifiers are integrated into the SeisComP3 operational monitoring system. The first one discards false events from the set of events obtained with a low short-term average/long-term average threshold; the second one labels the remaining events as either earthquakes or quarry blasts. When run in an operational setting, the first classifier correctly detected more than 99% of false events and just over 93% of earthquakes; the second classifier correctly labeled 95% of quarry blasts and 96% of earthquakes. After a manual review of the second classifier low-confidence outputs, the final catalog contained fewer than 2% of misclassified events. These results confirm that machine learning strengthens the quality of earthquake catalogs and that the performance of machine-learning classifiers can be improved through human expertise. Our study promotes a broader implication of hybrid intelligence monitoring within seismological observatories.","",""
4,"A. Mei, I. Milosavljevic, A. L. Simpson, Valerie A. Smetanka, Colin P. Feeney, Shay M. Seguin, S. D. Ha, W. Ha, M. Reed","Optimization of quantum-dot qubit fabrication via machine learning",2020,"","","","",119,"2022-07-13 09:40:40","","10.1063/5.0040967","","",,,,,4,2.00,0,9,2,"Precise nanofabrication represents a critical challenge to developing semiconductor quantum-dot qubits for practical quantum computation. Here, we design and train a convolutional neural network to interpret in-line scanning electron micrographs and quantify qualitative features affecting device functionality. The high-throughput strategy is exemplified by optimizing a model lithographic process within a five-dimensional design space and by demonstrating a new approach to address lithographic proximity effects. The present results emphasize the benefits of machine learning for developing robust processes, shortening development cycles, and enforcing quality control during qubit fabrication.","",""
4,"A. Serban, Joost Visser","An Empirical Study of Software Architecture for Machine Learning",2021,"","","","",120,"2022-07-13 09:40:40","","","","",,,,,4,4.00,2,2,1,"Specific developmental and operational characteristics of machine learning (ML) components, as well as their inherent uncertainty, demand robust engineering principles are used to ensure their quality. We aim to determine how software systems can be (re-) architected to enable robust integration of ML components. Towards this goal, we conducted a mixed-methods empirical study consisting of (i) a systematic literature review to identify the challenges and their solutions in software architecture for ML, (ii) semi-structured interviews with practitioners to qualitatively complement the initial findings, and (iii) a survey to quantitatively validate the challenges and their solutions. In total, we compiled and validated twenty challenges and solutions for (re-) architecting systems with ML components. Our results indicate, for example, that traditional software architecture challenges (e.g., component coupling) also play an important role when using ML components; along new ML specific challenges (e.g., the need for continuous retraining). Moreover, the results indicate that ML heightened decision drivers, such as privacy, play a marginal role compared to traditional decision drivers, such as scalability or interoperability. Using the survey, we were able to establish a link between architectural solutions and software quality attributes; which enabled us to provide twenty architectural tactics used for satisfying individual quality requirements of systems with ML components. Altogether, the results can be interpreted as an empirical framework that supports the process of (re-) architecting software systems with ML components.","",""
1,"Khansa Rasheed, A. Qayyum, M. Ghaly, A. Al-Fuqaha, A. Razi, Junaid Qadir","Explainable, Trustworthy, and Ethical Machine Learning for Healthcare: A Survey",2021,"","","","",121,"2022-07-13 09:40:40","","10.36227/TECHRXIV.14376179.V1","","",,,,,1,1.00,0,6,1,"With the advent of machine learning (ML) applications in daily life, the questions about liability, trust, and interpretability of their outputs are raising, especially for healthcare applications. The black-box nature of ML models is a roadblock for clinical utilization. Therefore, to gain the trust of clinicians and patients, researchers need to provide explanations of how and why the model is making a specific decision. With the promise of enhancing the trust and transparency of black-box models, researchers are in the phase of maturing the field of eXplainable ML (XML). In this paper, we provide a comprehensive review of explainable and interpretable ML techniques implemented for providing the reasons behind their decisions for various healthcare applications. Along with highlighting various security, safety, and robustness challenges that hinder the trustworthiness of ML we also discussed the ethical issues of healthcare ML and describe how explainable and trustworthy ML can resolve these ethical problems. Finally, we elaborate on the limitations of existing approaches and highlight various open research problems that require further development.","",""
3,"Tao Zhong, Zian Zhuang, Xiao-fei Dong, Ka-hing Wong, W. Wong, Jian Wang, D. He, Shengyuan Liu","Predicting Antituberculosis Drug–Induced Liver Injury Using an Interpretable Machine Learning Method: Model Development and Validation Study",2021,"","","","",122,"2022-07-13 09:40:40","","10.2196/29226","","",,,,,3,3.00,0,8,1,"Background Tuberculosis (TB) is a pandemic, being one of the top 10 causes of death and the main cause of death from a single source of infection. Drug-induced liver injury (DILI) is the most common and serious side effect during the treatment of TB. Objective We aim to predict the status of liver injury in patients with TB at the clinical treatment stage. Methods We designed an interpretable prediction model based on the XGBoost algorithm and identified the most robust and meaningful predictors of the risk of TB-DILI on the basis of clinical data extracted from the Hospital Information System of Shenzhen Nanshan Center for Chronic Disease Control from 2014 to 2019. Results In total, 757 patients were included, and 287 (38%) had developed TB-DILI. Based on values of relative importance and area under the receiver operating characteristic curve, machine learning tools selected patients’ most recent alanine transaminase levels, average rate of change of patients’ last 2 measures of alanine transaminase levels, cumulative dose of pyrazinamide, and cumulative dose of ethambutol as the best predictors for assessing the risk of TB-DILI. In the validation data set, the model had a precision of 90%, recall of 74%, classification accuracy of 76%, and balanced error rate of 77% in predicting cases of TB-DILI. The area under the receiver operating characteristic curve score upon 10-fold cross-validation was 0.912 (95% CI 0.890-0.935). In addition, the model provided warnings of high risk for patients in advance of DILI onset for a median of 15 (IQR 7.3-27.5) days. Conclusions Our model shows high accuracy and interpretability in predicting cases of TB-DILI, which can provide useful information to clinicians to adjust the medication regimen and avoid more serious liver injury in patients.","",""
3,"A. Ounajim, M. Billot, L. Goudman, P. Louis, Y. Slaoui, M. Roulaud, B. Bouche, P. Page, B. Lorgeoux, Sandrine Baron, Nihel Adjali, K. Nivole, Nicolas Naiditch, Chantal Wood, Raphaël Rigoard, R. David, M. Moens, P. Rigoard","Machine Learning Algorithms Provide Greater Prediction of Response to SCS Than Lead Screening Trial: A Predictive AI-Based Multicenter Study",2021,"","","","",123,"2022-07-13 09:40:40","","10.3390/jcm10204764","","",,,,,3,3.00,0,18,1,"Persistent pain after spinal surgery can be successfully addressed by spinal cord stimulation (SCS). International guidelines strongly recommend that a lead trial be performed before any permanent implantation. Recent clinical data highlight some major limitations of this approach. First, it appears that patient outco mes, with or without lead trial, are similar. In contrast, during trialing, infection rate drops drastically within time and can compromise the therapy. Using composite pain assessment experience and previous research, we hypothesized that machine learning models could be robust screening tools and reliable predictors of long-term SCS efficacy. We developed several algorithms including logistic regression, regularized logistic regression (RLR), naive Bayes classifier, artificial neural networks, random forest and gradient-boosted trees to test this hypothesis and to perform internal and external validations, the objective being to confront model predictions with lead trial results using a 1-year composite outcome from 103 patients. While almost all models have demonstrated superiority on lead trialing, the RLR model appears to represent the best compromise between complexity and interpretability in the prediction of SCS efficacy. These results underscore the need to use AI-based predictive medicine, as a synergistic mathematical approach, aimed at helping implanters to optimize their clinical choices on daily practice.","",""
3,"R. Barker, S. Barker, M. Cracknell, Elizabeth D. Stock, G. Holmes","Quantitative Mineral Mapping of Drill Core Surfaces II: Long-Wave Infrared Mineral Characterization Using μXRF and Machine Learning",2021,"","","","",124,"2022-07-13 09:40:40","","10.5382/econgeo.4804","","",,,,,3,3.00,1,5,1,"Long-wave infrared (LWIR) spectra can be interpreted using a Random Forest machine learning approach to predict mineral species and abundances. In this study, hydrothermally altered carbonate rock core samples from the Fourmile Carlin-type Au discovery, Nevada, were analyzed by LWIR and micro-X-ray fluorescence (μXRF). Linear programming-derived mineral abundances from quantified μXRF data were used as training data to construct a series of Random Forest regression models. The LWIR Random Forest models produced mineral proportion estimates with root mean square errors of 1.17 to 6.75% (model predictions) and 1.06 to 6.19% (compared to quantitative X-ray diffraction data) for calcite, dolomite, kaolinite, white mica, phlogopite, K-feldspar, and quartz. These results are comparable to the error of proportion estimates from linear spectral deconvolution (±7–15%), a commonly used spectral unmixing technique. Having a mineralogical and chemical training data set makes it possible to identify and quantify mineralogy and provides a more robust and meaningful LWIR spectral interpretation than current methods of utilizing a spectral library or spectral end-member extraction. Using the method presented here, LWIR spectroscopy can be used to overcome the limitations inherent with the use of short-wave infrared (SWIR) in fine-grained, low reflectance rocks. This new approach can be applied to any deposit type, improving the accuracy and speed of infrared data interpretation.","",""
1,"Ana Kostovska, Matej Petković, Tomaz Stepisnik, L. Lucas, T. Finn, José Antonio Martinez Heras, P. Panov, S. Džeroski, A. Donati, N. Simidjievski, D. Kocev","GalaxAI: Machine learning toolbox for interpretable analysis of spacecraft telemetry data",2021,"","","","",125,"2022-07-13 09:40:40","","10.1109/smc-it51442.2021.00013","","",,,,,1,1.00,0,11,1,"We present GalaxAI - a versatile machine learning toolbox for efficient and interpretable end-to-end analysis of spacecraft telemetry data. GalaxAI employs various machine learning algorithms for multivariate time series analyses, classification, regression and structured output prediction, capable of handling high-throughput heterogeneous data. These methods allow for the construction of robust and accurate predictive models, that are in turn applied to different tasks of spacecraft monitoring and operations planning. More importantly, besides the accurate building of models, GalaxAI implements a visualisation layer, providing mission specialists and operators with a full, detailed and interpretable view of the data analysis process. We show the utility and versatility of GalaxAI on two use-cases concerning two different spacecraft: i) analysis and planning of Mars Express thermal power consumption and ii) predicting of INTEGRAL’s crossings through Van Allen belts.","",""
2,"D. Devakumar, Goutham Sunny, B. Sasidharan, S. Bowen, Ambily Nadaraj, L. Jeyseelan, Manu Mathew, A. Irodi, R. Isiah, S. Pavamani, S. John, H. T. T Thomas","Framework for Machine Learning of CT and PET Radiomics to Predict Local Failure after Radiotherapy in Locally Advanced Head and Neck Cancers",2021,"","","","",126,"2022-07-13 09:40:40","","10.4103/jmp.JMP_6_21","","",,,,,2,2.00,0,12,1,"Context: Cancer Radiomics is an emerging field in medical imaging and refers to the process of converting routine radiological images that are typically qualitatively interpreted to quantifiable descriptions of the tumor phenotypes and when combined with statistical analytics can improve the accuracy of clinical outcome prediction models. However, to understand the radiomic features and their correlation to molecular changes in the tumor, first, there is a need for the development of robust image analysis methods, software tools and statistical prediction models which is often limited in low- and middle-income countries (LMIC). Aims: The aim is to build a framework for machine learning of radiomic features of planning computed tomography (CT) and positron emission tomography (PET) using open source radiomics and data analytics platforms to make it widely accessible to clinical groups. The framework is tested in a small cohort to predict local disease failure following radiation treatment for head-and-neck cancer (HNC). The predictors were also compared with the existing Aerts HNC radiomics signature. Settings and Design: Retrospective analysis of patients with locally advanced HNC between 2017 and 2018 and 31 patients with both pre- and post-radiation CT and evaluation PET were selected. Subjects and Methods: Tumor volumes were delineated on baseline PET using the semi-automatic adaptive-threshold algorithm and propagated to CT; PyRadiomics features (total of 110 under shape/intensity/texture classes) were extracted. Two feature-selection methods were tested for model stability. Models were built based on least absolute shrinkage and selection operator-logistic and Ridge regression of the top pretreatment radiomic features and compared to Aerts' HNC-signature. Average model performance across all internal validation test folds was summarized by the area under the receiver operator curve (ROC). Results: Both feature selection methods selected CT features MCC (GLCM), SumEntropy (GLCM) and Sphericity (Shape) that could predict the binary failure status in the cross-validated group and achieved an AUC >0.7. However, models using Aerts' signature features (Energy, Compactness, GLRLM-GrayLevelNonUniformity and GrayLevelNonUniformity-HLH wavelet) could not achieve a clear separation between outcomes (AUC = 0.51–0.54). Conclusions: Radiomics pipeline included open-source workflows which makes it adoptable in LMIC countries. Additional independent validation of data is crucial for the implementation of radiomic models for clinical risk stratification.","",""
2,"S. Newman, R. Furbank","Explainable machine learning models of major crop traits from satellite-monitored continent-wide field trial data",2021,"","","","",127,"2022-07-13 09:40:40","","10.1101/2021.03.08.434495","","",,,,,2,2.00,1,2,1,"Four species of grass generate half of all human-consumed calories1. However, abundant biological data on species that produce our food remains largely inaccessible, imposing direct barriers to understanding crop yield and fitness traits. Here, we assemble and analyse a continent-wide database of field experiments spanning ten years and hundreds of thousands of machine-phenotyped populations of ten major crop species. Training an ensemble of machine learning models, using thousands of variables capturing weather, ground-sensor, soil, chemical and fertiliser dosage, management, and satellite data, produces robust cross-continent yield models exceeding R2 = 0.8 prediction accuracy. In contrast to ‘black box’ analytics, detailed interrogation of these models reveals fundamental drivers of crop behaviour and complex interactions predicting yield and agronomic traits. These results demonstrate the capacity of machine learning models to build unified, interpretable, and explainable models of crop behaviour, and highlight the powerful role of data in the future of food.","",""
1,"Nicola Loi, C. Borile, Daniele Ucci","Towards an Automated Pipeline for Detecting and Classifying Malware through Machine Learning",2021,"","","","",128,"2022-07-13 09:40:40","","","","",,,,,1,1.00,0,3,1,"The constant growth in the number of malware software or code fragment potentially harmful for computers and information networks and the use of sophisticated evasion and obfuscation techniques have seriously hindered classic signature-based approaches. On the other hand, malware detection systems based on machine learning techniques started offering a promising alternative to standard approaches, drastically reducing analysis time and turning out to be more robust against evasion and obfuscation techniques. In this paper, we propose a malware taxonomic classification pipeline able to classify Windows Portable Executable files (PEs). Given an input PE sample, it is first classified as either malicious or benign. If malicious, the pipeline further analyzes it in order to establish its threat type, family, and behavior(s). We tested the proposed pipeline on the open source dataset EMBER, containing approximately 1 million PE samples, analyzed through static analysis. Obtained malware detection results are comparable to other academic works in the current state of art and, in addition, we provide an in-depth classification of malicious samples. Models used in the pipeline provides interpretable results which can help security analysts in better understanding decisions taken by the automated pipeline.","",""
2,"N. Frolov, Muhammad Salman Kabir, V. Maksimenko, A. Hramov","Machine learning evaluates changes in functional connectivity under a prolonged cognitive load.",2021,"","","","",129,"2022-07-13 09:40:40","","10.1063/5.0070493","","",,,,,2,2.00,1,4,1,"One must be aware of the black-box problem by applying machine learning models to analyze high-dimensional neuroimaging data. It is due to a lack of understanding of the internal algorithms or the input features upon which most models make decisions despite outstanding performance in classification, pattern recognition, and prediction. Here, we approach the fundamentally high-dimensional problem of classifying cognitive brain states based on functional connectivity by selecting and interpreting the most relevant input features. Specifically, we consider the alterations in the cortical synchrony under a prolonged cognitive load. Our study highlights the advances of this machine learning method in building a robust classification model and percept-related prestimulus connectivity changes over the conventional trial-averaged statistical analysis.","",""
0,"P. Rasouli, Ingrid Chieh Yu","Explainable Debugger for Black-box Machine Learning Models",2021,"","","","",130,"2022-07-13 09:40:40","","10.1109/IJCNN52387.2021.9533944","","",,,,,0,0.00,0,2,1,"The research around developing methods for debugging and refining Machine Learning (ML) models is still in its infancy. We believe employing tailored tools in the development process can help developers in creating more trustworthy and reliable models. This is particularly essential for creating black-box models such as deep neural networks and random forests, as their opaque decision-making and complex structure prevent detailed investigations. Although many explanation techniques provide interpretability in terms of predictive features for a mispredicted instance, it would be beneficial for a developer to find a partition of the training data that significantly influences the anomaly. Such responsible partitions can be subjected to data visualization and data engineering in the development phase to improve the model's accuracy. In this paper, we propose a systematic debugging framework for the development of ML models that guides the data engineering process using the model's decision boundary. Our approach finds the influential neighborhood of anomalous data points using observation-level feature importance and explains them via a novel quasi-global explanation technique. It is also equipped with a robust global explanation approach to reveal general trends and expose potential biases in the neighborhoods. We demonstrate the efficacy of the devised framework through several experiments on standard data sets and black-box models and propose various guidelines on how the framework's components can be practically useful from a developer's perspective.","",""
0,"Hiroto Mizutani, Masateu Tsunoda, K. Nakasai","How to Enlighten Novice Users on Behavior of Machine Learning Models?",2021,"","","","",131,"2022-07-13 09:40:40","","10.1109/SNPD51163.2021.9704891","","",,,,,0,0.00,0,3,1,"Background: Machine learning models are sometimes embedded in software to implement the required functions. As a result, non-experts in machine learning are becoming familiar with the models. However, the interpretability of the built models is often low in machine learning, such as deep learning, and the recognition process of such models is very different from that of humans. Therefore, it is not easy for novice users, such as end-users and beginners, to anticipate the behavior of models that they will use or build. Aim: We assist novice users to realize an aspect of the behavior of machine learning models relating to robustness intuitively. Method: We formalized and evaluated quiz-based analysis, which is often applied by practitioners to test the robustness of machine learning models arbitrarily. To generate test cases of the models, the analysis converts images towards the boundary of classification for both machine learning and humans. It can be regarded as a type of boundary value analysis of software development. Results: In the experiment, we evaluated whether the analysis quantitatively clarified the aspects of the models. The analysis clarified the robustness of the model for image conversion and misclassification quantitatively. Conclusion: The analysis is expected to enlighten novice users on the behavior of machine learning models. This may promote behavioral changes in the evaluation of models for novice users.","",""
0,"Aurélien Olivier, C. Hoffmann, A. Mansour, L. Bressollette, Benoit Clement","Survey on machine learning applied to medical image analysis",2021,"","","","",132,"2022-07-13 09:40:40","","10.1109/CISP-BMEI53629.2021.9624442","","",,,,,0,0.00,0,5,1,"This paper presents a selective survey on recent advances in machine learning applied to medical imaging. It aims to highlight both innovations that increase the performance of the models and methods that ensure certainty, interpretability and robustness of the trained models. The paper focuses particularly on new concepts such as attention modules that allow to gather specific features considering global context. Its second main focus is given to domain adaptation methods to enhance model robustness to distribution shifts. Finally, we discuss uncertainty estimation and interpretability methods to evaluate confidence in a trained model.","",""
1,"Jivitesh Sharma, Rohan Kumar Yadav, Ole-Christoffer Granmo, Lei Jiao","Drop Clause: Enhancing Performance, Interpretability and Robustness of the Tsetlin Machine",2021,"","","","",133,"2022-07-13 09:40:40","","","","",,,,,1,1.00,0,4,1,"In this article, we introduce a novel variant of the Tsetlin machine (TM) that randomly drops clauses, the key learning elements of a TM. In effect, TM with drop clause ignores a random selection of the clauses in each epoch, selected according to a predefined probability. In this way, additional stochasticity is introduced in the learning phase of TM. To explore the effects drop clause has on accuracy, training time, interpretability and robustness, we conduct extensive experiments on nine benchmark datasets in natural language processing (NLP) (IMDb, R8, R52, MR and TREC) and image classification (MNIST, Fashion MNIST, CIFAR-10 and CIFAR100). Our proposed model outperforms baseline machine learning algorithms by a wide margin and achieves competitive performance in comparison with recent deep learning model such as BERT and AlexNET-DFA. In brief, we observe up to +10% increase in accuracy and 2× to 4× faster learning compared with standard TM. We further employ the Convolutional TM to document interpretable results on the CIFAR datasets, visualizing how the heatmaps produced by the TM become more interpretable with drop clause. We also evaluate how drop clause affects learning robustness by introducing corruptions and alterations in the image/language test data. Our results show that drop clause makes TM more robust towards such changes1.","",""
0,"V. Yasaswini, P. Reeshika, K. Roy, N. Kalpana, Rajesh Yamparala","Drowsiness Detection Using Machine Learning Algorithms",2021,"","","","",134,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,5,1,": The abstract presents a literature review of driver drowsiness detection based on behavioural measures using machine learning techniques. Faces contain information that can be used to interpret levels of drowsiness. There are many facial features that can be extracted from the face to infer the level of drowsiness. However, the development of a drowsiness detection system that yields reliable and accurate results is a challenging task as it requires accurate and robust algorithms. A wide range of techniques has been examined to detect driver drowsiness in the past. As a result, machine learning techniques which include convolution neural networks in the context of drowsiness detection. Here convolution neural networks performed better than any other techniques.","",""
0,"A. Serban, Joost Visser","Adapting Software Architectures to Machine Learning Challenges",2021,"","","","",135,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,2,1,"Unique developmental and operational characteristics of machine learning (ML) components as well as their inherent uncertainty demand robust engineering principles are used to ensure their quality. We aim to determine how software systems can be (re-) architected to enable robust integration of ML components. Towards this goal, we conducted a mixed-methods empirical study consisting of (i) a systematic literature review to identify the challenges and their solutions in software architecture for ML, (ii) semi-structured interviews with practitioners to qualitatively complement the initial findings and (iii) a survey to quantitatively validate the challenges and their solutions. We compiled and validated twenty challenges and solutions for (re-) architecting systems with ML components. Our results indicate, for example, that traditional software architecture challenges (e.g., component coupling) also play an important role when using ML components; along with new ML specific challenges (e.g., the need for continuous retraining). Moreover, the results indicate that ML heightened decision drivers, such as privacy, play a marginal role compared to traditional decision drivers, such as scalability. Using the survey we were able to establish a link between architectural solutions and software quality attributes, which enabled us to provide twenty architectural tactics used to satisfy individual quality requirements of systems with ML components. Altogether, the results of the study can be interpreted as an empirical framework that supports the process of (re-) architecting software systems with ML components.","",""
0,"M. Ntampaka, Matthew Ho, B. Nord","Building Trustworthy Machine Learning Models for Astronomy",2021,"","","","",136,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,3,1,". Astronomy is entering an era of data-driven discovery, due in part to modern machine learning (ML) techniques enabling powerful new ways to interpret observations. This shift in our scientiﬁc approach requires us to consider whether we can trust the black box. Here, we overview methods for an often-overlooked step in the development of ML models: building community trust in the algorithms. Trust is an essential ingredient not just for creating more robust data analysis techniques, but also for building conﬁdence within the astronomy community to embrace machine learning methods and results.","",""
0,"Kaiyu Yang","1 Machine Learning for Reasoning",2021,"","","","",137,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,1,1,"Reasoning is a core component of human intelligence that machines still struggle with. I do research in the field of artificial intelligence, with the long-term goal of building machines that reason precisely, systematically, in ways that are interpretable and robust to ambiguity in real-world environments. My research advances towards this goal by attempting to combine the complementary strengths of machine learning and symbolic reasoning. My graduate research has focused on developing machine learning models that represent reasoning via symbolic proofs. They show the promise of new learning paradigms that I envision to be more robust, interpretable, and trustworthy for deployment in real-world high-stake applications. Symbolic reasoning is precise and generalizes systematically to unseen scenarios. But it has been restricted to domains amenable to rigid formalization. In contrast, machine learning has the flexibility to handle noisy and ambiguous domains that are hard to formalize. But predominant machine learning models, such as deep neural networks, are notoriously uninterpretable, data-hungry, and incapable of generalizing outside the training data distribution. Integrating the strengths of both approaches is essential for building flexible reasoning machines with precise and systematic generalization. However, due to the discrete nature of symbolic reasoning, such integration may require a radical departure from the predominant paradigm of gradient-based learning. And my research tries to answer what that alternative form of learning might look like.","",""
0,"Tochukwu Idika, Ismail Akturk","Attack-Centric Approach for Evaluating Transferability of Adversarial Samples in Machine Learning Models",2021,"","","","",138,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,2,1,"Transferability of adversarial samples became a serious concern due to their impact on the reliability of machine learning system deployments, as they find their way into many critical applications. Knowing factors that influence transferability of adversarial samples can assist experts to make informed decisions on how to build robust and reliable machine learning systems. The goal of this study is to provide insights on the mechanisms behind the transferability of adversarial samples through an attack-centric approach. This attack-centric perspective interprets how adversarial samples would transfer by assessing the impact of machine learning attacks (that generated them) on a given input dataset. To achieve this goal, we generated adversarial samples using attacker models and transferred these samples to victim models. We analyzed the behavior of adversarial samples on victim models and outlined four factors that can influence the transferability of adversarial samples. Although these factors are not necessarily exhaustive, they provide useful insights to researchers and practitioners of machine learning systems.","",""
2,"J. de Nijs, T. J. Burger, Ronald J. Janssen, S. M. Kia, Daniel P J van Opstal, M. D. de Koning, L. de Haan, Behrooz Z. Agna A. Nico J. Richard Lieuwe Philippe Jurjen  Alizadeh Bartels-Velthuis van Beveren Bruggeman de, B. Alizadeh, A. Bartels-Velthuis, N. V. van Beveren, R. Bruggeman, P. Delespaul, J. Luykx, I. Myin-Germeys, R. Kahn, F. Schirmbeck, C. Simons, T. van Amelsvoort, J. van os, R. van Winkel, W. Cahn, H. Schnack","Individualized prediction of three- and six-year outcomes of psychosis in a longitudinal multicenter study: a machine learning approach",2021,"","","","",139,"2022-07-13 09:40:40","","10.1038/s41537-021-00162-3","","",,,,,2,2.00,0,23,1,"","",""
25,"Nastaran Meftahi, M. Klymenko, A. Christofferson, U. Bach, D. Winkler, S. Russo","Machine learning property prediction for organic photovoltaic devices",2020,"","","","",140,"2022-07-13 09:40:40","","10.1038/s41524-020-00429-w","","",,,,,25,12.50,4,6,2,"","",""
4,"M. Valetich, C. Le Losq, R. Arculus, S. Umino, J. Mavrogenes","Compositions and Classification of Fractionated Boninite Series Melts from the Izu–Bonin–Mariana Arc: A Machine Learning Approach",2021,"","","","",141,"2022-07-13 09:40:40","","10.1093/PETROLOGY/EGAB013","","",,,,,4,4.00,1,5,1,"  Much of the boninite magmatism in the Izu–Bonin–Mariana arc is preserved as evolved boninite series compositions wherein extensive fractional crystallization of pyroxene and spinel have obscured the diagnostic geochemical indicators of boninite parentage, such as high Mg and low Ti at intermediate silica contents. As a result, the usual geochemical discriminants used for the classification of the broad range of parental boninites are inapplicable to such highly fractionated melts. These issues are compounded by the mixing of demonstrably different whole-rock and glass analyses in classification schemes and petrological interpretations based thereon. Whole-rock compositions are compromised by entrainment of variable proportions of crystalline phases resulting in inconsistent differences from corresponding in situ glass analyses, which arguably better reflect prior melt compositions. To circumvent such issues, we herein present a robust method for the classification of highly fractionated boninite series glasses. This new classification leverages the analysis of trace elements, which are much more sensitive to evolutionary processes than major elements, and benefits from the use of unsupervised machine learning as a classification tool. The results show that the most fractionated boninite series melts preserve geochemical indicators of their parentage, and highlight the pitfalls of interpreting whole-rock and glass analyses interchangeably.","",""
2,"I. M. Lei, Chen Jiang, Chon Lok Lei, S. D. de Rijk, Y. C. Tam, C. Swords, M. Sutcliffe, G. Malliaras, M. Bance, Yan Yan Shery Huang","3D printed biomimetic cochleae and machine learning co-modelling provides clinical informatics for cochlear implant patients",2021,"","","","",142,"2022-07-13 09:40:40","","10.1038/s41467-021-26491-6","","",,,,,2,2.00,0,10,1,"","",""
1,"T. Thung, Murray E. White, Wei Dai, J. Wilksch, R. Bamert, A. Rocker, C. Stubenrauch, Daniel Williams, Cheng Huang, Ralf Schittelhelm, J. Barr, E. Jameson, S. McGowan, Yanju Zhang, Jiawei Wang, R. Dunstan, T. Lithgow","The component parts of bacteriophage virions accurately defined by a machine-learning approach built on evolutionary features",2021,"","","","",143,"2022-07-13 09:40:40","","10.1101/2021.02.28.433281","","",,,,,1,1.00,0,17,1,"Antimicrobial resistance (AMR) continues to evolve as a major threat to human health and new strategies are required for the treatment of AMR infections. Bacteriophages (phages) that kill bacterial pathogens are being identified for use in phage therapies, with the intention to apply these bactericidal viruses directly into the infection sites in bespoke phage cocktails. Despite the great unsampled phage diversity for this purpose, an issue hampering the roll out of phage therapy is the poor quality annotation of many of the phage genomes, particularly for those from infrequently sampled environmental sources. We developed a computational tool called STEP3 to use the “evolutionary features” that can be recognized in genome sequences of diverse phages. These features, when integrated into an ensemble framework, achieved a stable and robust prediction performance when benchmarked against other prediction tools using phages from diverse sources. Validation of the prediction accuracy of STEP3 was conducted with high-resolution mass spectrometry analysis of two novel phages, isolated from a watercourse in the Southern Hemisphere. STEP3 provides a robust computational approach to distinguish specific and universal features in phages to improve the quality of phage cocktails, and is available for use at http://step3.erc.monash.edu/. IMPORTANCE In response to the global problem of antimicrobial resistance there are moves to use bacteriophages (phages) as therapeutic agents. Selecting which phages will be effective therapeutics relies on interpreting features contributing to shelf-life and applicability to diagnosed infections. However, the protein components of the phage virions that dictate these properties vary so much in sequence that best estimates suggest failure to recognize up to 90% of them. We have utilised this diversity in evolutionary features as an advantage, to apply machine learning for prediction accuracy for diverse components in phage virions. We benchmark this new tool showing the accurate recognition and evaluation of phage components parts using genome sequence data of phages from under-sampled environments, where the richest diversity of phage still lies.","",""
1973,"Finale Doshi-Velez, Been Kim","Towards A Rigorous Science of Interpretable Machine Learning",2017,"","","","",144,"2022-07-13 09:40:40","","","","",,,,,1973,394.60,987,2,5,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.","",""
19,"P. Choudhury, Ryan T. Allen, Michael G. Endres","Machine Learning for Pattern Discovery in Management Research",2020,"","","","",145,"2022-07-13 09:40:40","","10.2139/ssrn.3518780","","",,,,,19,9.50,6,3,2,"Supervised machine learning (ML) methods are a powerful toolkit for discovering robust patterns in quantitative data. The patterns identified by ML could be used for exploratory inductive or abductive research, or for post-hoc analysis of regression results to detect patterns that may have gone unnoticed. However, ML models should not be treated as the result of a deductive causal test. To demonstrate the application of ML for pattern discovery, we implement ML algorithms to study employee turnover at a large technology company. We interpret the relationships between variables using partial dependence plots, which uncover surprising nonlinear and interdependent patterns between variables that may have gone unnoticed using traditional methods. To guide readers evaluating ML for pattern discovery, we provide guidance for evaluating model performance, highlight human decisions in the process, and warn of common misinterpretation pitfalls. An online appendix provides code and data to implement the algorithms demonstrated in the paper.","",""
17,"Frank Male, J. Jensen, L. Lake","Comparison of permeability predictions on cemented sandstones with physics-based and machine learning approaches",2020,"","","","",146,"2022-07-13 09:40:40","","10.31223/osf.io/3w6jx","","",,,,,17,8.50,6,3,2,"Abstract Permeability prediction has been an important problem since the time of Darcy. Most approaches to solve this problem have used either idealized physical models or empirical relations. In recent years, machine learning (ML) has led to more accurate and robust, but less interpretable empirical models. Using 211 core samples collected from 12 wells in the Garn Sandstone from the North Sea, this study compared idealized physical models based on the Carman-Kozeny equation to interpretable ML models. We found that ML models trained on estimates of physical properties are more accurate than physical models. Also, the results show evidence of a threshold of about 10% volume fraction, above which pore-filling cement strongly affects permeability.","",""
15,"Niraj Thapa, Zhipeng Liu, KC DukkaB., B. Gokaraju, Kaushik Roy","Comparison of Machine Learning and Deep Learning Models for Network Intrusion Detection Systems",2020,"","","","",147,"2022-07-13 09:40:40","","10.3390/FI12100167","","",,,,,15,7.50,3,5,2,"The development of robust anomaly-based network detection systems, which are preferred over static signal-based network intrusion, is vital for cybersecurity. The development of a flexible and dynamic security system is required to tackle the new attacks. Current intrusion detection systems (IDSs) suffer to attain both the high detection rate and low false alarm rate. To address this issue, in this paper, we propose an IDS using different machine learning (ML) and deep learning (DL) models. This paper presents a comparative analysis of different ML models and DL models on Coburg intrusion detection datasets (CIDDSs). First, we compare different ML- and DL-based models on the CIDDS dataset. Second, we propose an ensemble model that combines the best ML and DL models to achieve high-performance metrics. Finally, we benchmarked our best models with the CIC-IDS2017 dataset and compared them with state-of-the-art models. While the popular IDS datasets like KDD99 and NSL-KDD fail to represent the recent attacks and suffer from network biases, CIDDS, used in this research, encompasses labeled flow-based data in a simulated office environment with both updated attacks and normal usage. Furthermore, both accuracy and interpretability must be considered while implementing AI models. Both ML and DL models achieved an accuracy of 99% on the CIDDS dataset with a high detection rate, low false alarm rate, and relatively low training costs. Feature importance was also studied using the Classification and regression tree (CART) model. Our models performed well in 10-fold cross-validation and independent testing. CART and convolutional neural network (CNN) with embedding achieved slightly better performance on the CIC-IDS2017 dataset compared to previous models. Together, these results suggest that both ML and DL methods are robust and complementary techniques as an effective network intrusion detection system.","",""
10,"Krupal P. Jethava, Jonathan A Fine, Yingqi Chen, Ahad Hossain, G. Chopra","Accelerated Reactivity Mechanism and Interpretable Machine Learning Model of N-Sulfonylimines toward Fast Multicomponent Reactions.",2020,"","","","",148,"2022-07-13 09:40:40","","10.26434/chemrxiv.12116163.v1","","",,,,,10,5.00,2,5,2,"We introduce chemical reactivity flowcharts to help chemists interpret reaction outcomes using statistically robust machine learning models trained on a small number of reactions. We developed fast N-sulfonylimine multicomponent reactions for understanding reactivity and to generate training data. Accelerated reactivity mechanisms were investigated using density functional theory. Intuitive chemical features learned by the model accurately predicted heterogeneous reactivity of N-sulfonylimine with different carboxylic acids. Validation of the predictions shows that reaction outcome interpretation is useful for human chemists.","",""
15,"Zihao Wang, Yang Su, Saimeng Jin, W. Shen, Jingzheng Ren, Xiang-ping Zhang, J. Clark","A novel unambiguous strategy of molecular feature extraction in machine learning assisted predictive models for environmental properties",2020,"","","","",149,"2022-07-13 09:40:40","","10.1039/d0gc01122c","","",,,,,15,7.50,2,7,2,"Environmental properties of compounds provide significant information in treating organic pollutants, which drives the chemical process and environmental science toward eco-friendly technology. Traditional group contribution methods play an important role in property estimations, whereas various disadvantages emerge in their applications, such as scattered predicted values for certain groups of compounds. In order to address such issues, an extraction strategy for molecular features is proposed in this research, which is characterized by interpretability and discriminating power with regard to isomers. Based on the Henry's law constant data of organic compounds in water, we developed a hybrid predictive model that integrates the proposed strategy in conjunction with a neural network framework. The structure of the predictive model is optimized using cross-validation and grid search to improve its robustness. Moreover, the predictive model is improved by introducing the plane of best fit descriptor as input and adopting k-means clustering in sampling. In contrast with reported models in the literature, the developed predictive model demonstrates improved generality, higher accuracy, and fewer molecular features used in its development.","",""
5,"M. Paggi","An Analysis of the Italian Lockdown in Retrospective Using Particle Swarm Optimization in Machine Learning Applied to an Epidemiological Model",2020,"","","","",150,"2022-07-13 09:40:40","","10.3390/physics2030020","","",,,,,5,2.50,5,1,2,"A critical analysis of the open data provided by the Italian Civil Protection Centre during phase 1 of Covid-19 epidemic—the so-called Italian lockdown—is herein proposed in relation to four of the most affected Italian regions, namely Lombardy, Reggio Emilia, Valle d’Aosta, and Veneto. A possible bias in the data induced by the extent in the use of medical swabs is found in relation to Valle d’Aosta and Veneto. Observed data are then interpreted using a Susceptible-Infectious-Recovered (SIR) epidemiological model enhanced with asymptomatic (infected and recovered) compartments, including lockdown effects through time-dependent model parameters. The initial number of susceptible individuals for each region is also considered as a parameter to be identified. The issue of parameters identification is herein addressed by a robust machine learning approach based on particle swarm optimization. Model predictions provide relevant information for policymakers in terms of the effect of lockdown measures in the different regions. The number of susceptible individuals involved in the epidemic, important for a safe release of lockdown during the next phases, is predicted to be around 10% of the population for Lombardy, 16% for Reggio Emilia, 18% for Veneto, and 40% for Valle d’Aosta.","",""
10,"T. Botari, Frederik Hvilshøj, Rafael Izbicki, A. Carvalho","MeLIME: Meaningful Local Explanation for Machine Learning Models",2020,"","","","",151,"2022-07-13 09:40:40","","","","",,,,,10,5.00,3,4,2,"Most state-of-the-art machine learning algorithms induce black-box models, preventing their application in many sensitive domains. Hence, many methodologies for explaining machine learning models have been proposed to address this problem. In this work, we introduce strategies to improve local explanations taking into account the distribution of the data used to train the black-box models. We show that our approach, MeLIME, produces more meaningful explanations compared to other techniques over different ML models, operating on various types of data. MeLIME generalizes the LIME method, allowing more flexible perturbation sampling and the use of different local interpretable models. Additionally, we introduce modifications to standard training algorithms of local interpretable models fostering more robust explanations, even allowing the production of counterfactual examples. To show the strengths of the proposed approach, we include experiments on tabular data, images, and text; all showing improved explanations. In particular, MeLIME generated more meaningful explanations on the MNIST dataset than methods such as GuidedBackprop, SmoothGrad, and Layer-wise Relevance Propagation. MeLIME is available on this https URL.","",""
506,"W. James Murdoch, Chandan Singh, Karl Kumbier, R. Abbasi-Asl, Bin Yu","Definitions, methods, and applications in interpretable machine learning",2019,"","","","",152,"2022-07-13 09:40:40","","10.1073/pnas.1900654116","","",,,,,506,168.67,101,5,3,"Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.","",""
4,"Ninghao Liu, Mengnan Du, Xia Hu","Adversarial Machine Learning: An Interpretation Perspective",2020,"","","","",153,"2022-07-13 09:40:40","","","","",,,,,4,2.00,1,3,2,"Recent years have witnessed the significant advances of machine learning in a wide spectrum of applications. However, machine learning models, especially deep neural networks, have been recently found to be vulnerable to carefully-crafted input called adversarial samples. The difference between normal and adversarial samples is almost imperceptible to human. Many work have been proposed to study adversarial attack and defense in different scenarios. An intriguing and crucial aspect among those work is to understand the essential cause of model vulnerability, which requires in-depth exploration of another concept in machine learning models, i.e., interpretability. Interpretable machine learning tries to extract human-understandable terms for the working mechanism of models, which also receives a lot of attention from both academia and industry. Recently, an increasing number of work start to incorporate interpretation into the exploration of adversarial robustness. Furthermore, we observe that many previous work of adversarial attacking, although did not mention it explicitly, can be regarded as natural extension of interpretation. In this paper, we review recent work on adversarial attack and defense, particularly, from the perspective of machine learning interpretation. We categorize interpretation into two types, according to whether it focuses on raw features or model components. For each type of interpretation, we elaborate on how it could be used in attacks, or defense against adversaries. After that, we briefly illustrate other possible correlations between the two domains. Finally, we discuss the challenges and future directions along tackling adversary issues with interpretation.","",""
8,"C. Rea, K. Montes, A. Pau, R. Granetz, O. Sauter","Progress Toward Interpretable Machine Learning–Based Disruption Predictors Across Tokamaks",2020,"","","","",154,"2022-07-13 09:40:40","","10.1080/15361055.2020.1798589","","",,,,,8,4.00,2,5,2,"Abstract In this paper we lay the groundwork for a robust cross-device comparison of data-driven disruption prediction algorithms on DIII-D and JET tokamaks. In order to consistently carry on a comparative analysis, we define physics-based indicators of disruption precursors based on temperature, density, and radiation profiles that are currently not used in many other machine learning predictors for DIII-D data. These profile-based indicators are shown to well-describe impurity accumulation events in both DIII-D and JET discharges that eventually disrupt. The univariate analysis of the features used as input signals in the data-driven algorithms applied on the data of both tokamaks statistically highlights the differences in the dominant disruption precursors. JET with its ITER-like wall is more prone to impurity accumulation events, while DIII-D is more subject to edge-cooling mechanisms that destabilize dangerous magnetohydrodynamic modes. Even though the analyzed data sets are characterized by such intrinsic differences, we show through a few examples that the inclusion of physics-based disruption markers in data-driven algorithms is a promising path toward the realization of a uniform framework to predict and interpret disruptive scenarios across different tokamaks. As long as the destabilizing precursors are diagnosed in a device-independent way, the knowledge that data-driven algorithms learn on one device can be re-used to explain a disruptive behavior on another device.","",""
2,"Xubo Leng, Margot Wohl, Kenichi Ishii, Pavan Nayak, Kenta Asahina","Quantifying influence of human choice on the automated detection of Drosophila behavior by a supervised machine learning algorithm",2020,"","","","",155,"2022-07-13 09:40:40","","10.1371/journal.pone.0241696","","",,,,,2,1.00,0,5,2,"Automated quantification of behavior is increasingly prevalent in neuroscience research. Human judgments can influence machine-learning-based behavior classification at multiple steps in the process, for both supervised and unsupervised approaches. Such steps include the design of the algorithm for machine learning, the methods used for animal tracking, the choice of training images, and the benchmarking of classification outcomes. However, how these design choices contribute to the interpretation of automated behavioral classifications has not been extensively characterized. Here, we quantify the effects of experimenter choices on the outputs of automated classifiers of Drosophila social behaviors. Drosophila behaviors contain a considerable degree of variability, which was reflected in the confidence levels associated with both human and computer classifications. We found that a diversity of sex combinations and tracking features was important for robust performance of the automated classifiers. In particular, features concerning the relative position of flies contained useful information for training a machine-learning algorithm. These observations shed light on the importance of human influence on tracking algorithms, the selection of training images, and the quality of annotated sample images used to benchmark the performance of a classifier (the ‘ground truth’). Evaluation of these factors is necessary for researchers to accurately interpret behavioral data quantified by a machine-learning algorithm and to further improve automated classifications.","",""
5,"T. Kootstra, J. Teuwen, J. Goudsmit, T. Nijboer, Michael D. Dodd, S. van der Stigchel","Machine learning-based classification of viewing behavior using a wide range of statistical oculomotor features",2020,"","","","",156,"2022-07-13 09:40:40","","10.1167/jov.20.9.1","","",,,,,5,2.50,1,6,2,"Since the seminal work of Yarbus, multiple studies have demonstrated the influence of task-set on oculomotor behavior and the current cognitive state. In more recent years, this field of research has expanded by evaluating the costs of abruptly switching between such different tasks. At the same time, the field of classifying oculomotor behavior has been moving toward more advanced, data-driven methods of decoding data. For the current study, we used a large dataset compiled over multiple experiments and implemented separate state-of-the-art machine learning methods for decoding both cognitive state and task-switching. We found that, by extracting a wide range of oculomotor features, we were able to implement robust classifier models for decoding both cognitive state and task-switching. Our decoding performance highlights the feasibility of this approach, even invariant of image statistics. Additionally, we present a feature ranking for both models, indicating the relative magnitude of different oculomotor features for both classifiers. These rankings indicate a separate set of important predictors for decoding each task, respectively. Finally, we discuss the implications of the current approach related to interpreting the decoding results.","",""
3,"Xubo Leng, Margot Wohl, Kenichi Ishii, Pavan Nayak, Kenta Asahina","Quantitative comparison of Drosophila behavior annotations by human observers and a machine learning algorithm",2020,"","","","",157,"2022-07-13 09:40:40","","10.1101/2020.06.16.153130","","",,,,,3,1.50,1,5,2,"Automated quantification of behavior is increasingly prevalent in neuroscience research. Human judgments can influence machine-learning-based behavior classification at multiple steps in the process, for both supervised and unsupervised approaches. Such steps include the design of the algorithm for machine learning, the methods used for animal tracking, the choice of training images, and the benchmarking of classification outcomes. However, how these design choices contribute to the interpretation of automated behavioral classifications has not been extensively characterized. Here, we quantify the effects of experimenter choices on the outputs of automated classifiers of Drosophila social behaviors. Drosophila behaviors contain a considerable degree of variability, which was reflected in the confidence levels associated with both human and computer classifications. We found that a diversity of sex combinations and tracking features was important for robust performance of the automated classifiers. In particular, features concerning the relative position of flies contained useful information for training a machine-learning algorithm. These observations shed light on the importance of human influence on tracking algorithms, the selection of training images, and the quality of annotated sample images used to benchmark the performance of a classifier (the ‘ground truth’). Evaluation of these factors is necessary for researchers to accurately interpret behavioral data quantified by a machine-learning algorithm and to further improve automated classifications. Significance Statement Accurate quantification of animal behaviors is fundamental to neuroscience. Here, we quantitatively assess how human choices influence the performance of automated classifiers trained by a machine-learning algorithm. We found that human decisions about the computational tracking method, the training images, and the images used for performance evaluation impact both the classifier outputs and how human observers interpret the results. These factors are sometimes overlooked but are critical, especially because animal behavior is itself inherently variable. Automated quantification of animal behavior is becoming increasingly prevalent: our results provide a model for bridging the gap between traditional human annotations and computer-based annotations. Systematic assessment of human choices is important for developing behavior classifiers that perform robustly in a variety of experimental conditions.","",""
0,"Taru Jain","Adversarial Machine Learning for Self Harm Disclosure Analysis (Workshop Paper)",2020,"","","","",158,"2022-07-13 09:40:40","","10.1109/BigMM50055.2020.00070","","",,,,,0,0.00,0,1,2,"Adversarial Machine Learning has been gaining attention from the NLP community due to low interpretability and low robustness of the current state-of-the-art systems. In this work, we study the effect of various adversarial attacks for detection of suicidal intent in social media setting. Suicide Ideation is a sensitive issue and is a leading cause of death. We show how various models are rendered useless after attacks and perform adversarial training using the most ideal attacks to improve their robustness. We also conduct several experiments with the attacks to study their effect and propose an approach for adversarial training using Generative Adversarial Networks.","",""
1,"W. Gou, Chu-wen Ling, Yan He, Zengliang Jiang, Yuanqing Fu, Fengzhe Xu, Z. Miao, Ting-yu Sun, Jie-sheng Lin, Hui-lian Zhu, Hongwei Zhou, Yu-ming Chen, Ju-Sheng Zheng","Interpretable machine learning framework reveals novel gut microbiome features in predicting type 2 diabetes",2020,"","","","",159,"2022-07-13 09:40:40","","10.1101/2020.04.05.024984","","",,,,,1,0.50,0,13,2,"Gut microbiome targets for type 2 diabetes (T2D) prevention among human cohorts have been controversial. Using an interpretable machine learning-based analytic framework, we identified robust human gut microbiome features, with their optimal threshold, in predicting T2D. Based on the results, we constructed a microbiome risk score (MRS), which was consistently associated with T2D across 3 independent Chinese cohorts involving 9111 participants (926 T2D cases). The MRS could also predict future glucose increment, and was correlated with a variety of gut microbiota-derived blood metabolites. Faecal microbiota transplantation from humans to germ-free mice demonstrated a causal role of the identified combination of microbes in the T2D development. We further identified adiposity and dietary factors which could prospectively modulate the MRS, and found that body fat distribution may be the key factor modulating the gut microbiome-T2D relationship. Taken together, we proposed a new analytical framework for the investigation of microbiome-disease relationship. The identified microbiota may serve as potential drug targets for T2D in future.","",""
1,"M. Richardson, A. Rivkin, A. Sickafoose","Investigating Machine Learning as a Basis for Asteroid Taxnomies in the 3-Micron Spectral Region",2020,"","","","",160,"2022-07-13 09:40:40","","10.5194/epsc2020-963","","",,,,,1,0.50,0,3,2,"<p>Abstract:</p> <p>As part of a larger study to elucidate the presence of hydrated minerals on asteroid surfaces, we are developing a robust taxonomic classification system using spectroscopic observations in the vicinity of 3 &#956;m. We have constructed a Python algorithm to identify band centers and band depths near 3 &#181;m for a set of normalized, thermally-corrected asteroid spectra for use to serve as inputs to Python&#8217;s Scikit-Learn library of Machine Learning (ML) algorithms. We anticipate a thorough investigation of both Principal Component Analysis and ML (supervised, unsupervised, and Artificial Neural Network) techniques to assess which technique is likely to be better suited for classifying the 3-&#181;m data. At this writing, we have run tests using Python&#8217;s Agglomerative clustering ML algorithm to examine possible clustering scenarios. These initial steps have given us some familiarity with the mechanics of using ML on the 3-&#181;m dataset as well as serving to identify some possible pitfalls or cul-de-sacs. Presented here are the preliminary results we have obtained.</p> <p>Introduction:</p> <p>Although various techniques have been used, asteroid classification has typically been done via Principal Component Analysis (PCA: [1,2]). PCA is a statistical technique that reduces the dimensionality of a dataset by identifying the most important parameters within a dataset based on their variance. Parameters that exhibit the greatest amount of variance are considered to be of greater importance while parameters with the least amount of variance are considered to be of lower importance. While the PCA technique produces better visualizations of the data by reducing the dimensionality of a dataset, the PCA technique comes with some drawbacks. Disadvantages such as its dependence on scale and information loss due to the orthogonal property of PCA can cause interpretation of PCA results to prove to be a more critical and time-consuming process. Therefore, exploring other means of classification may prove to be worthwhile.</p> <p>Machine Learning (ML) algorithms have had a significant impact on the way in which data is analyzed and interpreted, and have already proven to be a powerfully reliable resource in the field of planetary science. Accordingly, the application of ML to an asteroid taxonomy has the potential to be more efficient, objective, and easy-to-implement than PCA. ML algorithms can be supervised, in which the program &#8220;learns&#8221; from training data and is able to classify new inputs, or unsupervised, in which the program analyzes the dataset to determine patterns such as clusters. [3] used an Artificial Neural Network (ANN, a subset of ML) to classify asteroids, work followed up by [4]. Recent explorations of supervised ML for asteroid taxonomy are promising, and have applied training sets from existing databases to new visible and/or NIR photometric data (e.g. [5,6,7]).</p> <p>We seek to explore the benefits of ML algorithms, as well as compare and contrast to the PCA technique, in the production of an asteroid taxonomy. Our initial exploration has utilized a set of normalized, thermally-corrected asteroid spectra in the vicinity of 3 &#181;m. We have identified band centers and band depths and served this parameter space as inputs to Python&#8217;s Agglomerative clustering ML algorithm.</p> <p>Methodology:</p> <p>Thermal corrections of the asteroid spectra were performed via a forward model that uses a modified version of the Standard Thermal Model (STM: [8]). The forward model treats the beaming parameter as a free parameter adjusting its value for each iteration of the STM until it converges onto a value that yields expected long-wavelength continuum behavior. Spectra were then normalized to unity at a wavelength of 2.3 &#181;m, followed by identification of band centers and band depths near 3 &#181;m using both polynomial and Gaussian fits. In addition, band depths were measured at wavelengths of 2.9 &#181;m and 3.2 &#181;m to gather more information on asteroid band shapes. Lastly, the aforementioned calculated spectral features were input into Python&#8217;s Agglomerative clustering algorithm to determine which asteroid spectra shared similar features.</p> <p>Summary:</p> <p>As part of a larger investigation to better understand hydrated mineralogies as they apply to asteroids, we have begun work towards developing a quantitative taxonomic framework derived from asteroid spectra in the wavelength range from 2.0-4.0 &#181;m. Our exploration thus far of Python&#8217;s Agglomerative clustering algorithm has proven to be fruitful. Minor changes to the parameterization of this algorithm can yield very different results, which naturally can lead to different interpretations. The Agglomerative clustering algorithm is one of many the powerful ML algorithms we will explore against the PCA technique, all of which we will be discussing in our presentation.</p> <p>References:</p> <p>[1] Bus, S. J. and Binzel, R. P. (2002). Phase II of the Small Main-Belt Asteroid Spectroscopic Survey: A Feature-Based Taxonomy. Icarus, 158:146-177.</p> <p>[2] DeMeo, F. E., Binzel, R. P., Slivan, S. M., and Bus, S. J. (2009). An extension of the Bus asteroid taxonomy into the near-infrared. Icarus, 202:160-180.</p> <p>[3] Howell, E. S., Merenyi, E., and Lebofsky, L. A. (1994). Classification of asteroid spectra using a neural network. 99:10847-10865.</p> <p>[4] Merenyi, E., Howell E. S., Rivkin A. S., and Lebofsky L. A. (1997). Prediction of water in asteroids from spectral data shortward of 3 &#181;m. Icarus, 129:421-439.</p> <p>[5] Moment, M., Trilling, D. E., Borth, D., Jedicke, R., Butler, N., Reyes-Ruiz, M., Pichardo, B., Peterson, E., Axelrod, T., and Moskovitz, N. (2016). First results from the rapid-response spectrophotometric characterization of near-earth objects using UKIRT. The Astronomical Journal, 151:98</p> <p>[6] Popescu, Marcel & Licandro, Javier & Carvano, Jmf & Stoicescu, R. & De Le&#243;n, Julia & Morate, David & Boac\u{a, I.L. & Cristescu, C.. (2018). Taxonomic classification of asteroids based on MOVIS near-infrared colors. Astronomy & Astrophysics. 617. 10.1051/0004-6361/201833023.</p> <p>[7] Wallace, S. M., Burbine, T. H., Sheldon, D., Dyar, M. D. (2019). Machine Learning applied to asteroid taxonomy based on reflectance spectroscopy: An objective method. LPSC 50, Abstract # 1097.</p> <p>[8] Lebofsky, L. A., Sykes, M. V., Tedesco, E. F., Veeder, G. J., Matson, D. L., Brown, R. H., Gradie, J. C., Feierberg, M. A., Rudy, R. J. (1986). A refined standard thermal model for asteroids based on observations of 1 Ceres and 2 Palas. Icarus. 68:239-251.</p>","",""
200,"W. James Murdoch, Chandan Singh, Karl Kumbier, R. Abbasi-Asl, Bin Yu","Interpretable machine learning: definitions, methods, and applications",2019,"","","","",161,"2022-07-13 09:40:40","","10.1073/pnas.1900654116","","",,,,,200,66.67,40,5,3,"M learning (ML) has recently received considerable attention for its ability to accurately predict a wide variety of complex phenomena. However, there is a growing realization that, in addition to predictions, ML models are capable of producing knowledge about domain relationships contained in data, often referred to as interpretations. These interpretations have found uses both in their own right, e.g. medicine (1), policy-making (2), and science (3, 4), as well as in auditing the predictions themselves in response to issues such as regulatory pressure (5) and fairness (6). In the absence of a well-formed definition of interpretability, a broad range of methods with a correspondingly broad range of outputs (e.g. visualizations, natural language, mathematical equations) have been labeled as interpretation. This has led to considerable confusion about the notion of interpretability. In particular, it is unclear what it means to interpret something, what common threads exist among disparate methods, and how to select an interpretation method for a particular problem/audience. In this paper, we attempt to address these concerns. To do so, we first define interpretability in the context of machine learning and place it within a generic data science life cycle. This allows us to distinguish between two main classes of interpretation methods: model-based∗ and post hoc. We then introduce the Predictive, Descriptive, Relevant (PDR) framework, consisting of three desiderata for evaluating and constructing interpretations: predictive accuracy, descriptive","",""
3,"Shalaw R. Sallah, P. Sergouniotis, S. Barton, S. Ramsden, Rachel L. Taylor, Amro Safadi, M. Kabir, J. Ellingford, N. Lench, S. Lovell, G. Black","Using an integrative machine learning approach utilising homology modelling to clinically interpret genetic variants: CACNA1F as an exemplar",2020,"","","","",162,"2022-07-13 09:40:40","","10.1038/s41431-020-0623-y","","",,,,,3,1.50,0,11,2,"","",""
0,"Mimansa Jaiswal","Interpreting Multimodal Machine Learning Models Trained for Emotion Recognition to Address Robustness and Privacy Concerns",2020,"","","","",163,"2022-07-13 09:40:40","","10.1609/aaai.v34i10.7130","","",,,,,0,0.00,0,1,2,"Many mobile applications and virtual conversational agents now aim to recognize and adapt to emotions. These predicted emotions are used in variety of downstream applications: (a) generating more human like dialogues, (b) predicting mental health issues, and (c) hate speech detection and intervention. To enable this, data are transmitted from users' devices and stored on central servers. These data are then processed further, either annotated or used as inputs for training a model for a specific task. Yet, these data contain sensitive information that could be used by mobile applications without user's consent or, maliciously, by an eavesdropping adversary. My work focuses on two major issues that are faced while training emotion recognition algorithms: (a) privacy of the generated representations and, (b) explaining and ensuring that the predictions are robust to various situations. Tackling these issues would lead to emotion based algorithms that are deployable and helpful at a larger scale, thus enabling more human like experience when interacting with AI.","",""
166,"M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, S. Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Integrating machine learning and multiscale modeling—perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences",2019,"","","","",164,"2022-07-13 09:40:40","","10.1038/s41746-019-0193-y","","",,,,,166,55.33,17,11,3,"","",""
1981,"C. Rudin","Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",2018,"","","","",165,"2022-07-13 09:40:40","","10.1038/S42256-019-0048-X","","",,,,,1981,495.25,1981,1,4,"","",""
13,"Xinlei Mi, Baiming Zou, F. Zou, J. Hu","Permutation-based identification of important biomarkers for complex diseases via machine learning models",2021,"","","","",166,"2022-07-13 09:40:40","","10.1038/s41467-021-22756-2","","",,,,,13,13.00,3,4,1,"","",""
14,"M. Levine, A. Stuart","A Framework for Machine Learning of Model Error in Dynamical Systems",2021,"","","","",167,"2022-07-13 09:40:40","","","","",,,,,14,14.00,7,2,1,"The development of data-informed predictive models for dynamical systems is of widespread interest in many disciplines. We present a unifying framework for blending mechanistic and machine-learning approaches to identify dynamical systems from noisily and partially observed data. We compare pure data-driven learning with hybrid models which incorporate imperfect domain knowledge, referring to the discrepancy between an assumed truth model and the imperfect mechanistic model as model error. Our formulation is agnostic to the chosen machine learning model, is presented in both continuousand discrete-time settings, and is compatible both with model errors that exhibit substantial memory and errors that are memoryless. First, we study memoryless linear (w.r.t. parametric-dependence) model error from a learning theory perspective, defining excess risk and generalization error. For ergodic continuous-time systems, we prove that both excess risk and generalization error are bounded above by terms that diminish with the square-root of T , the time-interval over which training data is specified. Secondly, we study scenarios that benefit from modeling with memory, proving universal approximation theorems for two classes of continuous-time recurrent neural networks (RNNs): both can learn memory-dependent model error, assuming that it is governed by a finite-dimensional hidden variable and that, together, the observed and hidden variables form a continuous-time Markovian system. In addition, we connect one class of RNNs to reservoir computing, thereby relating learning of memory-dependent error to recent work on supervised learning between Banach spaces using random features. Numerical results are presented (Lorenz ’63, Lorenz ’96 Multiscale systems) to compare purely data-driven and hybrid approaches, finding hybrid methods less data-hungry and more parametrically efficient. We also find that, while a continuous-time framing allows for robustness to irregular sampling and desirable domain-interpretability, a discrete-time framing can provide similar or better predictive performance, especially when data are undersampled and the vector field defining the true dynamics cannot be identified. Finally, we demonstrate numerically how data assimilation can be leveraged to learn hidden dynamics from noisy, partially-observed data, and illustrate challenges in representing memory by this approach, and in the training of such models. Received by the editors July 13, 2021. 2020 Mathematics Subject Classification. Primary 68T30, 37A30, 37M10; Secondary 37M25,","",""
35,"Han Liu, Tony Zhang, N. M. Anoop Krishnan, M. Smedskjaer, J. Ryan, S. Gin, M. Bauchy","Predicting the dissolution kinetics of silicate glasses by topology-informed machine learning",2019,"","","","",168,"2022-07-13 09:40:40","","10.1038/s41529-019-0094-1","","",,,,,35,11.67,5,7,3,"","",""
13,"Thibault Laugel, Marie-Jeanne Lesot, C. Marsala, X. Renard, Marcin Detyniecki","Unjustified Classification Regions and Counterfactual Explanations in Machine Learning",2019,"","","","",169,"2022-07-13 09:40:40","","10.1007/978-3-030-46147-8_3","","",,,,,13,4.33,3,5,3,"","",""
18,"P. Fusar-Poli, Dominic Stringer, Alice M. S. Durieux, G. Rutigliano, I. Bonoldi, A. De Micheli, D. Ståhl","Clinical-learning versus machine-learning for transdiagnostic prediction of psychosis onset in individuals at-risk",2019,"","","","",170,"2022-07-13 09:40:40","","10.1038/s41398-019-0600-9","","",,,,,18,6.00,3,7,3,"","",""
1,"Hamed Zamani, Fernando Diaz, M. Dehghani, Donald Metzler, Michael Bendersky","Retrieval-Enhanced Machine Learning",2022,"","","","",171,"2022-07-13 09:40:40","","10.1145/3477495.3531722","","",,,,,1,1.00,0,5,1,"Although information access systems have long supportedpeople in accomplishing a wide range of tasks, we propose broadening the scope of users of information access systems to include task-driven machines, such as machine learning models. In this way, the core principles of indexing, representation, retrieval, and ranking can be applied and extended to substantially improve model generalization, scalability, robustness, and interpretability. We describe a generic retrieval-enhanced machine learning (REML) framework, which includes a number of existing models as special cases. REML challenges information retrieval conventions, presenting opportunities for novel advances in core areas, including optimization. The REML research agenda lays a foundation for a new style of information access research and paves a path towards advancing machine learning and artificial intelligence.","",""
5,"Rohan Chandra, Aniket Bera, D. Manocha","Using Graph-Theoretic Machine Learning to Predict Human Driver Behavior",2021,"","","","",172,"2022-07-13 09:40:40","","10.1109/tits.2021.3130218","","",,,,,5,5.00,2,3,1,"Studies have shown that autonomous vehicles (AVs) behave conservatively in a traffic environment composed of human drivers and do not adapt to local conditions and socio-cultural norms. It is known that socially aware AVs can be designed if there exists a mechanism to understand the behaviors of human drivers. We present an approach that leverages machine learning to predict, the behaviors of human drivers. This is similar to how humans implicitly interpret the behaviors of drivers on the road, by only observing the trajectories of their vehicles. We use graph-theoretic tools to extract driver behavior features from the trajectories and machine learning to obtain a computational mapping between the extracted trajectory of a vehicle in traffic and the driver behaviors. Compared to prior approaches in this domain, we prove that our method is robust, general, and extendable to broad-ranging applications such as autonomous navigation. We evaluate our approach on real-world traffic datasets captured in the U.S., India, China, and Singapore, as well as in simulation.","",""
5,"Gideon A. Lyngdoh, Mohd Zaki, N. Krishnan, Sumanta","Prediction of Concrete Strengths Enabled by Missing Data Imputation and Interpretable Machine Learning",2022,"","","","",173,"2022-07-13 09:40:40","","","","",,,,,5,5.00,1,4,1,"Machine learning (ML)-based prediction of non-linear composition-strength relationship in concretes requires a large, complete, and consistent dataset. However, the availability of such datasets is limited as the datasets often suffer from incompleteness because of missing data corresponding to different input features, which makes the development of robust ML-based predictive models challenging. Besides, as the degree of complexity in these ML models increases, the interpretation of the results becomes challenging. These interpretations of results are critical towards the development of efficient materials design strategies for enhanced materials performance. To address these challenges, this paper implements different data imputation approaches for enhanced dataset completeness. The imputed dataset is leveraged to predict the compressive and tensile strength of concrete using various hyperparameteroptimized ML approaches. Among all the approaches, Extreme Gradient Boosted Decision Trees (XGBoost) showed the highest prediction efficacy when the dataset is imputed using k-nearest neighbors (kNN) with a 10-neighbor configuration. To interpret the predicted results, SHapley Additive exPlanations (SHAP) is employed. Overall, by implementing efficient combinations of data imputation approach, machine learning, and data interpretation, this paper develops an efficient approach to evaluate the compositionstrength relationship in concrete. This work, in turn, can be used as a starting point toward the design and development of various performance-enhanced and sustainable concretes.","",""
0,"Gang Yu, Jiawang Tao, Jie Wang","Odysseia: Genetic Regulatory Feature Analysis with Interpretable Classification Machine Learning Models",2022,"","","","",174,"2022-07-13 09:40:40","","10.1101/2022.02.17.480852","","",,,,,0,0.00,0,3,1,"With rapid progress of robust single-cell transcriptome sequencing since last decade, numerous complex mechanisms underlying cell development has been revealed. Single-cell RNA sequencing (scRNA-seq) analysis is widely accepted as the main approach to define cell stages and phenotypes. As conversion of somatic cells into induced pluripotency cells succeeded, identification key genetic factors(GFs) with scRNA-seq for cell reprogramming in biological research and regenerative medicine fields gained increasing attention. Herein, we describe Odysseia, an interpretable machine learning classifier based single-cell gene expression profile(scGEP) analysis system, that assesses importances of genetic regulatory features in differentiating cell states(CSs). Furthermore, extracted factors, when combining with regulatory network analysis, can help to find key GFs in classifying CSs and possibly inducing CS conversions. Analyzed three published scRNA-seq datasets used to study divergent cell types, Odysseia correctly extracted GFs acclaimed to be capable of inducing CS conversions. Overall, Odysseia provides an automated alternative to obtain guidance information while explicating mechanism to engineer cellular phenotypes.","",""
1,"Aida Rahmattalabi, Alice Xiang","Promises and Challenges of Causality for Ethical Machine Learning",2022,"","","","",175,"2022-07-13 09:40:40","","","","",,,,,1,1.00,1,2,1,"In recent years, there has been increasing interest in causal reasoning for designing fair decision-making systems due to its compatibility with legal frameworks, interpretability for human stakeholders, and robustness to spurious correlations inherent in observational data, among other factors. The recent attention to causal fairness, however, has been accompanied with great skepticism due to practical and epistemological challenges with applying current causal fairness approaches in the literature. Motivated by the long-standing empirical work on causality in econometrics, social sciences, and biomedical sciences, in this paper we lay out the conditions for appropriate application of causal fairness under the""potential outcomes framework.""We highlight key aspects of causal inference that are often ignored in the causal fairness literature. In particular, we discuss the importance of specifying the nature and timing of interventions on social categories such as race or gender. Precisely, instead of postulating an intervention on immutable attributes, we propose a shift in focus to their perceptions and discuss the implications for fairness evaluation. We argue that such conceptualization of the intervention is key in evaluating the validity of causal assumptions and conducting sound causal analysis including avoiding post-treatment bias. Subsequently, we illustrate how causality can address the limitations of existing fairness metrics, including those that depend upon statistical correlations. Specifically, we introduce causal variants of common statistical notions of fairness, and we make a novel observation that under the causal framework there is no fundamental disagreement between different notions of fairness. Finally, we conduct extensive experiments where we demonstrate our approach for evaluating and mitigating unfairness, specially when post-treatment variables are present.","",""
1,"M. Lange, M. Kaden, T. Villmann","About analysis and robust classification of searchlight fMRI-data using machine learning classifiers",2013,"","","","",176,"2022-07-13 09:40:40","","10.1109/IJCNN.2013.6706990","","",,,,,1,0.11,0,3,9,"In the present paper we investigate the analysis of functional magnetic resonance image (fMRI) data based on voxel response analysis. All voxels in local spatial area (volume) of a considered voxel form its so-called searchlight. The searchlight for a presented task is taken as a complex pattern. Task dependent discriminant analysis of voxel is then performed by assessment of the discrimination behavior of the respective searchlight pattern for a given task. Classification analysis of these patterns is usually done using linear support vector machines (linSVMs) as a machine learning approach or another statistical classifier like linear discriminant classifier. The test classification accuracy determining the task sensitivity is interpreted as the discrimination ability of the related voxel. However, frequently, the number of voxels contributing to a searchlight is much larger than the number of available pattern samples in classification learning, i.e. the dimensionality of patterns is higher than the number of samples. Therefore, the respective underlying mathematical classification problem has not an unique solution such that a certain solution obtained by the machine learning classifier contains arbitrary (random) components. For this situation, the generalization ability of the classifier may drop down. We propose in this paper another data processing approach to reduce this problem. In particular, we reformulate the classification problem within the searchlight. Doing so, we avoid the dimensionality problem: We obtain a mathematically well-defined classification problem, such that generalization ability of a trained classifier is kept high. Hence, a better stability of the task discrimination is obtained. Additionally, we propose the utilization of generalized learning vector quantizers as an alternative machine learning classifier system compared to SVMs, to improve further the stability of the classifier model due to decreased model complexity.","",""
12,"E. M. Mortani Barbosa, B. Georgescu, S. Chaganti, G. Alemañ, Jordi Broncano Cabrero, G. Chabin, T. Flohr, P. Grenier, Sasa Grbic, Nakul Gupta, F. Mellot, S. Nicolaou, Thomas J. Re, P. Sanelli, A. Sauter, Y. Yoo, Valentin Ziebandt, D. Comaniciu","Machine learning automatically detects COVID-19 using chest CTs in a large multicenter cohort",2020,"","","","",177,"2022-07-13 09:40:40","","10.1007/s00330-021-07937-3","","",,,,,12,6.00,1,18,2,"","",""
11,"Tianfang Xu, F. Liang","Machine learning for hydrologic sciences: An introductory overview",2021,"","","","",178,"2022-07-13 09:40:40","","10.1002/wat2.1533","","",,,,,11,11.00,6,2,1,"The hydrologic community has experienced a surge in interest in machine learning in recent years. This interest is primarily driven by rapidly growing hydrologic data repositories, as well as success of machine learning in various academic and commercial applications, now possible due to increasing accessibility to enabling hardware and software. This overview is intended for readers new to the field of machine learning. It provides a non‐technical introduction, placed within a historical context, to commonly used machine learning algorithms and deep learning architectures. Applications in hydrologic sciences are summarized next, with a focus on recent studies. They include the detection of patterns and events such as land use change, approximation of hydrologic variables and processes such as rainfall‐runoff modeling, and mining relationships among variables for identifying controlling factors. The use of machine learning is also discussed in the context of integrated with process‐based modeling for parameterization, surrogate modeling, and bias correction. Finally, the article highlights challenges of extrapolating robustness, physical interpretability, and small sample size in hydrologic applications.","",""
11,"Markus Jaeger, Stephan Krügel, D. Marinelli, J. Papenbrock, P. Schwendner","Interpretable Machine Learning for Diversified Portfolio Construction",2020,"","","","",179,"2022-07-13 09:40:40","","10.2139/ssrn.3730144","","",,,,,11,5.50,2,5,2,"In this article, the authors construct a pipeline to benchmark hierarchical risk parity (HRP) relative to equal risk contribution (ERC) as examples of diversification strategies allocating to liquid multi-asset futures markets with dynamic leverage (volatility target). The authors use interpretable machine learning concepts (explainable AI) to compare the robustness of the strategies and to back out implicit rules for decision-making. The empirical dataset consists of 17 equity index, government bond, and commodity futures markets across 20 years. The two strategies are back tested for the empirical dataset and for about 100,000 bootstrapped datasets. XGBoost is used to regress the Calmar ratio spread between the two strategies against features of the bootstrapped datasets. Compared to ERC, HRP shows higher Calmar ratios and better matches the volatility target. Using Shapley values, the Calmar ratio spread can be attributed especially to univariate drawdown measures of the asset classes. TOPICS: Quantitative methods, statistical methods, big data/machine learning, portfolio construction, performance measurement Key Findings ▪ The authors introduce a procedure to benchmark rule-based investment strategies and to explain the differences in path-dependent risk-adjusted performance measures using interpretable machine learning. ▪ They apply the procedure to the Calmar ratio spread between hierarchical risk parity (HRP) and equal risk contribution (ERC) allocations of a multi-asset futures portfolio and find HRP to have superior risk-adjusted performance. ▪ The authors regress the Calmar ratio spread against statistical features of bootstrapped futures return datasets using XGBoost and apply the SHAP framework by Lundberg and Lee (2017) to discuss the local and global feature importance.","",""
0,"Daniel Grahn, Melonie Richey","The prediction management framework: ethical, governable, and interpretable deployment of artificial intelligence/machine learning systems",2022,"","","","",180,"2022-07-13 09:40:40","","10.1117/12.2617772","","",,,,,0,0.00,0,2,1,"As defense organizations integrate artificial intelligence (AI) into evermore critical operations, especially those near the tactical edge with real-time decision making, the necessity of a standardized, robust framework for deployment and management of AI systems is increasing. In this paper, we propose a Prediction Management Framework (PMF) that aligns with the Department of Defense’s Ethical Principles for AI for ethical, governable, and interpretable deployments. We explore different requirements for the framework with inspiration drawn from various regulatory, safety, and communication standards. In support of these requirements, we offer recommendations and implementation guidance to provide comprehensive visibility into the system.","",""
12,"Frank Male, I. Duncan","Lessons for machine learning from the analysis of porosity-permeability transforms for carbonate reservoirs",2019,"","","","",181,"2022-07-13 09:40:40","","10.31223/osf.io/fwndb","","",,,,,12,4.00,6,2,3,"Abstract Prediction of permeability is one of the most difficult aspects of reservoir characterization because permeability cannot be directly measured by current well logging technology. This is particularly challenging for carbonate rocks. Machine learning (ML) and robust multivariate methods have been developed that have been used in many fields of study to make accurate estimators for variables of interest from both large and small datasets. ML has been criticized for utilizing approaches that are typically not interpretable. That is, it is not clear how the answers are arrived at and what aspects of input data may be resulting in inaccurate results. The current study uses a number of the mathematical algorithms that operate inside ML modules. It applies them to developing porosity-permeability transforms, with or without rock types, to two well-characterized data sets for carbonate reservoirs. One data set is from Jerry Lucia's 1995 study of carbonate rock types, and the other is from a study of the Seminole, West Texas, San Andres Unit. This study of statistical analysis of porosity-permeability transforms includes: transforming the data to normal distributions; performing cross-validation blind testing; and detecting heteroscedasticity by creating plots of residuals. Heteroscedastic data (populations with variable variance) may have an adverse impact on ML algorithms such as Random Forests (RF). We find that including lithofacies information does not greatly improve porosity-permeability transforms. We also propose a number of strategies to make ML analyses of reservoir (and other geosciences) data sets more robust and accurate.","",""
9,"T. Botari, Rafael Izbicki, A. Carvalho","Local Interpretation Methods to Machine Learning Using the Domain of the Feature Space",2019,"","","","",182,"2022-07-13 09:40:40","","10.1007/978-3-030-43823-4_21","","",,,,,9,3.00,3,3,3,"","",""
11,"Tamer Karatekin, S. Sancak, G. Celik, S. Topçuoğlu, G. Karatekin, Pınar Kırcı, A. Okatan","Interpretable Machine Learning in Healthcare through Generalized Additive Model with Pairwise Interactions (GA2M): Predicting Severe Retinopathy of Prematurity",2019,"","","","",183,"2022-07-13 09:40:40","","10.1109/Deep-ML.2019.00020","","",,,,,11,3.67,2,7,3,"We have investigated the risk factors that lead to severe retinopathy of prematurity using statistical analysis and logistic regression as a form of generalized additive model (GAM) with pairwise interaction terms (GA2M). In this process, we discuss the trade-off between accuracy and interpretability of these machine learning techniques on clinical data. We also confirm the intuition of expert neonatologists on a few risk factors, such as gender, that were previously deemed as clinically not significant in RoP prediction.","",""
32,"Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, Jingdong Wang","Informative Dropout for Robust Representation Learning: A Shape-bias Perspective",2020,"","","","",184,"2022-07-13 09:40:40","","","","",,,,,32,16.00,5,6,2,"Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN's texture-bias and its robustness against distribution shift, adversarial perturbation, random corruption, etc. In this work, we attempt at improving various kinds of robustness universally by alleviating CNN's texture bias. With inspiration from the human visual system, we propose a light-weight model-agnostic method, namely Informative Dropout (InfoDrop), to improve interpretability and reduce texture bias. Specifically, we discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. Through extensive experiments, we observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation). To the best of our knowledge, this work is one of the earliest attempts to improve different kinds of robustness in a unified model, shedding new light on the relationship between shape-bias and robustness, also on new approaches to trustworthy machine learning algorithms. Code is available at this https URL.","",""
3,"Houxing Ren, Jingyuan Wang, Wayne Xin Zhao, Ning Wu","RAPT: Pre-training of Time-Aware Transformer for Learning Robust Healthcare Representation",2021,"","","","",185,"2022-07-13 09:40:40","","10.1145/3447548.3467069","","",,,,,3,3.00,1,4,1,"With the development of electronic health records (EHRs), prenatal care examination records have become available for developing automatic prediction or diagnosis approaches with machine learning methods. In this paper, we study how to effectively learn representations applied to various downstream tasks for EHR data. Although several methods have been proposed in this direction, they usually adapt classic sequential models to solve one specific diagnosis task or address unique EHR data issues. This makes it difficult to reuse these existing methods for the early diagnosis of pregnancy complications or provide a general solution to address the series of health problems caused by pregnancy complications. In this paper, we propose a novel model RAPT, which stands for RepresentAtion by Pre-training time-aware Transformer. To associate pre-training and EHR data, we design an architecture that is suitable for both modeling EHR data and pre-training, namely time-aware Transformer. To handle various characteristics in EHR data, such as insufficiency, we carefully devise three pre-training tasks to handle data insufficiency, data incompleteness and short sequence problems, namely similarity prediction, masked prediction and reasonability check. In this way, our representations can capture various EHR data characteristics. Extensive experimental results for four downstream tasks have shown the effectiveness of the proposed approach. We also introduce sensitivity analysis to interpret the model and design an interface to show results and interpretation for doctors. Finally, we implement a diagnosis system for pregnancy complications based on our pre-training model. Doctors and pregnant women can benefit from the diagnosis system in early diagnosis of pregnancy complications.","",""
0,"","Medical Big Data Analytics using Machine Learning Algorithms",2019,"","","","",186,"2022-07-13 09:40:40","","10.35940/ijitee.a5290.119119","","",,,,,0,0.00,0,0,3,"Artificial intelligence and expert systems plays a key role in modern medicine sciences for disease prediction, surveillance interventions, cost efficiency and better quality of life etc. With the arrival of new web-based data sources and systematic data collection through surveys and medical reporting, there is a need of the hour to develop effective recommendation systems which can support practitioners in better decision-making process. Machine Learning Algorithms (MLA) is a powerful tool which enables computers to learn from data. While many novel developed MLA constantly evolves, there is need to develop more systematic, robust algorithm which can interpret with highest possible accuracy, sensitivity and specificity. The study reviews previously published series on different algorithms their advantages and limitations which shall help make future recommendations for researchers and experts seeking to develop an effective algorithm for predicting the likelihood of various diseases.","",""
4,"S. Verstovsek, V. De Stefano, F. Heidel, M. Zuurman, Michael Zaiac, Erwan Bigan, Michael Ruhl, Christoph Meier, M. Beffy, J. Kiladjian","US Optum Database Study in Polycythemia Vera Patients: Thromboembolic Events (TEs) with Hydroxyurea (HU) Vs Ruxolitinib Switch Therapy and Machine-Learning Model to Predict Incidence of Tes and HU Failure",2019,"","","","",187,"2022-07-13 09:40:40","","10.1182/blood-2019-126410","","",,,,,4,1.33,0,10,3,"Introduction: Thromboembolic events (TEs) are one of the most prevalent complications in patients (pts) with polycythemia vera (PV). This real-world evidence study of the US OPTUM database evaluated the incidence of TEs in hydroxyurea (HU)-treated PV pts who either switched to ruxolitinib (RUX) after initial treatment (Tx) with HU (HU-RUX group) or continued HU Tx without switching (HU-alone group). Machine learning was then used to build a precise and scientifically robust model to predict the occurrence of TEs in PV pts with/without a history of TEs and HU failure (defined by either European LeukemiaNet [ELN] hematologic criteria or TEs).  Methods: The OPTUM database comprises claims data and electronic medical records from 90 million pts (2007-2017, median stay in the database=7 years), including 69,464 PV pts. To avoid any selection bias during comparison, only pts treated prior to the RUX market launch were included in the HU-alone group (HU-RUX, n=81; HU-alone, n=195). Due to unavailability of Tx duration, time difference between the first and the last prescription was used as a proxy, and overall Tx duration was matched in both groups. TEs were assessed before Tx initiation in both groups. For HU-RUX pts, it was also assessed while on HU (median duration 27 months) and on RUX (median duration 14 months). For HU-alone pts, it was assessed during the first 27 months of Tx (any pt included in the analysis was treated for longer than this due to duration matching) and during remaining period of Tx (median duration 14 months). TEs were identified by either a restrictive definition (a list of ICD codes containing keywords from the RESPONSE study was automatically generated and manually curated) or a less restrictive one (list of ICD codes was manually expanded to include any TEs matching those from the GEMFIN study).  PV pts who were exclusively treated with HU for ≥6 months were selected (n=2057) for modeling. Outcomes to be predicted were TEs in the 12 months following the end of the 6-month HU Tx period, and HU failure within 3 months of Tx. A logistic regression model was used for prediction. The baseline features extracted from the database included median lab parameters (3-6 months after HU initiation), history of thrombosis prior to primary diagnosis of PV, sociological features (age, gender), comorbidities, and concomitant medications (from inpatient/outpatient tables). Performance assessment methods included Receiver Operating Characteristic-Area Under the Curve (ROC-AUC) in early stages and confusion matrix in later stages; the findings were converted to clinically interpretable decision-tree classification algorithms.  Results: Based on the extensive definition, the annual incidence of TEs in the HU-RUX and HU-alone groups, respectively, was 9% and 7% before HU initiation, which increased to 17% and 13% on HU Tx. The small difference in baseline incidence may reflect residual differences between the two groups. After a median duration of 14 months, the incidence of TEs decreased to 15% in pts who switched to RUX vs an increase to 20% in pts who continued HU Tx. A similar trend was observed using less restrictive definition (Figure 1). This definition resulted in a substantially increased incidence of TEs and a decreased predictive power of the machine-learning model.  Using modeling, decision trees were developed to predict the occurrence of TEs in PV pts with/without a history of TEs. Lymphocyte percentage (<17%) and red cell distribution width (RDW; <15%) were predictors in pts without a history of TEs, whereas lymphocyte percentage (>13%) and platelet count (>393x103/µL) were predictors in pts with a history of TEs (Figure 2).  Based on the decision tree developed to predict HU failure, phlebotomy-dependent pts with >15% RDW had a higher risk of HU failure within 3 months of Tx (Figure 3).  Conclusions: A reduction in the incidence of TEs was observed in pts switching to RUX vs those who continued HU Tx. Based on the findings from this machine-learning model in PV pts, phlebotomy dependency and RDW were indicated as predictors of HU Tx failure within 3 months, whereas lymphocyte percentage+platelet count and lymphocyte percentage+RDW were predictors of incidence of TEs in pts with and without a history of TEs, respectively. Non-adjustment of the results for antiplatelet/anticoagulant Tx was a study limitation. Further validation of this machine-learning model is planned in other European databases.        Verstovsek: Celgene: Consultancy, Research Funding; Gilead: Research Funding; Promedior: Research Funding; CTI BioPharma Corp: Research Funding; Genetech: Research Funding; Protaganist Therapeutics: Research Funding; Constellation: Consultancy; Pragmatist: Consultancy; Incyte: Research Funding; Roche: Research Funding; NS Pharma: Research Funding; Blueprint Medicines Corp: Research Funding; Novartis: Consultancy, Research Funding; Sierra Oncology: Research Funding; Pharma Essentia: Research Funding; Astrazeneca: Research Funding; Ital Pharma: Research Funding. De Stefano:Celgene: Consultancy, Honoraria, Speakers Bureau; Janssen: Consultancy, Honoraria, Speakers Bureau; Amgen: Consultancy, Honoraria, Speakers Bureau; Novartis: Consultancy, Honoraria, Research Funding, Speakers Bureau; Alexion: Consultancy, Honoraria, Speakers Bureau. Heidel:Novartis: Consultancy, Honoraria, Research Funding; Celgene: Consultancy; CTI: Consultancy. Zuurman:Novartis Pharma B.V.: Employment. Zaiac:Novartis: Employment, Equity Ownership. Bigan:Novartis: Consultancy. Ruhl:Novartis: Consultancy. Meier:Novartis: Consultancy. Kiladjian:Celgene: Consultancy; Novartis: Honoraria, Research Funding; AOP Orphan: Honoraria, Research Funding. ","",""
2,"P. Choudhury, Ryan T. Allen, Michael G. Endres","A Machine Learning Methods Framework for Management Research: Application to Exploratory Pattern Detection",2019,"","","","",188,"2022-07-13 09:40:40","","10.2139/ssrn.3518780","","",,,,,2,0.67,1,3,3,"We provide a framework to implement, evaluate and interpret machine learning (ML) methods and apply this framework to identify nonlinear and interactive patterns in data. Compared to traditional causal inference methods, ML algorithms make fewer a priori assumptions about the functional form of the underlying model that best fits the data — thus researchers can use ML methods to inductively explore novel and robust patterns in data. Once identified, these patterns can be tested using traditional econometric methods. Using employee turnover data, we demonstrate how to implement three specific ML algorithms (decision trees, random forests, and neural networks) to uncover meaningful patterns not captured by our baseline econometric model. We demonstrate visual tools to evaluate machine learning model performance and interpret patterns identified in the predictive model. We also summarize human decisions and common pitfalls in implementing machine learning methods. Importantly, ML analysis should be interpreted as exploratory and inductive and not causal. An online appendix provides code to employ the ML algorithms described in the paper.","",""
1,"Arent van Korlaar","Field inversion and machine learning in turbulence modeling",2019,"","","","",189,"2022-07-13 09:40:40","","","","",,,,,1,0.33,1,1,3,"Turbulence closure models will continue to be necessary in order to perform computationally affordable simulations in the foreseeable future. It is expected that Reynolds-averaged Navier-Stokes (RANS) turbulence models will still be useful with the further development of the more accurate, but computationally expensive large eddy simulation (LES), especially in industry. The use of the robust but often inaccurate linear eddy viscosity closures is still widespread in industry. More complex closure models, such as Reynolds stress models and nonlinear eddy viscosity models, provide a more general description of the underlying physics of turbulent flows. Nevertheless, because of implementational difficulties or failure to provide consistent improvements over the more robust linear models, RANS turbulence modeling is considered to have reached a plateau. In the past few years, the availability of high-fidelity datasets, the increased accuracy of machine learning algorithms, and the rise in computational power led to the proposal of several data-driven approaches to turbulence modeling. The general idea is to use experimental and high-fidelity data to develop or enhance RANS turbulence models, instead of employing an approach purely based on physics. As in any emerging field, there are many possibilities for further developing the novel approaches to data-driven turbulence modeling. Recent work combined machine learning with statistical inversion. First, a spatially varying correction is applied to the RANS model and optimized by minimizing the discrepancy between the RANS output and the data for several flows. Machine learning is used to approximate a function between a set of flow features and the inferred corrections. The aim of this work is to further investigate this methodology, called the paradigm of field inversion and machine learning, in a broader set of test cases by inferring a spatially varying correction to the production term of the ω-equation in the k−ω model and to the eigenvalues of the Reynolds stress tensor. The gradients of this high-dimensional optimization process are obtained by implementing the continuous adjoint of the k−ω model in OpenFOAM. Gaussian processes and random forests are then used to approximate a function between mean flow features and the inferred corrections. It was found that both formulations are able to accurately infer the mean velocities and related quantities of interest, but that the inferred corrective terms are often non-unique or not physically interpretable. For several flow cases, the corrective terms were able to generalize to unseen Reynolds number and flow geometries.","",""
1,"William Briguglio, Sherif Saad","Interpreting Machine Learning Malware Detectors Which Leverage N-gram Analysis",2019,"","","","",190,"2022-07-13 09:40:40","","10.1007/978-3-030-45371-8_6","","",,,,,1,0.33,1,2,3,"","",""
48,"Clemens Stachl, F. Pargent, S. Hilbert, Gabriella M. Harari, Ramona Schoedel, Sumer S. Vaid, S. Gosling, M. Bühner","Personality Research and Assessment in the Era of Machine Learning",2019,"","","","",191,"2022-07-13 09:40:40","","10.1002/per.2257","","",,,,,48,16.00,6,8,3,"The increasing availability of high–dimensional, fine–grained data about human behaviour, gathered from mobile sensing studies and in the form of digital footprints, is poised to drastically alter the way personality psychologists perform research and undertake personality assessment. These new kinds and quantities of data raise important questions about how to analyse the data and interpret the results appropriately. Machine learning models are well suited to these kinds of data, allowing researchers to model highly complex relationships and to evaluate the generalizability and robustness of their results using resampling methods. The correct usage of machine learning models requires specialized methodological training that considers issues specific to this type of modelling. Here, we first provide a brief overview of past studies using machine learning in personality psychology. Second, we illustrate the main challenges that researchers face when building, interpreting, and validating machine learning models. Third, we discuss the evaluation of personality scales, derived using machine learning methods. Fourth, we highlight some key issues that arise from the use of latent variables in the modelling process. We conclude with an outlook on the future role of machine learning models in personality research and assessment.","",""
0,"Ali M’Rabeth","Model Risk in the age of Artificial Intelligence and Machine Learning What are the impacts on Model Risk?",2019,"","","","",192,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,1,3,"An increasing reliance on Artificial Intelligence for decision making is driving financial institutions, regulators, and supervisors towards a clarification of sources and control of risks. These risks were either already present (but marginal) or even non-existent in the usual model risk management framework. In a context where the use of machine learning is becoming massive and industrialized across banks and insurance companies, problematics such as interpretability and dynamic monitoring, robustness, ethics, bias and fairness require a specific attention.","",""
8,"Jo-Hsuan Wu, T. Y. A. Liu, W. Hsu, J. Ho, Chien-Chang Lee","Performance and Limitation of Machine Learning Algorithms for Diabetic Retinopathy Screening: Meta-analysis",2021,"","","","",193,"2022-07-13 09:40:40","","10.2196/23863","","",,,,,8,8.00,2,5,1,"Background Diabetic retinopathy (DR), whose standard diagnosis is performed by human experts, has high prevalence and requires a more efficient screening method. Although machine learning (ML)–based automated DR diagnosis has gained attention due to recent approval of IDx-DR, performance of this tool has not been examined systematically, and the best ML technique for use in a real-world setting has not been discussed. Objective The aim of this study was to systematically examine the overall diagnostic accuracy of ML in diagnosing DR of different categories based on color fundus photographs and to determine the state-of-the-art ML approach. Methods Published studies in PubMed and EMBASE were searched from inception to June 2020. Studies were screened for relevant outcomes, publication types, and data sufficiency, and a total of 60 out of 2128 (2.82%) studies were retrieved after study selection. Extraction of data was performed by 2 authors according to PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses), and the quality assessment was performed according to the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2). Meta-analysis of diagnostic accuracy was pooled using a bivariate random effects model. The main outcomes included diagnostic accuracy, sensitivity, and specificity of ML in diagnosing DR based on color fundus photographs, as well as the performances of different major types of ML algorithms. Results The primary meta-analysis included 60 color fundus photograph studies (445,175 interpretations). Overall, ML demonstrated high accuracy in diagnosing DR of various categories, with a pooled area under the receiver operating characteristic (AUROC) ranging from 0.97 (95% CI 0.96-0.99) to 0.99 (95% CI 0.98-1.00). The performance of ML in detecting more-than-mild DR was robust (sensitivity 0.95; AUROC 0.97), and by subgroup analyses, we observed that robust performance of ML was not limited to benchmark data sets (sensitivity 0.92; AUROC 0.96) but could be generalized to images collected in clinical practice (sensitivity 0.97; AUROC 0.97). Neural network was the most widely used method, and the subgroup analysis revealed a pooled AUROC of 0.98 (95% CI 0.96-0.99) for studies that used neural networks to diagnose more-than-mild DR. Conclusions This meta-analysis demonstrated high diagnostic accuracy of ML algorithms in detecting DR on color fundus photographs, suggesting that state-of-the-art, ML-based DR screening algorithms are likely ready for clinical applications. However, a significant portion of the earlier published studies had methodology flaws, such as the lack of external validation and presence of spectrum bias. The results of these studies should be interpreted with caution.","",""
8,"Lakshya Singhal, Yash Garg, Philip Yang, A. Tabaie, A. Wong, Akram Mohammed, Lokesh Chinthala, D. Kadaria, A. Sodhi, A. Holder, A. Esper, J. Blum, R. Davis, G. Clifford, G. Martin, R. Kamaleswaran","eARDS: A multi-center validation of an interpretable machine learning algorithm of early onset Acute Respiratory Distress Syndrome (ARDS) among critically ill adults with COVID-19",2021,"","","","",194,"2022-07-13 09:40:40","","10.1371/journal.pone.0257056","","",,,,,8,8.00,1,16,1,"We present an interpretable machine learning algorithm called ‘eARDS’ for predicting ARDS in an ICU population comprising COVID-19 patients, up to 12-hours before satisfying the Berlin clinical criteria. The analysis was conducted on data collected from the Intensive care units (ICU) at Emory Healthcare, Atlanta, GA and University of Tennessee Health Science Center, Memphis, TN and the Cerner® Health Facts Deidentified Database, a multi-site COVID-19 EMR database. The participants in the analysis consisted of adults over 18 years of age. Clinical data from 35,804 patients who developed ARDS and controls were used to generate predictive models that identify risk for ARDS onset up to 12-hours before satisfying the Berlin criteria. We identified salient features from the electronic medical record that predicted respiratory failure among this population. The machine learning algorithm which provided the best performance exhibited AUROC of 0.89 (95% CI = 0.88–0.90), sensitivity of 0.77 (95% CI = 0.75–0.78), specificity 0.85 (95% CI = 085–0.86). Validation performance across two separate health systems (comprising 899 COVID-19 patients) exhibited AUROC of 0.82 (0.81–0.83) and 0.89 (0.87, 0.90). Important features for prediction of ARDS included minimum oxygen saturation (SpO2), standard deviation of the systolic blood pressure (SBP), O2 flow, and maximum respiratory rate over an observational window of 16-hours. Analyzing the performance of the model across various cohorts indicates that the model performed best among a younger age group (18–40) (AUROC = 0.93 [0.92–0.94]), compared to an older age group (80+) (AUROC = 0.81 [0.81–0.82]). The model performance was comparable on both male and female groups, but performed significantly better on the severe ARDS group compared to the mild and moderate groups. The eARDS system demonstrated robust performance for predicting COVID19 patients who developed ARDS at least 12-hours before the Berlin clinical criteria, across two independent health systems.","",""
0,"M. Abbas","Success of Machine Learning algorithms in Dynamical Mass Measurements of Galaxy Clusters",2019,"","","","",195,"2022-07-13 09:40:40","","","","",,,,,0,0.00,0,1,3,"In recent years, machine learning (ML) algorithms have been successfully employed in Astronomy for analyzing and interpreting the data collected from various surveys. The need for new robust and efficient data analysis tools in Astronomy is imminently growing as we enter the new decade. Astronomical data sets are growing both in size and complexity at an exponential rate and ML methodologies can revolutionize our ability to interpret observations and provide new means of discovery. In this essay we focus on recent success of ML algorithms in predicting the dynamical mass of galaxy clusters. We discuss the results of the study performed by Ho et al. [1] and their implications, where it was found that ML algorithms outperform conventional statistical methods and can offer a robust and accurate tool for dynamical mass estimation.","",""
1111,"T. Baltrušaitis, Chaitanya Ahuja, Louis-Philippe Morency","Multimodal Machine Learning: A Survey and Taxonomy",2017,"","","","",196,"2022-07-13 09:40:40","","10.1109/TPAMI.2018.2798607","","",,,,,1111,222.20,370,3,5,"Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.","",""
37,"","Towards trustable machine learning",2018,"","","","",197,"2022-07-13 09:40:40","","10.1038/s41551-018-0315-x","","",,,,,37,9.25,0,0,4,"","",""
191,"M. Ahmad, A. Teredesai, C. Eckert","Interpretable Machine Learning in Healthcare",2018,"","","","",198,"2022-07-13 09:40:40","","10.1145/3233547.3233667","","",,,,,191,47.75,64,3,4,"This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.","",""
199,"Christoph Molnar, Giuseppe Casalicchio, B. Bischl","iml: An R package for Interpretable Machine Learning",2018,"","","","",199,"2022-07-13 09:40:40","","10.21105/joss.00786","","",,,,,199,49.75,66,3,4,"Complex, non-parametric models, which are typically used in machine learning, have proven to be successful in many prediction tasks. But these models usually operate as black boxes: While they are good at predicting, they are often not interpretable. Many inherently interpretable models have been suggested, which come at the cost of losing predictive power. Another option is to apply interpretability methods to a black box model after model training. Given the velocity of research on new machine learning models, it is preferable to have model-agnostic tools which can be applied to a random forest as well as to a neural network. Tools for model-agnostic interpretability methods should improve the adoption of machine learning.","",""
4,"Kengo Ito, Xiangru Xu, J. Kikuchi","Improved Prediction of Carbonless NMR Spectra by the Machine Learning of Theoretical and Fragment Descriptors for Environmental Mixture Analysis.",2021,"","","","",200,"2022-07-13 09:40:40","","10.1021/acs.analchem.1c00756","","",,,,,4,4.00,1,3,1,"As the first multidimensional NMR approach, 2D J-resolved (2DJ) spectroscopy is distinguished by signal resolution and detection sensitivity with remarkable advantages for the exhaustive evaluation of complex mixtures and environmental samples due to its carbonless feature without the requirement of 13C connectivity. Generally, the 2DJ signal assignment of metabolic mixtures is problematic in spite of references to experimental NMR databases, owing to the existence of metabolic ""dark matter."" In this study, a new method to predict 2DJ spectra was developed with a combination of quantum mechanical (QM) computation and machine learning (ML). The predictive accuracy of J-coupling constants was evaluated using validated data. The root-mean-square deviation (RMSD) for QM computation was 3.52 Hz, while the RMSD for QM + ML was 1.21 Hz, indicating a substantial increase in predictive accuracy. The proposed model was applied to predict the 2DJ spectra of 60 standard substances and 55 components of seawater. Furthermore, two practical environmental samples were used to evaluate the robustness of the constructed predictive model. A J-coupling tree and J-split spectra produced from QM + ML of aliphatic moieties had good consistency with the experimental data, as compared with the theoretical data produced by QM computation. The predicted J-coupling tree for the J-coupling multiplet analysis of freely rotating bonds in the complex mixture, which is traditionally difficult, was interpretable. In addition, in silico identification of the J-split 1H NMR signals, which was independent of experimental databases, aided in the discovery of new components in a mixture.","",""
