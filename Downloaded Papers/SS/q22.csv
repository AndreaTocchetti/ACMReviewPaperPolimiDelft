Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
1,"Liheng Gong, Xiao Zhang, Ling Li","An Artificial Intelligence Fusion Model for Cardiac Emergency Decision Making: Application and Robustness Analysis (Preprint)",2020,"","","","",1,"2022-07-13 09:21:43","","10.2196/preprints.19428","","",,,,,1,0.50,0,3,2,"  BACKGROUND  During cardiac emergency medical treatment, reducing the incidence of avoidable adverse events, ensuring the safety of patients, and generally improving the quality and efficiency of medical treatment have been important research topics in theoretical and practical circles.      OBJECTIVE  This paper examines the robustness of the decision-making reasoning process from the overall perspective of the cardiac emergency medical system.      METHODS  The principle of robustness was introduced into our study on the quality and efficiency of cardiac emergency decision making. We propose the concept of robustness for complex medical decision making by targeting the problem of low reasoning efficiency and accuracy in cardiac emergency decision making. The key bottlenecks such as anti-interference capability, fault tolerance, and redundancy were studied. The rules of knowledge acquisition and transfer in the decision-making process were systematically analyzed to reveal the core role of knowledge reasoning.      RESULTS  The robustness threshold method was adopted to construct the robustness criteria group of the system, and the fusion and coordination mechanism was realized through information entropy, information gain, and mutual information methods.      CONCLUSIONS  A set of fusion models and robust threshold methods such as the R2CMIFS (treatment mode of fibroblastic sarcoma) model and the RTCRF (clinical trial observation mode) model were proposed. Our study enriches the theoretical research on robustness in this field. ","",""
0,"Liheng Gong, Xiao Zhang, Ling Li","An Artificial Intelligence Fusion Model for Cardiac Emergency Decision Making: Application and Robustness Analysis",2020,"","","","",2,"2022-07-13 09:21:43","","10.2196/19428","","",,,,,0,0.00,0,3,2,"Background During cardiac emergency medical treatment, reducing the incidence of avoidable adverse events, ensuring the safety of patients, and generally improving the quality and efficiency of medical treatment have been important research topics in theoretical and practical circles. Objective This paper examines the robustness of the decision-making reasoning process from the overall perspective of the cardiac emergency medical system. Methods The principle of robustness was introduced into our study on the quality and efficiency of cardiac emergency decision making. We propose the concept of robustness for complex medical decision making by targeting the problem of low reasoning efficiency and accuracy in cardiac emergency decision making. The key bottlenecks such as anti-interference capability, fault tolerance, and redundancy were studied. The rules of knowledge acquisition and transfer in the decision-making process were systematically analyzed to reveal the core role of knowledge reasoning. Results The robustness threshold method was adopted to construct the robustness criteria group of the system, and the fusion and coordination mechanism was realized through information entropy, information gain, and mutual information methods. Conclusions A set of fusion models and robust threshold methods such as the R2CMIFS (treatment mode of fibroblastic sarcoma) model and the RTCRF (clinical trial observation mode) model were proposed. Our study enriches the theoretical research on robustness in this field.","",""
0,"Beilei Wang, Jie Jing, Xiaochun Huang, Cheng Hua, Qin Qin, Y. Jia, Zhiyong Wang, Lei Jiang, Bai Gao, Les J. Wu, Xianfei Zeng, Fubo Wang, Chuanbin Mao, Shanrong Liu","Establishment of a Knowledge‐and‐Data‐Driven Artificial Intelligence System with Robustness and Interpretability in Laboratory Medicine",2022,"","","","",3,"2022-07-13 09:21:43","","10.1002/aisy.202100204","","",,,,,0,0.00,0,14,1,"Laboratory medicine plays an important role in clinical diagnosis. However, no laboratory‐based artificial intelligence (AI) diagnostic system has been applied in current clinical practice due to the lack of robustness and interpretability. Although many attempts have been made, it is still difficult for doctors to adopt the existing machine learning (ML) patterns in interpreting laboratory (lab) big data. Here, a knowledge‐and‐data‐driven laboratory diagnostic system is developed, termed AI‐based Lab tEst tO diagNosis (AI LEON), by integrating an innovative knowledge graph analysis framework and “mixed XGboost and Genetic Algorithm (MiXG)” technique to simulate the doctor's laboratory‐based diagnosis. To establish AI LEON, we included 89 116 949 laboratory data and 10 423 581 diagnosis data points from 730 113 participants. Among them, 686 626 participants were recruited for training and validating purposes with the remaining for testing purposes. AI LEON automatically identified and analyzed 2071 lab indexes, resulting in multiple disease recommendations that involved 441 common diseases in ten organ systems. AI LEON exhibited outstanding transparency and interpretability in three universal clinical application scenarios and outperformed human physicians in interpreting lab reports. AI LEON is an advanced intelligent system that enables a comprehensive interpretation of lab big data, which substantially improves the clinical diagnosis.","",""
0,"D. Lange","Robustness of artificial intelligence in the face of novelty",2022,"","","","",4,"2022-07-13 09:21:43","","10.1117/12.2622912","","",,,,,0,0.00,0,1,1,"A critical factor in utilizing agents with Artificial Intelligence (AI) is their robustness to novelty. AI agents include models that are either engineered or trained. Engineered models include knowledge of those aspects of the environment that are known and considered important by the engineers. Learned models form embeddings of aspects of the environment based on connections made through the training data. In operation, however, a rich environment is likely to present challenges not seen in training sets or accounted for in engineered models. Worse still, adversarial environments are subject to change by opponents. A program at the Defense Advanced Research Project Agency (DARPA) seeks to develop the science necessary to develop and evaluate agents that are robust to novelty. This capability will be required, before AI has the role envisioned within mission critical environments.","",""
11,"S. Tripathi, David Muhr, Manuel Brunner, F. Emmert‐Streib, H. Jodlbauer, M. Dehmer","Ensuring the Robustness and Reliability of Data-Driven Knowledge Discovery Models in Production and Manufacturing",2020,"","","","",5,"2022-07-13 09:21:43","","10.3389/frai.2021.576892","","",,,,,11,5.50,2,6,2,"The Cross-Industry Standard Process for Data Mining (CRISP-DM) is a widely accepted framework in production and manufacturing. This data-driven knowledge discovery framework provides an orderly partition of the often complex data mining processes to ensure a practical implementation of data analytics and machine learning models. However, the practical application of robust industry-specific data-driven knowledge discovery models faces multiple data- and model development-related issues. These issues need to be carefully addressed by allowing a flexible, customized and industry-specific knowledge discovery framework. For this reason, extensions of CRISP-DM are needed. In this paper, we provide a detailed review of CRISP-DM and summarize extensions of this model into a novel framework we call Generalized Cross-Industry Standard Process for Data Science (GCRISP-DS). This framework is designed to allow dynamic interactions between different phases to adequately address data- and model-related issues for achieving robustness. Furthermore, it emphasizes also the need for a detailed business understanding and the interdependencies with the developed models and data quality for fulfilling higher business objectives. Overall, such a customizable GCRISP-DS framework provides an enhancement for model improvements and reusability by minimizing robustness-issues.","",""
25,"Abbas Abbaszadeh Shahri, S. Larsson, Crister Renkel","Artificial intelligence models to generate visualized bedrock level: a case study in Sweden",2020,"","","","",6,"2022-07-13 09:21:43","","10.1007/s40808-020-00767-0","","",,,,,25,12.50,8,3,2,"","",""
1,"K. Panetta, Landry Kezebou, Victor Oludare, J. Intriligator, S. Agaian","Artificial Intelligence for Text-Based Vehicle Search, Recognition, and Continuous Localization in Traffic Videos",2021,"","","","",7,"2022-07-13 09:21:43","","10.3390/ai2040041","","",,,,,1,1.00,0,5,1,"The concept of searching and localizing vehicles from live traffic videos based on descriptive textual input has yet to be explored in the scholarly literature. Endowing Intelligent Transportation Systems (ITS) with such a capability could help solve crimes on roadways. One major impediment to the advancement of fine-grain vehicle recognition models is the lack of video testbench datasets with annotated ground truth data. Additionally, to the best of our knowledge, no metrics currently exist for evaluating the robustness and performance efficiency of a vehicle recognition model on live videos and even less so for vehicle search and localization models. In this paper, we address these challenges by proposing V-Localize, a novel artificial intelligence framework for vehicle search and continuous localization captured from live traffic videos based on input textual descriptions. An efficient hashgraph algorithm is introduced to compute valid target information from textual input. This work further introduces two novel datasets to advance AI research in these challenging areas. These datasets include (a) the most diverse and large-scale Vehicle Color Recognition (VCoR) dataset with 15 color classes—twice as many as the number of color classes in the largest existing such dataset—to facilitate finer-grain recognition with color information; and (b) a Vehicle Recognition in Video (VRiV) dataset, a first of its kind video testbench dataset for evaluating the performance of vehicle recognition models in live videos rather than still image data. The VRiV dataset will open new avenues for AI researchers to investigate innovative approaches that were previously intractable due to the lack of annotated traffic vehicle recognition video testbench dataset. Finally, to address the gap in the field, five novel metrics are introduced in this paper for adequately accessing the performance of vehicle recognition models in live videos. Ultimately, the proposed metrics could also prove intuitively effective at quantitative model evaluation in other video recognition applications. T One major advantage of the proposed vehicle search and continuous localization framework is that it could be integrated in ITS software solution to aid law enforcement, especially in critical cases such as of amber alerts or hit-and-run incidents.","",""
0,"Poona Bahrebar, Leon Denis, Maxim Bonnaerens, Kristof Coddens, J. Dambre, W. Favoreel, I. Khvastunov, A. Munteanu, Hung Nguyen-Duc, S. Schulte, D. Stroobandt, Ramses Valvekens, N. V. D. Broeck, Geert Verbruggen","cREAtIve: reconfigurable embedded artificial intelligence",2021,"","","","",8,"2022-07-13 09:21:43","","10.1145/3457388.3458857","","",,,,,0,0.00,0,14,1,"cREAtIve targets the development of novel highly-adaptable embedded deep learning solutions for automotive and traffic monitoring applications, including position sensor processing, scene interpretation based on LiDAR, and object detection and classification in thermal images for traffic camera systems. These applications share the need for deep learning solutions tailored for deployment on embedded devices with limited resources and featuring high adaptability and robustness to changing environmental conditions. cREAtIve develops knowledge, tools and methods that enable hardware-efficient, adaptable, and robust deep learning.","",""
5,"David Abele, Sara D’Onofrio","Artificial Intelligence – The Big Picture",2020,"","","","",9,"2022-07-13 09:21:43","","10.1007/978-3-658-27941-7_2","","",,,,,5,2.50,3,2,2,"","",""
2,"Yuan Huang, Z. Cheng, Qianyu Zhou, Yuxing Xiang, Ruixiao Zhao","Data Mining Algorithm for Cloud Network Information Based on Artificial Intelligence Decision Mechanism",2020,"","","","",10,"2022-07-13 09:21:43","","10.1109/ACCESS.2020.2981632","","",,,,,2,1.00,0,5,2,"Due to the rapid development of information technology and network technology, there is a lot of data, but the phenomenon of lack of knowledge is becoming more and more serious. Data mining technology has developed vigorously in this environment, and it has shown more and more vitality. Based on Spark programming model, this paper designs the parallel extension of fuzzy c-means. In order to enhance the performance of fuzzy c-means parallel expansion, the improvement strategy of k-means during the initialization phase is borrowed, and k-means// is extended to fuzzy c-means to obtain better clustering performance. Combined with Spark’s programming model, this paper can obtain extended parallel fuzzy c-means algorithm. Several experiments on the data set of the algorithm proposed in this paper have shown good scalability and parallelism, effectively expanding fuzzy c-means clustering to distributed applications, greatly increasing the scale of the data processed by the algorithm. This improves the robustness of the algorithm and the adaptability of the algorithm to the shape and structure of the data, so that the parallel and scalable clustering algorithm can more effectively perform cluster analysis on big data. Three algorithms were simulated on MATLAB platform. We use simple data sets and complex two-dimensional data sets, and compare with the traditional fuzzy c-means algorithm and fuzzy c-means algorithm based on fuzzy entropy. Experiments show that the scalable parallel fuzzy c-means algorithm not only greatly improves the anti-noise performance, but also improves the convergence speed, and it can automatically determine the optimal number of clusters.","",""
1,"H. López-Fernández","Application of data mining and artificial intelligence techniques to mass spectrometry data for knowledge discovery",2016,"","","","",11,"2022-07-13 09:21:43","","","","",,,,,1,0.17,1,1,6,"Mass spectrometry using matrix assisted laser desorption ionization coupled to time of flight analyzers (MALDI-TOF MS) has become popular during the last decade due to its high speed, sensitivity and robustness for detecting proteins and peptides. This allows quickly analyzing large sets of samples are in one single batch and doing high-throughput proteomics. In this scenario, bioinformatics methods and computational tools play a key role in MALDI-TOF data analysis, as they are able handle the large amounts of raw data generated in order to extract new knowledge and useful conclusions. A typical MALDI-TOF MS data analysis workflow has three main stages: data acquisition, preprocessing and analysis. Although the most popular use of this technology is to identify proteins through their peptides, analyses that make use of artificial intelligence (AI), machine learning (ML), and statistical methods can be also carried out in order to perform biomarker discovery, automatic diagnosis, and knowledge discovery. In this research work, this workflow is deeply explored and new solutions based on the application of AI, ML, and statistical methods are proposed. In addition, an integrated software platform that supports the full MALDI-TOF MS data analysis workflow that facilitate the work of proteomics researchers without advanced bioinformatics skills has been developed and released to the scientific community.","",""
0,"H. L. Fernández","Application of data mining and artificial intelligence techniques to mass spectrometry data for knowledge discovery",2016,"","","","",12,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,1,6,"Mass spectrometry using matrix assisted laser desorption ionization coupled to time of flight analyzers (MALDI-TOF MS) has become popular during the last decade due to its high speed, sensitivity and robustness for detecting proteins and peptides. This allows quickly analyzing large sets of samples are in one single batch and doing high-throughput proteomics. In this scenario, bioinformatics methods and computational tools play a key role in MALDI-TOF data analysis, as they are able handle the large amounts of raw data generated in order to extract new knowledge and useful conclusions. A typical MALDI-TOF MS data analysis workflow has three main stages: data acquisition, preprocessing and analysis. Although the most popular use of this technology is to identify proteins through their peptides, analyses that make use of artificial intelligence (AI), machine learning (ML), and statistical methods can be also carried out in order to perform biomarker discovery, automatic diagnosis, and knowledge discovery. In this research work, this workflow is deeply explored and new solutions based on the application of AI, ML, and statistical methods are proposed. In addition, an integrated software platform that supports the full MALDI-TOF MS data analysis workflow that facilitate the work of proteomics researchers without advanced bioinformatics skills has been developed and released to the scientific community.","",""
75,"Qing Sun, Min Zhang, A. Mujumdar","Recent developments of artificial intelligence in drying of fresh food: A review",2019,"","","","",13,"2022-07-13 09:21:43","","10.1080/10408398.2018.1446900","","",,,,,75,25.00,25,3,3,"ABSTRACT Intellectualization is an important direction of drying development and artificial intelligence (AI) technologies have been widely used to solve problems of nonlinear function approximation, pattern detection, data interpretation, optimization, simulation, diagnosis, control, data sorting, clustering, and noise reduction in different food drying technologies due to the advantages of self-learning ability, adaptive ability, strong fault tolerance and high degree robustness to map the nonlinear structures of arbitrarily complex and dynamic phenomena. This article presents a comprehensive review on intelligent drying technologies and their applications. The paper starts with the introduction of basic theoretical knowledge of ANN, fuzzy logic and expert system. Then, we summarize the AI application of modeling, predicting, and optimization of heat and mass transfer, thermodynamic performance parameters, and quality indicators as well as physiochemical properties of dried products in artificial biomimetic technology (electronic nose, computer vision) and different conventional drying technologies. Furthermore, opportunities and limitations of AI technique in drying are also outlined to provide more ideas for researchers in this area.","",""
0,"Qi Deng","Artificial Intelligence BlockCloud (AIBC) Technical Whitepaper",2018,"","","","",14,"2022-07-13 09:21:43","","10.2139/ssrn.3464239","","",,,,,0,0.00,0,1,4,"The AIBC is an Artificial Intelligence and blockchain technology based large-scale decentralized ecosystem that allows system-wide low-cost sharing of computing and storage resources. The AIBC consists of four layers: a fundamental layer, a resource layer, an application layer, and an ecosystem layer. The AIBC implements a two-consensus scheme to enforce upper-layer economic policies and achieve fundamental layer performance and robustness: the DPoEV incentive consensus on the application and resource layers, and the DABFT distributed consensus on the fundamental layer. The DABFT uses deep learning techniques to predict and select the most suitable BFT algorithm in order to achieve the best balance of performance, robustness, and security. The DPoEV uses the knowledge map algorithm to accurately assess the economic value of digital assets.","",""
0,"Gang Li, Tongzhou Zhao","Approach of Intelligence Question-Answering System Based on Physical Fitness Knowledge Graph",2021,"","","","",15,"2022-07-13 09:21:43","","10.1109/RCAE53607.2021.9638824","","",,,,,0,0.00,0,2,1,"Artificial intelligence’s penetrating sports is a new development trend of modern sports. Physical Intelligence Question-Answering System (QAS) is a typical application of artificial intelligence in sports, which can quickly respond the physic fitness questions raised by people. This paper aims the construction method of physical fitness QAS based on knowledge graph. Firstly, the physical fitness knowledge graph is constructed based on the crawling data and expert knowledge. Secondly, several physical knowledge question templates are constructed. Thirdly, the Bayesian classifier is used to classify the questions and the Bidirectional Long Short-Term Memory (BiLSTM) combined with Conditional Random Fields (CRF) method is applied to extract contents from the input questions. Then a matching algorithm based on bidirectional slicing string and a statistical method are performed to implement the fuzzy query to enhance the accuracy and robustness of the QAS. The experiments show that the accuracy of physical fitness QAS can reach 93.5% when the question sentences are matched with the query templates, and 86.5% when not matched.","",""
22,"L. Valiant","Knowledge Infusion: In Pursuit of Robustness in Artificial Intelligence",2008,"","","","",16,"2022-07-13 09:21:43","","10.4230/LIPIcs.FSTTCS.2008.1770","","",,,,,22,1.57,22,1,14,"Endowing computers with the ability to apply commonsense knowledge with human- level performance is a primary challenge for computer science, comparable in importance to past great challenges in other fields of science such as the sequencing of the human genome. The right approach to this problem is still under debate. Here we shall discuss and attempt to justify one ap- proach, that of knowledge infusion. This approach is based on the view that the fundamental objective that needs to be achieved is robustness in the following sense: a framework is needed in which a computer system can represent pieces of knowledge about the world, each piece having some un- certainty, and the interactions among the pieces having even more uncertainty, such that the system can nevertheless reason from these pieces so that the uncertainties in its conclusions are at least controlled. In knowledge infusion rules are learned from the world in a principled way so that sub- sequent reasoning using these rules will also be principled, and subject only to errors that can be bounded in terms of the inverse of the effort invested in the learning process.","",""
20,"FarzinPiltan, MarziehKamgari, SaeedZare, FatemehShahryarZadeh, M. Mansoorzadeh","Design Novel Model Reference Artificial Intelligence Based Methodology to Optimized Fuel Ratio in IC Engine",2013,"","","","",17,"2022-07-13 09:21:43","","10.5815/IJIEEB.2013.02.07","","",,,,,20,2.22,4,5,9,"In this research, model reference fuzzy based control is presented as robust controls for IC engine. The objective of the study is to design controls for IC engines without the knowledge of the boundary of uncertainties and dynamic information by using fuzzy model reference PD p lus mass of air while improve the robustness of the PD p lus mass of air control. A PD plus mass of air provides for eliminate the mass of air and ultimate accuracy in the presence of the bounded disturbance/uncertainties, although this methods also causes some oscillation. The fuzzy PD plus mass of air is proposed as a solution to the problems crated by unstability. Th is method has a good performance in presence of uncertainty.","",""
0,"Zhijie Lu, Changzhu Zhang, Hao Zhang, Zhuping Wang, Chao Huang, Yuxiong Ji","Deep Reinforcement Learning Based Autonomous Racing Car Control With Priori Knowledge",2021,"","","","",18,"2022-07-13 09:21:43","","10.1109/cac53003.2021.9728289","","",,,,,0,0.00,0,6,1,"In the community of artificial intelligence, re-searchers have devoted much effort to the application of deep reinforcement learning algorithms for autonomous driving. Under deep reinforcement learning framework, it is important for the racing car agent to interact with its external environment to accumulate enough driving experience. However, the inter-action process is usually inefficient, risky and time-consuming. Furthermore, it is a common problem in relevant studies that brake policy is difficult to master. In this paper, we adopt some priori knowledge about vehicle dynamics to design the brake force and update it to the actor-critic network by soft-learning strategy. In addition, some effective strategies are developed to improve the training efficiency and control performance. The Open Racing Car Simulator(TORCS) is adopted to evaluate our algorithm. The simulation results show the effectiveness of our proposed algorithm with better learning efficiency, robustness and generalization performance.","",""
0,"P. Grenier, I. Álvarez, Jean-Marie Roger, V. Steinmetz","ARTIFICIAL INTELLIGENCE IN WINE-MAKING L ’ INTELLIGENCE ARTIFICIELLE EN ŒNOLOGIE",2008,"","","","",19,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,4,14,"In this paper, some terms of Artificial Intelligence are defined. Some present and potential applications of knowledge based systems are presented in the field of wine-making. Areas of concern were: multi sensor fusion, prediction by model cooperation, and diagnosis. Artificial intelligence techniques can indeed be applied for aiding the wine-maker in his choices. They facilitate the combination between experience and recent progress in technology. When associated with statistical processing, they allow knowledge sources to be used more effectively. Beyond wine-making, the prospects of artificial intelligence are promising for research and food industry, especially for improving the robustness of measurement systems (multi-sensors, sensors interpreted or validated by models), and for process diagnosis (risk prediction, action proposal). Résumé : Certains termes d’Intelligence Artificielle (IA) sont définis dans cette publication. Quelques applications en cours ou potentielles de systèmes fondés sur la connaissance sont présentés dans le domaine de la vinification. Les domaines d’étude sont : la fusion multi-capteurs, la prédiction par coopération de modèles, et le diagnostic. Les techniques IA peuvent aider le vinificateur dans ses choix par une symbiose entre expérience et progrès technologiques. En association avec des traitements statistiques, ces techniques permettent une utilisation plus efficace des sources de connaissances. Au-delà de la vinification, les perpectives de l’intelligence artificielle sont prometteuses en industrie alimentaire, en particulier pour améliorer la robustesse d’un système de mesure (fustion de capteurs, validation d’un capteur par un modèle) ou pour élaborer un diagnostic sur l’évolution d’un procédé (prédiction de risques, proposition d’intervention).","",""
3,"F. Lécué, B. Abeloos, Jonathan Anctil, Manuel Bergeron, Damien Dalla-Rosa, Simon Corbeil-Letourneau, Florian Martet, Tanguy Pommellet, L. Salvan, Simon Veilleux, M. Ziaeefard","Thales XAI Platform: Adaptable Explanation of Machine Learning Systems - A Knowledge Graphs Perspective",2019,"","","","",20,"2022-07-13 09:21:43","","","","",,,,,3,1.00,0,11,3,"Explanation in Machine Learning systems has been identified to be the main asset to have for large scale deployment of Artificial Intelligence (AI) in critical systems. Explanations could be example-, features-, semantics-based or even counterfactual to potentially action on an AI system; they could be represented in many different ways e.g., textual, graphical, or visual. All representations serve different means, purpose and operators. We built the first-of-its-kind XAI (eXplainable AI) platform for critical systems i.e., Thales XAI Platform which aims at serving explanations through various forms. This paper emphasizes on the semantics-based explanations for Machine Learning systems. 1 Explainable AI in Critical Systems Motivation: The current hype of Artificial Intelligence (AI) mostly refers to the success of Machine Learning (ML) and its sub-domain of deep learning. However industries operating with critical systems are either highly regulated, or require high level of certification and robustness. Therefore, such industry constraints do limit the adoption of non deterministic and ML systems. Answers to the question of explainability will be intrinsically connected to the adoption of AI in industry at scale. Indeed explanation, which could be used for debugging intelligent systems or deciding to follow a recommendation in real-time, will increase acceptance and (business) user trust. Explainable AI (XAI) is now referring to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. Focus: Thales XAI Platform is designed to provide explanation for a ML task (classification, regression, object detection, segmentation). Although Thales XAI Platform does provide different levels of explanation e.g., example-based, featuresbased, counterfactual using textual and visual representations, we emphasis only on the semantics-based explanation through knowledge graphs. ? Copyright c © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). Critical Applications: From adapting a plane trajectory, stopping a train, refitting a boat to reconfiguring a satellite, all are examples of critical situations where explanation is a must-to-have to follow an AI system decision. 2 Why Knowledge Graphs for Explainable AI? State-of-the-Art Limitations: Most approaches limits explanation of ML systems to features involved in the data and model, or at best to examples, prototypes or counterfactuals. Explanation should go beyond correlation (features importance) and numerical similarity (local explanation). Opportunity: By expanding and linking initial (training, validation and test) data with entities in knowledge graphs, (i) context is encoded, (ii) connections and relations are exposed, and (iii) inference and causation are natively supported. Knowledge graphs are used for encoding better representation of data, structuring a ML model in a more interpretable way, and adopt a semantic similarity for local (instance-based) and global (model-based) explanation. 3 Thales XAI Platform: A Knowledge Graph Perspective (Semantic) Perspective: The platform is combining ML and reasoning functionalities to expose a human-like rational as explanation when (i) recognizing an object (in a raw image) of any class in a knowledge graph, (ii) predicting a link in a knowledge graph. Thales XAI Platform is using state-of-the-art Semantic Web tools for enriching input, output (class) data with DBpedia (4, 233, 000 resources) and domain-specific knowledge graphs, usually enterprise knowledge graphs. This is a crucial step for contextualizing training, validation, test data. Explainable ML Classifications: Starting from raw images, as unstructured data, but with class labels augmented with a domain knowledge graph, Thales XAI Platform relies on existing neural network architectures to build the most appropriate models. All confidence scores of output classes on any input image are updated based on the semantic description of the output classes. For instance, an input classified as a car will have a higher overall confidence score in case some properties of car in the knowledge graph are retrieved e.g., having wheels, being on a road. In addition the platform is embedding naturally explanation i.e., properties of the objects retrieved in both the raw data and knowledge graph. Explainable Relational Learning: Starting from relational data, structured as graph, and augmented with a domain knowledge graph, Thales XAI Platform relies on existing knowledge graph embeddings frameworks to build the most appropriate models. Explanation of any link prediction is retrieved by identifying representative hotspots in the knowledge graph i.e., connected parts of the graphs that negatively impact prediction accuracy when removed.","",""
110,"C. Kulikowski","Artificial intelligence methods and systems for medical consultation",1980,"","","","",21,"2022-07-13 09:21:43","","10.1109/TPAMI.1980.6592368","","",,,,,110,2.62,110,1,42,"The major AI problems that arise in designing a consultation program involve choices of knowledge representations, diagnostic interpretation strategies, and treatment planning strategies. The need to justify decisions and update the knowledge base in the light of new research findings places a premium on the modularity of a representation and the ease with which its reasoning procedures can be explained. In both diagnosis and treatment decisions, the relative advantages and disadvantages of different schemes for quantifying the uncertainty of inferences raises difficult issues of a formal logical nature, as well as many specific practical problems of system design. An important insight that has resulted from the design of several artificial intelligence systems is that robustness of performance in the presence of many uncertainty relationships can be achieved by eliciting from the expert a segmentation of knowledge that will also provide a rich network of deterministic relationships to interweave the space of hypotheses.","",""
5,"M. Selfridge, D. J. Dickerson, S. F. Biggs","Cognitive Expert Systems and Machine Learning: Artificial Intelligence Research at the University of Connecticut",1987,"","","","",22,"2022-07-13 09:21:43","","10.1609/AIMAG.V8I1.577","","",,,,,5,0.14,2,3,35,"In order for next-generation expert systems to demonstrate the performance, robustness, flexibility, and learning ability of human experts, they will have to be based on cognitive models of expert human reasoning and learning. We call such next-generation systems cognitive expert systems. Research at the Artificial Intelligence Laboratory at the University of Connecticut is directed toward understanding the principles underlying cognitive expert systems and developing computer programs embodying those principles. The Causal Model Acquisition System (CMACS) learns causal models of physical mechanisms by understanding real-world natural language explanations of those mechanisms. The going Concern Expert ( GCX) uses business and environmental knowledge to assess whether a company will remain in business for at least the following year. The Business Information System (BIS) acquires business and environmental knowledge from in-depth reading of real-world news stories. These systems are based on theories of expert human reasoning and learning, and thus represent steps toward next-generation cognitive expert systems.","",""
2,"O. Deutsch","Artificial intelligence design challenge - Background, analysis, andrelative performance of algorithms",1988,"","","","",23,"2022-07-13 09:21:43","","10.2514/3.20326","","",,,,,2,0.06,2,1,34,"The Artificial Intelligence Design Challenge was an attempt to stimulate interest in a common problem involving the application of artificial intelligence technology to problems likely to be encountered in planning, scheduling, and battle management. These problems are characterized by high combinatorial complexity, uncertainty, constraints, and, in some cases, requirements for real-time performance on finite-speed processors. Participants in the design challenge submitted competing, alternative approaches, implemented in computer code executable on desktop microcomputers, for assessment and relative evaluation over a range of problems. The range of problems was generated by variation of an input data file at the time of contest judging. The participants were given a priori knowledge of only the range of data variations, and not the specific details. In this manner, robustness to problem variations was evaluated, as was normalized performance of competing algorithms and implementations.","",""
3,"D. Schutzer","Applications of Artificial Intelligence to Military Communications",1983,"","","","",24,"2022-07-13 09:21:43","","10.1109/MILCOM.1983.4794808","","",,,,,3,0.08,3,1,39,"This paper explores the field of artificial intelligence with respect to its application to military communication design problems. In particular it is shown how natural processing languages and knowledge-based system technologies can be used to reduce the required communications capacity and to improve a communication systems robustness and tolerance of errors by trading-off computation for communication. These technologies are also shown to improve the security of a military communications system operation. Other applications of artificial technology include the use of expert system technology to the operation, control and maintenance, and training areas.","",""
1,"P. Grenier, I. Álvarez, Jean-Marie Roger, V. Steinmetz, P. Barré, J. Sablayrolles","ARTIFICIAL INTELLIGENCE IN WINE-MAKING",2000,"","","","",25,"2022-07-13 09:21:43","","10.20870/OENO-ONE.2000.34.2.1007","","",,,,,1,0.05,0,6,22,"In this paper, some terms of Artificial Intelligence are defined. Some present and potential applications of knowledge based systems are presented in the field of wine-making. Areas of concern were: multi sensor fusion, prediction by model cooperation, and diagnosis. Artificial intelligence techniques can indeed be applied for aiding the wine-maker in his choices. They facilitate the combination between experience and recent progress in technology. When associated with statistical processing, they allow knowledge sources to be used more effectively. Beyond wine-making, the prospects of artificial intelligence are promising for research and food industry, especially for improving the robustness of measurement systems (multi-sensors, sensors interpreted or validated by models), and for process diagnosis (risk prediction, action proposal).","",""
1,"J. Delgado-Frias, W. Moore","A wafer-scale architecture for artificial intelligence",1989,"","","","",26,"2022-07-13 09:21:43","","10.1109/WAFER.1989.47543","","",,,,,1,0.03,1,2,33,"The architecture presented exploits the advantages of wafer-scale integration technology and has a defect-tolerant scheme to overcome silicon defects. It is in principle a two-dimensional array that is suited to process semantic network knowledge bases. The defect-tolerance approach is based on a combination of hardware redundancy and robust algorithms run on the architecture. The application that is presented here is the scene labeling that is used in computer vision. Due to the robustness of the scene labeling algorithms the machine can tolerate some hardware faults at run time.<<ETX>>","",""
6,"F. Barber, M. Salido","Robustness, stability, recoverability, and reliability in constraint satisfaction problems",2015,"","","","",27,"2022-07-13 09:21:43","","10.1007/s10115-014-0778-3","","",,,,,6,0.86,3,2,7,"","",""
9,"Avadh Kishor, P. Singh","Comparative Study of Artificial Bee Colony Algorithm and Real Coded Genetic Algorithm for Analysing Their Performances and Development of a New Algorithmic Framework",2015,"","","","",28,"2022-07-13 09:21:43","","10.1109/ISCMI.2015.29","","",,,,,9,1.29,5,2,7,"This paper compares performance of the artificial bee colony algorithm (ABC) and the real coded genetic algorithm (RCGA) on a suite of 9 standard benchmark problems. The problem suite comprises a diverse set of unimodal, multimodal and rotated multimodal numerical optimization functions and the comparison criteria include (i) solution quality, (ii) convergence speed, (iii) robustness, and (iv) scalability to test efficacy of the algorithms. To the best knowledge of the authors, such a comprehensive comparative study of the two algorithms is not available in the literature. An empirical study shows that the RCGA has advantages over the ABC in terms of all the criteria for the unimodal and the rotated multimodal functions. On other hand, the ABC outperforms the RCGA in terms of solution quality for the multimodal functions. Therefore, based on the insights gained out of this comparative study, the authors propose an algorithm ABC-GA with new algorithmic framework that comprises advantages of both the ABC and the GA. An empirical study of the proposed algorithm ABC-GA shows its promising performance as the obtained results are superior to both the comparative algorithms for all the problems in all the criteria.","",""
1,"Fang Zhang, Xiaochen Wang, J. Han, Jie Tang, Shiyin Wang","Fast Top-k Area Topics Extraction with Knowledge Base",2017,"","","","",29,"2022-07-13 09:21:43","","10.1109/DSC.2018.00016","","",,,,,1,0.20,0,5,5,"What are the most representative research topics in Artificial Intelligence (AI)? We formulate the problem as extracting top-k topics that can best represent a given area with the help of knowledge base. We theoretically prove that the problem is NP-hard and propose an optimization model, FastKATE, to address this problem by combining both explicit and latent representations for each topic. We leverage a large-scale knowledge base (Wikipedia) to generate topic embeddings using neural networks and use this kind of representations to help capture the representativeness of topics for given areas. We develop a fast heuristic algorithm to efficiently solve the problem with a provable error bound. We evaluate the proposed model on three real-world datasets. Experimental results demonstrate our model's effectiveness, robustness, real-timeness (return results in <1s), and its superiority over several alternative methods.","",""
29,"A. Umbrico, A. Cesta, Gabriella Cortellessa, Andrea Orlandini","A Holistic Approach to Behavior Adaptation for Socially Assistive Robots",2020,"","","","",30,"2022-07-13 09:21:43","","10.1007/s12369-019-00617-9","","",,,,,29,14.50,7,4,2,"","",""
25,"Anders Søgaard, Barbara Plank, Héctor Martínez Alonso","Using Frame Semantics for Knowledge Extraction from Twitter",2015,"","","","",31,"2022-07-13 09:21:43","","10.1609/aaai.v29i1.9524","","",,,,,25,3.57,8,3,7,"    Knowledge bases have the potential to advance artificial intelligence, but often suffer from recall problems, i.e., lack of knowledge of new entities and relations. On the contrary, social media such as Twitter provide abundance of data, in a timely manner: information spreads at an incredible pace and is posted long before it makes it into more commonly used resources for knowledge extraction. In this paper we address the question whether we can exploit social media to extract new facts, which may at first seem like finding needles in haystacks. We collect tweets about 60 entities in Freebase and compare four methods to extract binary relation candidates, based on syntactic and semantic parsing and simple mechanism for factuality scoring. The extracted facts are manually evaluated in terms of their correctness and relevance for search. We show that moving from bottom-up syntactic or semantic dependency parsing formalisms to top-down frame-semantic processing improves the robustness of knowledge extraction, producing more intelligible fact candidates of better quality. In order to evaluate the quality of frame semantic parsing on Twitter intrinsically, we make a multiply frame-annotated dataset of tweets publicly available.   ","",""
4,"S. Back, Seongju Lee, Sungho Shin, Yeonguk Yu, Taekyeong Yuk, Saepomi Jong, Seungjun Ryu, Kyoobin Lee","Robust Skin Disease Classification by Distilling Deep Neural Network Ensemble for the Mobile Diagnosis of Herpes Zoster",2021,"","","","",32,"2022-07-13 09:21:43","","10.1109/ACCESS.2021.3054403","","",,,,,4,4.00,1,8,1,"Herpes zoster (HZ) is a common cutaneous disease affecting one out of five people; hence, early diagnosis of HZ is crucial as it can progress to chronic pain syndrome if antiviral treatment is not provided within 72 hr. Mobile diagnosis of HZ with the assistance of artificial intelligence can prevent neuropathic pain while reducing clinicians’ fatigue and diagnosis cost. However, the clinical images captured from daily mobile devices likely contain visual corruptions, such as motion blur and noise, which can easily mislead the automated system. Hence, this paper aims to train a robust and mobile deep neural network (DNN) that can distinguish HZ from other skin diseases using user-submitted images. To enhance robustness while retaining low computational cost, we propose a knowledge distillation from ensemble via curriculum training (KDE-CT) wherein a student network learns from a stronger teacher network progressively. We established skin diseases dataset for HZ diagnosis and evaluated the robustness against 75 types of corruption. A total of 13 different DNNs was evaluated on both clean and corrupted images. The experiment result shows that the proposed KDE-CT significantly improves corruption robustness when compared with other methods. Our trained MobileNetV3-Small achieved more robust performance (93.5% overall accuracy, 67.6 mean corruption error) than the DNN ensemble with smaller computation (549x fewer multiply-and-accumulate operations), which makes it suitable for mobile skin lesion analysis.","",""
2,"Laura von Rueden, T. Wirtz, Fabian Hueger, Jan David Schneider, N. Piatkowski, C. Bauckhage","Street-Map Based Validation of Semantic Segmentation in Autonomous Driving",2021,"","","","",33,"2022-07-13 09:21:43","","10.1109/ICPR48806.2021.9413292","","",,,,,2,2.00,0,6,1,"Artificial intelligence for autonomous driving must meet strict requirements on safety and robustness, which motivates the thorough validation of learned models. However, current validation approaches mostly require ground truth data and are thus both cost-intensive and limited in their applicability. We propose to overcome these limitations by a model agnostic validation using a-priori knowledge from street maps. In particular, we show how to validate semantic segmentation masks and demonstrate the potential of our approach using OpenStreetMap. We introduce validation metrics that indicate false positive or negative road segments. Besides the validation approach, we present a method to correct the vehicle's GPS position so that a more accurate localization can be used for the street-map based validation. Lastly, we present quantitative results on the Cityscapes dataset indicating that our validation approach can indeed uncover errors in semantic segmentation masks.","",""
7,"Mary Jeyanthi Prem, M. Karnan","Business Intelligence-Hybrid Metaheuristics Techniques",2014,"","","","",34,"2022-07-13 09:21:43","","10.4018/ijbir.2014010105","","",,,,,7,0.88,4,2,8,"Business Intelligence BI is about getting the right information, to the right decision makers, at the right time. A business intelligence environment offers decision makers information and knowledge derived from data processing, through the application of mathematical models and algorithms. BI systems tend to promote a scientific and rational approach to managing enterprises and complex organizations. Soft computing is a collection of new techniques in artificial intelligence, which exploit the tolerance for imprecision, uncertainty and partial truth to achieve tractability, robustness and low solution cost. The purpose of this article is to provide an overview of soft computing techniques for the optimal and dynamic decision making system in the current business world.","",""
0,"Shamim Akhtar, M. Z. Sujod, Syed Sajjad Hussain Rizvi","A Novel Deep Learning Architecture for Data-Driven Energy Efficiency Management (D2EEM) - Systematic Survey",2021,"","","","",35,"2022-07-13 09:21:43","","10.1109/ICEET53442.2021.9659737","","",,,,,0,0.00,0,3,1,"The Energy Management System (EMS) is the cost-effectiveness, robustness, and flexible approach for energy efficiency management (EEM). Data-Driven Energy Efficiency Management (D2EEM) is a recent advancement in EMS. The D2EEM is the blend of data science and artificial intelligence for EEM. Due to the highly tolerant to the performance plateau and unconstraint to the feature extraction, Deep Learning (DL) facilitates handling big data-driven problems of EEM. To the best of the knowledge, the accurate and robust D2EEM is the pressing need. Moreover, the accurate pre-trained DL network for EEM is not available in the recent literature. In this work, a comprehensive study is presented to devise a D2EEM. Moreover, the architecture is suggested in connection to the research gap.","",""
0,"Zixuan Li, Xiaolong Li","Target Tracking Research Hotspots and Frontier Trends Based on Citespace",2021,"","","","",36,"2022-07-13 09:21:43","","10.1109/ICDSBA53075.2021.00106","","",,,,,0,0.00,0,2,1,"With the continuous development of artificial intelligence technology, target tracking is a hot problem in the field of computer vision, which has a wide range of application prospects in industrial, military, transportation, medical and other fields. In this paper, Citespace software is used to conduct descriptive statistical analysis and knowledge mapping analysis of target tracking based on domestic CNKI database literature, and to explore the development status and frontier trends in the field of target tracking in China. On this basis, point out three shortcomings of current research: low accuracy of target tracking in complex environments, poor real-time target tracking, and few application directions, and give suggestions to improve algorithm robustness, real-time, accelerate engineering implementation, and focus on future research trends.","",""
0,"Linna Zhu, Wei Li, Yongchuan Tang","Hierarchical Concept Learning by Fuzzy Semantic Cells",2021,"","","","",37,"2022-07-13 09:21:43","","10.3390/app112210723","","",,,,,0,0.00,0,3,1,"Concept modeling and learning have been important research topics in artificial intelligence and knowledge discovery. This paper studies a hierarchical concept learning method that requires a small amount of data to achieve competitive performances. The method starts from a set of fuzzy prototypes called Fuzzy Semantic Cells (FSCs). As a result of FSC parameter optimization, it creates a hierarchical structure of data–prototype–concept. Experiments are conducted to demonstrate the effectiveness of our approach in a classification problem. In particular, when faced with limited training data, our proposed method is comparable with traditional techniques in terms of robustness and generalization ability.","",""
0,"Edgar L Reinoso-Peláez, D. Gianola, O. González-Recio","Genome-Enabled Prediction Methods Based on Machine Learning.",2022,"","","","",38,"2022-07-13 09:21:43","","10.1007/978-1-0716-2205-6_7","","",,,,,0,0.00,0,3,1,"","",""
0,"P. Druzhinina, E. Kondrateva, M. Sharaev","The effect of skull-stripping on transfer learning for 3D MRI models: ADNI data",2022,"","","","",39,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,3,1,"In recent years, with the improvement of data collection and preprocessing, as well as the development of deep learning algorithms, there have been more opportunities for applying artificial intelligence to different areas, including neuroimaging. Various model learning pipelines are emerging to study the degree of cognitive impairment in diseases such as Alzheimer’s disease (AD). In this study, we explore knowledge transfer for the stability of the 3D computer vision models (CNN) for the classification of AD on ADNI data. To assess the model performance, and the quality of learned patterns and examine the ways of models overfitting we utilize conventional 3DCNN interpretation methods and swap tests. We imply that skull-stripping and knowledge transfer strategies can significantly impact the robustness and reproducibility of learned patterns, and suggest to apply swap tests to ensure the model stability.","",""
10,"Zheyan Shen, Peng Cui, Jiashuo Liu, Tong Zhang, Bo Li, Zhitang Chen","Stable Learning via Differentiated Variable Decorrelation",2020,"","","","",40,"2022-07-13 09:21:43","","10.1145/3394486.3403269","","",,,,,10,5.00,2,6,2,"Recently, as the applications of artificial intelligence gradually seeping into some risk-sensitive areas such as justice, healthcare and autonomous driving, an upsurge of research interest on model stability and robustness has arisen in the field of machine learning. Rather than purely fitting the observed training data, stable learning tries to learn a model with uniformly good performance under non-stationary and agnostic testing data. The key challenge of stable learning in practice is that we do not have any knowledge about the true model and test data distribution as a priori. Under such condition, we cannot expect a faithful estimation of model parameters and its stability over wild changing environments. Previous methods resort to a reweighting scheme to remove the correlations between all the variables through a set of new sample weights. However, we argue that such aggressive decorrelation between all the variables may cause the over-reduced sample size, which leads to the variance inflation and possible underperformance. In this paper, we incorporate the unlabled data from multiple environments into the variable decorrelation framework and propose a Differentiated Variable Decorrelation (DVD) algorithm based on the clustering of variables. Specifically, the variables are clustered according to the stability of their correlations and the variable decorrelation module learns a set of sample weights to remove the correlations merely between the variables of different clusters. Empirical studies on both synthetic and real world datasets clearly demonstrate the efficacy of our DVD algorithm on improving the model parameter estimation and the prediction stability over changing distributions.","",""
111,"M. Shahin, M. Jaksa, H. Maier","Recent Advances and Future Challenges for Artificial Neural Systems in Geotechnical Engineering Applications",2009,"","","","",41,"2022-07-13 09:21:43","","10.1155/2009/308239","","",,,,,111,8.54,37,3,13,"Artificial neural networks (ANNs) are a form of artificial intelligence that has proved to provide a high level of competency in solving many complex engineering problems that are beyond the computational capability of classicalmathematics and traditional procedures. In particular, ANNs have been applied successfully to almost all aspects of geotechnical engineering problems. Despite the increasing number and diversity of ANN applications in geotechnical engineering, the contents of reported applications indicate that the progress in ANN development and procedures is marginal and not moving forward since the mid-1990s. This paper presents a brief overview of ANN applications in geotechnical engineering, briefly provides an overview of the operation of ANN modeling, investigates the current research directions of ANNs in geotechnical engineering, and discusses some ANN modeling issues that need further attention in the future, including model robustness; transparency and knowledge extraction; extrapolation; uncertainty.","",""
4,"Sunday Iliya, E. Goodyer, J. Shell, M. Gongora, J. Gow","Optimized Neural Network using differential evolutionary and swarm intelligence optimization algorithms for RF power prediction in cognitive radio network: A comparative study",2014,"","","","",42,"2022-07-13 09:21:43","","10.1109/ICASTECH.2014.7068129","","",,,,,4,0.50,1,5,8,"Cognitive radio (CR) technology has emerged as a promising solution to many wireless communication problems including spectrum scarcity and underutilization. The a priory knowledge of Radio Frequency (RF) power (primary signals and/ or interfering signals plus noise) in the channels to be exploited by CR is of paramount importance. This will enable the selection of channel with less noise among idle (free) channels. Computational Intelligence (CI) techniques can be applied to these scenarios to predict the required RF power in the available channels to achieve optimum Quality of Service (QoS). In this paper, we developed a time domain based optimized Artificial Neural Network (ANN) model for the prediction of real world RF power within the GSM 900, Very High Frequency (VHF) and Ultra High Frequency (UHF) TV bands. The application of the models produced was found to increase the robustness of CR applications, specifically where the CR had no prior knowledge of the RF power related parameters such as signal to noise ratio, bandwidth and bit error rate. The models used, implemented a novel and innovative initial weight optimization of the ANN's through the use of differential evolutionary and swarm intelligence algorithms. This was found to enhance the accuracy and generalization of the ANN model. For this problem, DE/best/1/bin was found to yield a better performance as compared with the other algorithms implemented.","",""
0,"Youngchul Bae, Yong Soo Kim, F. Rhee, Yong-Tae Kim, C. Tao","Editorial Message: Special Issue on Fuzzy System in Data Mining and Knowledge Discovery: Modelling and Application",2017,"","","","",43,"2022-07-13 09:21:43","","10.1007/S40815-017-0359-1","","",,,,,0,0.00,0,5,5,"","",""
0,"Dr. Roberto Passailaigue Baquerizo, Vivian Estrada Sentí, Dr. Juan P. Febles Rodríguez","Evaluation of the socialization of knowledge and collaboration in educational management",2017,"","","","",44,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,3,5,"1Ph.D, Canciller Universidad Tecnológica ECOTEC, Ecuador 2 Ph.D, Universidad de las Ciencias Informáticas, Metodóloga de posgrado, La Habana, Cuba 3 Ph.D, Universidad de las Ciencias Informáticas, Metodólogo de posgrado, La Habana, Cuba ---------------------------------------------------------------------***--------------------------------------------------------------------Abstract The authors, using qualitative research methods, the Case-Based Reasoning as a paradigm of Artificial Intelligence and based on various data and opinions obtained from research on educational networks developed previously achieved knowledge representation for the status of socialization and collaboration in the process of educational management of selected educational institutions. The base case was tested with various situations and the results were 100% correct (in the test cases were used whose answers were known). The result intersects technological aspects associated with conclusions and recommendations to educational management, which were timely validated in previous processes and ensures the robustness of the proposal that article is performed.","",""
0,"Fang Zhang, Xiaochen Wang, J. Han, Jie Tang, Shiyin Wang","Fast Top-$\boldsymbol{k}$ Area Topics Extraction with Knowledge Base",2017,"","","","",45,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,5,5,"What are the most popular research topics in Artificial Intelligence (AI)? We formulate the problem as extracting top-$k$ topics that can best represent a given area with the help of knowledge base. We theoretically prove that the problem is NP-hard and propose an optimization model, FastKATE, to address this problem by combining both explicit and latent representations for each topic. We leverage a large-scale knowledge base (Wikipedia) to generate topic embeddings using neural networks and use this kind of representations to help capture the representativeness of topics for given areas. We develop a fast heuristic algorithm to efficiently solve the problem with a provable error bound. We evaluate the proposed model on three real-world datasets. Experimental results demonstrate our model's effectiveness, robustness, real-timeness (return results in $<1$s), and its superiority over several alternative methods.","",""
3,"Yongyue Wang, Chunhe Xia, Chengxiang Si, Beitong Yao, Tianbo Wang","Robust Reasoning Over Heterogeneous Textual Information for Fact Verification",2020,"","","","",46,"2022-07-13 09:21:43","","10.1109/ACCESS.2020.3019586","","",,,,,3,1.50,1,5,2,"Automatic fact verification (FV) based on artificial intelligence is considered as a promising approach which can be used to identify misinformation distributed on the web. Even though previous FV using deep learning have made great achievements in single dataset (e.g., FEVER), the trained systems are unlikely to be capable of extracting evidence from heterogeneous web-sources and validating claims in accordance with evidence found on the Internet. Nevertheless, the heterogeneity covers abundant semantic information, which will help FV system identify misinformation in a more accurate way. The current work is the first attempt to make the combination of knowledge graph (KG) and graph neural network (GNN) to enhance the robustness of FV systems for heterogeneous information. As a result, it can be generalized to multi-domain datasets after training on a sufficient single one. To make information update and aggregate well on the collaborative graph, the present study proposes a double graph attention network (DGAT) framework which recursively propagates the embeddings from a node’s neighbors to refine the node’s embedding as well as applies an attention mechanism to classify the importance of the neighbors. We train and evaluate our system on FEVER, a single and benchmark dataset for FV, and then re-evaluate our system on UKP Snopes Corpus, a new richly annotated corpus for FV tasks on the basis of heterogeneous web sources. According to experimental results, although DGAT has no excellent advantages in a single dataset, it shows outstanding performance in more realistic and multi-domain datasets. Moreover, the current study also provides a feasible method for deep learning to have the ability to infer heterogeneous information robustly.","",""
2,"H. Darabi, Xiang Zhai, A. Kianinejad, Zheren Ma, D. Castineira, R. Toronyi","Augmented AI Framework for Well Performance Prediction and Opportunity Identification in Unconventional Reservoirs",2020,"","","","",47,"2022-07-13 09:21:43","","10.2523/iptc-20099-ms","","",,,,,2,1.00,0,6,2,"  Many important business decisions and planning in unconventional reservoirs rely on a reliable forecast on well performance. Common practices like statistical type curves, analytical methods, and numerical simulation are not well suited to incorporate all the complexities involving rock/fluid properties, geological parameters, artificial lift systems, well and completion designs, etc. In this work, we introduce a novel ""Augmented AI"" (Artificial Intelligence) workflow for reliable forecasting of unconventional well performance and show its impact on decision making. Augmented AI represents smart integration of artificial intelligence and domain knowledge. In the application of well performance forecast, a smart DCA algorithm automatically estimates the short- and long-term performance of the historical wells; a spectrum of well attributes are aggregated/transformed with the consideration of uncertainty and robustness for training and prediction. Boosting and bootstrap tree-based models are ensembled to maximize the model generalization capability. In contrast to the commonly seen black-box modeling practices, the factor-specific impacts are deconvoluted, allowing for validation of the underlying physics. Furthermore, this gives guidelines for future well planning and completion designs. A case study is presented, where the workflow is implemented. Multi-disciplinary data (logs, completions, maps, fluid properties, etc.) from thousands of wells were integrated. During the feature engineering step, raw data was converted to a set of meaningful parameters leveraging the domain knowledge. As an example, some of the features were combined, some were transformed, and others were normalized. Then a machine learning model was created using an ensemble approach. The models showed a good model accuracy on the training, testing, and validation dataset. Leveraging the predictive model, thousands of field development opportunities including new vertical wells, new horizontal wells, recompletions, and completion optimization were identified that resulted in increased production, increased reserves, and improved capital efficiency. Using the model explanation techniques, the impact of various parameters on the well performance was quantified that resulted in best practices for future drilling and completion design.","",""
2,"T. Hauer","Machine Ethics, Allostery and Philosophical Anti-Dualism: Will AI Ever Make Ethically Autonomous Decisions?",2020,"","","","",48,"2022-07-13 09:21:43","","10.1007/s12115-020-00506-2","","",,,,,2,1.00,2,1,2,"","",""
2,"Cheng Jiang, Jun Liao, Pei Dong, Zhaoxuan Ma, De Cai, G. Zheng, Yueping Liu, H. Bu, Jianhua Yao","Blind deblurring for microscopic pathology images using deep learning networks",2020,"","","","",49,"2022-07-13 09:21:43","","","","",,,,,2,1.00,0,9,2,"Artificial Intelligence (AI)-powered pathology is a revolutionary step in the world of digital pathology and shows great promise to increase both diagnosis accuracy and efficiency. However, defocus and motion blur can obscure tissue or cell characteristics hence compromising AI algorithms'accuracy and robustness in analyzing the images. In this paper, we demonstrate a deep-learning-based approach that can alleviate the defocus and motion blur of a microscopic image and output a sharper and cleaner image with retrieved fine details without prior knowledge of the blur type, blur extent and pathological stain. In this approach, a deep learning classifier is first trained to identify the image blur type. Then, two encoder-decoder networks are trained and used alone or in combination to deblur the input image. It is an end-to-end approach and introduces no corrugated artifacts as traditional blind deconvolution methods do. We test our approach on different types of pathology specimens and demonstrate great performance on image blur correction and the subsequent improvement on the diagnosis outcome of AI algorithms.","",""
1,"Laura von Rueden, T. Wirtz, Fabian Hueger, Jan David Schneider, C. Bauckhage","Towards Map-Based Validation of Semantic Segmentation Masks",2020,"","","","",50,"2022-07-13 09:21:43","","","","",,,,,1,0.50,0,5,2,"Artificial intelligence for autonomous driving must meet strict requirements on safety and robustness. We propose to validate machine learning models for self-driving vehicles not only with given ground truth labels, but also with additional a-priori knowledge. In particular, we suggest to validate the drivable area in semantic segmentation masks using given street map data. We present first results, which indicate that prediction errors can be uncovered by map-based validation.","",""
15,"Jian Wang, Chen Xu, Yourui Huangfu, Rong Li, Yiqun Ge, Jun Wang","Deep Reinforcement Learning for Scheduling in Cellular Networks",2019,"","","","",51,"2022-07-13 09:21:43","","10.1109/WCSP.2019.8927868","","",,,,,15,5.00,3,6,3,"Integrating artificial intelligence (AI) into wireless networks has drawn significant interest in both industry and academia. A common solution is to replace partial or even all modules in the conventional systems, which is often lack of efficiency and robustness due to their ignoring of expert knowledge. In this paper, we take deep reinforcement learning (DRL) based scheduling as an example to investigate how expert knowledge can help with AI module in cellular networks. A simulation platform, which has considered link adaption, feedback and other practical mechanisms, is developed to facilitate the investigation. Besides the traditional way, which is learning directly from the environment, for training DRL agent, we propose two novel methods, i.e., learning from a dual AI module and learning from the expert solution. The results show that, for the considering scheduling problem, DRL training procedure can be improved on both performance and convergence speed by involving the expert knowledge. Hence, instead of replacing conventional scheduling module in the system, adding a newly introduced AI module, which is capable to interact with the conventional module and provide more flexibility, is a more feasible solution.","",""
1,"M. Masinde","ITIKI Success Story: Classic Application of Design Thinking",2020,"","","","",52,"2022-07-13 09:21:43","","","","",,,,,1,0.50,1,1,2,"ITIKI is an acronym for Information Technology and Indigenous Knowledge with Intelligence. It is a drought early warning system that integrates indigenous knowledge and scientific drought forecasting information to help small scale farmers make better and informed decisions, e.g. on when, what, how, and where to plant. The robustness of the system is anchored on artificial neural networks, that supports forecast models with accuracies of 70% to 98% for lead-times of up to 4 years. Fuzzy logic is used to store and manipulate the holistic indigenous knowledge. The success of ITIKI in three pilot areas in Kenya, Mozambique and South Africa is as a result of the novel application of design thinking in the development of entire system. Acknowledging that the main driver of ITIKI is the local people’s indigenous knowledge (IK) on droughts, three design science tools were applied in the entire innovation process leading to high rate of adoption. In this paper, this innovation process of ITIKI is presented and contextualized within the ethnography in service design.","",""
0,"Marko Kovandžić","Optimalno prepoznavanje i lokalizacija izvora zvuka primenom metoda veštačke inteligencije",2020,"","","","",53,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,1,2,"Тhe subject of the thesis is sound source recognition and sound source localization, in real  conditions, using artificial intelligence algorithms. The main goal is optimal procedure for sound  source observation using artificial neural networks for signal procesing, because of their extreme  procesing speed. It has to provide implementation of hybrid system capable to recognize and  locate sound source in the presence of disturbances. For the training and the testing of neural  networks two sets of data are provided, from two different experiments, and to increase  robustness genetic algorithm is applied. The results of the investigation will contribute the  existing body of acoustic observation knowledge.","",""
0,"Fangchen Liu","State-based Policy Representation for Deep Policy Learning",2020,"","","","",54,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,1,2,"Author(s): Liu, Fangchen | Advisor(s): Su, Hao | Abstract: Reinforcement Learning has achieved noticeable success in many fields, such as video game playing, continuous control, and the game of Go. One the other hand, current approaches usually require large sample complexity, and also lack the transferability to similar tasks. Imitation learning, also known as ``learning from demonstrations'', is possible to mitigate the former problem by providing successful experiences. However, current methods usually assume the expert and imitator are the same, which lack flexibility and robustness when the dynamics change.Generalizability is the core of artificial intelligence. An agent should be able to apply its knowledge for novel tasks after training in similar environments, or providing related demonstrations. Given current observation, it should have the ability to predict what can happen (modeling), and what needs to happen (planning). This brings out challenges on how to represent the knowledge and how to utilize the knowledge by learning from interactions or demonstrations.In this thesis, we will systematically study two important problems, the universal goal-reaching problem and the cross-morphology imitation learning problem, which are representative challenges in the field of reinforcement learning and imitation learning. Laying out our research work that attends to these challenging tasks unfolds our roadmap towards the holy-grail goal: make the agent generalizable by learning from observations and model the world.","",""
10,"Yi Huang, A. Kong, Kwok-Yan Lam","Adversarial Signboard against Object Detector",2019,"","","","",55,"2022-07-13 09:21:43","","","","",,,,,10,3.33,3,3,3,"Object detector is an indispensable component in many computer vision and artificial intelligence systems, such as autonomous robot and image analyzer for profiling social media users. Analyzing its vulnerabilities is essential for detecting and preventing attacks and minimizing potential loss. Researchers have proposed a number of adversarial examples to evaluate the robustness of object detectors. All these adversarial examples change pixels inside target objects to carry out attacks but only some of them are suitable for physical attacks. According to the best knowledge of the authors, no published work successfully attacks object detector without changing pixels inside the target object. In an unpublished work, the authors designed an adversarial border which tightly surrounds target object and successfully misleads Faster R-CNN and YOLOv3 digitally and physically. Adversarial border does not change pixels inside target object but makes it look weird. In this paper, a new adversarial example named adversarial signboard, which looks like a signboard, is proposed. By putting it below a target object, it can mislead the state-of-the-art object detectors. Using stop sign as a target object, adversarial signboard is evaluated on 48 videos with totally 5416 frames. The experimental results show that adversarial signboard derived from Faster R-CNN with ResNet-101 as a backbone network can mislead Faster R-CNN with a different backbone network, Mask R-CNN, YOLOv3 and R-FCN digitally and physically.","",""
32,"M. Jaksa, H. Maier, M. Shahin","Future challenges for artificial neural network modelling in geotechnical engineering",2008,"","","","",56,"2022-07-13 09:21:43","","","","",,,,,32,2.29,11,3,14,"Artificial neural networks (ANNs) are a form of artificial intelligence and, since the mid-1990s, ANNbased models have been successfully applied to virtually every problem in geotechnical engineering. This paper briefly examines the areas of geotechnical engineering to which ANNs have been applied, provides a brief overview of the operation of ANN models, and highlights and discusses four important issues which require further attention in the future. These are model robustness, transparency and knowledge extraction, extrapolation, and uncertainty. For ANN models to be more effective and useful in the future, it is essential that further work be undertaken in these four areas, particularly in the context of geotechnical engineering.","",""
8,"Szymon Bobek, G. J. Nalepa, M. Slazynski","HEARTDROID—Rule engine for mobile and context‐aware expert systems",2018,"","","","",57,"2022-07-13 09:21:43","","10.1111/exsy.12328","","",,,,,8,2.00,3,3,4,"Building mobile context‐aware systems is inherently complex and non‐trivial task. It consists of several phases starting from acquisition of context, through modeling to execution of contextual models. Today, such systems are mostly implemented on mobile platforms, that introduce specific requirements, such as intelligibility, robustness, privacy, and efficiency. Over the last decade, along with the rapid development of mobile industry, many approaches were developed that unevenly support these requirements. This is mainly caused by the fact that current modelling and reasoning methods are not crafted to operate in mobile environments. We argue that the use of rule‐based reasoning tailored to mobile environments is an optimal solution. Rules are based on symbolic knowledge representation, as such they meet the general tendency to enforce understandability, intelligibility, and controllability of artificial intelligence software, as stated in the recent European Union General Data Protection Regulation. To this goal, we introduce a lightweight rule engine dedicated for Android platform called HEARTDROID. It executes models in the HMR+ rule language that are capable of expressing uncertainty of knowledge, capturing dynamics of mobile environment and provide high level of intelligibility. We present a qualitative and quantitative comparison of HEARTDROID with the most popular rule engines available.","",""
3,"Marko Jereminov","Equivalent Circuit Programming",2019,"","","","",58,"2022-07-13 09:21:43","","10.1184/R1/9792233.V1","","",,,,,3,1.00,3,1,3,"Optimal decision-making processes represent the key component of everyday life and can be found everywhere, from the atomic level in nature, to the emerging technologies that are becoming increasingly dependent on various Machine Learning algorithms and Artificial Intelligence. Consequently, finding the mathematical modeling formulations and algorithms for solving these decision-making problems now represents one of the significant, on-going research topics in science. Most importantly, after the publication of the so-called “No Free Lunch” (NFL)theorems that proves existence of no algorithm that can efficiently and robustly solve all classes of optimization problems, it became clear that the algorithms and mathematical modeling of the optimization problems have to take into the consideration all of the available domain specific knowledge in order to achieve the best efficiency, robustness and scalability. The primary focus of this thesis is to develop a novel generic framework for continuousnetwork optimization problems. Our approach, Equivalent Circuit Programming (ECP), incorporates all of the available domain knowledge and translates it into the efficient and robust simulation algorithms. Inspired by the NFL theory and circuit simulation algorithms developed around the state-of-art circuit simulator SPICE, we first address the key issues of applying the generic local optimization algorithms to network optimization problems. To that effect, wegeneralize the adjoint circuit theory to include the nonlinear network models and show that the complete set of optimality conditions of a network optimization problem can be represented by a combination of the network and its uniquely defined adjoint circuit. With the circuit representation of the considered class of optimization problems established, we next embed the domain-specificknowledge within the existing optimization heuristics to develop a completely new set of algorithms to ensure a more efficient, robust and scalable solution process.To prove the concept and demonstrate the significant improvements in simulation efficiency, scalability and robustness, the proposed Equivalent Circuit Programming framework is applied to power system optimization problems. This is achieved by first introducing a generalized methodology for modeling the power grid steady-state response in terms of equivalent circuit equations that further allows us to incorporate them within the ECP framework. The examined power system optimization problems include the newly introduced Power Flow Feasibility analyses, AC Optimal Power Flow (AC-OPF) and Security Constrained AC Optimal Power Flow (SC-OPF) problems. Optimization results are compared with the existing state-of-art localoptimization algorithms for available network examples that include various realistic-size power system test cases of up to the 80k buses (nodes).","",""
7,"T. Bakardjieva, Galya Gercheva","Knowledge Management and e-Learning –An Agent-Based Approach",2011,"","","","",59,"2022-07-13 09:21:43","","","","",,,,,7,0.64,4,2,11,"In this paper an open agent-based modular framework for personalized and adaptive curriculum generation in e-learning environment is proposed. Agent-based approaches offer several potential advantages over alternative approaches. Agent-based systems exhibit high levels of flexibility and robustness in dynamic or unpredictable environments by virtue of their intrinsic autonomy. The presented framework enables integration of different types of expert agents, various kinds of learning objects and user modeling techniques. It creates possibilities for adaptive e-learning process. The KM e-learning system is in a process of implementation in Varna Free University and will be used for supporting the educational process at the University. Keywords—agents, e-Learning, knowledge management, knowledge sharing, artificial intelligence","",""
36,"Mario Rodríguez-Molins, L. Ingolotti, F. Barber, M. Salido, María R. Sierra, Jorge Puente","A genetic algorithm for robust berth allocation and quay crane assignment",2014,"","","","",60,"2022-07-13 09:21:43","","10.1007/s13748-014-0056-3","","",,,,,36,4.50,6,6,8,"","",""
233,"E. Papageorgiou, C. Stylios, P. Groumpos","Fuzzy Cognitive Map Learning Based on Nonlinear Hebbian Rule",2003,"","","","",61,"2022-07-13 09:21:43","","10.1007/978-3-540-24581-0_22","","",,,,,233,12.26,78,3,19,"","",""
1,"T. Dao, Federico Marin, M. Tho","MODELING OF MUSCULOSKELETAL SYSTEM USING BIOMECHANICS AND KNOWLEDGE ENGINEERING APPROACHES: CLINICAL BENEFITS AND LIMITATIONS",2011,"","","","",62,"2022-07-13 09:21:43","","","","",,,,,1,0.09,0,3,11,"Understanding of mechanical behaviors of the human body is a challenge to take appropriate medical decisions (e.g. patient's diagnosis or treatment). To achieve this objective, two modeling approaches were studied and confronted in order to highlight the benefits and limitations of each approach to clinical problems. The biomechanical model is including anatomical geometries, anthropometrical data, mechanical properties and motion analysis data. A new type of model named meta-model is based on knowledge engineering representation, data mining and artificial intelligence methods. Orthopedic pediatric pathologies (Polio, clubfoot, cerebral palsy) were studied to evaluate the accuracy and robustness of these modeling approaches. Methodological confrontation through clinical benefits and limitations of each modeling approach and their complementarities were analyzed and presented. To conclude, even if input data and modeling of each approach are different, these two approaches are closely complementary for better understanding of musculoskeletal disorders leading to best diagnosis and treatment prescriptions.","",""
31,"Vidhu Bhala R. Vidhu Bhala, S. Abirami","Trends in word sense disambiguation",2014,"","","","",63,"2022-07-13 09:21:43","","10.1007/s10462-012-9331-5","","",,,,,31,3.88,16,2,8,"","",""
102,"Christopher M. Cianci, X. Raemy, J. Pugh, A. Martinoli","Communication in a Swarm of Miniature Robots: The e-Puck as an Educational Tool for Swarm Robotics",2006,"","","","",64,"2022-07-13 09:21:43","","10.1007/978-3-540-71541-2_7","","",,,,,102,6.38,26,4,16,"","",""
1,"S. A. A. Shah, R. Shaikh, Rafaqat Hussain Arain","Reading the Moving Text in Animated Text-Based CAPTCHAs",2018,"","","","",65,"2022-07-13 09:21:43","","10.14569/IJACSA.2018.091209","","",,,,,1,0.25,0,3,4,"Having based on hard AI problems, CAPTCHA (Completely Automated Public Turing test to tell the Computers and Humans Apart) is a hot research topic in the field of computer vision and artificial intelligence. CAPTCHA is a challenge-response test conducted to single out humans and bots. It is ubiquitously implemented on the web since its introduction. As text-based CAPTCHAs are successfully broken by various researchers therefore several design variants have been proposed and implemented in order to further strengthen it. Animated Text-based CAPTCHAs are one of the design variant of it and are based on the difficulty of reading the moving text. They are based on zero knowledge per frame principle. Although it’s still easy for humans to read animated text but it’s a challenge for machines. As proposals for animated CAPTCHAs are on the rise so there is a strong need to scrutinize their strength against automated attacks. In this research, such CAPTCHAs are investigated to verify their robustness against automated attacks. The proposed methods proved that these CAPTCHAs are vulnerable and they do not guarantee the robustness against automated attacks. The proposed frame selection, noise removal, segmentation and recognition methods have successfully decoded these CAPTCHAs with an overall precision, segmentation accuracy and recognition rate of up to 53.8%, 92.9% and 93.5% respectively.","",""
2,"I. Kollia, N. Simou, G. Stamou, A. Stafylopatis","Interweaving Knowledge Representation and Adaptive Neural Networks",2009,"","","","",66,"2022-07-13 09:21:43","","","","",,,,,2,0.15,1,4,13,"Both symbolic knowledge representation systems and machine learning techniques, including artificial neural networks, play a significant role in Artificial Intelligence. Interweaving these techniques, in order to achieve adaptation and robustness in classification problems is of great importance. In this paper we present a novel architecture that can provide effective connectionist adaptation of ontological knowledge. The proposed architecture can be used to improve performance of multimedia analysis and man machine interaction systems.","",""
1,"I. Kollia, N. Simou, G. Stamou, A. Stafylopatis","Connectionist Models for Formal Knowledge Adaptation",2009,"","","","",67,"2022-07-13 09:21:43","","10.1007/978-3-642-04277-5_47","","",,,,,1,0.08,0,4,13,"","",""
5,"M. Schillo","Multiagent robustness: autonomy vs. organisation",2004,"","","","",68,"2022-07-13 09:21:43","","","","",,,,,5,0.28,5,1,18,"This thesis reports on work conducted under the Schwerpunktprogramm Sozionik,1 a basic research project funded by the Deutsche Forschungsgemeinschaft to transfer knowledge between sociology and distributed artificial intelligence (DAI). The motivation of this work is to use sociological notions and theories of ’organisation’ to blueprint more robust multiagent systems. Based on an analysis of the DAI literature, we give a precise and empirically verifiable definition of robustness, which we call τ-robustness. This notion consists of (i) the definition of a performance measure, (ii) the definition of a perturbation against which the performance measure is evaluated, and (iii) a threshold τ , which marks the maximally allowed deviation of the performance measure to call the tested system τ-robust. The main theoretic contribution of this thesis is a framework of design parameters for robust multiagent organisation based on a sociological notion of organisation. This framework, called the Framework for selfOrganisation and Robustness in Multiagent systems (FORM), can be applied to multiagent organisation using the concept of holonic multiagent systems, for which a new, extended definition is presented. By freely combining all possible values of the parameters in the framework, it allows to model more than 90 000 different forms of multiagent organisation. Furthermore, we discuss how the holonic design parameters relate to different dimensions of the autonomy of the participating agents. This discussion shows that, and how, the notion of self-organisation is connected to the notion of adjustable autonomy. As the notion of autonomy is central to the definition of an agent, this constitutes an important contribution to the general theory of multiagent systems. In order to make the large design space spanned by FORM concrete, we chose to model a subset of the possible organisational forms. This choice is based on a sociological analysis of organisations in today’s economy and makes a diverse use of the available values for the design parameters. The modelled forms of organisation are arranged on a spectrum of autonomy ranging from fully autonomous agents to more and more coupled agents, until we reach an organisational form where the boundaries between individual agents dissolve and only one single agent remains. This spectrum can be used to devise a mechanism for self-organisation in the sense that agents start to organise in a loosely coupled holon and increase the 1 Collaborative research group in Socionics.","",""
85,"Y. Bi, S. McClean, T. Anderson","Combining rough decisions for intelligent text mining using Dempster’s rule",2006,"","","","",69,"2022-07-13 09:21:43","","10.1007/s10462-007-9049-y","","",,,,,85,5.31,28,3,16,"","",""
11,"E. S. Abdolkarimi, M. Mosavi, A. Abedi, S. Mirzakuchaki","Optimization of the low-cost INS/GPS navigation system using ANFIS for high speed vehicle application",2015,"","","","",70,"2022-07-13 09:21:43","","10.1109/SPIS.2015.7422319","","",,,,,11,1.57,3,4,7,"Both Global Positioning System (GPS) and Inertial Navigation System (INS) have complementary characteristics and their integration provides continuous and accurate navigation solution, compared to standalone INS or GPS. Extended Kalman filtering (EKF) is the most common INS/GPS integration technique used for this purpose. Kalman filter methods require prior knowledge of the error model of INS, which increases the complexity of the system. These methods have some disadvantages in terms of stability, robustness, immunity to noise effect, and observability, especially when used with low-cost MEMS-based inertial sensors. Therefore, in this paper, low-cost INS/GPS integration is enhanced based on artificial intelligence (AI) techniques that are aimed at providing high-accuracy vehicle state estimates. First, the INS and GPS measurements are fused via an EKF method. Second, an artificial intelligence-based approach for the integration of INS/GPS measurements is improved based upon an Adaptive Neuro-Fuzzy Inference System (ANFIS). The performance of the two sensor fusion approaches are evaluated using a real field test data. The experiments have been conducted using a high speed vehicle. The results show great improvements in positioning for low-cost MEMS-based inertial sensors in terms of GPS blockage compared to the EKF-based approach.","",""
4,"P. Hu, T. Komura, Li Duan, Wu Ge, Y. Zhong","3D textile reconstruction based on KinectFusion and synthesized texture",2017,"","","","",71,"2022-07-13 09:21:43","","10.1108/IJCST-01-2017-0007","","",,,,,4,0.80,1,5,5,"The purpose of this paper is to present a novel framework of reconstructing the 3D textile model with synthesized texture.,First, a pipeline of 3D textile reconstruction based on KinectFusion is proposed to obtain a better 3D model. Second, “DeepTextures” method is applied to generate new textures for various three-dimensional textile models.,Experimental results show that the proposed method can conveniently reconstruct a three-dimensional textile model with synthesized texture.,A novel pipeline is designed to obtain 3D high-quality textile models based on KinectFusion. The accuracy and robustness of KinectFusion are improved via a turntable. To the best of the authors’ knowledge, this is the first paper to explore the synthesized textile texture for the 3D textile model. This is not only simply mapping the texture onto the 3D model, but also exploring the application of artificial intelligence in the field of textile.","",""
53,"R. Alcalá, J. Alcalá-Fdez, María José Gacto, F. Herrera","Improving fuzzy logic controllers obtained by experts: a case study in HVAC systems",2009,"","","","",72,"2022-07-13 09:21:43","","10.1007/s10489-007-0107-6","","",,,,,53,4.08,13,4,13,"","",""
0,"C. Brunger","Artificial Neural Network Modeling of Damaged Aircraft",1994,"","","","",73,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,1,28,"Abstract : Aircraft design and control techniques rely on the proper modeling of the aircraft's equations of motion. Many of the variables used in these equations are aerodynamic coefficients which are obtained from scale models in wind tunnel tests. In order to model damaged aircraft, every aerodynamic coefficient must be determined for every possible damage mechanism in every flight condition. Designing a controller for a damaged aircraft is particularly burdensome because knowledge of the effect of each damage mechanism on the model is required before the controller can be designed. Also, a monitoring system must be employed to decide when and how much damage has occurred in order to re configure the controller. Recent advances in artificial intelligence have made parallel distributed processors (artificial neural networks) feasible. Modeled on the human brain, the artificial neural network's strength lies in its ability to generalize from a given model. This thesis examines the robustness of the artificial neural network as a model for damaged aircraft.","",""
73,"L. Valiant","A neuroidal architecture for cognitive computation",1998,"","","","",74,"2022-07-13 09:21:43","","10.1145/355483.355486","","",,,,,73,3.04,73,1,24,"An architecture is described for designing systems that acquire and ma nipulate large amounts of unsystematized, or so-called commonsense, knowledge. Its aim is to exploit to the full those aspects of computational learning that are known to offer powerful solutions in the acquisition and maintenance of robust knowledge bases. The architecture makes explicit the requirements on the basic computational tasks that are to be performed and is designed to make this computationally tractable even for very large databases. The main claims are that (i) the basic learning and deduction tasks are provably tractable and (ii) tractable learning offers viable approaches to a range of issues that have been previously identified as problematic for artificial intelligence systems that are programmed. Among the issues that learning offers to resolve are robustness to inconsistencies, robustness to incomplete information and resolving among alternatives. Attribute-efficient learning algorithms, which allow learning from few examples in large dimensional systems, are fundamental to the approach. Underpinning the overall architecture is a new principled approach to manipulating relations in learning systems. This approach, of independently quantified arguments, allows propositional learning algorithms to be applied systematically to learning relational concepts in polynomial time and in modular fashion.","",""
0,"Stefan Zwicklbauer","Robust Entity Linking in Heterogeneous Domains",2017,"","","","",75,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,1,5,"Entity Linking is the task of mapping terms in arbitrary documents to entities in a knowledge base by identifying the correct semantic meaning. It is applied in the extraction of structured data in RDF (Resource Description Framework) from textual documents, but equally so in facilitating artificial intelligence applications, such as Semantic Search, Reasoning and Question and Answering. Most existing Entity Linking systems were optimized for specific domains (e.g., general domain, biomedical domain), knowledge base types (e.g., DBpedia, Wikipedia), or document structures (e.g., tables) and types (e.g., news articles, tweets). This led to very specialized systems that lack robustness and are only applicable for very specific tasks. In this regard, this work focuses on the research and development of a robust Entity Linking system in terms of domains, knowledge base types, and document structures and types.    To create a robust Entity Linking system, we first analyze the following three crucial components of an Entity Linking algorithm in terms of robustness criteria: (i) the underlying knowledge base, (ii) the entity relatedness measure, and (iii) the textual context matching technique. Based on the analyzed components, our scientific contributions are three-fold. First, we show that a federated approach leveraging knowledge from various knowledge base types can significantly improve robustness in Entity Linking systems. Second, we propose a new state-of-the-art, robust entity relatedness measure for topical coherence computation based on semantic entity embeddings. Third, we present the neural-network-based approach Doc2Vec as a textual context matching technique for robust Entity Linking.    Based on our previous findings and outcomes, our main contribution in this work is DoSeR (Disambiguation of Semantic Resources). DoSeR is a robust, knowledge-base-agnostic Entity Linking framework that extracts relevant entity information from multiple knowledge bases in a fully automatic way. The integrated algorithm represents a collective, graph-based approach that utilizes semantic entity and document embeddings for entity relatedness and textual context matching computation. Our evaluation shows, that DoSeR achieves state-of-the-art results over a wide range of different document structures (e.g., tables), document types (e.g., news documents) and domains (e.g., general domain, biomedical domain). In this context, DoSeR outperforms all other (publicly available) Entity Linking algorithms on most data sets.","",""
2,"H. L. Fernández, M. Reboiro-Jato, José A. Pérez Rodríguez, F. Fdez-Riverola, D. Glez-Peña","Implementing effective machine learning-based workflows for the analysis of mass spectrometry data",2016,"","","","",76,"2022-07-13 09:21:43","","10.5584/JIOMICS.V6I1.196","","",,,,,2,0.33,0,5,6,"Mass spectrometry using matrix assisted laser desorption ionization coupled to time of flight analyzers (MALDI-TOF MS) has become very popular during the last decade due to its high speed, sensitivity and robustness for accurately detecting proteins and peptides. This allows quickly analyzing large sets of samples in one single batch and doing high-throughput proteomics. In this scenario, bioinformatics methods and computational tools play a key role in MALDI-TOF MS data analysis, as they are able to correctly handle the large amount of raw data generated with the goal of discovering new knowledge and extracting useful conclusions. A typical MALDI-TOF MS data analysis workflow consists of three main stages: data acquisition, preprocessing and analysis. Although the most popular use of this technology is to identify proteins through their peptides, analyses that make use of artificial intelligence (AI), machine learning (ML), and statistical methods are of particular interest to conduct biomarker discovery, automatic diagnosis, and knowledge discovery. In this introductory work, the potential of these techniques are explored and novel solutions based on the application of AI, ML, and statistical methods are reviewed. In addition, an integrated software platform that supports full MALDI-TOF MS data analysis is presented with the goal of facilitating the work of proteomics researchers without advanced bioinformatics skills.","",""
2,"J. Olszewska","Tracking The Invisible Man - Hidden-object Detection for Complex Visual Scene Understanding",2016,"","","","",77,"2022-07-13 09:21:43","","10.5220/0005852302230229","","",,,,,2,0.33,2,1,6,"Reliable detection of objects of interest in complex visual scenes is of prime importance for video-surveillance applications. While most vision approaches deal with tracking visible or partially visible objects in single or multiple video streams, we propose a new approach to automatically detect all objects of interest being part of an analyzed scene, even those entirely hidden in a camera view whereas being present in the scene. For that, we have developed an innovative artificial-intelligence framework embedding a computer vision process fully integrating symbolic knowledge-based reasoning. Our system has been evaluated on standard datasets consisting of video streams with real-world objects evolving in cluttered, outdoor environment under difficult lighting conditions. Our proposed approach shows excellent performance both in detection accuracy and robustness, and outperforms state-of-the-art methods.","",""
1,"S. Sugiyama","Self evolving dynamic knowledge base",1998,"","","","",78,"2022-07-13 09:21:43","","10.1109/ICSMC.1998.728187","","",,,,,1,0.04,1,1,24,"There have been a lot of studies on artificial intelligence. But still we have a lot of problems left, like analysis of the environment for solving problems, low robustness, lack of information on a target, less matured computer languages, and so forth. Which means that a lot of human interactions are still necessary in order to get proper answers for given problems. Studies on how to reduce the human interactions are performed by using backpropagation neural networks with the idea of dynamic knowledge base which has an ability to evolve itself in order to get the most appropriate knowledge base and performance mechanisms for solving problems given. And as a result, it has been proved that it is possible to have a self evolving dynamic knowledge base which reduces a lot of human interactions which used to need.","",""
2,"M. Bringmann","Knowledge acquisition through the use of combined repertory grids",1990,"","","","",79,"2022-07-13 09:21:43","","","","",,,,,2,0.06,2,1,32,"Artificial intelligence is the study of the simulation of human cognitive faculties. Applications of various aspects of AI are important to the development of expert or knowledge-based systems. An individual expert system can be characterized by the methods selected for representing expertise, transfer of expertise and user interaction. Various aspects of a domain can be represented by separate knowledge sources acquired from different experts or through different models of reasoning. Combination of these knowledge sources can improve the capability or robustness of a model of a domain. The literature of clinical and experimental psychology contains much research relevant to the problem of improving the communication between knowledge engineers and domain experts. Specifically, the Personal Construct Theory of George A. Kelly (1955) has served as the basis for several recent experimental approaches to the design and construction of automated (i.e., computer-based) knowledge acquisition tools. A measure of the shared aspects of domain knowledge common to multiple experts is developed and illustrated with this model using knowledge gathered about two different domains. A model of the shared aspects of personal construct systems of a domain is developed and a system to draw conclusions based upon this information is also proposed.","",""
2,"Tobias Rodemann, Lars Gräning, K. Nishikawa","Automatic energy management controller design for hybrid electric vehicles",2016,"","","","",80,"2022-07-13 09:21:43","","10.1109/SSCI.2016.7850089","","",,,,,2,0.33,1,3,6,"Due to strict CO2 emission limits, the optimal design of controllers for hybrid cars is an increasingly important topic for the automotive industry. Most current approaches to controller design rely solely on engineering knowledge. Utilizing technologies from computational intelligence is not yet common practice. In this work we evaluate how simple controllers can automatically be extracted from optimal control strategies computed by Dynamic Programming. We compare artificial neural network and decision tree based controllers in terms of performance (fuel consumption), stability, robustness, and interpretability, and we investigate the dependency on specific drive cycles used for generating optimal control. Our findings indicate that automatically derived controllers can result in a performance 1–2% below optimal fuel economy, but we observe a large variety in performance and in the controller structure for different drive cycles, thus, underlining the relevance of the correct choice of the drive cycles used for controller development. We also outline the impact of typical learning related issues like overfitting on the practical development process.","",""
3,"Zhou Rong","Information System of Emergency Materials Management Based on Active Blackboard Structure Design",2014,"","","","",81,"2022-07-13 09:21:43","","10.1109/ISDEA.2014.133","","",,,,,3,0.38,3,1,8,"The existing disaster emergency materials management systems are established based on the traditional design mode, the information processing capabilities are limited by the communication control volume of control system, and the asynchronous / synchronous communication and time delay communication are seldom considered. The information sharing and resource integration capability among cities is low. The multi agent system has the good properties such as distribution, autonomy, coordination and robustness. These properties are taken into consideration in this paper, and the emergency material characteristics and management requirements are analyzed sufficiently. Combing with the distributed collaborative model of artificial intelligence, the agent communication model of blackboard structure is designed. An active blackboard design pattern is taken, and the emergency materials management information system is constructed. The system has good knowledge sharing and asynchronous/synchronous communication mechanism. Experiments show that the system can meet the requirements of emergency materials management effectively, and the operation efficiency is improved greatly, the system structure is easy to be implemented.","",""
3,"M. Geetha, J. Jerome","Fuzzy expert system based sensor and actuator fault diagnosis for continuous stirred tank reactor",2013,"","","","",82,"2022-07-13 09:21:43","","10.1109/IFUZZY.2013.6825445","","",,,,,3,0.33,2,2,9,"An approach on the degree in artificial intelligence applications to model-based diagnosis for dynamic nonlinear CSTR processes is proposed. Stress is placed on residual generation and residual evaluation employing fuzzy logic. Especially for residual generation, a novel Extended Kalman Filter concept, on knowledge Estimator, is introduced. An intelligent fuzzy approach for residual generation and evaluation is also defined. A fuzzy-reasoning approach is proposed to guarantee robustness to the inherent doubtfulness in the identified trends and to provide compact mapping. A two-staged strategy is employed: (i) identifying the most likely fault candidates based on a resemblance measure between the observed trends and the event-signatures in the cognition-base and, (ii) estimation of the fault magnitude. The fuzzy-cognition-base consists of a set of physically explainable if/then rules providing physical insight into the process. This technique provides multivariate differencing and is transparent for fault detection.","",""
35,"D. Cliff, J. Bruten","Simple Bargaining Agents for Decentralized Market-Based Control",1998,"","","","",83,"2022-07-13 09:21:43","","","","",,,,,35,1.46,18,2,24,"market-based control, economic agents, bargaining, autonomous agent Market-Based Control (MBC) is a resource allocation and control technique where multi-agent systems are built to resemble free-market economies. The aim is that MBC systems exhibit the same decentralization, robustness, and capacity for self-organization as do real economies. MBC systems are relevant to Artificial Intelligence (AI) and robotics in at least two ways: first, the agents in a MBC system need to be robot-like in their ability to autonomously coordinate perception and action in dynamic and uncertain environments that include other agents; second, MBC systems could be used as the control technologies for robots and other “intelligent” autonomous agents. We critically review a selection of MBC systems. We argue that the MBC systems reviewed here are either implicitly reliant on centralized knowledge, or require human operators and hence are not truly automatic. We identify a major issue in creating truly decentralized and automatic MBC systems: the need for the system's agents to be capable of bargaining behaviors. Following this, we briefly summarize our current results and ongoing work in creating multi-agent systems where each autonomous agent has the ability to bargain with other agents. We demonstrate that markets composed of such agents exhibit desirable behaviors, and that such agents could form the basis of truly decentralized MBC systems.","",""
33,"G. Baldassarre","Forward and Bidirectional Planning Based on Reinforcement Learning and Neural Networks in a Simulated Robot",2003,"","","","",84,"2022-07-13 09:21:43","","10.1007/978-3-540-45002-3_11","","",,,,,33,1.74,33,1,19,"","",""
36,"M. Tomita, H. Bunt","Recent Advances in Parsing Technology",2001,"","","","",85,"2022-07-13 09:21:43","","10.1007/978-94-010-9733-8","","",,,,,36,1.71,18,2,21,"","",""
11,"Kenneth D. Forbus, E. Tomai","A pragmatic approach to computational narrative understanding",2009,"","","","",86,"2022-07-13 09:21:43","","","","",,,,,11,0.85,6,2,13,"Narrative understanding is a hard problem for artificial intelligence that requires deep semantic understanding of natural language and broad world knowledge. Early research in this area stalled due to the difficulty of knowledge engineering and a trend in the field towards robustness at the expense of depth. This work explores how a practical integration of more recent resources and theories for natural language understanding can perform deep semantic interpretation of narratives when guided by specific pragmatic constraints. It shows how cognitive models can provide pragmatic context for narrative understanding in terms of well-defined reasoning tasks, and how those tasks can be used to guide interpretation and evaluate understanding.  This work presents an implemented system, EA NLU, which has been used to interpret narrative text input to cognitive modeling simulations. EA NLU integrates existing large-scale knowledge resources with a controlled grammar and a compositional semantic interpretation process to generate highly expressive logical representations of sentences. Delayed disambiguation and representations from dynamic logic are used to separate this compositional process from a querydriven discourse interpretation process that is guided by pragmatic concerns and uses world knowledge. By isolating explicit points of ambiguity and using limited evidential abduction, this query-driven process can automatically identify the disambiguation choices that entail relevant interpretations. This work shows how this approach maintains computational tractability without sacrificing expressive power. EA NLU is evaluated through a series of experiments with two cognitive models, showing that it is capable of meeting the deep reasoning requirements those models pose, and that the constraints provided by the models can effectively guide the interpretation process. By enforcing consistent interpretation principles, EA NLU benefits the cognitive modeling experiments by reducing the opportunities for tailoring the input.  This work also explores the use of a theory of narrative functions as a heuristic guide to interpretation in EA NLU. In contrast to potentially global task-specific queries, these narrative functions can be inferred on a sentence-by-sentence basis, providing incremental disambiguation. This method is evaluated by interpreting a set of Aesop's fables, and showing that the interpretations are sufficient to capture the intended lesson of each fable.","",""
2,"R. Azevedo, Gautam Biswas, D. Bohus, Ted Carmichael, Mark A. Finlayson, M. Hadzikadic, Catherine Havasi, E. Horvitz, T. Kanda, O. Koyejo, W. Lawless, D. Lenat, Felipe Meneguzzi, Bilge Mutlu, Jean Oh, R. Pirrone, Antoine Raux, D. Sofge, G. Sukthankar, Benjamin Van Durme","Reports of the AAAI 2010 Fall Symposia",2011,"","","","",87,"2022-07-13 09:21:43","","10.1609/aimag.v32i1.2338","","",,,,,2,0.18,0,20,11,"The Association for the Advancement of Artificial Intelligence was pleased to present the 2010 Fall Symposium Series, held Thursday through Saturday, November 11-13, at the Westin Arlington Gateway in Arlington, Virginia. The titles of the eight symposia are as follows: (1) Cognitive and Metacognitive Educational Systems; (2) Commonsense Knowledge; (3) Complex Adaptive Systems: Resilience, Robustness, and Evolvability; (4) Computational Models of Narrative; (5) Dialog with Robots; (6) Manifold Learning and Its Applications; (7) Proactive Assistant Agents ; and (8) Quantum Informatics for Cognitive, Social, and Semantic Processes. The highlights of each symposium are presented in this report.","",""
5,"Justin W. Hart, B. Scassellati","Robotic Self-Models Inspired by Human Development",2010,"","","","",88,"2022-07-13 09:21:43","","","","",,,,,5,0.42,3,2,12,"Traditionally, in the fields of artificial intelligence and robotics, representations of the self have been conspicuously absent. Capabilities of systems are listed explicitly by developers during construction and choices between behavioral options are decided based on search, inference, and planning. In robotics, while knowledge of the external world has often been acquired through experience, knowledge about the robot itself has generally been built in by the designer. Built-in models of the robot's kinematics, physical and sensory capabilities, and other equipment have stood in the place of self-knowledge, but none of these representations offer the flexibility, robustness, and functionality that are present in people. In this work, we seek to emulate forms of self-awareness developed during human infancy in our humanoid robot, Nico. In particular, we are interested in the ability to reason about the robot's embodiment and physical capabilities, with the robot building a model of itself through its experiences.","",""
16,"Matthias Nickles, Gerhard Weiss","Multiagent Systems Without Agents - Mirror-Holons for the Compilation and Enactment of Communication Structures",2005,"","","","",89,"2022-07-13 09:21:43","","10.1007/11594116_14","","",,,,,16,0.94,8,2,17,"","",""
3,"Yong Xiang, Shaohua Zhang, Yuzhu Shen, Meilin Shi","Pattern-Oriented Workflow Generation and Optimization",2009,"","","","",90,"2022-07-13 09:21:43","","10.3217/jucs-015-09-1924","","",,,,,3,0.23,1,4,13,"Automatic workflow generation is becoming an active research area for dealing with the dynamics of grid infrastructure, because it has a pervasive impact on system usability, flexibility and robustness. Artificial intelligence technology and explicit knowledge have been exploited in some research for workflow construction or composition. With the increasing use of knowledge, its quality has growing impact on system performance. In this report, we present the process pattern as a vehicle for knowledge representation to capture process expertise at the business level. A pattern-based planning approach is proposed for automated workflow generation. Our pattern-oriented approach decreases user-visible complexity and makes systems more scalable and flexible by utilizing explicit knowledge support. Then we propose a hybrid method of pattern knowledge optimization for pattern-based workflow generation planning; experts define the primary model, and subsequent classifier training adjusts and improves the pattern knowledge settings. Experiments with a prototype application demonstrated that this approach can substantially reduce modelling difficulties and effectively improve pattern knowledge quality.","",""
0,"Li Wei-hua","Automated Reasoning Algorithm in Extended Second Order Logic",2010,"","","","",91,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,1,12,"This proposed automatic reasoning algorithm that has been used in artificial intelligence decision making system can effectively extract knowledge and complete the unsupervised classification with the uncertain data sets.The purpose of the new method based on extended second-order logic(SOLe) is to improve the accuracy and robustness of decision-making system.By analyzing of unstructured knowledge,this paper built the automatic reasoning framework through adopting second-order logic that has these variables as well as additional variables that range over sets of individuals,and combining interval parameter estimation based on exponential distribution-cluster can effectively avoid paradox.Simulations show that the new algorithm can improve the accuracy by about 7% and the robustness by about 6.5% on UCI datasets in decision making system and expert system.","",""
0,"Peer-Olaf Siebers","Hongmei He (intelligent System Lab, University of Bristol): Soft Computing Approaches under the Framework of Hierarchical Decision Making or Classification System",2010,"","","","",92,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,1,12,"Hongmei He (Intelligent System Lab, University of Bristol): Soft Computing Approaches Under the Framework of Hierarchical Decision Making or Classification System With the development of AI, we can see there are a surprising number of the brain functions of the human Intelligent System (IS) are quite similar to those of an artificial IS, since most artificial ISs are modelled through naturally emulating human intelligence. A wide variety of approaches have been utilised in the functional design of artificial ISs. For example, fuzzy logic for robustness, decision trees for the transparency of reasoning, machine learning for knowledge learning, semantics for understandability, probabilistic reasoning, and neural computing, etc.","",""
6,"David Vallejo-Fernandez, J. Albusac, C. González-Morcillo, Luis Jiménez","A Service-Oriented MultiAgent Architecture for Cognitive Surveillance",2008,"","","","",93,"2022-07-13 09:21:43","","10.1007/978-3-540-85834-8_10","","",,,,,6,0.43,2,4,14,"","",""
0,"Guo Chao","Discussion of the Software Testing Programs of Foresty Expert System",2007,"","","","",94,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,1,15,"In view of the designing features of China's forestry expert system and the problems that exist in its research,this paper discusses several issues about the standardized software testing system.It makes an analytical study of the application of software testing technology,and of the important role this technology plays in designing the forestry expert system from the database testing,knowledge base testing,artificial intelligence field testing,maintaining,Robustness testing and other aspects,putting forward a new software testing scheme such as the software of forestry expert system.","",""
9,"Y. Wilks, D. Farwell","Building an Intelligent Second Language Tutoring System from Whatever Bits you Happen to Have Lying Around",1992,"","","","",95,"2022-07-13 09:21:43","","10.1007/978-3-642-77202-3_17","","",,,,,9,0.30,5,2,30,"","",""
6,"A. Whittaker, R. H. Thieme","Integration of Problem-Solving Techniques in Agriculture",1989,"","","","",96,"2022-07-13 09:21:43","","10.1609/aimag.v10i2.747","","",,,,,6,0.18,3,2,33,"Problem-solving techniques such as modeling, simulation, optimization, and network analysis have been used extensively to help agricultural scientists and practitioners understand and control biological systems. By their nature, most of these systems are difficult to quantitatively define. Many of the models and simulations that have been developed lack a user interface which enables people other than the developer to use them. As a result, several scientists are integrating knowledge-based- system (KBS) technology with conventional problem-solving techniques to increase the robustness and usability of their systems. To investigate the similarities and differences of leading scientists' approaches, a pioneer workshop, supported by the Association for the Advancement of Artificial Intelligence (AAAI) and the Knowledge Systems Area of the American Society of Agricultural Engineers, was held in San Antonio, Texas, on 10-12 August 1988. Part of the AAAI Applied Workshop Series, the meeting was intended to bring together researchers and practitioners active in applying AI concepts to agricultural problems.","",""
0,"L. Jayawardhana, A. D. Alwis, S. Pilapitiya, M. Ranasinghe","Bestcity: Developing Clean Cities - By Intelligently Management of Information",2005,"","","","",97,"2022-07-13 09:21:43","","10.1007/0-387-29295-0_68","","",,,,,0,0.00,0,4,17,"","",""
4,"T. E. Miller, L. Militello, J. Heaton","Evaluating Air Campaign Plan Quality in Operational Settings I",1996,"","","","",98,"2022-07-13 09:21:43","","","","",,,,,4,0.15,1,3,26,"Artificial Intelligence methods can rapidly generate plans for complex situations. However, these plans may be rejected by air campaign planning experts, who judge dimensions such as robustness using operational rather than computational criteria. Our goal is to capture the tactical and strategic concerns of air campaign planners, and incorporate these into planning technology to filter out the unacceptable options and highlight preferred plans. We are in the process of employing a complete Cognitive Task Analysis (CTA), which uses a variety of knowledge elicitation techniques to fully identify the process of plan evaluation and factors underlying judgments of plan robustness. The CTA will form the foundation of an ACP Plan Critic. The Plan Critic will evaluate plans for plan quality and will highlight potential problem areas and vulnerable assumptions. This paper summarizes the current status of this work. There is often a large gap between technological advances and operational feasibility. In the Artificial Intelligence (AI) planning domain, planning technology often approaches the problem via a process that does not match the way the targeted user thinks about and solves planning problems. In some cases, planning technologists and air campaign planners do not define certain concepts identically; what is robust to the planning technologist does not necessarily match robustness to the military planner. Subsequently, there is a considerable risk that current planning technologies will not be accepted in the field because they do not meet the decision-making needs of the commanders and planning staff. The following pages discuss the need for user-centered planning systems in the air campaign planning domain and describe a Decision-Centered Design approach to building systems, which will enable the generation of AI products that meet the operational needs of air campaign planners.","",""
0,"E. Burke, G. Kendall, E. Soubeiga, E. Costa, J. Marín-Blázquez, P. Ross","Constructive and local-search based hyperheuristics : A case for hybridisation ?",2004,"","","","",99,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,6,18,"Hyperheuristics can be defined to be heuristics which choose between heuristics in order to solve a given optimisation problem or class of optimisation problems[5, 1]. One aim of using hyperheuristic methods is to achieve robustness, that is, to generate good-quality solutions for various problems or problem instances using the same method with very limited problem-specific knowledge. Over the past half decade or so, a number of hyperheuristic methods have been developed and applied successfully to a number of relevant combinatorial optimisation problems including timetabling, scheduling, bin packing, rostering, and space allocation [5, 1, 2, 7, 6, 4]. This development has gained increasing recognition in the operational research and artificial intelligence communities (See [10, 1] for an overview of the literature on hyperheuristics). Of interest to us are two types of hyperheuristics which were recently developed at Napier and Nottingham universities:","",""
3,"T. E. Miller, R. R. Copeland, Jennifer K. Phillips, M. McCloskey","A Cognitive Approach to Developing Planning Tools to Support Air Campaign Planners",1999,"","","","",100,"2022-07-13 09:21:43","","10.1037/e444602005-001","","",,,,,3,0.13,1,4,23,"Abstract : Artificial Intelligence methods can rapidly generate detailed plans for complex situations. However, these plans may be rejected by planning experts, who judge dimensions such as robustness using operational rather than computational criteria. Our goal in this research was to capture the tactical and strategic concerns of air campaign planners, and incorporate these into planning technology to assist with filtering out the unacceptable options and highlighting preferred plans. Specifically, we focused on identifying characteristics of quality plans and how these characteristics are judged in operational settings. We used a variety of knowledge elicitation techniques in the Cognitive Task Analysis (CTA) to identify the process of plan evaluation and the factors underlying judgment of plan robustness. Our research drew on observations and interviews in a variety of settings: The primary data sources were from Joint Force Air Component Commander (JFACC) exercises and from a simulation exercise with Pentagon planning staff. The CTA formed the foundation of a software tool, the Bed Down Critic, which highlights potential problem areas, vulnerable assumptions, and summarizes aspects of quality to the user.","",""
4,"O. Deutsch","THE A.I. DESIGN CHALLENGE - BACKGROUND, ANALYSIS, AND RELATIVE PERFORMANCE OF ALGORITHMS",1987,"","","","",101,"2022-07-13 09:21:43","","10.2514/6.1987-2339","","",,,,,4,0.11,4,1,35,"The Artificial Intelligence (A.I.) Design Challenge was an attempt to stimulate interest in a common problem involving the application of A.I. technology to problems likely to be encountered in planning, scheduling, and battle management. These problems are characterized by high combinatorial complexity, uncertainty, constraints, and in some cases, requirements for real-time performance on finite speed processors. Participants in the design challenge submitted competing, alternative approaches, implemented in computer code executable on desktop microcomputers, for assessment and relative evaluation over a range of problems. The range of problems was generated by variation of an input data file at the time of contest judging. The participants were given a priori knowledge of only the range of data variations, and not the specific details. In this manner, robustness to problem variations was evaluated, as well as normalized performance of competing algorithms and implementations.","",""
0,"D. Florian, V. Haase","Software for computer control 1986 : selected papers from the Fourth IFAC/IFIP Symposium, Graz, Austria, 20-23 May 1986",1987,"","","","",102,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,2,35,"(partial) Software project management, P F Elzer. Software development for distributed systems, G M Bull. Abstractions with explicit performance attributes for process-control software development, L Krzanik. MUSIC: a tool for simulation and real-time control, J Cser et al . The real time language PLZRTC and its application, St. Burkhardt. Computer aided design of robust multivariable control systems, Dj B Petkovski. Graphics in an artificial intelligence language, PROLOG, A Doman. Knowledge based support for online program changes, R J Whiddett. Software and hardware to support the teaching of real-time distributed systems, G M Bull & D A Fenso e. Concurrent architectures for power system control, A Viegas De Vasconcelos. Methods and tools for the development of software for complex real time control systems, M Bruns & H Rake. S nthesis of well behaved synchronized processes, C Hao. Improved time domain robustness criteria for multivariable control systems, Dj B Petkovski. Traffic control and optimization in process control communication systems, E I Stoilov et al .","",""
1,"H. Schultz, C. Jaynes, M. Marengoni, A. Schwickerath, F. Stolle, Xiaoguang Wang, A. Hanson, E. Riseman","D RECONSTRUCTION OF TOPOGRAPHIC OBJECTS AT THE UNIVERSITY OF MASSACHUSETTS",1997,"","","","",103,"2022-07-13 09:21:43","","","","",,,,,1,0.04,0,8,25,"The goal of interpreting and reconstructing a scene from multiple aerial images requires use of many techniques from photogrammetry, image processing and artificial intelligence. Indeed, the domain includes many of the outstanding problems within the three fields. At the University of Massachusetts Computer Vision Laboratory, we are concerned with developing specific algorithms for automatic or semi-automatic scene reconstruction from aerial images. Although many of these research efforts have proven at least partially successful on their own, our view is that higher degrees of generality and robustness can be achieved only by fusing individual algorithms into larger systems. This paper highlights several of the more successful components that have been developed as part of this effort and discusses how we intend to combine them into a system. We argue that the use of knowledge and intelligent control allows algorithms to be applied in their specific application domain, and fused into a complete topographic modeling system.","",""
1,"Thomas Augustin, S. Pöhlmann","On Robust Sequential Analysis - Kiefer-Weiss Optimal Testing under Interval Probability",2001,"","","","",104,"2022-07-13 09:21:43","","10.5282/ubm/epub.1641","","",,,,,1,0.05,1,2,21,"Usual sequential testing procedures often are very sensitive against even small deviations from the `ideal model' underlying the hypotheses. This makes robust procedures highly desirable. To rely on a clearly defined optimality criterion, we incorporate robustness aspects directly into the formulation of the hypotheses considering the problem of sequentially testing between two interval probabilities (imprecise probabilities). We derive the basic form of the Kiefer-Weiss optimal testing procedure and show how it can be calculated by an easy-to-handle optimization problem. These results are based on the reinterpretation of our testing problem as the task to test between nonparametric composite hypotheses, which allows to adopt the framework of Pavlov (1991). From this we obtain a general result applicable to any interval probability field on a finite sample space, making the approach powerful far beyond robustness considerations, for instance for applications in artificial intelligence dealing with imprecise expert knowledge.","",""
0,"P. Domański","Fuzzy logic supervised predictive controller",1994,"","","","",105,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,1,28,"To obtain better performance of a control system and greater robustness to disturbances we need an additional loop of supervision, which would be responsible for an adaptation of controller parameters. In this paper a control scheme with knowledge-based supervision of a single loop SISO internal model control (IMC) predictive controller is proposed. This supervision is based on artificial intelligence methods using rule bases and fuzzy logic.","",""
1,"M. Hagiwara, Y. Anzai","Connectionist Model Data Base System with a Template for Association",1992,"","","","",106,"2022-07-13 09:21:43","","10.1541/IEEJEISS1987.112.3_165","","",,,,,1,0.03,1,2,30,"Combination of Artificial Intelligence (AI) and Connectionist model is very effective to construct an intelligent information processing system. There are several studies based on such a concept. An example, a knowledge base system based on connectionist model has the following features. (1) Robustness : even when there are some errors in a user's question or in a knowledge base, a near right answer can be obtained. (2) Context dependency : retrieval with prediction is possible. (3) Easy parallel retrieval for multiple concepts. (4) Easy maintenance of knowledge base : facts are expressed by symbols. In spite of the above advantages, the conventional system has a great shortcoming : it can only make inference of facts. This paper proposes a connectionist model data base system with a template for association. The proposed system has two features : inference is possible when data is insufficient, and new knowl edge can be generated by inference. The proposed system uses a template to create a network for associative reasoning, and it can be done by interactive activation and competition process. Computer simulation result indicates the effectiveness of the proposed system.","",""
0,"B. Lu, Hongbing Xu","The innovation of a variable structure control system-a striking fact about how A.I. can help system",1997,"","","","",107,"2022-07-13 09:21:43","","10.1109/ICIPS.1997.672885","","",,,,,0,0.00,0,2,25,"The paper attempts to show how through artificial intelligence (AI) techniques such as knowledge based production systems etc., together with some novel ideas about making use of variable structure controllers (VSC), an innovative variable structure control system with better performance could be built. The system identifies online, the control characteristics related tightly to switching surfaces. With the aid of AI techniques and a novel approach of using each control mode according to the needs decided by judgement from the obtained control characteristic information, the system can be operated in a more favorable manner. Such an innovative variable structure controller (IVSC) would act more intelligently in dealing with plant/system with significant uncertainty, nonlinearity and other undesired properties for control. It can alleviate the chattering problem, and at the same time retain fast response, good robustness and disturbance rejection. The benefits achieved in IVSC are proved by computer simulation results.","",""
1,"D. Kimes, J. Smith, P. Harrison, P. Harrison","Application of AI techniques to infer vegetation characteristics from directional reflectance(s)",1994,"","","","",108,"2022-07-13 09:21:43","","","","",,,,,1,0.04,0,4,28,"Traditionally, the remote sensing community has relied totally on spectral knowledge to extract vegetation characteristics. However, there are other knowledge bases (KB's) that can be used to significantly improve the accuracy and robustness of inference techniques. Using AI (artificial intelligence) techniques a KB system (VEG) was developed that integrates input spectral measurements with diverse KB's. These KB's consist of data sets of directional reflectance measurements, knowledge from literature, and knowledge from experts which are combined into an intelligent and efficient system for making vegetation inferences. VEG accepts spectral data of an unknown target as input, determines the best techniques for inferring the desired vegetation characteristic(s), applies the techniques to the target data, and provides a rigorous estimate of the accuracy of the inference. VEG was developed to: infer spectral hemispherical reflectance from any combination of nadir and/or off-nadir view angles; infer percent ground cover from any combination of nadir and/or off-nadir view angles; infer unknown view angle(s) from known view angle(s) (known as view angle extension); and discriminate between user defined vegetation classes using spectral and directional reflectance relationships developed from an automated learning algorithm. The errors for these techniques were generally very good ranging between 2 to 15% (proportional root mean square). The system is designed to aid scientists in developing, testing, and applying new inference techniques using directional reflectance data.","",""
2,"C. Lindley","Extending the behavioural paradigm for intelligent systems",1994,"","","","",109,"2022-07-13 09:21:43","","10.1109/HICSS.1994.323361","","",,,,,2,0.07,2,1,28,"Based upon fundamentally different ideas about the nature of knowledge and intelligence, the behavioural approach to artificial intelligence is an alternative to traditional representational AI paradigms that overcomes specific limitations of traditional approaches. Behavioural control systems for autonomous mobile robots have demonstrated robust and effective performance. However, a comprehensive methodology for the behavioural paradigm has not yet been fully developed. Concentrating upon a behavioural methodology called the subsumption architecture, this paper describes methodological enhancements to the behavioural paradigm based upon the use of logic. The logical definition of behaviours allows many techniques from traditional artificial intelligence to be applied to the analysis, definition, and verification of behavioural systems, and supports strategies for achieving robustness, without incurring the problems associated with traditional AI.<<ETX>>","",""
0,"A. Abraham, C. Grosan","ON SOFT COMPUTING FOR MODELING AND SIMULATION",1998,"","","","",110,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,2,24,"It is well known that the intelligent systems, which can provide human like expertise such as domain knowledge, uncertain reasoning, and adaptation to a noisy and time varying environment, are important in tackling practical computing problems. In contrast with conventional artificial intelligence techniques which only deal with precision, certainty and rigor the guiding principle of soft computing is to exploit the tolerance for imprecision, uncertainty, low solution cost, robustness, partial truth to achieve tractability, and better rapport with reality [Zadeh, 1998]. Soft computing is a consortium of technologies involving approximate reasoning, function approximation, learning capabilities, and a methodology for systematic random search and optimization. These capabilities are combined in a complementary and synergetic fashion. Soft computing has evolved not only from a theoretical point of view but also with a large variety of realistic applications to consumer products and industrial systems. Applications of soft computing have provided the opportunity to integrate humanlike vagueness and real-life uncertainty into an otherwise hard computer programs.","",""
1,"Pingping Sun, Lingang Gu","Fuzzy knowledge graph system for artificial intelligence-based smart education",2021,"","","","",111,"2022-07-13 09:21:43","","10.3233/JIFS-189332","","",,,,,1,1.00,1,2,1,"Fuzzy knowledge graph system is a semantic network that reveals the relationships between entities, and a tool or methodology that can formally describe things in the real world and their relationships. Smart education is an educational concept or model that uses advanced information technology to build a smart environment, integrates theory and practice to build an educational framework for information age, and provides paths to practice it. Artificial intelligence (AI) is a comprehensive discipline developed by the interpenetration of computer science, cybernetics, information theory, linguistics, neurophysiology and other disciplines, which is a direction for the development of information technology in the future. On the basis of summarizing and analyzing of previous research works, this paper expounded the research status and significance of AI technology, elaborated the development background, current status and future challenges of the construction and application of fuzzy knowledge graph system for smart education, introduced the methods and principles of data acquisition methods and digitalized apprenticeship, realized the process design, information extraction, entity recognition and relationship mining of smart education, constructed a systematic framework for fuzzy knowledge graph, and analyzed the high-quality resources sharing and personalized service of AI-assisted smart education, discussed automatic knowledge acquisition and fusion of fuzzy knowledge graph, performed co-occurrence relationship analysis, and finally conducted application case analysis. The results show that the smart education knowledge graph for AI-assisted smart education can integrate teaching experience and domain knowledge of discipline experts, enhance explainable and robust machine intelligence for AI-assisted smart education, and provide data-driven and knowledge-driven information processing methods; it can also discover the analysis hotspots and main content of research objects through clustering of high-frequency topic words, reveal the corresponding research structure in depth, and then systematically explore its research dimensions, subject background and theoretical basis.","",""
0,"C. Hamel, Mona Hersi, S. Kelly, A. Tricco, S. Straus, G. Wells, B. Pham, B. Hutton","Guidance for using artificial intelligence for title and abstract screening while conducting knowledge syntheses",2021,"","","","",112,"2022-07-13 09:21:43","","10.1186/s12874-021-01451-2","","",,,,,0,0.00,0,8,1,"","",""
21,"D. Ali, S. Frimpong","Artificial intelligence, machine learning and process automation: existing knowledge frontier and way forward for mining sector",2020,"","","","",113,"2022-07-13 09:21:43","","10.1007/s10462-020-09841-6","","",,,,,21,10.50,11,2,2,"","",""
13,"Francesca Iandolo, F. Loia, Irene Fulco, Chiara Nespoli, F. Caputo","Combining Big Data and Artificial Intelligence for Managing Collective Knowledge in Unpredictable Environment—Insights from the Chinese Case in Facing COVID-19",2020,"","","","",114,"2022-07-13 09:21:43","","10.1007/s13132-020-00703-8","","",,,,,13,6.50,3,5,2,"","",""
16,"","Knowledge Graphs for eXplainable Artificial Intelligence: Foundations, Applications and Challenges",2020,"","","","",115,"2022-07-13 09:21:43","","","","",,,,,16,8.00,0,0,2,"","",""
23,"M. Mitchell","Abstraction and analogy‐making in artificial intelligence",2021,"","","","",116,"2022-07-13 09:21:43","","10.1111/nyas.14619","","",,,,,23,23.00,23,1,1,"Conceptual abstraction and analogy‐making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.","",""
111,"Zhihan Lv, Yang Han, A. Singh, Gunasekaran Manogaran, Haibin Lv","Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",2021,"","","","",117,"2022-07-13 09:21:43","","10.1109/TII.2020.2994747","","",,,,,111,111.00,22,5,1,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","",""
4,"Shubham Yadav, S. Ganesh, Debanjan Das, U. Venkanna, R. Mahapatra, A. Shrivastava, Prantar Chakrabarti, A. Talukder","Suśruta: Artificial Intelligence and Bayesian Knowledge Network in Health Care - Smartphone Apps for Diagnosis and Differentiation of Anemias with Higher Accuracy at Resource Constrained Point-of-Care Settings",2019,"","","","",118,"2022-07-13 09:21:43","","10.1007/978-3-030-37188-3_10","","",,,,,4,1.33,1,8,3,"","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",119,"2022-07-13 09:21:43","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
86,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu","Artificial-Intelligence-Enabled Intelligent 6G Networks",2019,"","","","",120,"2022-07-13 09:21:43","","10.1109/MNET.011.2000195","","",,,,,86,28.67,14,6,3,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","",""
7,"Redmond R. Shamshiri, Ibrahim A. Hameed, Kelly R. Thorp, Siva K. Balasundram, S. Shafian, Mohammad Fatemieh, M. Sultan, B. Mahns, S. Samiei","Greenhouse Automation Using Wireless Sensors and IoT Instruments Integrated with Artificial Intelligence",2021,"","","","",121,"2022-07-13 09:21:43","","10.5772/INTECHOPEN.97714","","",,,,,7,7.00,1,9,1,"Automation of greenhouse environment using simple timer-based actuators or by means of conventional control algorithms that require feedbacks from offline sensors for switching devices are not efficient solutions in large-scale modern greenhouses. Wireless instruments that are integrated with artificial intelligence (AI) algorithms and knowledge-based decision support systems have attracted growers’ attention due to their implementation flexibility, contribution to energy reduction, and yield predictability. Sustainable production of fruits and vegetables under greenhouse environments with reduced energy inputs entails proper integration of the existing climate control systems with IoT automation in order to incorporate real-time data transfer from multiple sensors into AI algorithms and crop growth models using cloud-based streaming systems. This chapter provides an overview of such an automation workflow in greenhouse environments by means of distributed wireless nodes that are custom-designed based on the powerful dual-core 32-bit microcontroller with LoRa modulation at 868 MHz. Sample results from commercial and research greenhouse experiments with the IoT hardware and software have been provided to show connection stability, robustness, and reliability. The presented setup allows deployment of AI on embedded hardware units such as CPUs and GPUs, or on cloud-based streaming systems that collect precise measurements from multiple sensors in different locations inside greenhouse environments.","",""
0,"S. Sadeghi, M. Amiri, Farzaneh Mansoori Mooseloo","Artificial Intelligence and Its Application in Optimization under Uncertainty",2021,"","","","",122,"2022-07-13 09:21:43","","10.5772/intechopen.98628","","",,,,,0,0.00,0,3,1,"Nowadays, the increase in data acquisition and availability and complexity around optimization make it imperative to jointly use artificial intelligence (AI) and optimization for devising data-driven and intelligent decision support systems (DSS). A DSS can be successful if large amounts of interactive data proceed fast and robustly and extract useful information and knowledge to help decision-making. In this context, the data-driven approach has gained prominence due to its provision of insights for decision-making and easy implementation. The data-driven approach can discover various database patterns without relying on prior knowledge while also handling flexible objectives and multiple scenarios. This chapter reviews recent advances in data-driven optimization, highlighting the promise of data-driven optimization that integrates mathematical programming and machine learning (ML) for decision-making under uncertainty and identifies potential research opportunities. This chapter provides guidelines and implications for researchers, managers, and practitioners in operations research who want to advance their decision-making capabilities under uncertainty concerning data-driven optimization. Then, a comprehensive review and classification of the relevant publications on the data-driven stochastic program, data-driven robust optimization, and data-driven chance-constrained are presented. This chapter also identifies fertile avenues for future research that focus on deep-data-driven optimization, deep data-driven models, as well as online learning-based data-driven optimization. Perspectives on reinforcement learning (RL)-based data-driven optimization and deep RL for solving NP-hard problems are discussed. We investigate the application of data-driven optimization in different case studies to demonstrate improvements in operational performance over conventional optimization methodology. Finally, some managerial implications and some future directions are provided.","",""
5,"Ayodeji Oseni, Nour Moustafa, H. Janicke, Peng Liu, Z. Tari, A. Vasilakos","Security and Privacy for Artificial Intelligence: Opportunities and Challenges",2021,"","","","",123,"2022-07-13 09:21:43","","","","",,,,,5,5.00,1,6,1,"The increased adoption of Artificial Intelligence (AI) presents an opportunity to solve many socio-economic and environmental challenges; however, this cannot happen without securing AI-enabled technologies. In recent years, most AI models are vulnerable to advanced and sophisticated hacking techniques. This challenge has motivated concerted research efforts into adversarial AI, with the aim of developing robust machine and deep learning models that are resilient to different types of adversarial scenarios. In this paper, we present a holistic cyber security review that demonstrates adversarial attacks against AI applications, including aspects such as adversarial knowledge and capabilities, as well as existing methods for generating adversarial examples and existing cyber defence models. We explain mathematical AI models, especially new variants of reinforcement and federated learning, to demonstrate how attack vectors would exploit vulnerabilities of AI models. We also propose a systematic framework for demonstrating attack techniques against AI applications, and reviewed several cyber defences that would protect the AI applications against those attacks. We also highlight the importance of understanding the adversarial goals and their capabilities, especially the recent attacks against industry applications, to develop adaptive defences that assess to secure AI applications. Finally, we describe the main challenges and future research directions in the domain of security and privacy of AI technologies.","",""
31,"I. Habli, T. Lawton, Zoe Porter","Artificial intelligence in health care: accountability and safety",2020,"","","","",124,"2022-07-13 09:21:43","","10.2471/BLT.19.237487","","",,,,,31,15.50,10,3,2,"Abstract The prospect of patient harm caused by the decisions made by an artificial intelligence-based clinical tool is something to which current practices of accountability and safety worldwide have not yet adjusted. We focus on two aspects of clinical artificial intelligence used for decision-making: moral accountability for harm to patients; and safety assurance to protect patients against such harm. Artificial intelligence-based tools are challenging the standard clinical practices of assigning blame and assuring safety. Human clinicians and safety engineers have weaker control over the decisions reached by artificial intelligence systems and less knowledge and understanding of precisely how the artificial intelligence systems reach their decisions. We illustrate this analysis by applying it to an example of an artificial intelligence-based system developed for use in the treatment of sepsis. The paper ends with practical suggestions for ways forward to mitigate these concerns. We argue for a need to include artificial intelligence developers and systems safety engineers in our assessments of moral accountability for patient harm. Meanwhile, none of the actors in the model robustly fulfil the traditional conditions of moral accountability for the decisions of an artificial intelligence system. We should therefore update our conceptions of moral accountability in this context. We also need to move from a static to a dynamic model of assurance, accepting that considerations of safety are not fully resolvable during the design of the artificial intelligence system before the system has been deployed.","",""
1,"Lana Sinapayen","Perspective: Purposeful Failure in Artificial Life and Artificial Intelligence",2021,"","","","",125,"2022-07-13 09:21:43","","","","",,,,,1,1.00,1,1,1,"Complex systems fail. I argue that failures can be a blueprint characterizing living organisms and biological intelligence, a control mechanism to increase complexity in evolutionary simulations, and an alternative to classical fitness optimization. Imitating biological successes in Artificial Life and Artificial Intelligence can be misleading; imitating failures offers a path towards understanding and emulating life it in artificial systems. Failure is Knowledge, Knowledge is Power You are handed a mysterious box containing the most complex object in the universe, and must find how the object works. Where do you start? “The human brain is the most complex object in the universe” is a well worn cliche (Constable (1918)). While the claim might not be true, the human brain is definitely very complex. In neuroscience and psychology, one of the most compelling ways to understand how the brain works is to study how it fails. Brain damage, irrational decisions, sensory illusions: internal or external changes that make the brain fail are how we find how the brain succeeds. Failure is used to understand complex systems beyond neuroscience: reverse-engineering computer software, understanding animal behavior, identifying solid materials... Failure even defines Science itself. an hypothesis is considered scientific if and only if it is “falsifiable”: if it can reproducibly fail (Popper (1934)). Why default to observing failures when we don’t know what is going on? Because the success-failure boundary is full of information. Let me define “failure” in the context of this discussion. Imagine being an ant dropped somewhere on top of Fig.1-(a). What is the fastest way to map your surroundings? Rather than walking every inch of the surface, find boundaries. When you are investigating a complex system that is working as expected, you are an ant dropped on Mount Success. To find the boundary, you have to push the system into failure mode. Staying inside the success space can inform you about the robustness of the system to perturbations (at best the system recovered from the perturbation, at worst your perturbation was irrelevant), but it is not explanatory. Neither is going from failure to failure. You can only investigate causes and effects if your intervention actually changes something: the failure boundary is not just more informative, it is a different kind of information altogether. Boundaries and failures are not exactly the same. If you are observing a function of the system that does not change when it crosses the failure boundary, you will not notice the transition. If you are observing the right function, you might see the system performance on that function become better or worse. Let us call “failure points” abrupt transitions from “some performance” to 0: they are the most salient of transitions. Going back to Fig.1, the ant might not notice the transition from a gentle slope to a flat terrain, but a cliff will be noticeable. Ideally, you would want to map the entire failure boundary; in practice, you will focus on failure points. ? Mount Success Failure Bog","",""
1,"M. Panahiazar, Nolan Chen, D. Lituiev, D. Hadley","Empowering study of breast cancer data with application of artificial intelligence technology: promises, challenges, and use cases",2021,"","","","",126,"2022-07-13 09:21:43","","10.1007/s10585-021-10125-8","","",,,,,1,1.00,0,4,1,"","",""
1,"Y. Sheng, Jiahan Zhang, Y. Ge, Xinyi Li, Wentao Wang, H. Stephens, F. Yin, Qiuwen Wu, Q. Wu","Artificial intelligence applications in intensity modulated radiation treatment planning: an overview.",2021,"","","","",127,"2022-07-13 09:21:43","","10.21037/qims-21-208","","",,,,,1,1.00,0,9,1,"Artificial intelligence (AI) refers to methods that improve and automate challenging human tasks by systematically capturing and applying relevant knowledge in these tasks. Over the past decades, a number of approaches have been developed to address different types and needs of system intelligence ranging from search strategies to knowledge representation and inference to robotic planning. In the context of radiation treatment planning, multiple AI approaches may be adopted to improve the planning quality and efficiency. For example, knowledge representation and inference methods may improve dose prescription by integrating and reasoning about the domain knowledge described in many clinical guidelines and clinical trials reports. In this review, we will focus on the most studied AI approach in intensity modulated radiation therapy (IMRT)/volumetric modulated arc therapy (VMAT)-machine learning (ML) and describe our recent efforts in applying ML to improve the quality, consistency, and efficiency of IMRT/VMAT planning. With the available high-quality data, we can build models to accurately predict critical variables for each step of the planning process and thus automate and improve its outcomes. Specific to the IMRT/VMAT planning process, we can build models for each of the four critical components in the process: dose-volume histogram (DVH), Dose, Fluence, and Human Planner. These models can be divided into two general groups. The first group focuses on encoding prior experience and knowledge through ML and more recently deep learning (DL) from prior clinical plans and using these models to predict the optimal DVH (DVH prediction model), or 3D dose distribution (dose prediction model), or fluence map (fluence map model). The goal of these models is to reduce or remove the trial-and-error process and guarantee consistently high-quality plans. The second group of models focuses on mimicking human planners' decision-making process (planning strategy model) during the iterative adjustments/guidance of the optimization engine. Each critical step of the IMRT/VMAT treatment planning process can be improved and automated by AI methods. As more training data becomes available and more sophisticated models are developed, we can expect that the AI methods in treatment planning will continue to improve accuracy, efficiency, and robustness.","",""
18,"J. Hellwig, Sarah Huggett, Mark Siebert","Data for report ""Artificial Intelligence: How knowledge is created, transferred, and used""",2019,"","","","",128,"2022-07-13 09:21:43","","10.17632/7YDFS62GD6.2","","",,,,,18,6.00,6,3,3,"The growing importance and relevance of artificial intelligence (AI) to humanity is undisputed. However, AI does not seem to have a universally agreed definition, and different sectors of society use very different vocabulary to describe AI. Using AI to define AI, we were able to detect the relevant body of research, further structure it in sub-fields, and give a comprehensive overview of the research landscape.    There are strong regional differences in AI activity:  • China aspires to lead globally in AI and focuses on computer vision. It shows a rapid rise in scholarly output and citation impact. A net brain gain of AI researchers to China also suggests an attractive research environment.  • Europe is the largest producer of AI scholarly output, but appears to be losing academic AI talent. The broad spectrum of AI research in Europe reflects the diversity of European countries, each with their own agenda and specialties.  • AI research in the United States is robust, both in terms of scholarly output and talent retention. The US benefits from a strong corporate sector. The corpus shows less diversity in AI research than Europe but more than China.    A key area of further development in AI research worldwide is on ethical issues pertaining to AI. While a major topic in daily conversation, there is surprisingly little formal research published on AI ethics to date. We believe there is a need for more AI ethics research, which would bring many benefits to the field, its development, and its applications.","",""
0,"Canan Tiftik","Investigation of Human Resources Dimension in Management and Organization Structure of the Effects of Artificial Intelligence",2021,"","","","",129,"2022-07-13 09:21:43","","10.21733/IBAD.833256","","",,,,,0,0.00,0,1,1,"In the competitive time, there has been a great deal of progress in the industry. It is one of the most serious obstacles to the industry in many industries that adopt contemporary technologies to manage continuous development and faster than ordinary jobs. Many of the scientists and researchers recommend using AI tools and digital technologies for industries. Machine language and artificial intelligence are used by many organizations in the human resources unit, where it undertakes an integrated task in recruiting, performance analysis, personnel selection, data collection for employees, providing real-time information and obtaining the right information. Artificial intelligence-based Human Resources (HR) applications have a solid potential to increase employee productivity and support HR experts to become knowledge and trained consultants that increase the success of the employee. HR applications authorized by artificial intelligence have the ability to analyze, predict, diagnose and seek and find more robust and capable resources.","",""
0,"James M. White, R. Lidskog","Ignorance and the regulation of artificial intelligence",2021,"","","","",130,"2022-07-13 09:21:43","","10.1080/13669877.2021.1957985","","",,,,,0,0.00,0,2,1,"Abstract Much has been written about the risks posed by artificial intelligence (AI). This article is interested not only in what is known about these risks, but what remains unknown and how that unknowing is and should be approached. By reviewing and expanding on the scientific literature, it explores how social knowledge contributes to the understanding of AI and its regulatory challenges. The analysis is conducted in three steps. First, the article investigates risks associated with AI and shows how social scientists have challenged technically-oriented approaches that treat the social instrumentally. It then identifies the invisible and visible characteristics of AI, and argues that not only is it hard for outsiders to comprehend risks attached to the technology, but also for developers and researchers. Finally, it asserts the need to better recognise ignorance of AI, and explores what this means for how their risks are handled. The article concludes by stressing that proper regulation demands not only independent social knowledge about the pervasiveness, economic embeddedness and fragmented regulation of AI, but a social non-knowledge that is attuned to its complexity, and inhuman and incomprehensible behaviour. In properly allowing for ignorance of its social implications, the regulation of AI can proceed in a more modest, situated, plural and ultimately robust manner.","",""
11,"A. Massaro, A. Calicchio, Vincenzo Maritati, A. Galiano, Vitangelo Birardi, L. Pellicani, Maria Gutierrez Millan, Barbara Dalla Tezza, Mauro Bianchi, Guido Vertua, Antonello Puggioni","A Case Study of Innovation of an Information Communication System and Upgrade of the Knowledge Base in Industry by ESB, Artificial Intelligence, and Big Data System Integration",2018,"","","","",131,"2022-07-13 09:21:43","","10.5121/IJAIA.2018.9503","","",,,,,11,2.75,1,11,4,"In this paper, a case study is analyzed. This case study is about an upgrade of an industry communication system developed by following Frascati research guidelines. The knowledge Base (KB) of the industry is gained by means of different tools that are able to provide data and information having different formats and structures into an unique bus system connected to a Big Data. The initial part of the research is focused on the implementation of strategic tools, which can able to upgrade the KB. The second part of the proposed study is related to the implementation of innovative algorithms based on a KNIME (Konstanz Information Miner) Gradient Boosted Trees workflow processing data of the communication system which travel into an Enterprise Service Bus (ESB) infrastructure. The goal of the paper is to prove that all the new KB collected into a Cassandra big data system could be processed through the ESB by predictive algorithms solving possible conflicts between hardware and software. The conflicts are due to the integration of different database technologies and data structures. In order to check the outputs of the Gradient Boosted Trees algorithm an experimental dataset suitable for machine learning testing has been tested. The test has been performed on a prototype network system modeling a part of the whole communication system. The paper shows how to validate industrial research by following a complete design and development of a whole communication system network improving business intelligence (BI).","",""
26,"T. Denœux, D. Dubois, H. Prade","Representations of Uncertainty in Artificial Intelligence: Probability and Possibility",2020,"","","","",132,"2022-07-13 09:21:43","","10.1007/978-3-030-06164-7_3","","",,,,,26,13.00,9,3,2,"","",""
0,"Chengbing Tan, Qun Chen","Application of an artificial intelligence algorithm model of memory retrieval and roaming in sorting Chinese medicinal materials",2021,"","","","",133,"2022-07-13 09:21:43","","10.3233/jcm-215477","","",,,,,0,0.00,0,2,1,"In order to capture autobiographical memory, inspired by the development of human intelligence, a computational AM model for autobiographical memory is proposed in this paper, which is a three-layer network structure, in which the bottom layer encodes the event-specific knowledge comprising 5W1H, and provides retrieval clues to the middle layer, encodes the related events, and the top layer encodes the event set. According to the bottom-up memory search process, the corresponding events and event sets can be identified in the middle layer and the top layer respectively; At the same time, AM model can simulate human memory roaming through the process of rule-based memory retrieval. The computational AM model proposed in this paper not only has robust and flexible memory retrieval, but also has better response performance to noisy memory retrieval cues than the commonly used memory retrieval model based on keyword query method, and can also imitate the roaming phenomenon in memory.","",""
0,"Alex Mathew﻿","Artificial Intelligence and Cognitive Computing for 6G Communications & Networks﻿",2021,"","","","",134,"2022-07-13 09:21:43","","10.47760/IJCSMC.2021.V10I03.003","","",,,,,0,0.00,0,1,1,"With the fast improvement of smart infrastructures and terminals, as well as an enhanced applications (such as augmented and virtual reality, holographic projection and remote surgery) with vivid prerequisites, modern networks (forthcoming 5G and 4G networks) will most likely be unable to satisfy the rapidly rising traffic needs. Likewise, efforts from both the scholarly and academia realm have effectively been put to the examination on 6G systems. Lately, man-made intelligence (AI) has been widely used as another worldview for the plan and enhancement of 6G systems with an undeniable degree of knowledge. Accordingly, the paper proposes AI-empowered architecture engineering for 6G systems to acknowledge information discovery, intelligent service provisioning, mechanic system adjustment, and smart resource management where the network architecture if segregated in four layers: smart application layer, intelligent control layer, information search and logic layer, and intelligent sensing layer. The article further survey and examine the uses of AI techniques for 6G organizations and expand how to utilize the AI procedures to efficiently and viably streamline the exaction of networks, including smart spectrum management, handover management, intelligent mobility, and AI-empowered mobile edge computing. The paper also highlights crucial future study directions and possible solutions for 6G networks such as energy management, hardware development, algorithms robustness, and computation efficiency. Keywords— 6G networks, Artificial Intelligence, Remote networks, device-to-device (D2D) technologies, and massive machine-type communications (mMTC)","",""
19,"Simone Castagno, Mohamed Khalifa","Perceptions of Artificial Intelligence Among Healthcare Staff: A Qualitative Survey Study",2020,"","","","",135,"2022-07-13 09:21:43","","10.3389/frai.2020.578983","","",,,,,19,9.50,10,2,2,"Objectives: The medical community is in agreement that artificial intelligence (AI) will have a radical impact on patient care in the near future. The purpose of this study is to assess the awareness of AI technologies among health professionals and to investigate their perceptions toward AI applications in medicine. Design: A web-based Google Forms survey was distributed via the Royal Free London NHS Foundation Trust e-newsletter. Setting: Only staff working at the NHS Foundation Trust received an invitation to complete the online questionnaire. Participants: 98 healthcare professionals out of 7,538 (response rate 1.3%; CI 95%; margin of error 9.64%) completed the survey, including medical doctors, nurses, therapists, managers, and others. Primary outcome: To investigate the prior knowledge of health professionals on the subject of AI as well as their attitudes and worries about its current and future applications. Results: 64% of respondents reported never coming across applications of AI in their work and 87% did not know the difference between machine learning and deep learning, although 50% knew at least one of the two terms. Furthermore, only 5% stated using speech recognition or transcription applications on a daily basis, while 63% never utilize them. 80% of participants believed there may be serious privacy issues associated with the use of AI and 40% considered AI to be potentially even more dangerous than nuclear weapons. However, 79% also believed AI could be useful or extremely useful in their field of work and only 10% were worried AI will replace them at their job. Conclusions: Despite agreeing on the usefulness of AI in the medical field, most health professionals lack a full understanding of the principles of AI and are worried about potential consequences of its widespread use in clinical practice. The cooperation of healthcare workers is crucial for the integration of AI into clinical practice and without it the NHS may miss out on an exceptionally rewarding opportunity. This highlights the need for better education and clear regulatory frameworks.","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",136,"2022-07-13 09:21:43","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
0,"E. Cambouropoulos, Maximos A. Kaliakatsos-Papakostas","Cognitive Musicology and Artificial Intelligence: Harmonic Analysis, Learning, and Generation",2021,"","","","",137,"2022-07-13 09:21:43","","10.1007/978-3-030-72116-9_10","","",,,,,0,0.00,0,2,1,"","",""
9,"Kuansong Wang, Gang Yu, Chao Xu, Xiang-He Meng, Jian-hua Zhou, C. Zheng, Z. Deng, L. Shang, Ruijie Liu, S. Su, Xunjian Zhou, Qingling Li, Juanni Li, Jing Wang, K. Ma, J. Qi, Zhenmin Hu, P. Tang, Jeffrey Deng, X. Qiu, Bo Li, W. Shen, R. Quan, Juntao Yang, Lin Huang, Yao Xiao, Zhichun Yang, Zhongming Li, Shengchun Wang, Hongzheng Ren, C. Liang, Wei Guo, Yanchun Li, Heng Xiao, Yong-hong Gu, J. Yun, Dan Huang, Zhigang Song, Xiangshan Fan, Ling Chen, Xiaochu Yan, Zhi Li, Zhongjun Huang, Jufang Huang, Joseph Luttrell, Chaoyang Zhang, Weihua Zhou, Kun Zhang, C. Yi, Hui Shen","Accurate diagnosis of colorectal cancer based on histopathology images using artificial intelligence",2020,"","","","",138,"2022-07-13 09:21:43","","10.1186/s12916-021-01942-5","","",,,,,9,4.50,1,50,2,"","",""
301,"E. Davis, G. Marcus","Commonsense reasoning and commonsense knowledge in artificial intelligence",2015,"","","","",139,"2022-07-13 09:21:43","","10.1145/2701413","","",,,,,301,43.00,151,2,7,"AI has seen great advances of many kinds recently, but there is one critical area where progress has been extremely slow: ordinary commonsense.","",""
19,"B. Verheij","Artificial intelligence as law",2020,"","","","",140,"2022-07-13 09:21:43","","10.1007/s10506-020-09266-0","","",,,,,19,9.50,19,1,2,"","",""
8,"I. Wiafe, F. N. Koranteng, Emmanuel Nyarko Obeng, Nana Assyne, Abigail Wiafe, S. Gulliver","Artificial Intelligence for Cybersecurity: A Systematic Mapping of Literature",2020,"","","","",141,"2022-07-13 09:21:43","","10.1109/ACCESS.2020.3013145","","",,,,,8,4.00,1,6,2,"Due to the ever-increasing complexities in cybercrimes, there is the need for cybersecurity methods to be more robust and intelligent. This will make defense mechanisms to be capable of making real-time decisions that can effectively respond to sophisticated attacks. To support this, both researchers and practitioners need to be familiar with current methods of ensuring cybersecurity (CyberSec). In particular, the use of artificial intelligence for combating cybercrimes. However, there is lack of summaries on artificial intelligent methods for combating cybercrimes. To address this knowledge gap, this study sampled 131 articles from two main scholarly databases (ACM digital library and IEEE Xplore). Using a systematic mapping, the articles were analyzed using quantitative and qualitative methods. It was observed that artificial intelligent methods have made remarkable contributions to combating cybercrimes with significant improvement in intrusion detection systems. It was also observed that there is a reduction in computational complexity, model training times and false alarms. However, there is a significant skewness within the domain. Most studies have focused on intrusion detection and prevention systems, and the most dominant technique used was support vector machines. The findings also revealed that majority of the studies were published in two journal outlets. It is therefore suggested that to enhance research in artificial intelligence for CyberSec, researchers need to adopt newer techniques and also publish in other related outlets.","",""
8,"M. Erp, C. Reynolds, D. Maynard, A. Starke, R. Martín, Frédéric Andrès, Maria C. A. Leite, Damien Alvarez de Toledo, X. S. Rivera, C. Trattner, Steven D. Brewer, C. A. Martins, A. Kluczkovski, A. Frankowska, S. Bridle, R. Levy, F. Rauber, Jacqueline Tereza da Silva, U. Bosma","Using Natural Language Processing and Artificial Intelligence to Explore the Nutrition and Sustainability of Recipes and Food",2021,"","","","",142,"2022-07-13 09:21:43","","10.3389/frai.2020.621577","","",,,,,8,8.00,1,19,1,"In this paper, we discuss the use of natural language processing and artificial intelligence to analyze nutritional and sustainability aspects of recipes and food. We present the state-of-the-art and some use cases, followed by a discussion of challenges. Our perspective on addressing these is that while they typically have a technical nature, they nevertheless require an interdisciplinary approach combining natural language processing and artificial intelligence with expert domain knowledge to create practical tools and comprehensive analysis for the food domain.","",""
6,"Óscar Álvarez-Machancoses, Enrique J DeAndrés Galiana, A. Cernea, J. Fernández de la Viña, J. Fernández-Martínez","On the Role of Artificial Intelligence in Genomics to Enhance Precision Medicine",2020,"","","","",143,"2022-07-13 09:21:43","","10.2147/PGPM.S205082","","",,,,,6,3.00,1,5,2,"Abstract The complexity of orphan diseases, which are those that do not have an effective treatment, together with the high dimensionality of the genetic data used for their analysis and the high degree of uncertainty in the understanding of the mechanisms and genetic pathways which are involved in their development, motivate the use of advanced techniques of artificial intelligence and in-depth knowledge of molecular biology, which is crucial in order to find plausible solutions in drug design, including drug repositioning. Particularly, we show that the use of robust deep sampling methodologies of the altered genetics serves to obtain meaningful results and dramatically decreases the cost of research and development in drug design, influencing very positively the use of precision medicine and the outcomes in patients. The target-centric approach and the use of strong prior hypotheses that are not matched against reality (disease genetic data) are undoubtedly the cause of the high number of drug design failures and attrition rates. Sampling and prediction under uncertain conditions cannot be avoided in the development of precision medicine.","",""
1,"Alicia Lai","Artificial Intelligence, LLC: Corporate Personhood as Tort Reform",2020,"","","","",144,"2022-07-13 09:21:43","","10.2139/ssrn.3677360","","",,,,,1,0.50,1,1,2,"Our legal system has long tried to fit the square peg of artificial intelligence (AI) technologies into the round hole of the current tort regime, overlooking the inability of traditional liability schemes to address the nuances of how AI technology creates harms. The current tort regime deals out rough justice—using strict liability for some AI products and using the negligence rule for other AI services—both of which are insufficiently tailored to achieve public policy objectives.    Under a strict liability regime where manufacturers are always held liable for the faults of their technology regardless of knowledge or precautionary measures, firms are incentivized to play it safe and stifle innovation. But even with this cautionary stance, the goals of strict liability cannot be met due to the unique nature of AI technology: its mistakes are merely “efficient errors”—they appropriately surpass the human baseline, they are game theory problems intended for a jury, they are necessary to train a robust system, or they are harmless but misclassified.    Under a negligence liability regime where the onus falls entirely on consumers to prove the element of causation, victimized consumers are left without sufficient recourse or compensation. Many critiques have been leveled against the “black-box” nature of algorithms.    This paper proposes a new framework to regulate artificial intelligence technologies: bestowing corporate personhood to AI systems. First, the corporate personality trait of “limited liability” strikes an optimal balance in determining liability—it would both compensate victims (for instance, through obligations to carry insurance and a straightforward burden of causation) while holding manufacturers responsible only when the infraction is egregious (for instance, through veil-piercing). Second, corporate personhood is “divisible”—meaning not all corporate personality traits need to be granted—which circumvents many of the philosophical criticisms of giving AI the complete set of rights of full legal personhood.","",""
1,"A. Zarzeczny, P. Babyn, S. Adams, Justin Longo","Artificial intelligence-based imaging analytics and lung cancer diagnostics: Considerations for health system leaders",2020,"","","","",145,"2022-07-13 09:21:43","","10.1177/0840470420975062","","",,,,,1,0.50,0,4,2,"Lung cancer is a leading cause of cancer death in Canada, and accurate, early diagnosis are critical to improving clinical outcomes. Artificial Intelligence (AI)-based imaging analytics are a promising healthcare innovation that aim to improve the accuracy and efficiency of lung cancer diagnosis. Maximizing their clinical potential while mitigating their risks and limitations will require focused leadership informed by interdisciplinary expertise and system-wide insight. We convened a knowledge exchange workshop with diverse Saskatchewan health system leaders and stakeholders to explore issues surrounding the use of AI in diagnostic imaging for lung cancer, including implementation opportunities, challenges, and priorities. This technology is anticipated to improve patient outcomes, reduce unnecessary healthcare spending, and increase knowledge. However, health system leaders must also address the needs for robust data, financial investment, effective communication and collaboration between healthcare sectors, privacy and data protections, and continued interdisciplinary research to achieve this technology’s potential benefits.","",""
1,"Joanna Black, Cody Fullerton","Digital Deceit: Fake News, Artificial Intelligence, and Censorship in Educational Research",2020,"","","","",146,"2022-07-13 09:21:43","","10.4236/jss.2020.87007","","",,,,,1,0.50,1,2,2,"Never has it been more urgent for educators to be aware of the perils of  research in education using digital searches in today’s world of  disinformation, misinformation, artificial intelligence and censorship. As a  result, we are more reliant on strong researchers than ever before. In the  discipline of Education, students are often asked to research issues pertaining  to curricula, pedagogy, educational information and theories. Pupils are using  Internet and digital library searches to gain knowledge within public and  private K-12 schools and within higher education. In this article, an  Educational Librarian and an Education Professor outline their approach to  educating all Faculty of Education students about using digital platforms in  relation to unmasking fake news, artificial intelligence (AI) usage,  and increasing Internet censorship. Using case study research, we  examined 34 Bachelor of Education students in training at the high school level who created environmental digital art  projects. Information/media literacy was taught in order to provide students with  the necessary tools to identify credible, diverse, well-informed, strong, and  robust research. In addition, they  needed to be able to discern when artificial intelligence was utilized. Outlined are students’ projects. Our findings include “top ten” practical suggestions for educators at all levels when teaching students about  effective researching in our current digital era.","",""
24,"Maxime Sermesant, H. Delingette, H. Cochet, P. Jaïs, N. Ayache","Applications of artificial intelligence in cardiovascular imaging",2021,"","","","",147,"2022-07-13 09:21:43","","10.1038/s41569-021-00527-2","","",,,,,24,24.00,5,5,1,"","",""
0,"S. Prusty, U. K. Jena, Sidhanta Kumar Balabantaray","APPLICATION OF ARTIFICIAL INTELLIGENCE IN FUZZY LOGIC FOR CROP MANAGEMENT IN AGRICULTURE",2020,"","","","",148,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,3,2,"This paper provides a systematic implementation of th e techniques of artificial intelligence for agricultural crop man agement. Agriculture faces many challenges, such as disease a nd infestation of pests, unsuitable soil treatment, inadequate dr ainage and irrigation and many more. Such result in severe crop failure along with environmental hazards caused by excessive chemical use. Several researches were carried out to deal with these issues. With its robust learning capabilities, the fields of artificial intelligence have become a crucial technique for solving various agricultural related problems. Systems are being developed to assist the agricultural experts around the world in finding better solutions. The sector faces numerous challenges to optimize its production, including inadequate soil care, disease and infestation of pests, big data requirements, low output and knowledge gap between farmers and","",""
109,"Shilin Qiu, Qihe Liu, Shijie Zhou, Chunjiang Wu","Review of Artificial Intelligence Adversarial Attack and Defense Technologies",2019,"","","","",149,"2022-07-13 09:21:43","","10.3390/APP9050909","","",,,,,109,36.33,27,4,3,"In recent years, artificial intelligence technologies have been widely used in computer vision, natural language processing, automatic driving, and other fields. However, artificial intelligence systems are vulnerable to adversarial attacks, which limit the applications of artificial intelligence (AI) technologies in key security fields. Therefore, improving the robustness of AI systems against adversarial attacks has played an increasingly important role in the further development of AI. This paper aims to comprehensively summarize the latest research progress on adversarial attack and defense technologies in deep learning. According to the target model’s different stages where the adversarial attack occurred, this paper expounds the adversarial attack methods in the training stage and testing stage respectively. Then, we sort out the applications of adversarial attack technologies in computer vision, natural language processing, cyberspace security, and the physical world. Finally, we describe the existing adversarial defense methods respectively in three main categories, i.e., modifying data, modifying models and using auxiliary tools.","",""
5,"F. Morandin, G. Amato, M. Fantozzi, R. Gini, C. Metta, M. Parton","SAI: A Sensible Artificial Intelligence That Plays with Handicap and Targets High Scores in 9×9 Go",2020,"","","","",150,"2022-07-13 09:21:43","","10.3233/FAIA200119","","",,,,,5,2.50,1,6,2,"We develop a new model that can be applied to any perfect information two-player zero-sum game to target a high score, and thus a perfect play. We integrate this model into the Monte Carlo tree search-policy iteration learning pipeline introduced by Google DeepMind with AlphaGo. Training this model on 9×9 Go produces a superhuman Go player, thus proving that it is stable and robust. We show that this model can be used to effectively play with both positional and score handicap, and to minimize suboptimal moves. We develop a family of agents that can target high scores against any opponent, and recover from very severe disadvantage against weak opponents. To the best of our knowledge, these are the first effective achievements in this direction.","",""
1,"O. Jenkins, D. Lopresti, M. Mitchell","Next Wave Artificial Intelligence: Robust, Explainable, Adaptable, Ethical, and Accountable",2020,"","","","",151,"2022-07-13 09:21:43","","","","",,,,,1,0.50,0,3,2,"The history of AI has included several ""waves"" of ideas. The first wave, from the mid-1950s to the 1980s, focused on logic and symbolic hand-encoded representations of knowledge, the foundations of so-called ""expert systems"". The second wave, starting in the 1990s, focused on statistics and machine learning, in which, instead of hand-programming rules for behavior, programmers constructed ""statistical learning algorithms"" that could be trained on large datasets. In the most recent wave research in AI has largely focused on deep (i.e., many-layered) neural networks, which are loosely inspired by the brain and trained by ""deep learning"" methods. However, while deep neural networks have led to many successes and new capabilities in computer vision, speech recognition, language processing, game-playing, and robotics, their potential for broad application remains limited by several factors.  A concerning limitation is that even the most successful of today's AI systems suffer from brittleness-they can fail in unexpected ways when faced with situations that differ sufficiently from ones they have been trained on. This lack of robustness also appears in the vulnerability of AI systems to adversarial attacks, in which an adversary can subtly manipulate data in a way to guarantee a specific wrong answer or action from an AI system. AI systems also can absorb biases-based on gender, race, or other factors-from their training data and further magnify these biases in their subsequent decision-making. Taken together, these various limitations have prevented AI systems such as automatic medical diagnosis or autonomous vehicles from being sufficiently trustworthy for wide deployment. The massive proliferation of AI across society will require radically new ideas to yield technology that will not sacrifice our productivity, our quality of life, or our values.","",""
169,"Amisha, Paras Malik, Monika Pathania, V. Rathaur","Overview of artificial intelligence in medicine",2019,"","","","",152,"2022-07-13 09:21:43","","10.4103/jfmpc.jfmpc_440_19","","",,,,,169,56.33,42,4,3,"Background: Artificial intelligence (AI) is the term used to describe the use of computers and technology to simulate intelligent behavior and critical thinking comparable to a human being. John McCarthy first described the term AI in 1956 as the science and engineering of making intelligent machines. Objective: This descriptive article gives a broad overview of AI in medicine, dealing with the terms and concepts as well as the current and future applications of AI. It aims to develop knowledge and familiarity of AI among primary care physicians. Materials and Methods: PubMed and Google searches were performed using the key words 'artificial intelligence'. Further references were obtained by cross-referencing the key articles. Results: Recent advances in AI technology and its current applications in the field of medicine have been discussed in detail. Conclusions: AI promises to change the practice of medicine in hitherto unknown ways, but many of its practical applications are still in their infancy and need to be explored and developed better. Medical professionals also need to understand and acclimatize themselves with these advances for better healthcare delivery to the masses.","",""
0,"J. Blay, Jurgi Camblong, F. Sigaux","Artificial Intelligence Applied to Oncology",2020,"","","","",153,"2022-07-13 09:21:43","","10.1007/978-3-030-32161-1_24","","",,,,,0,0.00,0,3,2,"","",""
73,"Valentina Bellemo, Gilbert Lim, T. Rim, G. Tan, C. Cheung, S. Sadda, M. He, A. Tufail, M. Lee, W. Hsu, D. Ting","Artificial Intelligence Screening for Diabetic Retinopathy: the Real-World Emerging Application",2019,"","","","",154,"2022-07-13 09:21:43","","10.1007/s11892-019-1189-3","","",,,,,73,24.33,7,11,3,"","",""
27,"N. S. Saravana Kumar","IMPLEMENTATION OF ARTIFICIAL INTELLIGENCE IN IMPARTING EDUCATION AND EVALUATING STUDENT PERFORMANCE",2019,"","","","",155,"2022-07-13 09:21:43","","10.36548/jaicn.2019.1.001","","",,,,,27,9.00,27,1,3,"Simulation of human intelligence process is made possible with the help of artificial intelligence. The learning, reasoning and self-correction properties are made possible in computer systems. Along with AI, other technologies are combined effectively in order to create remarkable applications. We apply the changing role of AI and its techniques in new educational paradigms to create a personalised teaching-learning environment. Features like recognition, pattern matching, decision making, reasoning, problem solving and so on are applied along with knowledge based system and supervised machine learning for a complete learning and assessment process.","",""
4,"Qi Deng","Blockchain Economical Models, Delegated Proof of Economic Value and Delegated Adaptive Byzantine Fault Tolerance and their implementation in Artificial Intelligence BlockCloud",2019,"","","","",156,"2022-07-13 09:21:43","","10.3390/jrfm12040177","","",,,,,4,1.33,4,1,3,"The Artificial Intelligence BlockCloud (AIBC) is an artificial intelligence and blockchain technology based large-scale decentralized ecosystem that allows system-wide low-cost sharing of computing and storage resources. The AIBC consists of four layers: a fundamental layer, a resource layer, an application layer, and an ecosystem layer (the latter three are the collective “upper-layers”). The AIBC layers have distinguished responsibilities and thus performance and robustness requirements. The upper layers need to follow a set of economic policies strictly and run on a deterministic and robust protocol. While the fundamental layer needs to follow a protocol with high throughput without sacrificing robustness. As such, the AIBC implements a two-consensus scheme to enforce economic policies and achieve performance and robustness: Delegated Proof of Economic Value (DPoEV) incentive consensus on the upper layers, and Delegated Adaptive Byzantine Fault Tolerance (DABFT) distributed consensus on the fundamental layer. The DPoEV uses the knowledge map algorithm to accurately assess the economic value of digital assets. The DABFT uses deep learning techniques to predict and select the most suitable BFT algorithm in order to enforce the DPoEV, as well as to achieve the best balance of performance, robustness, and security. The DPoEV-DABFT dual-consensus architecture, by design, makes the AIBC attack-proof against risks such as double-spending, short-range and 51% attacks; it has a built-in dynamic sharding feature that allows scalability and eliminates the single-shard takeover. Our contribution is four-fold: that we develop a set of innovative economic models governing the monetary, trading and supply-demand policies in the AIBC; that we establish an upper-layer DPoEV incentive consensus algorithm that implements the economic policies; that we provide a fundamental layer DABFT distributed consensus algorithm that executes the DPoEV with adaptability; and that we prove the economic models can be effectively enforced by AIBC’s DPoEV-DABFT dual-consensus architecture.","",""
19,"M. Komorowski","Clinical management of sepsis can be improved by artificial intelligence: yes",2019,"","","","",157,"2022-07-13 09:21:43","","10.1007/s00134-019-05898-2","","",,,,,19,6.33,19,1,3,"","",""
10,"S. Mukhopadhyay, Sumarga Kumar Sah Tyagi, N. Suryadevara, V. Piuri, F. Scotti, S. Zeadally","Artificial Intelligence-Based Sensors for Next Generation IoT Applications: A Review",2021,"","","","",158,"2022-07-13 09:21:43","","10.1109/JSEN.2021.3055618","","",,,,,10,10.00,2,6,1,"Sensors play a vital role in our daily lives and are an essential component for Internet of Things (IoT) based systems as they enable the IoT to collect data to take smart and intelligent decisions. Recent advances in IoT systems, applications, and technologies, including industrial Cyber-Physical Systems (CPSs), are being supported by a wide range of different types of sensors based on artificial intelligence (AI). These smart AI-based sensors are typically characterized by onboard intelligence and have the ability to communicate collaboratively or through the Internet. To achieve the high level of automation required in today’s smart IoT applications, sensors incorporated into nodes must be efficient, intelligent, context-aware, reliable, accurate, and connected. Such sensors must also be robust, safety- and privacy-aware for users interacting with them. Sensors leveraging advanced AI technologies, new capabilities have recently emerged which have the potential to detect, identify, and avoid performance degradation and discover new patterns. Along with knowledge from complex sensor datasets, they can promote product innovation, improve operation level, and open up novel business models. We review sensors, smart data processing, communication protocol, and artificial intelligence which will enable the deployment of AI-based sensors for next-generation IoT applications.","",""
46,"K. Berezina, Olena Ciftci, C. Cobanoglu","Robots, Artificial Intelligence, and Service Automation in Restaurants",2019,"","","","",159,"2022-07-13 09:21:43","","10.1108/978-1-78756-687-320191010","","",,,,,46,15.33,15,3,3,"Originality/value: To the best of the authors’ knowledge, this chapter presents the first systematic and in-depth review of RAISA technologies in the restaurant industry.","",""
0,"Chris Yang","Explainable Artificial Intelligence for Predictive Modeling in Healthcare",2022,"","","","",160,"2022-07-13 09:21:43","","10.1007/s41666-022-00114-1","","",,,,,0,0.00,0,1,1,"","",""
0,"Bukhoree Sahoh, Kanjana Haruehansapong, Mallika Kliangkhlao","Causal Artificial Intelligence for High-Stakes Decisions: The Design and Development of a Causal Machine Learning Model",2022,"","","","",161,"2022-07-13 09:21:43","","10.1109/access.2022.3155118","","",,,,,0,0.00,0,3,1,"A high-stakes decision requires deep thought to understand the complex factors that stop a situation from becoming worse. Such decisions are carried out under high pressure, with a lack of information, and in limited time. This research applies Causal Artificial Intelligence to high-stakes decisions, aiming to encode causal assumptions based on human-like intelligence, and thereby produce interpretable and argumentative knowledge. We develop a Causal Bayesian Networks model based on causal science using $d$ -separation and do-operations to discover the causal graph aligned with cognitive understanding. Causal odd ratios are used to measure the causal assumptions integrated with the real-world data to prove the proposed causal model compatibility. Causal effect relationships in the model are verified based on causal P-values and causal confident intervals and approved less than 1% by random chance. It shows that the causal model can encode cognitive understanding as precise, robust relationships. The concept of model design allows software agents to imitate human intelligence by inferring potential knowledge and be employed in high-stakes decision applications.","",""
0,"Hamza Ejaz, Hari McGrath, Brian Lh Wong, Andrew Guise, Tom Kamiel Magda Vercauteren, J. Shapey","Artificial intelligence and medical education: A global mixed-methods study of medical students’ perspectives",2022,"","","","",162,"2022-07-13 09:21:43","","10.1177/20552076221089099","","",,,,,0,0.00,0,6,1,"Objective Medical students, as clinicians and healthcare leaders of the future, are key stakeholders in the clinical roll-out of artificial intelligence-driven technologies. The authors aim to provide the first report on the state of artificial intelligence in medical education globally by exploring the perspectives of medical students. Methods The authors carried out a mixed-methods study of focus groups and surveys with 128 medical students from 48 countries. The study explored knowledge around artificial intelligence as well as what students wished to learn about artificial intelligence and how they wished to learn this. A combined qualitative and quantitative analysis was used. Results Support for incorporating teaching on artificial intelligence into core curricula was ubiquitous across the globe, but few students had received teaching on artificial intelligence. Students showed knowledge on the applications of artificial intelligence in clinical medicine as well as on artificial intelligence ethics. They were interested in learning about clinical applications, algorithm development, coding and algorithm appraisal. Hackathon-style projects and multidisciplinary education involving computer science students were suggested for incorporation into the curriculum. Conclusions Medical students from all countries should be provided teaching on artificial intelligence as part of their curriculum to develop skills and knowledge around artificial intelligence to ensure a patient-centred digital future in medicine. This teaching should focus on the applications of artificial intelligence in clinical medicine. Students should also be given the opportunity to be involved in algorithm development. Students in low- and middle-income countries require the foundational technology as well as robust teaching on artificial intelligence to ensure that they can drive innovation in their healthcare settings.","",""
10,"Peter Szolovits","Artificial Intelligence and Medicine",2019,"","","","",163,"2022-07-13 09:21:43","","10.4324/9780429052071-1","","",,,,,10,3.33,10,1,3,"The introduction of artificial intelligence (AI) has resulted in numerous technological advancements in the medical profession and a radical transformation of the old medical model. Artificial intelligence in medicine consists mostly of machine learning, deep learning, expert systems, intelligent robotics, the internet of medical things, and other prevalent and new AI technology. The primary applications of AI in the medical industry are intelligent screening, intelligent diagnosis, risk prediction, and supplemental treatment. Presently, medical AI has achieved significant advances, and big data quality management, new technology empower-ment innovation, multi-domain knowledge integration, and personalized medical deci-sion-making will exhibit greater growth potential in the clinical arena.","",""
8,"Erwan Moreau, Carl Vogel, Marguerite Barry","A Paradigm for Democratizing Artificial Intelligence Research",2019,"","","","",164,"2022-07-13 09:21:43","","10.1007/978-3-030-15939-9_8","","",,,,,8,2.67,3,3,3,"","",""
7,"David K. Spencer, Stephen Duncan, Adam Taliaferro","Operationalizing artificial intelligence for multi-domain operations: a first look",2019,"","","","",165,"2022-07-13 09:21:43","","10.1117/12.2524227","","",,,,,7,2.33,2,3,3,"Artificial Intelligence / Machine Learning (AI/ML) is a foundational requirement for Multi-Domain Operations (MDO). To solve some of MDO’s most critical problems, for example, penetrating and dis-integrating an adversary’s antiaccess/area denial (A2/AD) systems, the future force requires the ability to converge capabilities from across multiple domains at speeds and scales beyond human cognitive abilities. This requires robust, interoperable AI/ML that operates across multiple layers: from optimizing technologies and platforms, to fusing data from multiple sources, to transferring knowledge across joint functions to accomplish critical MDO tactical tasks. This paper provides an overview of ongoing work from the Unified Quest Future Study Plan and other events with the Army’s Futures and Concepts Center to operationalize AI/ML to address MDO problems with this layered approach. It includes insights and required AI/ML capabilities determined with subject matter experts from various organizations at these learning events over the past two years, as well as vignettes that illustrate how AI/ML can be operationalized to enable successful Multi-Domain Operations against a near peer adversary.","",""
7,"D. G. Harkut, K. Kasat","Introductory Chapter: Artificial Intelligence - Challenges and Applications",2019,"","","","",166,"2022-07-13 09:21:43","","10.5772/INTECHOPEN.84624","","",,,,,7,2.33,4,2,3,"Artificial intelligence (AI) is any task performed by program or machine, which otherwise human needs to apply intelligence to accomplish it. It is the science and engineering of making machines to demonstrate intelligence especially visual perception, speech recognition, decision-making, and translation between languages like human beings. AI is the simulation of human intelligence processes by machines, especially computer systems. This includes learning, reasoning, planning, self-correction, problem solving, knowledge representation, perception, motion, manipulation, and creativity. It is a science and a set of computational techniques that are inspired by the way in which human beings use their nervous system and their body to feel, learn, reason, and act. AI is related to machine learning and deep learning wherein machine learning makes use of algorithms to discover patterns and generate insights from the data they are working on. Deep learning is a subset of machine learning, one that brings AI closer to the goal of enabling machines to think and work as human as possible. AI is a debatable topic and is often represented in a negative way; some would call it a blessing in disguise for businesses, while for some it is a technology that endangers the mere existence of humankind as it is potentially capable of taking over and dominating human being, but in reality artificial intelligence has affected our lifestyle either directly or indirectly and shaping the future of tomorrow. AI has already become an intrinsic part of our daily life and has greatly impacted our lifestyle despite the imperative uses of digital assistants of mobile phones, driverassistance systems, the bots, texts and speech translators, and systems that assist in recommending products and services and customized learning. Every emerging technology is a source of both enthusiasm and skepticism. AI is a source of both advantages and disadvantages in different perspectives. However, we need to overcome certain challenges before we can realize the true potential and immense transformational capabilities of this emerging technology. Some of the challenges related to artificial intelligence are:","",""
4,"T. Schmid","Deconstructing the Final Frontier of Artificial Intelligence: Five Theses for a Constructivist Machine Learning",2019,"","","","",167,"2022-07-13 09:21:43","","","","",,,,,4,1.33,4,1,3,"Ambiguity and diversity in human cognition can be regarded a final frontier in developing equivalent systems of artificial intelligence. Despite astonishing accomplishments, modern machine learning algorithms are still hardly more than adaptive systems. Deep neural networks, for example, represent complexity through complex connectivity but are not able to allow for abstraction and differentiation of interpretable knowledge, i.e., for key mechanisms of human cognition. Like support vector machines, random forests and other statistically motivated algorithms, they do neither reflect nor yield structures and strategies of human thinking. Therefore, we suggest to realign the use of existing machine learning tools with respect to the philosophical paradigm of constructivism, which currently is the key concept in human learning and professional teaching. Based on the idea that learning units like classifiers can be considered models with limited validity, we formulate five principles to guide a constructivist machine learning. We describe how to define such models and model limitations, how to relate them and how relationships allow to abstract and differentiate models. To this end, we propose the use of meta data for classifiers and other models. Moreover, we argue that such meta data-based machine learning results in a knowledge base that is both created by the means of automation and interpretable for humans. Over the last decade, it has become widely accepted to address computational systems intelligent. Not only journalists, but also scientists have adapted this habit in their publications. In fact, many classical engineering tasks like monitoring or regulating have profited from the employment of machine learning (Abellan-Nebot and Romero Subirón 2010; Mohanraj, Jayaraj, and Muraleedharan 2012). The same holds true for pattern recognition, most prominently in automated image and video analysis (Zafeiriou, Zhang, and Zhang 2015; Yang et al. 2011). And even though ultimate challenges like the infamous Turing test are left unsatisfied (You 2015), some exceptional results in specialized tasks like playing the game of go (Silver et al. 2016) make current learning machines look intelligent on a human level. Copyright held by the author(s). In A. Martin, K. Hinkelmann, A. Gerber, D. Lenat, F. van Harmelen, P. Clark (Eds.), Proceedings of the AAAI 2019 Spring Symposium on Combining Machine Learning with Knowledge Engineering (AAAI-MAKE 2019). Stanford University, Palo Alto, California, USA, March 25-27, 2019. A final frontier for learning systems, however, is the variety of alternative cognitive functions observable in a diverse set of individuals or from ambiguous stimuli (Kornmeier and Bach 2012). While philosophy has acknowledged and embraced the subjectivity and limitations of human cognition during the last decades (Prawat and Floden 1994), current learning systems regard cognition a complex, yet technical task to be solved. In particular, established algorithms do neither provide convincing answers to the challenges provided by an ambiguous environment; nor do they offer concepts that explicitly allow for contradictory judgements comparable to differences in social perception. The main reason for this shortcoming is that so far both algorithms and researchers have failed to incorporate a constructivist point of view. Constructivism implies not only cognition to be a highly individual phenomenon, but also humans to take an active role in their perception of the world – and that there is no such thing as a human-independent reality (Reich 2009). Yet algorithms and applications aiming to predict things other than laws of nature are implicitly founded on exactly this outdated asumption. In the following, we introduce axioms that allow machine learning to follow constructivist principles. Key features of this approach are the use of modern tools from empirical sciences, model-oriented learning, the ability to handle ambiguity, the ability to integrate supervised and unsupervised learning into a unified framework, the ability to create an individual knowledge base and the ability to abstract, differentiate or discard learned knowledge automatically. 1. The key component of cognitive functionality is a model. Since the introduction of artificial neural networks as a theoretical concept (McCulloch and Pitts 1943), many mathematicians and computer scientists have considered neurons the key component of learning systems. In education and psychology, however, cognitive functions are often seen as certain skills or abilities acquired and exposed by an individual human and described in terms like the concept of competence, which, e.g., is widely used in the modern European education system (Méhaut and Winch 2012). Functionalistic psychology explains cognitive functions of humans by the concept of mental models (Rouse and Morris 1986). Initially, mental models have been used to understand motor control, e.g., of hand movements (Veldhuyzen and Stassen 1977). In a more general sense, however, mental models are described as “hypothetical constructs” (Wickens 2000) that can be ordered hierarchically (Rasmussen 1979) and allow a human to make predictions about his physical and social environment (Oatley 1985). It has also been postulated that such models cannot be of static nature but rather underlay continuous modifications (Oatley 1985). Philosophers, too, consider models an important tool in human knowledge acquisition (Klaus 1967, p. 412) or even the only tool, respectively (Stachowiak 1973, p. 56). While varying and concurring theoretical definitions exist, most model concepts assume an image, an origin of the image and a relationship between them. This definition is, e.g., matched by the idea of mathematical modeling as proposed by Heinrich Hertz and others (Hertz 1894; Hamilton 1982). With the rise of robotics and artificial intelligence, engineers have adapted and extended this idea by postulating the concept of a cybernetic model, which involves a generalized subject and an object of the model (Rose 2009). Cybernetics, however, did neither reflect time-related aspects nor issues involved with individual model subjects. This matter was adressed by Herbert Stachowiak, who was influenced by cybernetics when developing his General Model Theory (Hof 2018). He postulated any model to be limited to specific subjects, specific temporal ranges and specific purposes (Stachowiak 1973, p. 133). Limitations, to this end, are considered a matter of fact rather than a matter of definition. Thus, such models circumvent ambiguity by viewing an otherwise ambiguous model with unknown validity limits as a number of models of limited validity. 2. Learning constitutes from constructing, reconstructing or deconstructing models. Modern education is dominated by the ideas of constructivism and constructivist learning (Fox 2001). At its heart, this approach is based on the assumption that humans acquire knowledge and competences actively and individually through processes called construction, reconstruction and deconstruction (Duffy and Jonassen 1992). Construction is associated with creation, innovation and production and implies searching for variations, combinations or transfers of knowledge (Reich 2004, p. 145). Analogously, reconstruction is associated with application, repetition or imitation and implies searching for order, patterns or models (Reich 2004, p. 145). Deconstruction is in the context of constructivism associated with reconsideration, doubt and modification and implies searching for omissions, additions and defective parts of acquired knowledge (Reich 2004, p. 145). Learning algorithms have been used for half a century to transform sample data into models in a mathematical sense, that is: into generalized mathematical relationships between image and origin. The two major approaches or objectives, known as supervised and unsupervised learning, either do or do not require a given target parameter. Artificial neural networks and their relatives are among the most popular and prominent algorithms for learning with a given target parameter (Singh, Thakur, and Sharma 2016), but statistically motivated approaches like support vector machines (Cristianini and Shawe-Taylor 2000) or random forests (Breiman 2001) are also widely used for supervised learning; a specialized field of supervised learning is reinforcement learning, which is popular in robotics (Kober and Peters 2012) and adaptive control (Lewis, Vrabie, and Vamvoudakis 2012). For unsupervised learning, too, biologically inspired approaches like self-organizing maps (Kohonen 2001) as well as statistically motivated approaches like k-means (Jain 2010) are employed. To some extent, machine learning parallels modern education concepts. A construction process in the constructivist sense may be matched by an unsupervised learning, i.e., identifying clusterings or dimensionality reduction, and can, e.g., be implemented with self-organizing maps, kmeans, autoencoders or feature clustering (Schmid 2018). A reconstruction process in the constructivist sense may be matched by a supervised learning, i.e., classification or regression tasks, and can, e.g., be implemented with artificial neural networks or random forests (Schmid 2018). Few researchers, however, have discussed a constructivist approach to machine learning (Drescher 1989; Quartz 1993), and even less how to design a deconstruction process. While domainspecific applications with manual re-engineering options exist (Herbst and Karagiannis 2000), to the best of our knowledge, there is currently only one working implementation of an algorithmic deconstruction process (Schmid 2018). 3. Deconstructing models computationally requires model-based meta data. In order to automate and implement a deconstruction process, successfully learned models must be held available for comparison or re-training. More over, possible matchings with n","",""
6,"Yaron Einhorn, M. Einhorn, Adaia Kamshov, Oron Lev, A. Trabelsi, N. Paz-Yaacov, S. Gross","Gene-specific artificial intelligence-based variant classification engine: results of a time-capsule experiment",2019,"","","","",168,"2022-07-13 09:21:43","","10.21203/rs.2.11834/v1","","",,,,,6,2.00,1,7,3,"  Background: Interpretation of genetic variation remains an impediment to cost-effective application of genomics to medicine. An advanced artificial intelligence (AI)-based Variant Classification Engine (aiVCE), rooted in ACMG/AMP guidelines, employs data-driven methods to expedite gene-specific classification (franklin.genoox.com). In this blinded study, the aiVCE’s overall and rule-level performances were evaluated using ClinVar (v. 2018-10) variants with creation dates after 5/01/2017. By removing any prior knowledge of these variants from the aiVCE training data, they were treated as novel variants. Using a ‘Full’ dataset (75,801 variants with ≥1 star) and an ‘Increased-Certainty’ dataset (3,993 variants with ≥2 stars), the aiVCE classified variants as pathogenic (P), likely-pathogenic (LP), uncertain significance (VUS), likely-benign (LB), or benign (B). VUS with sufficient supporting data were subclassified as VUS-leaning benign or VUS-leaning pathogenic. aiVCE results were evaluated to determine concordance with final ClinVar classification and rule-level determinations. Results: The aiVCE demonstrated >97% concordance among Increased-Certainty variants. Concordance was >95% across variant effects (e.g., missense, null, splice region), and was >93.5% for the Full dataset. When assessing the aiVCE’s application of specific ACMG rules, significant differences were observed between ClinVar P/LP and B/LB variants rule-met proportions (all P<0.00001), thus supporting gene-specific rule selections. Evaluation of discordance between the aiVCE and ClinVar uncovered evidences that might have been unavailable to submitting laboratories, highlighting AI utility in variant classification. Conclusions: The aiVCE exhibited robust performance, despite lacking past evidence, in determining whether variants would be categorized as P/LP. Applying latest computational advances to existing guidelines may assist scientists and clinicians interpret variants with limited clinical information and greatly reduce analytical bottlenecks.","",""
4,"A. Samareh, Xiangyu Chang, W. Lober, H. Evans, Zhangyang Wang, Xiaoning Qian, Shuai Huang","Artificial Intelligence Methods for Surgical Site Infection: Impacts on Detection, Monitoring, and Decision Making.",2019,"","","","",169,"2022-07-13 09:21:43","","10.1089/sur.2019.150","","",,,,,4,1.33,1,7,3,"Background: There has been tremendous growth in the amount of new surgical site infection (SSI) data generated. Key challenges exist in understanding the data for robust clinical decision-support. Limitations of traditional methodologies to handle these data led to the emergence of artificial intelligence (AI). This article emphasizes the capabilities of AI to identify patterns of SSI data. Method: Artificial intelligence comprises various subfields that present potential solutions to identify patterns of SSI data. Discussions on opportunities, challenges, and limitations of applying these methods to derive accurate SSI prediction are provided. Results: Four main challenges in dealing with SSI data were defined: (1) complexities in using SSI data, (2) disease knowledge, (3) decision support, and (4) heterogeneity. The implications of some of the recent advances in AI methods to optimize clinical effectiveness were discussed. Conclusions: Artificial intelligence has the potential to provide insight in detecting and decision-support of SSI. As we turn SSI data into intelligence about the disease, we increase the possibility of improving surgical practice with the promise of a future optimized for the highest quality patient care.","",""
8,"M. Peters, P. Jandrić","Artificial Intelligence, Human Evolution, and the Speed of Learning",2019,"","","","",170,"2022-07-13 09:21:43","","10.1007/978-981-13-8161-4_12","","",,,,,8,2.67,4,2,3,"","",""
1,"Sonal Modak, D. Sehgal, J. Valadi","Applications of Artificial Intelligence and Machine Learning in Viral Biology",2019,"","","","",171,"2022-07-13 09:21:43","","10.1007/978-3-030-29022-1_1","","",,,,,1,0.33,0,3,3,"","",""
0,"Yaxin Peng, S. Du, T. Zeng","Preface: Special Issue on Optimization Models and Algorithms in Artificial Intelligence",2019,"","","","",172,"2022-07-13 09:21:43","","10.1007/s40305-019-00278-5","","",,,,,0,0.00,0,3,3,"","",""
3,"H. Mohammed, S. Ismail","Proposition of new computer artificial intelligence models for shear strength prediction of reinforced concrete beams",2021,"","","","",173,"2022-07-13 09:21:43","","10.1007/S00366-021-01400-Z","","",,,,,3,3.00,2,2,1,"","",""
3,"T. Kaur, Anirudra Diwakar, Kirandeep, Pranav Mirpuri, M. Tripathi, P. Chandra, T. Gandhi","Artificial Intelligence in Epilepsy",2021,"","","","",174,"2022-07-13 09:21:43","","10.4103/0028-3886.317233","","",,,,,3,3.00,0,7,1,"Background: The study of seizure patterns in electroencephalography (EEG) requires several years of intensive training. In addition, inadequate training and human error may lead to misinterpretation and incorrect diagnosis. Artificial intelligence (AI)-based automated seizure detection systems hold an exciting potential to create paradigms for proper diagnosis and interpretation. AI holds the promise to transform healthcare into a system where machines and humans can work together to provide an accurate, timely diagnosis, and treatment to the patients. Objective: This article presents a brief overview of research on the use of AI systems for pattern recognition in EEG for clinical diagnosis. Material and Methods: The article begins with the need for understanding nonstationary signals such as EEG and simplifying their complexity for accurate pattern recognition in medical diagnosis. It also explains the core concepts of AI, machine learning (ML), and deep learning (DL) methods. Results and Conclusions: In this present context of epilepsy diagnosis, AI may work in two ways; first by creating visual representations (e.g., color-coded paradigms), which allow persons with limited training to make a diagnosis. The second is by directly explaining a complete automated analysis, which of course requires more complex paradigms than the previous one. We also clarify that AI is not about replacing doctors and strongly emphasize the need for domain knowledge in building robust AI models that can work in real-time scenarios rendering good detection accuracy in a minimum amount of time.","",""
41,"Guangnan Zhang, Z. H. Ali, M. Aldlemy, Mohamed H. Mussa, Sinan Q. Salih, M. Hameed, Z. Al-khafaji, Z. Yaseen","Reinforced concrete deep beam shear strength capacity modelling using an integrative bio-inspired algorithm with an artificial intelligence model",2020,"","","","",175,"2022-07-13 09:21:43","","10.1007/s00366-020-01137-1","","",,,,,41,20.50,5,8,2,"","",""
7,"F. Morandin, G. Amato, M. Fantozzi, R. Gini, C. Metta, M. Parton","SAI: a Sensible Artificial Intelligence that plays with handicap and targets high scores in 9x9 Go (extended version)",2019,"","","","",176,"2022-07-13 09:21:43","","","","",,,,,7,2.33,1,6,3,"We develop a new model that can be applied to any perfect information two-player zero-sum game to target a high score, and thus a perfect play. We integrate this model into the Monte Carlo tree search-policy iteration learning pipeline introduced by Google DeepMind with AlphaGo. Training this model on 9x9 Go produces a superhuman Go player, thus proving that it is stable and robust. We show that this model can be used to effectively play with both positional and score handicap, and to minimize suboptimal moves. We develop a family of agents that can target high scores against any opponent, and recover from very severe disadvantage against weak opponents. To the best of our knowledge, these are the first effective achievements in this direction.","",""
0,"Nigamanth Sridhar, Li Yang, J. Joshi, Victor P. Piotrowski","Cybersecurity Education in the Age of Artificial Intelligence",2021,"","","","",177,"2022-07-13 09:21:43","","10.1145/3408877.3439525","","",,,,,0,0.00,0,4,1,"The 2019 Federal Cybersecurity Research and Development Strategic Plan highlighted the mutual needs and benefits of artificial intelligence (AI) and cybersecurity. AI techniques are expected to enhance cybersecurity by assisting human system managers with automated monitoring, analysis, and responses to cybersecurity attacks. Conversely, it is essential to guard AI technologies from unintended uses and hostile exploitation by leveraging cybersecurity practices. Research results at the intersection of AI and cybersecurity can help us to be better equipped with tools and techniques to tackle the growing cybersecurity challenges, while also presenting an opportunity to devise fundamentally new ways to motivate and educate students about cybersecurity in the age of AI. Likewise, a June 2019 technical workshop on 'Artificial Intelligence and Cybersecurity: Opportunities and Challenges' noted how the interplay between AI, machine learning, and cybersecurity will continue to introduce new opportunities and challenges in the security of AI as well as AI for cybersecurity. Basic research at the intersection of AI, cybersecurity, and education has the potential to expand existing AI opportunities and resources in cybersecurity education and workforce development. Education efforts are needed to foster workforce knowledge and skills about applying AI expertise to cybersecurity as well as building robust and trustworthy AI. This BOF session will bring together researchers who are interested in these collaborative explorations.","",""
0,"C. Carpenter","The Future of Plunger Lift Control Using Artificial Intelligence",2021,"","","","",178,"2022-07-13 09:21:43","","10.2118/0321-0044-JPT","","",,,,,0,0.00,0,1,1,"This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 201132, “The Future of Plunger Lift Control Using Artificial Intelligence,” by Ferdinand Hingerl and Brian Arnst, SPE, Ambyint, and David Cosby, SPE, Shale Tec, et al., prepared for the 2020 SPE Virtual Artificial Lift Conference and Exhibition - Americas, 10-12 November. The paper has not been peer reviewed.  Dozens of plunger lift control algorithms have been developed to account for different well conditions and optimization protocols. However, challenges exist that prevent optimization at scale. To address these challenges, a plunger lift optimization software was developed. One aspect of this software is enabling set-point optimization at scale. This paper will present the methodology to do so, detailing three separate areas working in unison to offer significant value to plunger lift well operators.  Introduction  Even in vertical wells, plunger lift presents significant challenges to optimization. Despite their mechanical simplicity, plunger lifted wells produce large amounts of data, and the combinations of possible set points to optimize the well are many. Additionally, plunger lift wells can present a variety of different types of anomalies and problems that require a robust understanding of the underlying physics and mathematics.  Such problems then are amplified when applied to horizontal well applications. The underlying physics and mathematics applied throughout the years for vertical wells do not produce accurate results for horizontal wells. Additionally, the anomalies produced in horizontal wells are more complex. Finally, typical production engineers and well optimizers now regularly look after more than 150—and often more than 500—wells, creating additional resource constraints to optimizing a field of plunger lift wells.  The presented plunger lift optimization software was implemented by creating a secure connection between the operator’s supervisory control and data acquisition (SCADA) network and the cloud. As new data are generated by the SCADA network, they are automatically transmitted to the cloud and processed.  Plunger Lift Control Algorithm Overview  These algorithms are the software code that determines when the well opens and when the well closes. Even though the algorithms only control well open/close, the plunger moves through four stages of plunger operation to complete one cycle: plunger fall time, casing pressure build time, plunger rise, and after flow (or production). Optimizing each individual stage is critical to ideal well production.  Plunger fall time is the time required for the plunger to descend from the lubricator to the bottomhole assembly (BHA). Currently, operators use the manufacturer’s anticipated fall time, trial and error, previous knowledge, acoustical plunger tracking, and plunger fall applications to set the appropriate fall time in the controller. A “fudge factor” is often applied to help ensure that the fall timer does not expire before the plunger reaches the BHA. Plunger fall time is affected by many changing variables: plunger condition, tubing condition, liquid height, and gas and liquid density. These variables make it difficult for a fall timer set once to represent accurately the correct time required for the plunger to reach the BHA on every cycle.","",""
4,"Alexander Tischbirek","Artificial Intelligence and Discrimination: Discriminating Against Discriminatory Systems",2019,"","","","",179,"2022-07-13 09:21:43","","10.1007/978-3-030-32361-5_5","","",,,,,4,1.33,4,1,3,"","",""
2,"M. Shahid, G. Abbas, Mohammad Rashid Hussain, M. Asad, U. Farooq, J. Gu, V. Balas, M. Uzair, A. Awan, T. Yazdan","Artificial Intelligence-Based Controller for DC-DC Flyback Converter",2019,"","","","",180,"2022-07-13 09:21:43","","10.3390/app9235108","","",,,,,2,0.67,0,10,3,"This paper presents an intelligent voltage controller designed on the basis of an adaptive neuro-fuzzy inference system (ANFIS) for a flyback converter (FC) working in continuous conduction mode (CCM). The union of fuzzy logic (FL) and adaptive neural networks (ANN) makes ANFIS more robust against model parameters’ uncertainties and perturbations in input voltage or load current. ANFIS inherits the advantages of structured knowledge representation from FL and learning capability from NN. Comparative analysis showed that the ANFIS controller offers not only the superior transient response characteristics, but also excellent steady-state characteristics compared to those of the FL controller (FLC) and proportional–integral–derivative (PID) controllers, thus validating its superiority over these traditional controllers. For this purpose, MATLAB/Simulink environment-based simulation results are presented for validation of the proposed converter compensated system under all operating conditions.","",""
6,"Herut Uzan, Shira Sardi, A. Goldental, R. Vardi, I. Kanter","Biological learning curves outperform existing ones in artificial intelligence algorithms",2019,"","","","",181,"2022-07-13 09:21:43","","10.1038/s41598-019-48016-4","","",,,,,6,2.00,1,5,3,"","",""
0,"F. Morandin, G. Amato, M. Fantozzi, R. Gini, C. Metta, M. Parton","Sensible Artificial Intelligence that plays with handicap and targets high scores in 9 × 9",2019,"","","","",182,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,6,3,"We develop a new model that can be applied to any perfect information two-player zero-sum game to target a high score, and thus a perfect play. We integrate this model into the Monte Carlo tree search-policy iteration learning pipeline introduced by Google DeepMind with AlphaGo. Training this model on 9×9 Go produces a superhuman Go player, thus proving that it is stable and robust. We show that this model can be used to effectively play with both positional and score handicap. We develop a family of agents that can target high scores against any opponent, and recover from very severe disadvantage against weak opponents. To the best of our knowledge, these are the first effective achievements in this direction.","",""
81,"Thomas G. Dietterich","Steps Toward Robust Artificial Intelligence",2017,"","","","",183,"2022-07-13 09:21:43","","10.1609/aimag.v38i3.2756","","",,,,,81,16.20,81,1,5,"Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system’s models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world","",""
11,"K. Mudgal, Neelanjan Das","The ethical adoption of artificial intelligence in radiology",2019,"","","","",184,"2022-07-13 09:21:43","","10.1259/bjro.20190020","","",,,,,11,3.67,6,2,3,"Artificial intelligence (AI) is rapidly transforming healthcare—with radiology at the pioneering forefront. To be trustfully adopted, AI needs to be lawful, ethical and robust. This article covers the different aspects of a safe and sustainable deployment of AI in radiology during: training, integration and regulation. For training, data must be appropriately valued, and deals with AI companies must be centralized. Companies must clearly define anonymization and consent, and patients must be well-informed about their data usage. Data fed into algorithms must be made AI-ready by refining, purification, digitization and centralization. Finally, data must represent various demographics. AI needs to be safely integrated with radiologists-in-the-loop: guiding forming concepts of AI solutions and supervising training and feedback. To be well-regulated, AI systems must be approved by a health authority and agreements must be made upon liability for errors, roles of supervised and unsupervised AI and fair workforce distribution (between AI and radiologists), with a renewal of policy at regular intervals. Any errors made must have a root-cause analysis, with outcomes fedback to companies to close the loop—thus enabling a dynamic best prediction system. In the distant future, AI may act autonomously with little human supervision. Ethical training and integration can ensure a ""transparent"" technology that will allow insight: helping us reflect on our current understanding of imaging interpretation and fill knowledge gaps, eventually moulding radiological practice. This article proposes recommendations for ethical practise that can guide a nationalized framework to build a sustainable and transparent system.","",""
7,"Ashley Kras, L. Celi, John B. Miller","Accelerating ophthalmic artificial intelligence research: the role of an open access data repository.",2020,"","","","",185,"2022-07-13 09:21:43","","10.1097/ICU.0000000000000678","","",,,,,7,3.50,2,3,2,"PURPOSE OF REVIEW Artificial intelligence has already provided multiple clinically relevant applications in ophthalmology. Yet, the explosion of nonstandardized reporting of high-performing algorithms are rendered useless without robust and streamlined implementation guidelines. The development of protocols and checklists will accelerate the translation of research publications to impact on patient care.   RECENT FINDINGS Beyond technological scepticism, we lack uniformity in analysing algorithmic performance generalizability, and benchmarking impacts across clinical settings. No regulatory guardrails have been set to minimize bias or optimize interpretability; no consensus clinical acceptability thresholds or systematized postdeployment monitoring has been set. Moreover, stakeholders with misaligned incentives deepen the landscape complexity especially when it comes to the requisite data integration and harmonization to advance the field. Therefore, despite increasing algorithmic accuracy and commoditization, the infamous 'implementation gap' persists. Open clinical data repositories have been shown to rapidly accelerate research, minimize redundancies and disseminate the expertise and knowledge required to overcome existing barriers. Drawing upon the longstanding success of existing governance frameworks and robust data use and sharing agreements, the ophthalmic community has tremendous opportunity in ushering artificial intelligence into medicine. By collaboratively building a powerful resource of open, anonymized multimodal ophthalmic data, the next generation of clinicians can advance data-driven eye care in unprecedented ways.   SUMMARY This piece demonstrates that with readily accessible data, immense progress can be achieved clinically and methodologically to realize artificial intelligence's impact on clinical care. Exponentially progressive network effects can be seen by consolidating, curating and distributing data amongst both clinicians and data scientists.","",""
5,"Cathy O'Neil, H. Gunn","Near-Term Artificial Intelligence and the Ethical Matrix",2020,"","","","",186,"2022-07-13 09:21:43","","10.1093/oso/9780190905033.003.0009","","",,,,,5,2.50,3,2,2,"This chapter takes up the issue of near-term artificial intelligence, or the algorithms that are already in place in a variety of public and private sectors, guiding decisions from advertising and to credit ratings to sentencing in the justice system. There is a pressing need to recognize and evaluate the ways that structural racism, sexism, classism, and ableism may be embedded in and amplified by these systems. The chapter proposes a framework for ethical analysis that can be used to facilitate more robust ethical reflection in AI development and implementation. It presents an ethical matrix that incorporates the language of data science as a tool that data scientists can build themselves in order to integrate ethical analysis into the design process, addressing the need for immediate analysis and accountability over the design and deployment of near-term AI.","",""
19,"P. Svenmarck, L. Luotsinen, Mattias Nilsson, J. Schubert","Possibilities and Challenges for Artificial Intelligence in Military Applications",2018,"","","","",187,"2022-07-13 09:21:43","","","","",,,,,19,4.75,5,4,4,"Recent developments in artificial intelligence (AI) have resulted in a breakthrough for many classical AIapplications, such as computer vision, natural language processing, robotics, and data mining. Therefore, there are many efforts to exploit these developments for military applications, such as surveillance, reconnaissance, threat evaluation, underwater mine warfare, cyber security, intelligence analysis, command and control, and education and training. However, despite the possibilities for AI in military applications, there are many challenges to consider. For instance, 1) high risks means that military AI-systems need to be transparent to gain decision maker trust and to facilitate risk analysis; this is a challenge since many AItechniques are black boxes that lack sufficient transparency, 2) military AI-systems need to be robust and reliable; this is a challenge since it has been shown that AI-techniques may be vulnerable to imperceptible manipulations of input data even without any knowledge about the AI-technique that is used, and 3) many AItechniques are based on machine learning that requires large amounts of training data; this is challenge since there is often a lack of sufficient data in military applications. This paper present results from ongoing projects to identity possibilities for AI in military applications, as well as how to address these challenges.","",""
1,"Latifa Mrisho, N. Mbilinyi, Mathias Ndalahwa, Amanda Ramcharan, Annalyse Kehs, Peter McCloskey, H. Murithi, David P. Hughes, J. Legg","Evaluating the accuracy of a smartphone-based artificial intelligence system, PlantVillage Nuru, in diagnosing of the viral diseases of cassava",2020,"","","","",188,"2022-07-13 09:21:43","","10.1101/2020.01.26.919449","","",,,,,1,0.50,0,9,2,"Premise of the study Nuru is an artificial intelligence system for diagnosis of plant diseases and pests developed as a public good by PlantVillage (Penn State University), FAO, IITA and CIMMYT. It provides a simple, inexpensive and robust means of conducting in-field diagnosis without requiring internet connection and provides real-time results and advice. The present work evaluates the effectiveness of Nuru as an in-field diagnostic tool by comparing the diagnosis capability of Nuru to that of cassava experts (researchers trained on cassava pests and diseases), agricultural extension agents and farmers. Methods The diagnosis capability of Nuru and that of the assessed individuals was determined by inspecting cassava plants in-field and by using the cassava symptom recognition assessment tool (CaSRAT) to score images of cassava leaves. Results Nuru’s accuracy for symptom recognition when using six leaves (74 - 88%, depending on the condition) was similar to that of experts, 1.5-times higher than agricultural extension agents and two-times higher than farmers. Discussion These findings suggests that Nuru can be an effective tool for in-field diagnosis of cassava diseases and has a potential of being a quick and cost-effective means of disseminating knowledge from researchers to agricultural extension agents and farmers.","",""
0,"R. Brachman, David Gunning, Murray Burke","Integrated Artificial Intelligence Systems",2020,"","","","",189,"2022-07-13 09:21:43","","","","",,,,,0,0.00,0,3,2,"Copyright © 2020, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 66 AI MAGAZINE When one thinks about what it might take to build an intelligent system, it is evident that multiple capabilities will be required. Intelligence is generally considered to be reflected in the ability of a system to learn and understand the world around it, and to deal successfully with new or challenging situations. A closer look at what it might take to accomplish this reveals a surprisingly complex set of abilities that must work together. There are many variations on these themes, but roughly speaking, a robustly intelligent, autonomous agent embedded in the real world will need perceptual capabilities to sense and help interpret external signals and phenomena; a set of beliefs about the world, including itself and other agents, cause and effect, and a host of other things relevant to its survival and success in achieving its goals; a variety of reasoning capabilities to determine implications of its beliefs, understand its environment, plan ahead, solve problems, and so forth; a wide array of learning and adaptation capabilities; the ability to affect the world through action; and, some kind of rich communication mechanism along the lines of natural human language generation and understanding.  From Shakey the Robot to self-driving cars, from the personal computer to personal assistants on our phones, the Defense Advanced Research Projects Agency (DARPA) has led the development of integrated artificial intelligence (AI) systems for more than half a century. From the earliest days of AI, it was apparent that a robust, generally intelligent system should include a complete set of capabilities: perception, memory, reasoning, learning, planning, and action; and when DARPA initiated AI research in the 1960s, ambitious projects such as Shakey the Robot went after the complete package. As DARPA realized the challenges, they backed away from the ultimate goal of integrated AI and tried to make progress on the individual problems of image understanding, speech and language understanding, knowledge representation and reasoning, planning and decision aids, machine learning, and robotic manipulation. Yet, even as researchers struggled to make progress in these subdisciplines, DARPA periodically resurrected the challenge of integrated intelligent systems and pushed the community to try again. In the 1980s, DARPA’s Strategic Computing Initiative took on challenges of integrated AI projects such as the Autonomous Land Vehicle and the Pilot’s Associate. These did not succeed, but instead set the stage for the several decades of more siloed research that followed, until it was time to try again. In the 2000s, DARPA took on the integrated AI problem again with its Grand Challenges, which led to the first self-driving cars, and projects such as the Personalized Assistant that Learns, which produced Apple’s Siri. These efforts created complex, richlyintegrated systems that represented quantum leaps ahead in machine intelligence. The integration of sophisticated capabilities in a fundamental way is the key to general intelligence. This is the story of DARPA’s persistent long-term support for this essential premise of AI. Integrated Artificial Intelligence Systems","",""
0,"A. Paic","Policies for Artificial Intelligence in Science and Innovation",2020,"","","","",190,"2022-07-13 09:21:43","","10.22323/1.372.0045","","",,,,,0,0.00,0,1,2,"This contribution synthesizes the discussions of the special session on policies for Artificial Intelligence in Science and Innovation, organized by the OECD’s Directorate for Science, Technology and Innovation. The session was opened by Dr Judith Arrieta, Minister of the Foreign Service at the Chief of Staff’s Office of the Secretary of Foreign Affairs of Mexico, and the two panels included speakers from governments, industry and civil society from European countries, USA,Canada China and Australia. Participants discussed the disruptive nature of AI and the formidable challenges it poses. Most of the discussion focused under the umbrella title of ethics, but they span very different issues of human-centered values, fairness, transparency, explainability, and many more. Other challenges include employment, education, SME policy, enabling environment, access to data and computing technology. Responses by governments were also discussed with a particular focus on national strategies, whose main pillars are oriented toward knowledge creation through AI research, knowledge diffusion through linkages to the private sector, development of human capital which will underpin the development of the sector, and a strong values, ethical and regulatory framework to create the conditions for the development of trustworthy AI.  In a world of finite resources, discussants concluded that one cannot apply very stringent requirements to all AI decisions, and there is clearly a need to require more transparency, explainability and robustness from systems which have the greatest impact on human lives. Therefore an approach based on algorithmic impact assessment seems reasonable. Such an approach needs to be further developed and standardized.","",""
0,"S. Bandyopadhyay, R. Mukherjee, S. Sarkar","A Report on the First Workshop on Software Engineering for Artificial Intelligence (SE4AI 2020)",2020,"","","","",191,"2022-07-13 09:21:43","","10.1145/3385032.3385055","","",,,,,0,0.00,0,3,2,"With advancement in technology-driven decision making, the software-intensive systems for decisions have become more robust, dynamic, adaptive, context-aware, dependable. Architectural designs of such systems crave for new approaches where the data-driven decision making has to be incorporated in the solution. Methods for recommendation mechanism, prediction of operation failures, dealing with unsafe conditions etc are going to be part of the solution itself. Integrating such features to conceive an intelligent system that will directly influence the business solution is mostly appreciated. This would not have been possible without the direct interference of Artificial Intelligence which has been a standard procedure of industrial repertoire since 1980s. The direct impact of AI on social and economic life has been been felt mostly in last decade (since 2007) with the advent of smart phone, which contribute largely to ""big data"". The era of ""big data"" has witnessed the efficacy of Machine Learning and there is a need of the hour to combine data-driven machine intelligence with human intelligence (insights and domain knowledge) to effectively make the software development (requirement, design, testing, deployment and operation management) intelligent. The research community has shown a keen interest in this emerging field. In this report, we present a pre-organization summary of the workshop to be held on February 27, 2020, at IIIT Jabbalpur (India), co-located with the 13th Innovations in Software Engineering Conference (ISEC 2020).","",""
0,"S. Cuddy","THE BENEFITS AND DANGERS OF USING ARTIFICIAL INTELLIGENCE IN PETROPHYSICS",2020,"","","","",192,"2022-07-13 09:21:43","","10.30632/spwla-5066","","",,,,,0,0.00,0,1,2,"Abstract Artificial Intelligence, or AI, is a method of data analysis that learns from data, identify patterns and makes predictions with the minimal human intervention. AI is bringing many benefits to petrophysical evaluation. Using case studies, this paper describes several successful applications. The future of AI has even more potential. However, if used carelessly there are potentially grave consequences. A complex Middle East Carbonate field needed a bespoke shaly water saturation equation. AI was used to ‘evolve’ an ideal equation, together with field specific saturation and cementation exponents. One UKCS gas field had an ‘oil problem’. Here, AI was used to unlock the hidden fluid information in the NMR T1 and T2 spectra and successfully differentiate oil and gas zones in real time. A North Sea field with 30 wells had shear velocity data (Vs) in only 4 wells. Vs was required for reservoir modelling and well bore stability prediction. AI was used to predict Vs in all 30 wells. Incorporating high vertical resolution data, the Vs predictions were even better than the recorded logs. As it is not economic to take core data on every well, AI is used to discover the relationships between logs, core, litho-facies and permeability in multi-dimensional data space. As a consequence, all wells in a field were populated with these data to build a robust reservoir model. In addition, the AI predicted data upscaled correctly unlike many conventional techniques. AI gives impressive results when automatically log quality controlling (LQC) and repairing electrical logs for bad hole and sections of missing data. AI doesn’t require prior knowledge of the petrophysical response equations and is self-calibrating. There are no parameters to pick or cross-plots to make. There is very little user intervention and AI avoids the problem of ‘garbage in, garbage out’ (GIGO), by ignoring noise and outliers. AI programs work with an unlimited number of electrical logs, core and gas chromatography data; and don’t ‘fall-over’ if some of those inputs are missing. AI programs currently being developed include ones where their machine code evolves using similar rules used by life’s DNA code. These AI programs pose considerable dangers far beyond the oil industry as described in this paper. A ‘risk assessment’ is essential on all AI programs so that all hazards and risk factors, that could cause harm, are identified and mitigated.","",""
128,"D. Bonderman","Artificial intelligence in cardiology",2017,"","","","",193,"2022-07-13 09:21:43","","10.1007/s00508-017-1275-y","","",,,,,128,25.60,128,1,5,"","",""
496,"Christopher J. Kelly, A. Karthikesalingam, Mustafa Suleyman, Greg Corrado, Dominic King","Key challenges for delivering clinical impact with artificial intelligence",2019,"","","","",194,"2022-07-13 09:21:43","","10.1186/s12916-019-1426-2","","",,,,,496,165.33,99,5,3,"","",""
30,"S. Elkatatny, Zeeshan Tariq, M. Mahmoud, I. Mohamed, A. Abdulraheem","Development of New Mathematical Model for Compressional and Shear Sonic Times from Wireline Log Data Using Artificial Intelligence Neural Networks (White Box)",2018,"","","","",195,"2022-07-13 09:21:43","","10.1007/S13369-018-3094-5","","",,,,,30,7.50,6,5,4,"","",""
10,"M. Alomar, M. Hameed, N. Al‐Ansari, M. Alsaadi","Data-Driven Model for the Prediction of Total Dissolved Gas: Robust Artificial Intelligence Approach",2020,"","","","",196,"2022-07-13 09:21:43","","10.1155/2020/6618842","","",,,,,10,5.00,3,4,2,"Saturated total dissolved gas (TDG) is recently considered as a serious issue in the environmental engineering field since it stands behind the reasons for increasing the mortality rates of fish and aquatic organisms. The accurate and more reliable prediction of TDG has a very significant role in preserving the diversity of aquatic organisms and reducing the phenomenon of fish deaths. Herein, two machine learning approaches called support vector regression (SVR) and extreme learning machine (ELM) have been applied to predict the saturated TDG% at USGS 14150000 and USGS 14181500 stations which are located in the USA. For the USGS 14150000 station, the recorded samples from 13 October 2016 to 14 March 2019 (75%) were used for training set, and the rest from 15 March 2019 to 13 October 2019 (25%) were used for testing requirements. Similarly, for USGS 14181500 station, the hourly data samples which covered the period from 9 June 2017 till 11 March 2019 were used for calibrating the models and from 12 March 2019 until 9 October 2019 were used for testing the predictive models. Eight input combinations based on different parameters have been established as well as nine statistical performance measures have been used for evaluating the accuracy of adopted models, for instance, not limited, correlation of determination (        R      2        ), mean absolute relative error (MAE), and uncertainty at 95% (        U      95        ). The obtained results of the study for both stations revealed that the ELM managed efficiently to estimate the TDG in comparison to SVR technique. For USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.986 (0.986), MAE of 0.316 (0.441), and         U      95        of 3.592 (3.869). Lastly, for USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.991 (0.991), MAE of 0.338 (0.396), and         U      95        of 0.832 (0.837). In addition, ELM’s training process computational time is stated to be much shorter than that of SVM. The results also showed that the temperature parameter was the most significant variable that influenced TDG relative to the other parameters. Overall, the proposed model (ELM) proved to be an appropriate and efficient computer-assisted technology for saturated TDG modeling that will contribute to the basic knowledge of environmental considerations.","",""
47,"Chengjie Zheng, T. V. Johnson, Aakriti Garg, Michael V. Boland","Artificial intelligence in glaucoma",2019,"","","","",197,"2022-07-13 09:21:43","","10.1097/ICU.0000000000000552","","",,,,,47,15.67,12,4,3,"Purpose of review The use of computers has become increasingly relevant to medical decision-making, and artificial intelligence methods have recently demonstrated significant advances in medicine. We therefore provide an overview of current artificial intelligence methods and their applications, to help the practicing ophthalmologist understand their potential impact on glaucoma care. Recent findings Techniques used in artificial intelligence can successfully analyze and categorize data from visual fields, optic nerve structure [e.g., optical coherence tomography (OCT) and fundus photography], ocular biomechanical properties, and a combination thereof to identify disease severity, determine disease progression, and/or recommend referral for specialized care. Algorithms have become increasingly complex in recent years, utilizing both supervised and unsupervised methods of artificial intelligence. Impressive performance of these algorithms on previously unseen data has been reported, often outperforming standard global indices and expert observers. However, there remains no clearly defined gold standard for determining the presence and severity of glaucoma, which undermines the training of these algorithms. To improve upon existing methodologies, future work must employ more robust definitions of disease, optimize data inputs for artificial intelligence analysis, and improve methods of extracting knowledge from learned results. Summary Artificial intelligence has the potential to revolutionize the screening, diagnosis, and classification of glaucoma, both through the automated processing of large data sets, and by earlier detection of new disease patterns. In addition, artificial intelligence holds promise for fundamentally changing research aimed at understanding the development, progression, and treatment of glaucoma, by identifying novel risk factors and by evaluating the importance of existing ones.","",""
12,"S. Craw, A. Aamodt","Case Based Reasoning as a Model for Cognitive Artificial Intelligence",2018,"","","","",198,"2022-07-13 09:21:43","","10.1007/978-3-030-01081-2_5","","",,,,,12,3.00,6,2,4,"","",""
32,"Matt Taddy","The Technological Elements of Artificial Intelligence",2018,"","","","",199,"2022-07-13 09:21:43","","10.3386/W24301","","",,,,,32,8.00,32,1,4,"We have seen in the past decade a sharp increase in the extent that companies use data to optimize their businesses. Variously called the `Big Data' or `Data Science' revolution, this has been characterized by massive amounts of data, including unstructured and nontraditional data like text and images, and the use of fast and flexible Machine Learning (ML) algorithms in analysis. With recent improvements in Deep Neural Networks (DNNs) and related methods, application of high-performance ML algorithms has become more automatic and robust to different data scenarios. That has led to the rapid rise of an Artificial Intelligence (AI) that works by combining many ML algorithms together – each targeting a straightforward prediction task – to solve complex problems. We will define a framework for thinking about the ingredients of this new ML-driven AI. Having an understanding of the pieces that make up these systems and how they fit together is important for those who will be building businesses around this technology. Those studying the economics of AI can use these definitions to remove ambiguity from the conversation on AI's projected productivity impacts and data requirements. Finally, this framework should help clarify the role for AI in the practice of modern business analytics and economic measurement.","",""
15,"Yun-he Pan","Special issue on artificial intelligence 2.0",2017,"","","","",200,"2022-07-13 09:21:43","","10.1631/FITEE.1710000","","",,,,,15,3.00,15,1,5,"With the ever-growing popularization of the Internet, universal existence of sensors, emergence of big data, development of e-commerce, rise of the information community, and interconnection and fusion of data and knowledge in human society, physical space, and cyberspace, the information environment surrounding artificial intelligence (AI) development has changed profoundly, leading to a new evolutionary stage: AI 2.0. The emergence of new technologies also promotes AI to a new stage (Pan, 2016). The next-generation AI, namely AI 2.0, is a more explainable, robust, open, and general AI with the following attractive merits: It effectively integrates data-driven machine learning approaches (bottom-up) with knowledge-guided methods (top-down). In addition, it can employ data with different modalities (e.g., visual, auditory, and natural language processing) to perform cross-media learning and inference. Furthermore, there will be a step from the pursuit of an intelligent machine to the hybridaugmented intelligence (i.e., high-level man-machine collaboration and fusion). AI 2.0 will also promote crowd-based intelligence and autonomous-intelligent systems. In the next decades, AI2.0 will probably achieve remarkable progress in aforementioned trends, and therefore significantly change our cities, products, services, economics, environments, even how we advance our society. This special issue aims at reporting recent re-thinking of AI 2.0 from aforementioned aspects as well as practical methodologies, efficient implementations, and applications of AI 2.0. The papers in this special issue can be categorized into two groups. The first group consists of six review papers and the second group five research papers. In the first group, Zhuang et al. (2017) reviewed recent emerging theoretical and technological advances of AI in big data settings. The authors concluded that integrating data-driven machine learning with human knowledge (common priors or implicit intuitions) can effectively lead to explainable, robust, and general AI. Li W et al. (2017) described the concepts of crowd intelligence, and explained its relationship to the existing related concepts, e.g., crowdsourcing and human computation. In addition, the authors introduced four categories of representative crowd intelligence platforms. Peng et al. (2017) presented approaches, advances, and future directions in cross-media analysis and reasoning. This paper covers cross-media representation, mining, reasoning, and cross-media knowledge evolution. Tian et al. (2017) reviewed the state-of-the-art research of the perception in terms of visual perception, auditory perception, and speech perception. It also covered perceptual information processing and learning engines. Zhang et al. (2017) introduced the trends in the development of intelligent unmanned autonomous systems. It covered unmanned vehicles, unmanned aerial vehicles, service robots, space robots, marine robots, and unmanned Editorial: Frontiers of Information Technology & Electronic Engineering www.zju.edu.cn/jzus; engineering.cae.cn; www.springerlink.com ISSN 2095-9184 (print); ISSN 2095-9230 (online) E-mail: jzus@zju.edu.cn","",""
