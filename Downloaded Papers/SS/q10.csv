Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
0,"Svitlana Volkova, Dustin L. Arendt, Emily Saldanha, M. Glenski, Ellyn Ayton, Joseph A. Cottam, Sinan G. Aksoy, Brett Jefferson, Karthnik Shrivaram","Explaining and predicting human behavior and social dynamics in simulated virtual worlds: reproducibility, generalizability, and robustness of causal discovery methods",2021,"","","","",1,"2022-07-13 09:20:03","","10.1007/s10588-021-09351-y","","",,,,,0,0.00,0,9,1,"","",""
22,"L. Valiant","Knowledge Infusion: In Pursuit of Robustness in Artificial Intelligence",2008,"","","","",2,"2022-07-13 09:20:03","","10.4230/LIPIcs.FSTTCS.2008.1770","","",,,,,22,1.57,22,1,14,"Endowing computers with the ability to apply commonsense knowledge with human- level performance is a primary challenge for computer science, comparable in importance to past great challenges in other fields of science such as the sequencing of the human genome. The right approach to this problem is still under debate. Here we shall discuss and attempt to justify one ap- proach, that of knowledge infusion. This approach is based on the view that the fundamental objective that needs to be achieved is robustness in the following sense: a framework is needed in which a computer system can represent pieces of knowledge about the world, each piece having some un- certainty, and the interactions among the pieces having even more uncertainty, such that the system can nevertheless reason from these pieces so that the uncertainties in its conclusions are at least controlled. In knowledge infusion rules are learned from the world in a principled way so that sub- sequent reasoning using these rules will also be principled, and subject only to errors that can be bounded in terms of the inverse of the effort invested in the learning process.","",""
0,"Tongjie Pan, Ziwei Huang, Yalan Ye, Yunfei Cheng, Wenwen He, Chong Wang","Joint Transfer Strategy for Cross-Domain Human Activity Recognition",2021,"","","","",3,"2022-07-13 09:20:03","","10.1109/ICET51757.2021.9451080","","",,,,,0,0.00,0,6,1,"Human activity recognition based on wearable sensors has been considered as an important work in the increasingly developed artificial intelligence and Internet of Things. Since acquiring enough activity labels is often expensive and time-consuming, an intuitive way is to leverage existing data from one sensor (source domain) to recognize target data from another sensor (target domain). Unfortunately, the data from different domains may have a large discrepancy. Existing approaches typically consider reducing the domain discrepancy to transfer knowledge. However, these approaches do not take full advantage of data structure, which is important for activity recognition. In this paper, we propose an effective method, named Joint Transfer Strategy (JTS). The method has high accuracy and robustness for cross-domain human activity recognition. Specifically, JTS first obtains pseudo labels for the target domain via a majority voting technique. Then, it leverages a joint transfer strategy to map the source and target domain data into a shared domain-invariant subspace and preserve the data structure information. Finally, an adaptive classifier is learned to label target data. Comprehensive experiments on three large public activity recognition datasets demonstrate that JTS outperforms other state-of-the-art methods in terms of classification accuracy.","",""
5,"M. Selfridge, D. J. Dickerson, S. F. Biggs","Cognitive Expert Systems and Machine Learning: Artificial Intelligence Research at the University of Connecticut",1987,"","","","",4,"2022-07-13 09:20:03","","10.1609/AIMAG.V8I1.577","","",,,,,5,0.14,2,3,35,"In order for next-generation expert systems to demonstrate the performance, robustness, flexibility, and learning ability of human experts, they will have to be based on cognitive models of expert human reasoning and learning. We call such next-generation systems cognitive expert systems. Research at the Artificial Intelligence Laboratory at the University of Connecticut is directed toward understanding the principles underlying cognitive expert systems and developing computer programs embodying those principles. The Causal Model Acquisition System (CMACS) learns causal models of physical mechanisms by understanding real-world natural language explanations of those mechanisms. The going Concern Expert ( GCX) uses business and environmental knowledge to assess whether a company will remain in business for at least the following year. The Business Information System (BIS) acquires business and environmental knowledge from in-depth reading of real-world news stories. These systems are based on theories of expert human reasoning and learning, and thus represent steps toward next-generation cognitive expert systems.","",""
0,"Youngchul Bae, Yong Soo Kim, F. Rhee, Yong-Tae Kim, C. Tao","Editorial Message: Special Issue on Fuzzy System in Data Mining and Knowledge Discovery: Modelling and Application",2017,"","","","",5,"2022-07-13 09:20:03","","10.1007/S40815-017-0359-1","","",,,,,0,0.00,0,5,5,"","",""
233,"E. Papageorgiou, C. Stylios, P. Groumpos","Fuzzy Cognitive Map Learning Based on Nonlinear Hebbian Rule",2003,"","","","",6,"2022-07-13 09:20:03","","10.1007/978-3-540-24581-0_22","","",,,,,233,12.26,78,3,19,"","",""
5,"Justin W. Hart, B. Scassellati","Robotic Self-Models Inspired by Human Development",2010,"","","","",7,"2022-07-13 09:20:03","","","","",,,,,5,0.42,3,2,12,"Traditionally, in the fields of artificial intelligence and robotics, representations of the self have been conspicuously absent. Capabilities of systems are listed explicitly by developers during construction and choices between behavioral options are decided based on search, inference, and planning. In robotics, while knowledge of the external world has often been acquired through experience, knowledge about the robot itself has generally been built in by the designer. Built-in models of the robot's kinematics, physical and sensory capabilities, and other equipment have stood in the place of self-knowledge, but none of these representations offer the flexibility, robustness, and functionality that are present in people. In this work, we seek to emulate forms of self-awareness developed during human infancy in our humanoid robot, Nico. In particular, we are interested in the ability to reason about the robot's embodiment and physical capabilities, with the robot building a model of itself through its experiences.","",""
1,"T. Dao, Federico Marin, M. Tho","MODELING OF MUSCULOSKELETAL SYSTEM USING BIOMECHANICS AND KNOWLEDGE ENGINEERING APPROACHES: CLINICAL BENEFITS AND LIMITATIONS",2011,"","","","",8,"2022-07-13 09:20:03","","","","",,,,,1,0.09,0,3,11,"Understanding of mechanical behaviors of the human body is a challenge to take appropriate medical decisions (e.g. patient's diagnosis or treatment). To achieve this objective, two modeling approaches were studied and confronted in order to highlight the benefits and limitations of each approach to clinical problems. The biomechanical model is including anatomical geometries, anthropometrical data, mechanical properties and motion analysis data. A new type of model named meta-model is based on knowledge engineering representation, data mining and artificial intelligence methods. Orthopedic pediatric pathologies (Polio, clubfoot, cerebral palsy) were studied to evaluate the accuracy and robustness of these modeling approaches. Methodological confrontation through clinical benefits and limitations of each modeling approach and their complementarities were analyzed and presented. To conclude, even if input data and modeling of each approach are different, these two approaches are closely complementary for better understanding of musculoskeletal disorders leading to best diagnosis and treatment prescriptions.","",""
0,"C. Brunger","Artificial Neural Network Modeling of Damaged Aircraft",1994,"","","","",9,"2022-07-13 09:20:03","","","","",,,,,0,0.00,0,1,28,"Abstract : Aircraft design and control techniques rely on the proper modeling of the aircraft's equations of motion. Many of the variables used in these equations are aerodynamic coefficients which are obtained from scale models in wind tunnel tests. In order to model damaged aircraft, every aerodynamic coefficient must be determined for every possible damage mechanism in every flight condition. Designing a controller for a damaged aircraft is particularly burdensome because knowledge of the effect of each damage mechanism on the model is required before the controller can be designed. Also, a monitoring system must be employed to decide when and how much damage has occurred in order to re configure the controller. Recent advances in artificial intelligence have made parallel distributed processors (artificial neural networks) feasible. Modeled on the human brain, the artificial neural network's strength lies in its ability to generalize from a given model. This thesis examines the robustness of the artificial neural network as a model for damaged aircraft.","",""
35,"D. Cliff, J. Bruten","Simple Bargaining Agents for Decentralized Market-Based Control",1998,"","","","",10,"2022-07-13 09:20:03","","","","",,,,,35,1.46,18,2,24,"market-based control, economic agents, bargaining, autonomous agent Market-Based Control (MBC) is a resource allocation and control technique where multi-agent systems are built to resemble free-market economies. The aim is that MBC systems exhibit the same decentralization, robustness, and capacity for self-organization as do real economies. MBC systems are relevant to Artificial Intelligence (AI) and robotics in at least two ways: first, the agents in a MBC system need to be robot-like in their ability to autonomously coordinate perception and action in dynamic and uncertain environments that include other agents; second, MBC systems could be used as the control technologies for robots and other “intelligent” autonomous agents. We critically review a selection of MBC systems. We argue that the MBC systems reviewed here are either implicitly reliant on centralized knowledge, or require human operators and hence are not truly automatic. We identify a major issue in creating truly decentralized and automatic MBC systems: the need for the system's agents to be capable of bargaining behaviors. Following this, we briefly summarize our current results and ongoing work in creating multi-agent systems where each autonomous agent has the ability to bargain with other agents. We demonstrate that markets composed of such agents exhibit desirable behaviors, and that such agents could form the basis of truly decentralized MBC systems.","",""
1,"S. Sugiyama","Self evolving dynamic knowledge base",1998,"","","","",11,"2022-07-13 09:20:03","","10.1109/ICSMC.1998.728187","","",,,,,1,0.04,1,1,24,"There have been a lot of studies on artificial intelligence. But still we have a lot of problems left, like analysis of the environment for solving problems, low robustness, lack of information on a target, less matured computer languages, and so forth. Which means that a lot of human interactions are still necessary in order to get proper answers for given problems. Studies on how to reduce the human interactions are performed by using backpropagation neural networks with the idea of dynamic knowledge base which has an ability to evolve itself in order to get the most appropriate knowledge base and performance mechanisms for solving problems given. And as a result, it has been proved that it is possible to have a self evolving dynamic knowledge base which reduces a lot of human interactions which used to need.","",""
2,"M. Bringmann","Knowledge acquisition through the use of combined repertory grids",1990,"","","","",12,"2022-07-13 09:20:03","","","","",,,,,2,0.06,2,1,32,"Artificial intelligence is the study of the simulation of human cognitive faculties. Applications of various aspects of AI are important to the development of expert or knowledge-based systems. An individual expert system can be characterized by the methods selected for representing expertise, transfer of expertise and user interaction. Various aspects of a domain can be represented by separate knowledge sources acquired from different experts or through different models of reasoning. Combination of these knowledge sources can improve the capability or robustness of a model of a domain. The literature of clinical and experimental psychology contains much research relevant to the problem of improving the communication between knowledge engineers and domain experts. Specifically, the Personal Construct Theory of George A. Kelly (1955) has served as the basis for several recent experimental approaches to the design and construction of automated (i.e., computer-based) knowledge acquisition tools. A measure of the shared aspects of domain knowledge common to multiple experts is developed and illustrated with this model using knowledge gathered about two different domains. A model of the shared aspects of personal construct systems of a domain is developed and a system to draw conclusions based upon this information is also proposed.","",""
0,"Peer-Olaf Siebers","Hongmei He (intelligent System Lab, University of Bristol): Soft Computing Approaches under the Framework of Hierarchical Decision Making or Classification System",2010,"","","","",13,"2022-07-13 09:20:03","","","","",,,,,0,0.00,0,1,12,"Hongmei He (Intelligent System Lab, University of Bristol): Soft Computing Approaches Under the Framework of Hierarchical Decision Making or Classification System With the development of AI, we can see there are a surprising number of the brain functions of the human Intelligent System (IS) are quite similar to those of an artificial IS, since most artificial ISs are modelled through naturally emulating human intelligence. A wide variety of approaches have been utilised in the functional design of artificial ISs. For example, fuzzy logic for robustness, decision trees for the transparency of reasoning, machine learning for knowledge learning, semantics for understandability, probabilistic reasoning, and neural computing, etc.","",""
0,"Bordeaux-Sud-Ouest","Project-Team FLOWERS Flowing Epigenetic Robots and Systems",2009,"","","","",14,"2022-07-13 09:20:03","","","","",,,,,0,0.00,0,1,13,"In spite of considerable and impressive work in artificial intelligence, machine learning, and pattern recognition in the past 50 years, we have no machine capable of adapting to the physical and social environment with the flexibility, robustness and versatility of a 6-months old human child. Instead of trying to simulate directly the adult’s intelligence, EXPLORERS proposes to focus on the developmental processes that give rise to intelligence in infants by re-implementing them in machines. Framed in the developmental/epigenetic robotics research agenda, and grounded in research in human developmental psychology, its main target is to build robotic machines capable of autonomously learning and re-using a variety of skills and know-how that were not specified at design time, and with initially limited knowledge of the body and of the environment in which it will operate. This implies several fundamental issues: How can a robot discover its body and its relationships with the physical and social environment? How can it learn new skills without the intervention of an engineer? What internal motivations shall guide its exploration of vast spaces of skills? Can it learn through natural social interactions with humans? How to represent the learnt skills and how can they be re-used? EXPLORERS attacks directly those questions by proposing a series of scientific and technological advances: 1) we will formalize and implement sophisticated systems of intrinsic motivation, responsible of organized spontaneous exploration in humans, for the regulation of the growth of complexity of learning situations; 2) intrinsic motivation systems will be used to drive the learning of forward/anticipative sensorimotor models in high-dimensional multimodal spaces, as well as the building of reusable behavioural macros; 3) intrinsically motivated exploration will be coupled with social guidance from non-engineer humans; 4) an informationtheoretic framework will complement intrinsically motivated exploration to allow for the inference of body maps; 5) we will show how learnt basic sensorimotor skills can be re-used to learn the meaning of early concrete words, pushing forward human-robot mutual understanding. Furthermore, we will setup large scale experiments, in order to show how these advances can allow a high-dimensional multimodal robot to learn collections of skills continuously in a weeks-to-months time scale. This project not only addresses fundamental scientific questions, but also relates to important societal issues: personal home robots are bound to become part of everyday life in the 21st century, in particular as helpful social companions in an aging society. EXPLORERS’ objectives converge to the challenges implied by this vision: robots will have to be able to adapt and learn new skills in the unknown homes of users who are not engineers. The ERC EXPLORERS is a central scientific driver of the FLOWERS team. 8.4. International Initiatives 8.4.1. Inria International Partners • Luis Montesano, University of Zaragoza, Spain. Manuel Lopes collaborated with Luis Montesano on several topics. Recently on active learning approaches for grasping point learning [103] and clustering activities. • Francisco Melo Instituto Superior Técnico, Portugal. Manuel Lopes collaborated with Francisco Melo on the development of active learning for inverse reinforcement learning. Recent developments consider the extension to more cues available to the learner and sampling complexity on the algorithm. • José Santos-Victor, Instituto Superior Técnico, Portugal. Manuel Lopes collaborated with José Santos-Victor on the extension of affordances models to higher levels of representations, e.g. relational models. • Maya Cakmak, Andrea Thomaz, Georgia Tech, USA. Manuel Lopes collaborated with Maya Cakmak on the development of optimal teaching algorithms for sequential decision problems (modeled as markov decision processes). The algorithm provides optimal demonstrations for systems that learn using inverse reinforcement learning. The joint work considers not only the algorithmic aspects but also a comparison with human behavior and the possibility of using insights from the algorithm to elicit better teaching behavior on humans [32]. 50 Activity Report INRIA 2012 • Marc Toussaint, Tobias Lang, Free University of Berlin, Germany. Manuel Lopes and PierreYves Oudeyer are collaborating with FUB in the unification of exploration algorithms based on intrinsic motivation with methods for exploration in reinforcement learning such asRmax. We intend to develop a general framework for exploration in non-stationary domains [46]. Another project consider how to learn efficient representation for robotic hierarchical planning [44]. • Todd Hester and Peter Stone, University of Texas, USA ( 2012 ) Peter Stone is a leading expert on reinforcement learning applied to real robots ( he won the RobotCup competition several times) and to multi-agent problems. We started this collaboration by introducing a new method to automatically select the best exploration strategy to use in a particular problem [42]. Future directions of the collaboration will include ad-hoc teams, exploration in continuous space and human-guided machine learning. • Jacqueline Gottlieb and Adrien Baranes, Columbia University, New-York, US. Pierre-Yves Oudeyer and Manuel Lopes continued a collaboration with Jacqueline Gottlied, neuroscientist at Columbia University and specialist of visual attention and exploration in monkeys, and Adrien Baranes, postdoc in Gottlieb’s lab and previously working in Flowers team. An experimental setup with brain imaging and behavioural observations of monkeys, and made to evaluate new families of computational models of visual attention and exploration (some of which developped in the team around the concept of intrinsic motivation) is being elaborated. • Louis ten Bosch, Radboud University, The Netherlands. Pierre-Yves Oudeyer and David Filliat continued to work with Louis ten Bosch on the modelling of multimodal language acquisition using techniques based on Non-Negative Matrix Factorization. We showed that these techniques can allow a robot to discover audio-video invariants starting from a continuous unlabelled and unsegmented flow of low-level auditory and visual stimuli. A journal article is in preparation. • Britta Wrede, Katharina Rohlfing, Jochen Steil and Sebastian Wrede, Bielefeld University, Germany, Jun Tani KAIST, South Korea. Pierre-Yves Oudeyer collaborated with Wrede, Rohlfing, Steil, Wrede and Tani on the elaboration of a novel conceptual vision of teleoogical language and action development in robots. This led to the publication of a joint workshop article [64]. • Michael A. Arbib, University of Southern California (Los Angeles, USA). Clément Moulin-Frier is continuing his collaborative work with Michael Arbib since his 6-month visit at USC in 2009. See the section entitled “Recognizing speech in a novel accent: the Motor Theory of Speech Perception reframed” for more information. • Paul Vogt (Tillburg University, The Netherlands), Linda Smith (Indiana University, Bloomington, US), Aslo Ozyurek (Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands), Tony Belpaeme (University of Plymouth, UK). Pierre-Yves Oudeyer began collaboration with partners of the NWO SCMSC project to set up a research network on modeling of social cognition and symbolic communication. • Michael Gienger from Honda Research Institute Europe. Alexander Gepperth collaborated with Principal Scientist Dr.Michael Gienger from Honda Research Institute Europe GmbH about robotic grasping: this activity will result in a jointly supervised internship (""stage de fine d’études"") and a publication. • Ursula Korner from Honda Research Institute Europe. Alexander Gepperth collaborated with Dr. Usula Körner of Honda Research Institute Europe GmbH, Offenbach (Germany), on the topic of biologically inspired learning architectures for visual categorization of behaviorally relevant entities. This work is intended to be summitted to the International Conference on Development and Learning, as well as the journal ""Frontiers in Cognitive Systems"". • Michael Garcia Ortiz, Laboratory for Cognitive Robotics (CoR-Lab) in Bielefeld, Germany. Alexander Gepperth collaborated with Michael Garcia Ortiz, a PhD student from the Laboratory for Cognitive Robotics (CoR-Lab) in Bielefeld, Germany, on the exploitation of scene context for object detection in intelligent vehicles. This collaboration resulted in the submission of a journal publication to the journal ”Neurocomputing”. Project-Team FLOWERS 51 • Martha White and Richard Sutton from the University of Alberta, Canada. Thomas Degris collaborated with Martha White and Richard Sutton on the paper “Off-Policy Actor–Critic” [38]. • Patrick Pilarski and Richard Sutton from the University of Alberta (Canada). Thomas Degris collaborated with Patrick Pilarski on the following papers: “Model-Free Reinforcement Learning with Continuous Action in Practice” [37], “Apprentissage par Renforcement sans Modèle et avec Action Continue” [65], “Dynamic Switching and Real-time Machine Learning for Improved Human Control of Assistive Biomedical Robots” [57], “Towards Prediction-Based Prosthetic Control” [58], and “Prediction and Anticipation for Adaptive Artificial Limbs” [27]. • Joseph Modayil from the University of Alberta, Canada. Thomas Degris collaborated with Joseph Modayil on the following paper: “Scaling-up Knowledge for a Cognizant Robot” [35]. • Ashique Rupam Mahmood from the University of Alberta, Canada. Thomas Degris collaborated with Ashique Rupam Mahmood on the following paper: “Tuning-Free Step-Size Adaptation” [50]. 8.5. International Research Visitors 8.5.1. Visits of International Scientists • Andrew Barto, Reinforcement learning and intrinsic motivation, University of Massachusetts Amherst, USA (oct 2012) • Adam White, Reinforcement Learning and Art","",""
0,"A. Abraham, C. Grosan","ON SOFT COMPUTING FOR MODELING AND SIMULATION",1998,"","","","",15,"2022-07-13 09:20:03","","","","",,,,,0,0.00,0,2,24,"It is well known that the intelligent systems, which can provide human like expertise such as domain knowledge, uncertain reasoning, and adaptation to a noisy and time varying environment, are important in tackling practical computing problems. In contrast with conventional artificial intelligence techniques which only deal with precision, certainty and rigor the guiding principle of soft computing is to exploit the tolerance for imprecision, uncertainty, low solution cost, robustness, partial truth to achieve tractability, and better rapport with reality [Zadeh, 1998]. Soft computing is a consortium of technologies involving approximate reasoning, function approximation, learning capabilities, and a methodology for systematic random search and optimization. These capabilities are combined in a complementary and synergetic fashion. Soft computing has evolved not only from a theoretical point of view but also with a large variety of realistic applications to consumer products and industrial systems. Applications of soft computing have provided the opportunity to integrate humanlike vagueness and real-life uncertainty into an otherwise hard computer programs.","",""
1,"","The Future of Human Knowledge and Artificial Intelligence",2019,"","","","",16,"2022-07-13 09:20:03","","10.34190/km.19.129","","",,,,,1,0.33,0,0,3,"","",""
1,"Elizabeth Real de Oliveira, P. Rodrigues","A Review of Literature on Human Behaviour and Artificial Intelligence: Contributions Towards Knowledge Management",2021,"","","","",17,"2022-07-13 09:20:03","","10.34190/ejkm.19.2.2459","","",,,,,1,1.00,1,2,1,"The main purpose of this research paper is to understand how artificial intelligence and machine learning applied to human behaviour has been treated, both theoretically and empirically, over the last twenty years, regarding predictive analytics and human organizational behaviour analysis. To achieve this goal, the authors performed a systematic literature review, as proposed by Tranfield, Denyer and Smart (2003), on selected databases and followed the PRISMA framework (Preferred Reporting Items for Systematic reviews and Meta-Analyses). The method is particularly suited for assessing emerging trends within multiple disciplines and therefore deemed the most suitable method for the purposes of this paper, which intends to survey and select papers according to their contribute towards theory building. By mapping what is known, this review will lay the groundwork, providing a timely insight into the current state of research on human organisational behaviour and its applications. A total of 17795 papers resulted from the application of the search equations. The papers’ abstracts were screened according to the inclusion / exclusion criterions which resulted in 199 papers for analysis. The authors have analysed the papers through VOSviewer software and R programming statistical computing software. This review showed that 60% of the research undertaken in the field has been done in the last three and a half years and there is no prominent author or academic journal, showing the emergence and the novelty of this research. The other key finds of the research relate to the evolution of the concept, from data-driven (hard) towards emotions-driven (soft) organisations.","",""
0,"Beilei Wang, Jie Jing, Xiaochun Huang, Cheng Hua, Qin Qin, Y. Jia, Zhiyong Wang, Lei Jiang, Bai Gao, Les J. Wu, Xianfei Zeng, Fubo Wang, Chuanbin Mao, Shanrong Liu","Establishment of a Knowledge‐and‐Data‐Driven Artificial Intelligence System with Robustness and Interpretability in Laboratory Medicine",2022,"","","","",18,"2022-07-13 09:20:03","","10.1002/aisy.202100204","","",,,,,0,0.00,0,14,1,"Laboratory medicine plays an important role in clinical diagnosis. However, no laboratory‐based artificial intelligence (AI) diagnostic system has been applied in current clinical practice due to the lack of robustness and interpretability. Although many attempts have been made, it is still difficult for doctors to adopt the existing machine learning (ML) patterns in interpreting laboratory (lab) big data. Here, a knowledge‐and‐data‐driven laboratory diagnostic system is developed, termed AI‐based Lab tEst tO diagNosis (AI LEON), by integrating an innovative knowledge graph analysis framework and “mixed XGboost and Genetic Algorithm (MiXG)” technique to simulate the doctor's laboratory‐based diagnosis. To establish AI LEON, we included 89 116 949 laboratory data and 10 423 581 diagnosis data points from 730 113 participants. Among them, 686 626 participants were recruited for training and validating purposes with the remaining for testing purposes. AI LEON automatically identified and analyzed 2071 lab indexes, resulting in multiple disease recommendations that involved 441 common diseases in ten organ systems. AI LEON exhibited outstanding transparency and interpretability in three universal clinical application scenarios and outperformed human physicians in interpreting lab reports. AI LEON is an advanced intelligent system that enables a comprehensive interpretation of lab big data, which substantially improves the clinical diagnosis.","",""
5985,"David Silver, Julian Schrittwieser, K. Simonyan, Ioannis Antonoglou, Aja Huang, A. Guez, T. Hubert, Lucas baker, Matthew Lai, A. Bolton, Yutian Chen, T. Lillicrap, Fan Hui, L. Sifre, George van den Driessche, T. Graepel, D. Hassabis","Mastering the game of Go without human knowledge",2017,"","","","",19,"2022-07-13 09:20:03","","10.1038/nature24270","","",,,,,5985,1197.00,599,17,5,"","",""
11,"Michael Gofman, Zhao Jin","Artificial Intelligence, Human Capital, and Innovation",2020,"","","","",20,"2022-07-13 09:20:03","","10.2139/ssrn.3449440","","",,,,,11,5.50,6,2,2,"The scarcity of the human capital needed for R&D in Artificial Intelligence (AI) created an unprecedented brain drain of AI professors from universities in 2004-2018. We exploit this brain drain as a source of variation in students’ domain-specific knowledge and provide causal evidence that domain-specific knowledge is important for startup formation, seed funding, round A funding, and funding growth rate. The effect is the largest for top universities, PhD students, and startups in the area of deep learn- ing. These results contribute to the entrepreneurial finance literature and challenge the current view that entrepreneurs are jacks-of-all-trades, master of none (Lazear 2004).","",""
8,"M. Peters, P. Jandrić","Artificial Intelligence, Human Evolution, and the Speed of Learning",2019,"","","","",21,"2022-07-13 09:20:03","","10.1007/978-981-13-8161-4_12","","",,,,,8,2.67,4,2,3,"","",""
40,"B. Cope, M. Kalantzis, Duane Searsmith","Artificial intelligence for education: Knowledge and its assessment in AI-enabled learning ecologies",2020,"","","","",22,"2022-07-13 09:20:03","","10.1080/00131857.2020.1728732","","",,,,,40,20.00,13,3,2,"Abstract Over the past ten years, we have worked in a collaboration between educators and computer scientists at the University of Illinois to imagine futures for education in the context of what is loosely called “artificial intelligence.” Unhappy with the first generation of digital learning environments, our agenda has been to design alternatives and research their implementation. Our starting point has been to ask, what is the nature of machine intelligence, and what are its limits and potentials in education? This paper offers some tentative answers, first conceptually, and then practically in an overview of the results of a number of experimental implementations documented in greater detail elsewhere. Our key finding is that artificial intelligence—in the context of the practices of electronic computing developing over the past three quarters of a century—will never in any sense “take over” the role of teacher, because how it works and what it does are so profoundly different from human intelligence. However, within the limits that we describe in this paper, it offers the potential to transform education in ways that—counterintuitively perhaps—make education more human, not less.","",""
0,"A. D. W. Sumari, I. Syamsiana","A Simple Introduction to Cognitive Artificial Intelligence’s Knowledge Growing System",2021,"","","","",23,"2022-07-13 09:20:03","","10.1109/DATABIA53375.2021.9650179","","",,,,,0,0.00,0,2,1,"Knowledge Growing System (KGS) since its introduction in 2009, has been stated as the foundation of Cognitive Artificial Intelligence (CAI). Because of its computation mechanism simplicity and it does not burden the computation resources, various use-cases have applied KGS to solve their problems. The KGS development was inspired by the growing of knowledge within human brain when thinking during carrying out interactions to a phenomenon in its environment. KGS learns to the data received at that time and at the next series of time that are sensed and perceived by its sensory organs, and uses them to generate knowledge. By combining approaches and techniques from cognitive psychology, mathematics, social science, and AI fields, we created simple mathematics formulas called ASSA2010 (Arwin Sumari-Suwandi Ahmad year 2010) information-inferencing fusion method for KGS’ knowledge growing mechanism. In this article, we deliver a simple introduction to KGS and also some of its utilizations for humankind.","",""
0,"Canan Tiftik","Investigation of Human Resources Dimension in Management and Organization Structure of the Effects of Artificial Intelligence",2021,"","","","",24,"2022-07-13 09:20:03","","10.21733/IBAD.833256","","",,,,,0,0.00,0,1,1,"In the competitive time, there has been a great deal of progress in the industry. It is one of the most serious obstacles to the industry in many industries that adopt contemporary technologies to manage continuous development and faster than ordinary jobs. Many of the scientists and researchers recommend using AI tools and digital technologies for industries. Machine language and artificial intelligence are used by many organizations in the human resources unit, where it undertakes an integrated task in recruiting, performance analysis, personnel selection, data collection for employees, providing real-time information and obtaining the right information. Artificial intelligence-based Human Resources (HR) applications have a solid potential to increase employee productivity and support HR experts to become knowledge and trained consultants that increase the success of the employee. HR applications authorized by artificial intelligence have the ability to analyze, predict, diagnose and seek and find more robust and capable resources.","",""
1,"Noris Binti Mohd Norowi","Human-Centred Artificial Intelligence in Concatenative Sound Synthesis",2021,"","","","",25,"2022-07-13 09:20:03","","10.1007/978-3-030-72116-9_21","","",,,,,1,1.00,1,1,1,"","",""
13,"Francesca Iandolo, F. Loia, Irene Fulco, Chiara Nespoli, F. Caputo","Combining Big Data and Artificial Intelligence for Managing Collective Knowledge in Unpredictable Environment—Insights from the Chinese Case in Facing COVID-19",2020,"","","","",26,"2022-07-13 09:20:03","","10.1007/s13132-020-00703-8","","",,,,,13,6.50,3,5,2,"","",""
9,"Yanyan Dong, Jie Hou, Ning Zhang, Maocong Zhang","Research on How Human Intelligence, Consciousness, and Cognitive Computing Affect the Development of Artificial Intelligence",2020,"","","","",27,"2022-07-13 09:20:03","","10.1155/2020/1680845","","",,,,,9,4.50,2,4,2,"Artificial intelligence (AI) is essentially the simulation of human intelligence. Today’s AI can only simulate, replace, extend, or expand part of human intelligence. In the future, the research and development of cutting-edge technologies such as brain-computer interface (BCI) together with the development of the human brain will eventually usher in a strong AI era, when AI can simulate and replace human’s imagination, emotion, intuition, potential, tacit knowledge, and other kinds of personalized intelligence. Breakthroughs in algorithms represented by cognitive computing promote the continuous penetration of AI into fields such as education, commerce, and medical treatment to build up AI service space. As to human concern, namely, who controls whom between humankind and intelligent machines, the answer is that AI can only become a service provider for human beings, demonstrating the value rationality of following ethics.","",""
8,"Sheikh Rabiul Islam, W. Eberle, S. Ghafoor, A. Siraj, Mike Rogers","Domain Knowledge Aided Explainable Artificial Intelligence for Intrusion Detection and Response",2019,"","","","",28,"2022-07-13 09:20:03","","","","",,,,,8,2.67,2,5,3,"Artificial Intelligence (AI) has become an integral part of modern-day security solutions for its ability to learn very complex functions and handling ""Big Data"". However, the lack of explainability and interpretability of successful AI models is a key stumbling block when trust in a model's prediction is critical. This leads to human intervention, which in turn results in a delayed response or decision. While there have been major advancements in the speed and performance of AI-based intrusion detection systems, the response is still at human speed when it comes to explaining and interpreting a specific prediction or decision. In this work, we infuse popular domain knowledge (i.e., CIA principles) in our model for better explainability and validate the approach on a network intrusion detection test case. Our experimental results suggest that the infusion of domain knowledge provides better explainability as well as a faster decision or response. In addition, the infused domain knowledge generalizes the model to work well with unknown attacks, as well as opens the path to adapt to a large stream of network traffic from numerous IoT devices.","",""
3,"C. Kolski, G. Boy, G. Melançon, M. Ochs, J. Vanderdonckt","Cross-Fertilisation Between Human-Computer Interaction and Artificial Intelligence",2020,"","","","",29,"2022-07-13 09:20:03","","10.1007/978-3-030-06170-8_11","","",,,,,3,1.50,1,5,2,"","",""
0,"Patrick Lambe","AI: Artificial Intelligence or Autistic Intelligence? Keeping knowledge organisation human",2019,"","","","",30,"2022-07-13 09:20:03","","10.5771/9783956505508-255","","",,,,,0,0.00,0,1,3,"","",""
2,"Aidan Murphy, Gráinne Murphy, Jorge Amaral, D. M. Dias, Enrique Naredo, C. Ryan","Towards Incorporating Human Knowledge in Fuzzy Pattern Tree Evolution",2021,"","","","",31,"2022-07-13 09:20:03","","10.1007/978-3-030-72812-0_5","","",,,,,2,2.00,0,6,1,"","",""
41,"Anja Bechmann, G. Bowker","Unsupervised by any other name: Hidden layers of knowledge production in artificial intelligence on social media",2019,"","","","",32,"2022-07-13 09:20:03","","10.1177/2053951718819569","","",,,,,41,13.67,21,2,3,"Artificial Intelligence (AI) in the form of different machine learning models is applied to Big Data as a way to turn data into valuable knowledge. The rhetoric is that ensuing predictions work well—with a high degree of autonomy and automation. We argue that we need to analyze the process of applying machine learning in depth and highlight at what point human knowledge production takes place in seemingly autonomous work. This article reintroduces classification theory as an important framework for understanding such seemingly invisible knowledge production in the machine learning development and design processes. We suggest a framework for studying such classification closely tied to different steps in the work process and exemplify the framework on two experiments with machine learning applied to Facebook data from one of our labs. By doing so we demonstrate ways in which classification and potential discrimination take place in even seemingly unsupervised and autonomous models. Moving away from concepts of non-supervision and autonomy enable us to understand the underlying classificatory dispositifs in the work process and that this form of analysis constitutes a first step towards governance of artificial intelligence.","",""
94,"A. Zappone, M. Di Renzo, M. Debbah, T. T. Lam, Xuewen Qian","Model-Aided Wireless Artificial Intelligence: Embedding Expert Knowledge in Deep Neural Networks for Wireless System Optimization",2018,"","","","",33,"2022-07-13 09:20:03","","10.1109/MVT.2019.2921627","","",,,,,94,23.50,19,5,4,"Deep learning based on artificial neural networks (ANNs) is a powerful machine-learning method that, in recent years, has been successfully used to realize tasks such as image classification, speech recognition, and language translation, among others, that are usually simple for human beings but extremely difficult for machines. This is one reason deep learning is considered one of the main enablers for realizing artificial intelligence (AI). The current methodology in deep learning consists of employing a data-driven approach to identify the best architecture of an ANN that allows input-output data pairs to be fitted. Once the ANN is trained, it is capable of responding to never-observed inputs by providing the optimum output based on past acquired knowledge. In this context, a recent trend in the deep-learning community complements purely data-driven approaches with prior information based on expert knowledge. In this article, we describe two methods that implement this strategy to optimize wireless communication networks. In addition, we provide numerical results to assess the performance of the proposed approaches compared with purely data-driven implementations.","",""
66,"Fabio Massimo Zanzotto","Viewpoint: Human-in-the-loop Artificial Intelligence",2017,"","","","",34,"2022-07-13 09:20:03","","10.1613/jair.1.11345","","",,,,,66,13.20,66,1,5,"Little by little, newspapers are revealing the bright future that Artificial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future may have a possible dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers may need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Many learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, many of these workers are shooting themselves in the feet.  In this paper, we propose Human-in-the-loop Artificial Intelligence (HitAI) as a fairer paradigm for AI systems. Recognizing that any AI system has humans in the loop, HitAI will reward these aware and unaware knowledge producers with a different scheme: decisions of AI systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Merry Men, HitAI researchers should fight for a fairer Robin Hood Artificial Intelligence that gives back what it steals.      This article is part of the special track on AI and Society.    ","",""
7,"Fauzia Jabeen, Shehla Abbas Zaidi, Maryam Hamad Al Dhaheri","Automation and artificial intelligence in hospitality and tourism",2021,"","","","",35,"2022-07-13 09:20:03","","10.1108/tr-09-2019-0360","","",,,,,7,7.00,2,3,1," Purpose This study aims to develop a framework to identify and prioritize the key factors in automation and artificial intelligence (AI) implementation in the hospitality and tourism industry.   Design/Methodology/Approach This paper used the analytic hierarchy process, a multi-criteria decision-making method, to prioritize the factors influencing automation and AI implementation. This paper developed a model with five criteria (human knowledge, services, robotics applications, internal environment and institutional environment) and 23 sub-criteria obtained from previous studies. This paper designed a questionnaire in the form of pairwise comparisons based on the proposed hierarchical structure. This paper used a nine-point ranking scale to show the relative significance of each variable in the hierarchy and tested the model among staff from 35 five-star hotels and top-rated tourism agencies in the United Arab Emirates.   Findings Human knowledge, services and robotics applications were the most significant factors influencing automation and AI implementation. Practitioners and researchers in the hospitality and tourism industry could apply the proposed framework to develop sustainable strategies for implementing and managing automation and AI. The proposed framework may also be useful in future studies examining AI implementation in the hospitality and tourism industry.   Originality/Value This paper developed a framework for policymakers that identifies and could help to overcome some of the challenges in implementing automation and AI in the hospitality and tourism sector around the world. The results provide an agenda for future research in this area. ","",""
0,"R. Hariharan, P. He, C. Hickman, J. Chambost, C. Jacques, M. Hentschke, B. Cunegatto, C. Dutra, A. Drakeley, Q. Zhan, R. Miller, G. Verheyen, M. Rosselot, S. Loubersac, K. Kelley","P–165 Using Artificial Intelligence to Classify Embryo Shape: An International Perspective",2021,"","","","",36,"2022-07-13 09:20:03","","10.1093/humrep/deab130.164","","",,,,,0,0.00,0,15,1,"      Is a pre-trained machine learning algorithm able to accurately detect cellular arrangement in 4-cell embryos from a different continent?        Artificial Intelligence (AI) analysis of 4-cell embryo classification is transferable across clinics globally with 79% accuracy.        Previous studies observing four-cell human embryo configurations have demonstrated that non-tetrahedral embryos (embryos in which cells make contact with fewer than 3 other cells) are associated with compromised blastulation and implantation potential. Previous research by this study group has indicated the efficacy of AI models in classification of tetrahedral and non-tetrahedral embryos with 87% accuracy, with a database comprising 2 clinics both from the same country (Brazil). This study aims to evaluate the transferability and robustness of this model on blind test data from a different country (France).        The study was a retrospective cohort analysis in which 909 4-cell embryo images (“tetrahedral”, n = 749; “non-tetrahedral”, n = 160) were collected from 3 clinics (2 Brazilian, 1 French). All embryos were captured at the central focal plane using Embryoscope™ time-lapse incubators. The training data consisted solely of embryo images captured in Brazil (586 tetrahedral; 87 non-tetrahedral) and the test data consisted exclusively of embryo images captured in France (163 tetrahedral; 72 non-tetrahedral).        The embryo images were labelled as either “tetrahedral” or “non-tetrahedral” at their respective clinics. Annotations were then validated by three operators. A ResNet–50 neural network model pretrained on ImageNet was fine-tuned on the training dataset to predict the correct annotation for each image. We used the cross entropy loss function and the RMSprop optimiser (lr = 1e–5). Simple data augmentations (flips and rotations) were used during the training process to help counteract class imbalances.        Our model was capable of classifying embryos in the blind French test set with 79% accuracy when trained with the Brazilian data. The model had sensitivity of 91% and 51% for tetrahedral and non-tetrahedral embryos respectively; precision was 81% and 73%; F1 score was 86% and 60%; and AUC was 0.61 and 0.64. This represents a 10% decrease in accuracy compared to when the model both trained and tested on different data from the same clinics.        Although strict inclusion and exclusion criteria were used, inter-operator variability may affect the pre-processing stage of the algorithm. Moreover, as only one focal plane was used, ambiguous cases were interpoloated and further annotated. Analysing embryos at multiple focal planes may prove crucial in improving the accuracy of the model.  Wider implications of the findings: Though the use of machine learning models in the analysis of embryo imagery has grown in recent years, there has been concern over their robustness and transferability. While previous results have demonstrated the utility of locally-trained models, our results highlight the potential for models to be implemented across different clinics.        Not applicable ","",""
2,"Frank Guerin","Projection: A Mechanism for Human-like Reasoning in Artificial Intelligence",2021,"","","","",37,"2022-07-13 09:20:03","","10.1080/0952813x.2022.2078889","","",,,,,2,2.00,2,1,1,"Artiﬁcial Intelligence systems cannot yet match human abilities to apply knowledge to situations that vary from what they have been programmed for, or trained for. In visual object recognition, methods of inference exploiting top-down information (from a model) have been shown to be eﬀective for recognising entities in diﬃcult conditions. Here a component of this type of inference, called ‘projection’, is shown to be a key mechanism to solve the problem of applying knowledge to varied or challenging situations, across a range of AI domains, such as vision, robotics, or language. Finally the relevance of projection to tackling the commonsense knowledge problem is discussed.","",""
0,"Iván Manuel De la Vega Hernández, Angel Serrano Urdaneta, E. Carayannis","Global bibliometric mapping of the frontier of knowledge in the field of artificial intelligence for the period 1990–2019",2022,"","","","",38,"2022-07-13 09:20:03","","10.1007/s10462-022-10206-4","","",,,,,0,0.00,0,3,1,"","",""
0,"A. Chavez Badiola, A. Flores-Saiffe, R. Valencia, G. Mendizabal-Ruiz, J. Villavicencio, D. Gonzalez, D. Griffin, A. Drakeley, J. Cohen","P-241 ‘Augmented intelligence’ to possibly shorten euploid identification time: A human-machine interaction study for euploid identification using ERICA, an Artificial Intelligence software to assist embryo ranking",2022,"","","","",39,"2022-07-13 09:20:03","","10.1093/humrep/deac107.231","","",,,,,0,0.00,0,9,1,"      What is the mean number of transfers needed to achieve a euploid transfer selected by embryologists plus ERICA’s assistance?        Augmented intelligence (ERICA plus human collaboration) outperforms both the embryologists and artificial intelligence's individual performance alone.        Euploid embryos are more likely to implant successfully. Artificial intelligence (AI) could improve embryo selection over current techniques, but scepticism exists. Augmented intelligence (AuI) combines both the mathematical reproducibility of machine learning and the knowledge and experience of humans. This approach employs AI tools as an assistant, where the user shall learn to interpret the AI. A recent study suggested that embryologists assisted by AI improved the embryo selection of euploid transfers. ERICA (IVF2.0 Limited, UK) was designed to rank blastocysts according to their probability of euploidy.        We prospectively studied embryo selection for ERICA alone, embryologists only and when interacting (embryologists and ERICA) in 150 synthetically generated (reconstructed on real-data) embryo transfer cycles. Embryos were ranked in order, and performance was assessed by time to identify a euploid embryo within each cycle cohort correctly. Embryologists were allowed to rank a maximum of 10 cycles per day for three weeks starting in January 2022, using a mobile phone application designed for this purpose.        Using real-life cycle distributions of euploid/aneuploid blastocysts and the number of embryos in a cycle (according to ERICA’s database), we created 150 synthetic cycles, 30 for each age bracket (< 35, 35-37, 38-40, 41-42, and >42). These were randomly populated with blastocyst images preserving their actual ploidy status correspondingly. Each synthetic cycle contained between 2 to 6 authentic embryo images with at least one euploid and one aneuploid.        The total database had a euploid rate of 37.4% (n = 513), and by age brackets from 1 to 5 were 45.7% (n = 116), 43.8% (n = 105), 35.9% (n = 92), 31.2% (n = 96), and 28.8% (n = 104) respectively.  The mean number of cycles analysed by each participant was 113.5 (CI: 100.8-126.2). The mean time-to-euploid transfer for embryologists alone was 2.07 (CI:2.00-2.13); for the ERICA alone was 1.86 (CI:1.82-1.91); and for embryologists assisted by ERICA was 1.62 (CI:1.55-1.68). All study groups compared to each other were statistically significant using a paired two-tailed student’s t-test (p < 0.001).  The proportion of euploid transfer at the first try for embryologists alone was 0.40 (CI:0.37-0.43), for ERICA alone was 0.54 (CI:0.53-0.54), and for embryologists assisted by ERICA was 0.47 (CI:0.44-0.50). All study groups compared with each other were statistically significant with a paired two-tailed student’s t-test (p < 0.01).        Although our findings suggest that Aul outperforms both AI and humans alone, this study needs to be replicated with a larger cohort of embryologists with different experience levels in different countries to confirm these results.        Combining machine-human interaction through a well-designed process could improve embryo selection and reduce inter-operator variability amongst staff with different experience levels. It could also set a frame for adequate agency and accountability, and enhance trust and adoption.        NA ","",""
1,"C. Bormann, M. Kanakasabapathy, Prudhvi Thirumalaraju, I. Dimitriadis, I. Souter, K. Hammer, H. Shafiee","O-125 Development of an artificial intelligence embryo witnessing system to accurately track and identify patient specific embryos in a human IVF laboratory",2021,"","","","",40,"2022-07-13 09:20:03","","10.1093/humrep/deab126.050","","",,,,,1,1.00,0,7,1,"      Can convolutional neural networks (CNN) be used as a witnessing system to accurately track and identify patient specific embryos at the cleavage stage of development?        We developed the first artificial intelligence driven witnessing system to accurately track cleavage and blastocyst stage embryos in a human ART laboratory.        There are reports of human errors in embryo tracking that have led to the births of children with different genetic makeup than their birth parents. Clinical practices rely on manual identification, barcodes or radio-frequency identification technology to track embryos. These systems are designed to track culture dishes but are unable to monitor developing embryos within the dish to help ensure an error-free patient match. Previously, we developed an AI witnessing system to track blastocysts with 100% accuracy. The goal of this study was to determine whether an AI witnessing system could be developed that accurately tracks cleavage stage embryos.        A pre-developed deep neural network technology was first trained and tested on 4944 embryos images. The algorithm processed embryo images for each patient and produced a unique key that was associated with the patient ID at 60 hpi, which formed our library. When the algorithm evaluated embryos at 64 hpi it generated another key that was matched with the patient’s unique key available in the library.        A total of 3068 embryos from 412 patients were examined by the CNN at both 60 hpi and 64 hpi. These timepoints were chosen as they reflect the time our laboratory evaluates Day 3 embryos (60 hpi) and the time we move them to another dish and prepare them for transfer (64 hpi). The patient cohorts ranged from 3-12 embryos per patient.        The accuracy of the CNN in correctly matching the patient identification with the patient embryo cohort was 100% (CI: 99.1% to 100.0%, n = 412).        Limitations of this study include that all embryos were imaged under identical conditions and within the same EmbryoScope. Additionally, this study only examined fresh Day 3 embryos cultured over a span of 4 hours. Future studies should include images of fresh and frozen/thawed embryos captured using different imaging systems.        This study describes the first artificial intelligence-based approach for cleavage stage embryo tracking and patient specimen identification in the IVF laboratory. This technology offers a robust witnessing step based on unique morphological features that are specific to each individual embryo.        This work was partially supported by the Brigham Precision Medicine Developmental Award (Brigham Precision Medicine Program, Brigham and Women’s Hospital), Partners Innovation Discovery Grant (Partners Healthcare), and R01AI118502, and R01AI138800. ","",""
23,"M. Mitchell","Abstraction and analogy‐making in artificial intelligence",2021,"","","","",41,"2022-07-13 09:20:03","","10.1111/nyas.14619","","",,,,,23,23.00,23,1,1,"Conceptual abstraction and analogy‐making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.","",""
3,"R. Kreutzer, Marie Sirrenberg","Fields of Application of Artificial Intelligence—Health Care, Education and Human Resource Management",2019,"","","","",42,"2022-07-13 09:20:03","","10.1007/978-3-030-25271-7_6","","",,,,,3,1.00,2,2,3,"","",""
34,"C. Thongprayoon, W. Kaewput, K. Kovvuru, P. Hansrivijit, S. Kanduri, T. Bathini, A. Chewcharat, N. Leeaphorn, M. Gonzalez-Suarez, W. Cheungpasitporn","Promises of Big Data and Artificial Intelligence in Nephrology and Transplantation",2020,"","","","",43,"2022-07-13 09:20:03","","10.3390/jcm9041107","","",,,,,34,17.00,3,10,2,"Kidney diseases form part of the major health burdens experienced all over the world. Kidney diseases are linked to high economic burden, deaths, and morbidity rates. The great importance of collecting a large quantity of health-related data among human cohorts, what scholars refer to as “big data”, has increasingly been identified, with the establishment of a large group of cohorts and the usage of electronic health records (EHRs) in nephrology and transplantation. These data are valuable, and can potentially be utilized by researchers to advance knowledge in the field. Furthermore, progress in big data is stimulating the flourishing of artificial intelligence (AI), which is an excellent tool for handling, and subsequently processing, a great amount of data and may be applied to highlight more information on the effectiveness of medicine in kidney-related complications for the purpose of more precise phenotype and outcome prediction. In this article, we discuss the advances and challenges in big data, the use of EHRs and AI, with great emphasis on the usage of nephrology and transplantation.","",""
31,"I. Habli, T. Lawton, Zoe Porter","Artificial intelligence in health care: accountability and safety",2020,"","","","",44,"2022-07-13 09:20:03","","10.2471/BLT.19.237487","","",,,,,31,15.50,10,3,2,"Abstract The prospect of patient harm caused by the decisions made by an artificial intelligence-based clinical tool is something to which current practices of accountability and safety worldwide have not yet adjusted. We focus on two aspects of clinical artificial intelligence used for decision-making: moral accountability for harm to patients; and safety assurance to protect patients against such harm. Artificial intelligence-based tools are challenging the standard clinical practices of assigning blame and assuring safety. Human clinicians and safety engineers have weaker control over the decisions reached by artificial intelligence systems and less knowledge and understanding of precisely how the artificial intelligence systems reach their decisions. We illustrate this analysis by applying it to an example of an artificial intelligence-based system developed for use in the treatment of sepsis. The paper ends with practical suggestions for ways forward to mitigate these concerns. We argue for a need to include artificial intelligence developers and systems safety engineers in our assessments of moral accountability for patient harm. Meanwhile, none of the actors in the model robustly fulfil the traditional conditions of moral accountability for the decisions of an artificial intelligence system. We should therefore update our conceptions of moral accountability in this context. We also need to move from a static to a dynamic model of assurance, accepting that considerations of safety are not fully resolvable during the design of the artificial intelligence system before the system has been deployed.","",""
32,"W. Villegas-Ch., Adrián Arias-Navarrete, Xavier Palacios-Pacheco","Proposal of an Architecture for the Integration of a Chatbot with Artificial Intelligence in a Smart Campus for the Improvement of Learning",2020,"","","","",45,"2022-07-13 09:20:03","","10.3390/su12041500","","",,,,,32,16.00,11,3,2,"Traditional teaching based on masterclasses or techniques where the student develops a passive role has proven to be inefficient methods in the learning process. The use of technology in universities helps to generate active learning where the student’s interest improves making him the main actor in his education. However, implementing an environment where active learning takes place requires a great deal of effort given the number of variables involved in this objective. To identify these variables, it is necessary to analyze the data generated by the students in search of patterns that allow them to be classified according to their needs. Once these needs are identified, it is possible to make decisions that contribute to the learning of each student; for this, the use of artificial intelligence is considered. These techniques emulate the processes of human thought using structures that contain knowledge and experience of human experts.","",""
1,"Lana Sinapayen","Perspective: Purposeful Failure in Artificial Life and Artificial Intelligence",2021,"","","","",46,"2022-07-13 09:20:03","","","","",,,,,1,1.00,1,1,1,"Complex systems fail. I argue that failures can be a blueprint characterizing living organisms and biological intelligence, a control mechanism to increase complexity in evolutionary simulations, and an alternative to classical fitness optimization. Imitating biological successes in Artificial Life and Artificial Intelligence can be misleading; imitating failures offers a path towards understanding and emulating life it in artificial systems. Failure is Knowledge, Knowledge is Power You are handed a mysterious box containing the most complex object in the universe, and must find how the object works. Where do you start? “The human brain is the most complex object in the universe” is a well worn cliche (Constable (1918)). While the claim might not be true, the human brain is definitely very complex. In neuroscience and psychology, one of the most compelling ways to understand how the brain works is to study how it fails. Brain damage, irrational decisions, sensory illusions: internal or external changes that make the brain fail are how we find how the brain succeeds. Failure is used to understand complex systems beyond neuroscience: reverse-engineering computer software, understanding animal behavior, identifying solid materials... Failure even defines Science itself. an hypothesis is considered scientific if and only if it is “falsifiable”: if it can reproducibly fail (Popper (1934)). Why default to observing failures when we don’t know what is going on? Because the success-failure boundary is full of information. Let me define “failure” in the context of this discussion. Imagine being an ant dropped somewhere on top of Fig.1-(a). What is the fastest way to map your surroundings? Rather than walking every inch of the surface, find boundaries. When you are investigating a complex system that is working as expected, you are an ant dropped on Mount Success. To find the boundary, you have to push the system into failure mode. Staying inside the success space can inform you about the robustness of the system to perturbations (at best the system recovered from the perturbation, at worst your perturbation was irrelevant), but it is not explanatory. Neither is going from failure to failure. You can only investigate causes and effects if your intervention actually changes something: the failure boundary is not just more informative, it is a different kind of information altogether. Boundaries and failures are not exactly the same. If you are observing a function of the system that does not change when it crosses the failure boundary, you will not notice the transition. If you are observing the right function, you might see the system performance on that function become better or worse. Let us call “failure points” abrupt transitions from “some performance” to 0: they are the most salient of transitions. Going back to Fig.1, the ant might not notice the transition from a gentle slope to a flat terrain, but a cliff will be noticeable. Ideally, you would want to map the entire failure boundary; in practice, you will focus on failure points. ? Mount Success Failure Bog","",""
1,"Y. Sheng, Jiahan Zhang, Y. Ge, Xinyi Li, Wentao Wang, H. Stephens, F. Yin, Qiuwen Wu, Q. Wu","Artificial intelligence applications in intensity modulated radiation treatment planning: an overview.",2021,"","","","",47,"2022-07-13 09:20:03","","10.21037/qims-21-208","","",,,,,1,1.00,0,9,1,"Artificial intelligence (AI) refers to methods that improve and automate challenging human tasks by systematically capturing and applying relevant knowledge in these tasks. Over the past decades, a number of approaches have been developed to address different types and needs of system intelligence ranging from search strategies to knowledge representation and inference to robotic planning. In the context of radiation treatment planning, multiple AI approaches may be adopted to improve the planning quality and efficiency. For example, knowledge representation and inference methods may improve dose prescription by integrating and reasoning about the domain knowledge described in many clinical guidelines and clinical trials reports. In this review, we will focus on the most studied AI approach in intensity modulated radiation therapy (IMRT)/volumetric modulated arc therapy (VMAT)-machine learning (ML) and describe our recent efforts in applying ML to improve the quality, consistency, and efficiency of IMRT/VMAT planning. With the available high-quality data, we can build models to accurately predict critical variables for each step of the planning process and thus automate and improve its outcomes. Specific to the IMRT/VMAT planning process, we can build models for each of the four critical components in the process: dose-volume histogram (DVH), Dose, Fluence, and Human Planner. These models can be divided into two general groups. The first group focuses on encoding prior experience and knowledge through ML and more recently deep learning (DL) from prior clinical plans and using these models to predict the optimal DVH (DVH prediction model), or 3D dose distribution (dose prediction model), or fluence map (fluence map model). The goal of these models is to reduce or remove the trial-and-error process and guarantee consistently high-quality plans. The second group of models focuses on mimicking human planners' decision-making process (planning strategy model) during the iterative adjustments/guidance of the optimization engine. Each critical step of the IMRT/VMAT treatment planning process can be improved and automated by AI methods. As more training data becomes available and more sophisticated models are developed, we can expect that the AI methods in treatment planning will continue to improve accuracy, efficiency, and robustness.","",""
1,"Pranav Gupta, A. Woolley","Articulating the Role of Artificial Intelligence in Collective Intelligence: A Transactive Systems Framework",2021,"","","","",48,"2022-07-13 09:20:03","","10.1177/1071181321651354c","","",,,,,1,1.00,1,2,1,"Human society faces increasingly complex problems that require coordinated collective action. Artificial intelligence (AI) holds the potential to bring together the knowledge and associated action needed to find solutions at scale. In order to unleash the potential of human and AI systems, we need to understand the core functions of collective intelligence. To this end, we describe a socio-cognitive architecture that conceptualizes how boundedly rational individuals coordinate their cognitive resources and diverse goals to accomplish joint action. Our transactive systems framework articulates the inter-member processes underlying the emergence of collective memory, attention, and reasoning, which are fundamental to intelligence in any system. Much like the cognitive architectures that have guided the development of artificial intelligence, our transactive systems framework holds the potential to be formalized in computational terms to deepen our understanding of collective intelligence and pinpoint roles that AI can play in enhancing it.","",""
2,"Davis K. Cope","Music, Artificial Intelligence and Neuroscience",2021,"","","","",49,"2022-07-13 09:20:03","","10.1007/978-3-030-72116-9_7","","",,,,,2,2.00,2,1,1,"","",""
1,"Alaa Amjed Abdulateef, Alaa Hamid Mohammed, Ihsan Amjad Abdulateef","The Avoidance And Detection Function Of Artificial Intelligence In Covid-19",2021,"","","","",50,"2022-07-13 09:20:03","","10.1109/HORA52670.2021.9461280","","",,,,,1,1.00,0,3,1,"The whole planet today is having to fight COVID-19 with big obstacles. The COVID-19 influenced several countries around the world between December 2019 and the present day. Many organisations and scientists seek to find a vaccine and to minimize the spread of COVID-19. Artificial Intelligence is one technology that can successfully address this virus (AI). In the case of other pathogens, artificial intelligence performed very well and could help us cope with the virus COVID-19, too. It is the imagination and the information of the people who use it which will help to overcome this dilemma. In some previous instances AI played a major role in virus prevention and identification. We have an ability to detect certain aspects of the AI because of the COVID-19 crisis. Machine learning that an AI subclass is used to identify patterns and to plan valuable knowledge based on recorded data sets. At the point where used entirely, AI can exceed human efforts by speed and differentiate designs from knowledge previously ignored. However many correct and appropriate data are needed for effective implementation of AI systems. This paper discusses the AI's role in COVID-19 prevention and detection and examines numerous technological aspects of AI. This paper would also clarify where AI will contribute with likely solutions to stop the spread of COVID-19.","",""
3,"Edirlei Soares de Lima, B. Feijó","Artificial Intelligence in Human-Robot Interaction",2019,"","","","",51,"2022-07-13 09:20:03","","10.1007/978-3-319-96722-6_11","","",,,,,3,1.00,2,2,3,"","",""
0,"Junxi Chen, Xiying Zhan, Yuping Wang, Xuping Huang","Medical Robots based on Artificial Intelligence in the Medical Education",2021,"","","","",52,"2022-07-13 09:20:03","","10.1109/ICAIE53562.2021.00008","","",,,,,0,0.00,0,4,1,"With the rapid development of artificial intelligence, more and more scientific and technological achievements in the information age have been widely used in various fields of society. At the same time, the rapid development of intelligence has also continuously promoted the progress of medical education, and more and more robots have been applied to the field of medical education. The main purpose of this research is to analyze the current situation of using artificial intelligence-based medical robots in medical education to determine its effect on improving medical education. We have proposed a method for using medical robots based on artificial intelligence technology to help medical students conduct more efficient skills training and professional knowledge acquisition. Medical education based on AI use the robots to act as a human body model, perform scene simulation, act as a ""medical encyclopedia"", perform pharmacological simulation reactions, and finally automatically evaluate students’ medical knowledge mastery level. All in all, integrating various artificial intelligence technologies into medical education, aroused the interest of medical students in learning, and greatly improved the results of teaching.","",""
0,"V. Maphosa, M. Maphosa","The Trajectory of Artificial Intelligence Research in Higher Education: A Bibliometric Analysis and Visualisation",2021,"","","","",53,"2022-07-13 09:20:03","","10.1109/icABCD51485.2021.9519368","","",,,,,0,0.00,0,2,1,"The rapid development of artificial intelligence (AI) and its applications is gaining global attention and promises to revolutionise every aspect of human life, including education. AI will transform higher education (HE) institutions through improved adaptation and competitiveness. The application of AI in HE is an emerging research area with limited review. Our paper seeks to provide a comprehensive bibliometric analysis and visualisation of research on AI application in HE in the past two decades. We evaluated 283 articles published by researchers from 59 countries in the Scopus database over the past two decades based on explicit inclusion and exclusion criteria. The study applied various bibliometric indicators and word analysis to examine emergent trends. VOSviewer was used for visualisations to map a knowledge base by uncovering keywords used within AI in HE. The results show the number of AI articles published per year, their geographic distribution, analysis by subject area and keywords, and research trends. Research in AI is interdisciplinary, dominated by computer science and engineering fields. AI research in HE is growing, the first 15 years contributed 22%, and the last five years yielded 78%. Countries with a high investment in research dominated AI research, with China and the United States leading. There is very little research from developing countries. Our paper highlights current and future research directions in AI in education, and its limitations.","",""
0,"Ewa Szewczyk","Artificial intelligence in administrative law and procedure",2021,"","","","",54,"2022-07-13 09:20:03","","10.13166/wsge//krcl6757","","",,,,,0,0.00,0,1,1,"Introduction Since J. McCarthy used the term ‘artificial intelligence’ (AI) in the 1950s, it has become a key concept in the technological development of all mankind. It has appeared in every area of life and science. AI has become established in areas of life that were previously thought to be reserved for decision-making by human beings. Artificial intelligence is based on the analysis of large volumes of data, used in algorithms. According to the modern definition, artificial intelligence encompasses the area of knowledge that includes fuzzy logic, evolutionary computation, neural networks, artificial life, and robotics, and one of its essential features is the ability to learn1 and take into account new circumstances when solving a given problem2. In other words, artificial intelligence is the ability of a machine to mimic or imitate human intelligence3. Algorithms are nothing new. They have been used in computer programmes for decades. Today, however, advanced algorithms have become digital robots – often sophisticated computer programmes (rather than physical entities) with the ability to adapt and ‘learn’. However, there is no denying that the unhindered development of AI technologies is marred with public concern and is by no means universally embraced, even though the Covid-19 pandemic has","",""
0,"Qingchu Jiang, Tianmei Mao","Research on Future Education Development under the trend of Information Technology and Artificial Intelligence in the Sixth Scientific and Technological Revolution",2021,"","","","",55,"2022-07-13 09:20:03","","10.1109/ICAIE53562.2021.00131","","",,,,,0,0.00,0,2,1,"With the rapid rise of big data, cloud computing, artificial intelligence and other emerging technologies and their wide application in various fields of society, the Sixth Scientific and Technological Revolution is quietly infiltrating human habitat, which Includes the field of education. At present, China's education development is also facing the challenges of education reform. Based on the influence of technological revolution on the future, this paper investigates the development of educational technology for China's educational reform. Our education should change the teaching form through multimedia and online classroom tools, and expand the new teaching content under the analysis of Artificial Intelligence (AI). On the one hand, informatization, internationalization and knowledge economy bring severe external challenges to education. On the other hand, China's education is facing such problems as educational injustice, lack of innovation and lack of lifelong education system. Traditional educational concepts, models and methods are no longer able to adapt to the new development situation, and there is an urgent need to promote systematic changes in education as a whole. Based on the future impact of the Sixth Scientific and Technological Revolution, this study aims to clarify the scientific trend and law of education in the future. In view of these problems, this paper puts forward concrete analysis and countermeasures for China's educational reform in combination with the technological development in the field of education.","",""
0,"Jingjing Li, Hao Wang","Application of Artificial Intelligence in Libraries",2021,"","","","",56,"2022-07-13 09:20:03","","10.1109/AIAM54119.2021.00072","","",,,,,0,0.00,0,2,1,"Artificial intelligence (AI) has shown great promise for applications with large breakthroughs in pattern recognition, knowledge mapping, computer vision, and human-machine interaction. In this paper, firstly, the key technical features of AI in library application are studied, and its application in library stack building, stack management and information retrieval service is analyzed; then the existing problems of artificial intelligence in library application are analyzed; and finally the corresponding application service countermeasures are put forward, which can provide reference for related researchers and staff.","",""
111,"Zhihan Lv, Yang Han, A. Singh, Gunasekaran Manogaran, Haibin Lv","Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",2021,"","","","",57,"2022-07-13 09:20:03","","10.1109/TII.2020.2994747","","",,,,,111,111.00,22,5,1,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","",""
23,"S. Jamil","Artificial Intelligence and Journalistic Practice: The Crossroads of Obstacles and Opportunities for the Pakistani Journalists",2020,"","","","",58,"2022-07-13 09:20:03","","10.1080/17512786.2020.1788412","","",,,,,23,11.50,23,1,2,"ABSTRACT In the past two decades, technological advancement has reshaped the news media landscape in many developed and developing countries of the world. Journalistic practice, especially in large-scale economies, is experiencing transformations due to the introduction of automatic artificial intelligence processes into different aspects of news production and dissemination. Now devices and machines are taking on the role of communicator, replacing journalists as communicators. It is important to analyse whether journalism in low income countries has begun to feeling the ripple of artificial intelligence for technologically advanced journalistic practices and newsrooms. Using the Human-Machine Communication framework, this study advances knowledge about what AI can do to transform journalism practice in low-income countries, which is vital to compare the implications of AI in other similar socioeconomic contexts and news media ecologies. Thus, this study specifically reflects upon the case of Pakistan. It investigates how the Pakistani journalists perceive AI technology in the role of communicator and how they view the human-machine communication process. This study identifies constraints and explores opportunities for the use of artificial intelligence in Pakistan’s mainstream news media. To address these aims, this study uses the qualitative method of in-depth interviews, and presents the findings using thematic analysis.","",""
0,"Chengbing Tan, Qun Chen","Application of an artificial intelligence algorithm model of memory retrieval and roaming in sorting Chinese medicinal materials",2021,"","","","",59,"2022-07-13 09:20:03","","10.3233/jcm-215477","","",,,,,0,0.00,0,2,1,"In order to capture autobiographical memory, inspired by the development of human intelligence, a computational AM model for autobiographical memory is proposed in this paper, which is a three-layer network structure, in which the bottom layer encodes the event-specific knowledge comprising 5W1H, and provides retrieval clues to the middle layer, encodes the related events, and the top layer encodes the event set. According to the bottom-up memory search process, the corresponding events and event sets can be identified in the middle layer and the top layer respectively; At the same time, AM model can simulate human memory roaming through the process of rule-based memory retrieval. The computational AM model proposed in this paper not only has robust and flexible memory retrieval, but also has better response performance to noisy memory retrieval cues than the commonly used memory retrieval model based on keyword query method, and can also imitate the roaming phenomenon in memory.","",""
15,"F. Pesapane, P. Tantrige, F. Patella, P. Biondetti, L. Nicosia, A. Ianniello, U. Rossi, G. Carrafiello, A. Ierardi","Myths and facts about artificial intelligence: why machine- and deep-learning will not replace interventional radiologists",2020,"","","","",60,"2022-07-13 09:20:03","","10.1007/s12032-020-01368-8","","",,,,,15,7.50,2,9,2,"","",""
61,"L. Metcalf, David A. Askay, Louis B. Rosenberg","Keeping Humans in the Loop: Pooling Knowledge through Artificial Swarm Intelligence to Improve Business Decision Making",2019,"","","","",61,"2022-07-13 09:20:03","","10.1177/0008125619862256","","",,,,,61,20.33,20,3,3,"This article explores how a collaboration technology called Artificial Swarm Intelligence (ASI) addresses the limitations associated with group decision making, amplifies the intelligence of human groups, and facilitates better business decisions. It demonstrates of how ASI has been used by businesses to harness the diverse perspectives that individual participants bring to groups and to facilitate convergence upon decisions. It advances the understanding of how artificial intelligence (AI) can be used to enhance, rather than replace, teams as they collaborate to make business decisions.","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",62,"2022-07-13 09:20:03","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
39,"R. Confalonieri, Ludovik Çoba, Benedikt Wagner, Tarek R. Besold","A historical perspective of explainable Artificial Intelligence",2020,"","","","",63,"2022-07-13 09:20:03","","10.1002/widm.1391","","",,,,,39,19.50,10,4,2,"Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the “how” and “why” of automated decision‐making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge‐based) expert systems. Since then, the definition, understanding, and implementation of explainability have been picked up in several lines of research work, namely, expert systems, machine learning, recommender systems, and in approaches to neural‐symbolic learning and reasoning, mostly happening during different periods of AI history. In this article, we present a historical perspective of Explainable Artificial Intelligence. We discuss how explainability was mainly conceived in the past, how it is understood in the present and, how it might be understood in the future. We conclude the article by proposing criteria for explanations that we believe will play a crucial role in the development of human‐understandable explainable systems.","",""
10,"T. Guo","Alan Turing: Artificial intelligence as human self‐knowledge",2015,"","","","",64,"2022-07-13 09:20:03","","10.1111/1467-8322.12209","","",,,,,10,1.43,10,1,7,"This article explores the life and ideas of Alan Turing (1912–1954), commonly known as the father of artificial intelligence (AI), and highlights the process whereby the human self is reconceptualized in the development of Turing's ideas of machine intelligence. I will further illustrate how this process of self-reconceptualization – composed of the pursuit, adaptation and transformation of self-knowledge – is closely related to contemporary digital life. In doing so, I wish to reveal the ways in which Turing's underlying self-transforming agenda of AI can contribute to our understanding of the human self, for AI, as I will argue, leads to questions of existence and existential anxieties.","",""
45,"H. Abbass","Social Integration of Artificial Intelligence: Functions, Automation Allocation Logic and Human-Autonomy Trust",2019,"","","","",65,"2022-07-13 09:20:03","","10.1007/s12559-018-9619-0","","",,,,,45,15.00,45,1,3,"","",""
0,"C. Saraswathy, S. Sarumathi","Analysis of Thief Identification Techniques using Artificial Intelligence",2021,"","","","",66,"2022-07-13 09:20:03","","10.46610/JOIPAI.2021.V07I02.005","","",,,,,0,0.00,0,2,1,"Theft is a major cause of violence around the world. Several valuable things are stolen as a result of security issues in the workplace, bank, and home. In previous years, a variety of techniques were used to reduce the risks. Two of these methods are burglar alarms and CCTV recording. However, these methods are inaccurate and disorganized due to a lack of human attention in such processes. Since such a system requires human maintenance to control the data collected by the camera, it presents a greater problem for shop or home owners. The that rate of crime causes people to struggle both financially and emotionally. As a result, there is a need to prevent theft and develop a security system. It has to be simple to use, free of false alarms, human interference-free, and cost-effective. The primary goal of this paper is to serve as both a concise overview and a reference by providing primary knowledge of various techniques used as well as various research opportunities in this field. A variety of methods for identifying the thief using artificial intelligence based on the face and behaviour recognition are demonstrated in this survey.","",""
0,"F. LeRon Shults","Progress in simulating human geography: Assemblage theory and the practice of multi-agent artificial intelligence modeling",2021,"","","","",67,"2022-07-13 09:20:03","","10.1177/03091325211059567","","",,,,,0,0.00,0,1,1,"Over the last few years, there has been an explosion of interest in assemblage theory among human geographers. During this same period, a growing number of scholars in the field have utilized computational methodologies to simulate the complex adaptive systems they study. However, very little attention has been paid to the connections between these two developments. This article outlines those connections and argues that more explicitly integrating assemblage theory and computer modeling can encourage a more robust philosophical understanding of both and facilitate progress in scientific research on the ways in which complex socio-material systems form and transform.","",""
22,"Shengze Cai, He Li, Fuyin Zheng, Fang Kong, M. Dao, G. Karniadakis, S. Suresh","Artificial intelligence velocimetry and microaneurysm-on-a-chip for three-dimensional analysis of blood flow in physiology and disease",2021,"","","","",68,"2022-07-13 09:20:03","","10.1073/pnas.2100697118","","",,,,,22,22.00,3,7,1,"Significance Microfluidics is an important in vitro platform to gain insights into mechanics of blood flow and mechanisms of pathophysiology of human diseases. Extraction of 3D fields in microfluidics with dense cell suspensions remains a formidable challenge. We present artificial-intelligence velocimetry (AIV) as a general platform to determine 3D flow fields and a microaneurysm-on-a-chip to simulate blood flow in microaneurysms in patients with diabetic retinopathy. AIV is built on physics-informed neural networks that integrate seamlessly 2D images from microfluidic experiments or in vivo observations with physical laws to estimate full 3D velocity and stress fields. AIV could be integrated into imaging technologies to automatically infer key hemodynamic metrics from in vivo and in vitro biomedical images. Understanding the mechanics of blood flow is necessary for developing insights into mechanisms of physiology and vascular diseases in microcirculation. Given the limitations of technologies available for assessing in vivo flow fields, in vitro methods based on traditional microfluidic platforms have been developed to mimic physiological conditions. However, existing methods lack the capability to provide accurate assessment of these flow fields, particularly in vessels with complex geometries. Conventional approaches to quantify flow fields rely either on analyzing only visual images or on enforcing underlying physics without considering visualization data, which could compromise accuracy of predictions. Here, we present artificial-intelligence velocimetry (AIV) to quantify velocity and stress fields of blood flow by integrating the imaging data with underlying physics using physics-informed neural networks. We demonstrate the capability of AIV by quantifying hemodynamics in microchannels designed to mimic saccular-shaped microaneurysms (microaneurysm-on-a-chip, or MAOAC), which signify common manifestations of diabetic retinopathy, a leading cause of vision loss from blood-vessel damage in the retina in diabetic patients. We show that AIV can, without any a priori knowledge of the inlet and outlet boundary conditions, infer the two-dimensional (2D) flow fields from a sequence of 2D images of blood flow in MAOAC, but also can infer three-dimensional (3D) flow fields using only 2D images, thanks to the encoded physics laws. AIV provides a unique paradigm that seamlessly integrates images, experimental data, and underlying physics using neural networks to automatically analyze experimental data and infer key hemodynamic indicators that assess vascular injury.","",""
24,"Maxime Sermesant, H. Delingette, H. Cochet, P. Jaïs, N. Ayache","Applications of artificial intelligence in cardiovascular imaging",2021,"","","","",69,"2022-07-13 09:20:03","","10.1038/s41569-021-00527-2","","",,,,,24,24.00,5,5,1,"","",""
24,"Muhammad Javed Iqbal, Z. Javed, H. Sadia, I. Qureshi, Asma Irshad, R. Ahmed, K. Malik, S. Raza, A. Abbas, R. Pezzani, J. Sharifi‐Rad","Clinical applications of artificial intelligence and machine learning in cancer diagnosis: looking into the future",2021,"","","","",70,"2022-07-13 09:20:03","","10.1186/s12935-021-01981-1","","",,,,,24,24.00,2,11,1,"","",""
58,"Dominik Dellermann, A. Calma, Nikolaus Lipusch, Thorsten Weber, Sascha Weigel, P. Ebel","The Future of Human-AI Collaboration: A Taxonomy of Design Knowledge for Hybrid Intelligence Systems",2019,"","","","",71,"2022-07-13 09:20:03","","10.24251/HICSS.2019.034","","",,,,,58,19.33,10,6,3,"Recent technological advances, especially in the field of machine learning, provide astonishing progress on the road towards artificial general intelligence. However, tasks in current real-world business applications cannot yet be solved by machines alone. We, therefore, identify the need for developing socio-technological ensembles of humans and machines. Such systems possess the ability to accomplish complex goals by combining human and artificial intelligence to collectively achieve superior results and continuously improve by learning from each other. Thus, the need for structured design knowledge for those systems arises. Following a taxonomy development method, this article provides three main contributions: First, we present a structured overview of interdisciplinary research on the role of humans in the machine learning pipeline. Second, we envision hybrid intelligence systems and conceptualize the relevant dimensions for system design for the first time. Finally, we offer useful guidance for system developers during the implementation of such applications.","",""
19,"B. Verheij","Artificial intelligence as law",2020,"","","","",72,"2022-07-13 09:20:03","","10.1007/s10506-020-09266-0","","",,,,,19,9.50,19,1,2,"","",""
11,"Atte Oksanen, N. Savela, Rita Latikka, Aki Koivula","Trust Toward Robots and Artificial Intelligence: An Experimental Approach to Human–Technology Interactions Online",2020,"","","","",73,"2022-07-13 09:20:03","","10.3389/fpsyg.2020.568256","","",,,,,11,5.50,3,4,2,"Robotization and artificial intelligence (AI) are expected to change societies profoundly. Trust is an important factor of human–technology interactions, as robots and AI increasingly contribute to tasks previously handled by humans. Currently, there is a need for studies investigating trust toward AI and robots, especially in first-encounter meetings. This article reports findings from a study investigating trust toward robots and AI in an online trust game experiment. The trust game manipulated the hypothetical opponents that were described as either AI or robots. These were compared with control group opponents using only a human name or a nickname. Participants (N = 1077) lived in the United States. Describing opponents with robots or AI did not impact participants’ trust toward them. The robot called jdrx894 was the most trusted opponent. Opponents named “jdrx894” were trusted more than opponents called “Michael.” Further analysis showed that having a degree in technology or engineering, exposure to robots online and robot use self-efficacy predicted higher trust toward robots and AI. Out of Big Five personality characteristics, openness to experience predicted higher trust, and conscientiousness predicted lower trust. Results suggest trust on robots and AI is contextual and it is also dependent on individual differences and knowledge on technology.","",""
179,"S. Michie, James Thomas, M. Johnston, Pol Mac Aonghusa, J. Shawe-Taylor, M. Kelly, L. Deleris, Ailbhe N. Finnerty, M. Marques, E. Norris, A. O'Mara-Eves, R. West","The Human Behaviour-Change Project: harnessing the power of artificial intelligence and machine learning for evidence synthesis and interpretation",2017,"","","","",74,"2022-07-13 09:20:03","","10.1186/s13012-017-0641-5","","",,,,,179,35.80,18,12,5,"","",""
11,"Sheikh Rabiul Islam, W. Eberle, S. Ghafoor","Towards Quantification of Explainability in Explainable Artificial Intelligence Methods",2019,"","","","",75,"2022-07-13 09:20:03","","","","",,,,,11,3.67,4,3,3,"Artificial Intelligence (AI) has become an integral part of domains such as security, finance, healthcare, medicine, and criminal justice. Explaining the decisions of AI systems in human terms is a key challenge--due to the high complexity of the model, as well as the potential implications on human interests, rights, and lives . While Explainable AI is an emerging field of research, there is no consensus on the definition, quantification, and formalization of explainability. In fact, the quantification of explainability is an open challenge. In our previous work, we incorporated domain knowledge for better explainability, however, we were unable to quantify the extent of explainability. In this work, we (1) briefly analyze the definitions of explainability from the perspective of different disciplines (e.g., psychology, social science), properties of explanation, explanation methods, and human-friendly explanations; and (2) propose and formulate an approach to quantify the extent of explainability. Our experimental result suggests a reasonable and model-agnostic way to quantify explainability","",""
25,"U. Lichtenthaler","Substitute or Synthesis: The Interplay between Human and Artificial Intelligence",2018,"","","","",76,"2022-07-13 09:20:03","","10.1080/08956308.2018.1495962","","",,,,,25,6.25,25,1,4,"Business leaders in a range of industries, from machinery and electronics to retail and insurance, agree that artificial intelligence (AI) has the potential to damage their established business lines substantially, leading to a significant loss of traditional jobs (Agrawal, Gans, and Goldfarb 2017; Davenport and Ronanki 2018). At the same time, many knowledge workers, including those in R&D, feel unthreatened—the unique competencies required by creative work, they assert, are not replicable by AI. Even enthusiasts acknowledge the limitations of AI, now and for at least the next 10 years, if not well beyond this period (Ili and Lichtenthaler 2017). So who is right—the camp highlighting the potential of AI to replace humans or the camp arguing for the irreplaceable uniqueness of human intelligence? In fact, these two perspectives are parts of a more complex story. So far, no coherent perspective has emerged around these issues, either in academic studies or in more general discussions. Industry experts and consulting professionals also disagree about the likely consequences of AI (see, for instance, Kolbjornsrud, Amico, and Thomas 2016). Some recent studies from a variety of organizations, including the OECD, the World Economic Forum, Forrester, and McKinsey, expect that the advancement of AI will result in only a limited net loss of jobs if the new jobs AI will create are also taken into consideration. Winick (2018), who summarizes these studies, suggests that it may still be too early to forecast the number of job cuts and new jobs. In fact, the bulk of the attention to AI in recent years has focused on the efficiency gains that could be reaped from advanced automation based on intelligent algorithms, which have the capability to replace at least some human work. The potential for developing completely new solutions and achieving growth by combining human and artificial intelligence has received insufficient attention until very recently (Agrawal, Gans, and Goldfarb 2017). This strategic perspective, which considers human and artificial intelligence as complementary rather than competitive, may help to bridge the inconsistent results of previous studies, articles, and discussions. The relationship between human and artificial intelligence may be described in terms of a matrix that maps out four different interplays, each engaging different combinations of human and artificial intelligence (Figure 1).","",""
53,"Fabio Massimo Zanzotto","Human-in-the-loop Artificial Intelligence",2017,"","","","",77,"2022-07-13 09:20:03","","","","",,,,,53,10.60,53,1,5,"Little by little, newspapers are revealing the bright future that Artificial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future has a dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers will need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, these workers are digging their own graves.  In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI) as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward aware and unaware knowledge producers with a different scheme: decisions of AI systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Robin Hoods, HIT-AI researchers should fight for a fairer Artificial Intelligence that gives back what it steals.","",""
1,"Alicia Lai","Artificial Intelligence, LLC: Corporate Personhood as Tort Reform",2020,"","","","",78,"2022-07-13 09:20:03","","10.2139/ssrn.3677360","","",,,,,1,0.50,1,1,2,"Our legal system has long tried to fit the square peg of artificial intelligence (AI) technologies into the round hole of the current tort regime, overlooking the inability of traditional liability schemes to address the nuances of how AI technology creates harms. The current tort regime deals out rough justice—using strict liability for some AI products and using the negligence rule for other AI services—both of which are insufficiently tailored to achieve public policy objectives.    Under a strict liability regime where manufacturers are always held liable for the faults of their technology regardless of knowledge or precautionary measures, firms are incentivized to play it safe and stifle innovation. But even with this cautionary stance, the goals of strict liability cannot be met due to the unique nature of AI technology: its mistakes are merely “efficient errors”—they appropriately surpass the human baseline, they are game theory problems intended for a jury, they are necessary to train a robust system, or they are harmless but misclassified.    Under a negligence liability regime where the onus falls entirely on consumers to prove the element of causation, victimized consumers are left without sufficient recourse or compensation. Many critiques have been leveled against the “black-box” nature of algorithms.    This paper proposes a new framework to regulate artificial intelligence technologies: bestowing corporate personhood to AI systems. First, the corporate personality trait of “limited liability” strikes an optimal balance in determining liability—it would both compensate victims (for instance, through obligations to carry insurance and a straightforward burden of causation) while holding manufacturers responsible only when the infraction is egregious (for instance, through veil-piercing). Second, corporate personhood is “divisible”—meaning not all corporate personality traits need to be granted—which circumvents many of the philosophical criticisms of giving AI the complete set of rights of full legal personhood.","",""
0,"M. Boughanem, I. Akermi, G. Pasi, Karam Abdulahhad","Information Retrieval and Artificial Intelligence",2020,"","","","",79,"2022-07-13 09:20:03","","10.1007/978-3-030-06170-8_5","","",,,,,0,0.00,0,4,2,"","",""
169,"Amisha, Paras Malik, Monika Pathania, V. Rathaur","Overview of artificial intelligence in medicine",2019,"","","","",80,"2022-07-13 09:20:03","","10.4103/jfmpc.jfmpc_440_19","","",,,,,169,56.33,42,4,3,"Background: Artificial intelligence (AI) is the term used to describe the use of computers and technology to simulate intelligent behavior and critical thinking comparable to a human being. John McCarthy first described the term AI in 1956 as the science and engineering of making intelligent machines. Objective: This descriptive article gives a broad overview of AI in medicine, dealing with the terms and concepts as well as the current and future applications of AI. It aims to develop knowledge and familiarity of AI among primary care physicians. Materials and Methods: PubMed and Google searches were performed using the key words 'artificial intelligence'. Further references were obtained by cross-referencing the key articles. Results: Recent advances in AI technology and its current applications in the field of medicine have been discussed in detail. Conclusions: AI promises to change the practice of medicine in hitherto unknown ways, but many of its practical applications are still in their infancy and need to be explored and developed better. Medical professionals also need to understand and acclimatize themselves with these advances for better healthcare delivery to the masses.","",""
167,"C. Langlotz, Bibb Allen, B. Erickson, Jayashree Kalpathy-Cramer, K. Bigelow, T. Cook, A. Flanders, M. Lungren, D. Mendelson, J. Rudie, Ge Wang, K. Kandarpa","A Roadmap for Foundational Research on Artificial Intelligence in Medical Imaging: From the 2018 NIH/RSNA/ACR/The Academy Workshop.",2019,"","","","",81,"2022-07-13 09:20:03","","10.1148/radiol.2019190613","","",,,,,167,55.67,17,12,3,"Imaging research laboratories are rapidly creating machine learning systems that achieve expert human performance using open-source methods and tools. These artificial intelligence systems are being developed to improve medical image reconstruction, noise reduction, quality assurance, triage, segmentation, computer-aided detection, computer-aided classification, and radiogenomics. In August 2018, a meeting was held in Bethesda, Maryland, at the National Institutes of Health to discuss the current state of the art and knowledge gaps and to develop a roadmap for future research initiatives. Key research priorities include: 1, new image reconstruction methods that efficiently produce images suitable for human interpretation from source data; 2, automated image labeling and annotation methods, including information extraction from the imaging report, electronic phenotyping, and prospective structured image reporting; 3, new machine learning methods for clinical imaging data, such as tailored, pretrained model architectures, and federated machine learning methods; 4, machine learning methods that can explain the advice they provide to human users (so-called explainable artificial intelligence); and 5, validated methods for image de-identification and data sharing to facilitate wide availability of clinical imaging data sets. This research roadmap is intended to identify and prioritize these needs for academic research laboratories, funding agencies, professional societies, and industry.","",""
0,"D. Anderlini, Luigi F. Agnati, D. Guidolin, M. Marcoli, Amina S. Woods, G. Maura","From Gilgamesh’s quest for immortality to everlasting cloud hyper-collective mind: ethical implications for artificial intelligence",2022,"","","","",82,"2022-07-13 09:20:03","","10.1108/gkmc-08-2021-0130","","",,,,,0,0.00,0,6,1," Purpose This conceptual paper aims to explore the possibility of human beings reaching a virtual form of immortality.   Design/methodology/approach The paper is an investigation of the path from an early example of human knowledge to the birth of artificial intelligence (AI) and robots. A critical analysis of different point of views, from philosophers to scientists, is presented.   Findings From ancient rock art paintings to the moon landing, human knowledge has made a huge progress to the point of creating robots resembling human features. While these humanoid robots can successfully undertake risky tasks, they also generate ethical issues for the society they interact with.   Research limitations/implications The paper is conceptual, and it does attempt to provide one theory by which human beings can achieve the dream of immortality. It is part of a work in progress on the use of AI and the issues related to the creation/use of humanoid robots in society.   Originality/value This paper provides an overview of some of the key issues and themes impacting our modern society. Its originality resides in the linking of human knowledge to collective knowledge and then of collective mind to the hyper-collective mind. The idea of humans reaching immortality is burdened by the imperative need to define ethical guidelines for the field of AI and its uses. ","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",83,"2022-07-13 09:20:03","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
86,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu","Artificial-Intelligence-Enabled Intelligent 6G Networks",2019,"","","","",84,"2022-07-13 09:20:03","","10.1109/MNET.011.2000195","","",,,,,86,28.67,14,6,3,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","",""
6,"A. D. W. Sumari, A. S. Ahmad","Knowledge-growing system: The origin of the cognitive artificial intelligence",2017,"","","","",85,"2022-07-13 09:20:03","","10.1109/ICEEI.2017.8312382","","",,,,,6,1.20,3,2,5,"Knowledgde-Growing System (KGS) has been there for a while since invented in 2009. Since some real-life problems have been approached by applying KGS and prospective results show the benefits of using it especially for cases in decision making and estimating or predicting the probability of occurrence of a phenomenon being observed by the system. It is also good to be applied to obtain second opinion or alternative decision for decision maker regarding a phenomenon being observed. The excellence of KGS is its mechanism which mimics how human thinks, in this case is how brain grows the knowledge based on the information it receives from the sensory organs. Thinking is a cognitive process which is not easy to be emulated to a system. In this paper we share our endeavor in building new perspective in Artificial Intelligence called Cognitive Artificial Intelligence (CAI) where KGS is the primary example of our achievement.","",""
42,"H. Khayyam, B. Javadi, M. Jalili, R. Jazar","Artificial Intelligence and Internet of Things for Autonomous Vehicles",2019,"","","","",86,"2022-07-13 09:20:03","","10.1007/978-3-030-18963-1_2","","",,,,,42,14.00,11,4,3,"","",""
63,"M. VerMilyea, J. Hall, S. Diakiw, A. Johnston, T. Nguyen, D. Perugini, A. Miller, A. Picou, A. P. Murphy, M. Perugini","Development of an artificial intelligence-based assessment model for prediction of embryo viability using static images captured by optical light microscopy during IVF",2020,"","","","",87,"2022-07-13 09:20:03","","10.1093/humrep/deaa013","","",,,,,63,31.50,6,10,2,"Abstract STUDY QUESTION Can an artificial intelligence (AI)-based model predict human embryo viability using images captured by optical light microscopy? SUMMARY ANSWER We have combined computer vision image processing methods and deep learning techniques to create the non-invasive Life Whisperer AI model for robust prediction of embryo viability, as measured by clinical pregnancy outcome, using single static images of Day 5 blastocysts obtained from standard optical light microscope systems. WHAT IS KNOWN ALREADY Embryo selection following IVF is a critical factor in determining the success of ensuing pregnancy. Traditional morphokinetic grading by trained embryologists can be subjective and variable, and other complementary techniques, such as time-lapse imaging, require costly equipment and have not reliably demonstrated predictive ability for the endpoint of clinical pregnancy. AI methods are being investigated as a promising means for improving embryo selection and predicting implantation and pregnancy outcomes. STUDY DESIGN, SIZE, DURATION These studies involved analysis of retrospectively collected data including standard optical light microscope images and clinical outcomes of 8886 embryos from 11 different IVF clinics, across three different countries, between 2011 and 2018. PARTICIPANTS/MATERIALS, SETTING, METHODS The AI-based model was trained using static two-dimensional optical light microscope images with known clinical pregnancy outcome as measured by fetal heartbeat to provide a confidence score for prediction of pregnancy. Predictive accuracy was determined by evaluating sensitivity, specificity and overall weighted accuracy, and was visualized using histograms of the distributions of predictions. Comparison to embryologists’ predictive accuracy was performed using a binary classification approach and a 5-band ranking comparison. MAIN RESULTS AND THE ROLE OF CHANCE The Life Whisperer AI model showed a sensitivity of 70.1% for viable embryos while maintaining a specificity of 60.5% for non-viable embryos across three independent blind test sets from different clinics. The weighted overall accuracy in each blind test set was >63%, with a combined accuracy of 64.3% across both viable and non-viable embryos, demonstrating model robustness and generalizability beyond the result expected from chance. Distributions of predictions showed clear separation of correctly and incorrectly classified embryos. Binary comparison of viable/non-viable embryo classification demonstrated an improvement of 24.7% over embryologists’ accuracy (P = 0.047, n = 2, Student’s t test), and 5-band ranking comparison demonstrated an improvement of 42.0% over embryologists (P = 0.028, n = 2, Student’s t test). LIMITATIONS, REASONS FOR CAUTION The AI model developed here is limited to analysis of Day 5 embryos; therefore, further evaluation or modification of the model is needed to incorporate information from different time points. The endpoint described is clinical pregnancy as measured by fetal heartbeat, and this does not indicate the probability of live birth. The current investigation was performed with retrospectively collected data, and hence it will be of importance to collect data prospectively to assess real-world use of the AI model. WIDER IMPLICATIONS OF THE FINDINGS These studies demonstrated an improved predictive ability for evaluation of embryo viability when compared with embryologists’ traditional morphokinetic grading methods. The superior accuracy of the Life Whisperer AI model could lead to improved pregnancy success rates in IVF when used in a clinical setting. It could also potentially assist in standardization of embryo selection methods across multiple clinical environments, while eliminating the need for complex time-lapse imaging equipment. Finally, the cloud-based software application used to apply the Life Whisperer AI model in clinical practice makes it broadly applicable and globally scalable to IVF clinics worldwide. STUDY FUNDING/COMPETING INTEREST(S) Life Whisperer Diagnostics, Pty Ltd is a wholly owned subsidiary of the parent company, Presagen Pty Ltd. Funding for the study was provided by Presagen with grant funding received from the South Australian Government: Research, Commercialisation and Startup Fund (RCSF). ‘In kind’ support and embryology expertise to guide algorithm development were provided by Ovation Fertility. J.M.M.H., D.P. and M.P. are co-owners of Life Whisperer and Presagen. Presagen has filed a provisional patent for the technology described in this manuscript (52985P pending). A.P.M. owns stock in Life Whisperer, and S.M.D., A.J., T.N. and A.P.M. are employees of Life Whisperer.","",""
1,"Shengtao Dong, J. Li, Haozong Zhao, Yu-Zhen Zheng, Yaoning Chen, Junxi Shen, Huan Yang, Jieyang Zhu","Risk Factor Analysis for Predicting the Onset of Rotator Cuff Calcific Tendinitis Based on Artificial Intelligence",2022,"","","","",88,"2022-07-13 09:20:03","","10.1155/2022/8978878","","",,,,,1,1.00,0,8,1,"Background Symptomatic rotator cuff calcific tendinitis (RCCT) is a common shoulder disorder, and approaches combined with artificial intelligence greatly facilitate the development of clinical practice. Current scarce knowledge of the onset suggests that clinicians may need to explore this disease thoroughly. Methods Clinical data were retrospectively collected from subjects diagnosed with RCCT at our institution within the period 2008 to 2020. A standardized questionnaire related to shoulder symptoms was completed in all cases, and standardized radiographs of both shoulders were extracted using a human-computer interactive electronic medical system (EMS) to clarify the clinical diagnosis of symptomatic RCCT. Based on the exclusion of asymptomatic subjects, risk factors in the baseline characteristics significantly associated with the onset of symptomatic RCCT were assessed via stepwise logistic regression analysis. Results Of the 1,967 consecutive subjects referred to our academic institution for shoulder discomfort, 237 were diagnosed with symptomatic RCCT (12.05%). The proportion of women and the prevalence of clinical comorbidities were significantly higher in the RCCT cohort than those in the non-RCCT cohort. Stepwise logistic regression analysis confirmed that female gender, hyperlipidemia, diabetes mellitus, and hypothyroidism were independent risk factors for the entire cohort. Stratified by gender, the study found a partial overlap of risk factors contributing to morbidity in men and women. Diagnosis of hyperlipidemia, diabetes mellitus, and hypothyroidism in male cases and diabetes mellitus in female cases were significantly associated with symptomatic RCCT. Conclusion Independent predictors of symptomatic RCCT are female, hyperlipidemia, diabetes mellitus, and hypothyroidism. Men diagnosed with hyperlipidemia, diabetes mellitus, and hypothyroidism are at high risk for symptomatic RCCT, while more medical attention is required for women with diabetes mellitus. Artificial intelligence offers pioneering innovations in the diagnosis and treatment of musculoskeletal disorders, and careful assessment through individualized risk stratification can help predict onset and targeted early stage treatment.","",""
0,"Chris Yang","Explainable Artificial Intelligence for Predictive Modeling in Healthcare",2022,"","","","",89,"2022-07-13 09:20:03","","10.1007/s41666-022-00114-1","","",,,,,0,0.00,0,1,1,"","",""
0,"Alexandre Ardichvili","The Impact of Artificial Intelligence on Expertise Development: Implications for HRD",2022,"","","","",90,"2022-07-13 09:20:03","","10.1177/15234223221077304","","",,,,,0,0.00,0,1,1,"Problem The implementation of artificial intelligence (AI) is assumed to lead to increased productivity of knowledge workers. However, AI could also have negative effects on the development of professional expertise. Solution A review of the literature on expertise development is provided, followed by examples of AI implementation in a knowledge-intensive profession, accounting. The analysis of these examples suggests that automation can result in the loss of expertise due to reduced opportunities for learning from deliberate practice and experienced colleagues, and from working on progressively more complex tasks. Implications for human resource development (HRD) include creating alternative individual development opportunities and promoting organizational cultures conducive to expertise development in human-machine interaction modes. Stakeholders The results of this study will be of interest to scholars of HRD, accounting education, and human-machine interaction. Practical implications will be of relevance to HRD professionals and managers responsible for the implementation of artificial intelligence solutions.","",""
0,"R. Philipsen, P. Brauner, Hannah Biermann, M. Ziefle","I Am What I Am – Roles for Artificial Intelligence from the Users’ Perspective",2022,"","","","",91,"2022-07-13 09:20:03","","10.54941/ahfe1001453","","",,,,,0,0.00,0,4,1,"With increasing digitization, intelligent software systems are taking over more tasks in everyday human life, both in private and professional contexts. So-called artificial intelligence (AI) ranges from subtle and often unnoticed improvements in daily life, optimizations in data evaluation, assistance systems with which the people interact directly, to perhaps artificial anthropomorphic entities in the future. How-ever, no etiquette yet exists for integrating AI into the human living environment, which has evolved over millennia for human interaction. This paper addresses what roles AI may take, what knowledge AI may have, and how this is influenced by user characteristics. The results show that roles with personal relationships, such as an AI as a friend or partner, are not preferred by users. The higher the confidence in an AI's handling of data, the more likely personal roles are seen as an option for the AI, while the preference for subordinate roles, such as an AI as a servant or a subject, depends on general technology acceptance and belief in a dangerous world. The role attribution is independent from the usage intention and the semantic perception of artificial intelligence, which differs only slightly, e.g., in terms of morality and controllability, from the perception of human intelligence.","",""
0,"Bukhoree Sahoh, Kanjana Haruehansapong, Mallika Kliangkhlao","Causal Artificial Intelligence for High-Stakes Decisions: The Design and Development of a Causal Machine Learning Model",2022,"","","","",92,"2022-07-13 09:20:03","","10.1109/access.2022.3155118","","",,,,,0,0.00,0,3,1,"A high-stakes decision requires deep thought to understand the complex factors that stop a situation from becoming worse. Such decisions are carried out under high pressure, with a lack of information, and in limited time. This research applies Causal Artificial Intelligence to high-stakes decisions, aiming to encode causal assumptions based on human-like intelligence, and thereby produce interpretable and argumentative knowledge. We develop a Causal Bayesian Networks model based on causal science using $d$ -separation and do-operations to discover the causal graph aligned with cognitive understanding. Causal odd ratios are used to measure the causal assumptions integrated with the real-world data to prove the proposed causal model compatibility. Causal effect relationships in the model are verified based on causal P-values and causal confident intervals and approved less than 1% by random chance. It shows that the causal model can encode cognitive understanding as precise, robust relationships. The concept of model design allows software agents to imitate human intelligence by inferring potential knowledge and be employed in high-stakes decision applications.","",""
0,"A. Campbell, R. Smith, B. Petersen, L. Moore, A. Khan, A. Barrie","O-125 Application of artificial intelligence using big data to devise and train a machine learning model on over 63,000 human embryos to automate time-lapse embryo annotation",2022,"","","","",93,"2022-07-13 09:20:03","","10.1093/humrep/deac105.025","","",,,,,0,0.00,0,6,1,"      Can a machine learning (ML) model, developed using modern neural network architecture produce comparable annotation data; utilisable for algorithmic outcome prediction, to manual time-lapse annotations?        The model automatically annotated unseen embryos with comparable results to manual methods, generating morphokinetic data to enable comparably predictive outputs from an embryo selection algorithm.        The application of artificial intelligence across healthcare industries, including fertility, is increasing. Several ML models are available that seek to generate or analyse embryo images and morphokinetic data, and to determine embryo viability potential. Along with photographic images, the use of time-lapse in IVF laboratories has amassed numeric data, resulting predominantly from annotated manual assessment of images over time. Embryo annotation practice is variable in quality, can be subjective and is time-consuming; commonly taking several minutes per embryo. The development of rapid, accurate automatic annotation would represent a significant time-saving as well as an increase in reproducibility and accuracy.        Multicentre quality assured annotation data from 63,383 time-lapse monitored embryos (EmbryoScope®), comprising over 400 million individual images, were used to train a ML model to automatically generate morphokinetic annotations. Data was derived from 8 UK clinics within a cohesive group between 2012-2021. Accuracy was assessed using 900 unseen embryos (with live birth outcome) by comparing the output of an established in-house, prospectively validated embryo selection model when the input was either ML-automated, or manual annotations.        Multi-focal plane images were processed on the Azure cloud (Microsoft) and resampled to 300x300 pixels. A Laplacian-based focal stacking algorithm merged frames into a single image. The model consisted of an EfficientNetB4 Convolutional Neural Network classifier to extract features and classify the stage of embryo images. A Temporal Convolutional Network  interpreted a time-series of image features; producing annotations from pronuclear fading through to blastocyst. Soft localisation loss function used QA data to integrate annotation subjectivities.        The ML model rapidly and automatically generated annotations. Efficacy and comparability of the ML model to automate reliable, utilisable annotations was demonstrated by comparison with manual annotation data and the ML model’s ability to auto-generate annotations which could be used to predict live birth by providing annotation data to an established, validated in house embryo selection model. Live birth-predictive capability was measured, and benchmarked against manual annotation, using the area under the receiver operating characteristic curve (AUC).  When tested on time-lapse images, collected from pronuclear fading to full blastulation, representing 900 previously unseen, transferred blastocysts where live birth outcomes were blinded, the in-house developed auto-annotation ML model resulted in an AUC of 0.686 compared with 0.661 for manual annotations, for live birth prediction.  Auto annotation using the developed model took only milliseconds to complete per embryo. The developed auto-annotation model, built and tested on large data, is considered suitable for productionisation with the aim of being validated and integrated into an application to support IVF laboratory practice.        Whilst this model was trained to recognise key morphokinetic events, there are other morphokinetic variables that may be useful in the prediction of live birth and further improve embryo selection, or deselection, ability. Akin to manual interpretation, some embryos may fail to be annotated or need second opinion.        There is increasing evidence supporting the application of ML to utilise big data from time-lapse imaging and fertility care generally. Whilst promising benefits to IVF clinics and patients, responsible use of data is required alongside large high-quality datasets, and rigorous validation, to ensure safe and robust applications.        N/A ","",""
39,"Urmil Bharti, Deepali Bajaj, Hunar Batra, Shreya Lalit, Shweta Lalit, Aayushi Gangwani","Medbot: Conversational Artificial Intelligence Powered Chatbot for Delivering Tele-Health after COVID-19",2020,"","","","",94,"2022-07-13 09:20:03","","10.1109/ICCES48766.2020.9137944","","",,,,,39,19.50,7,6,2,"Telemedicine can be used by medical practitioners to connect with their patients during the recent Coronavirus outbreak, whilst attempting to reduce COVID-19 transmission among patients and clinicians. Amidst the pandemic, Telemedicine has the potential to help by permitting patients to receive supportive care without having to physically visit a hospital by using a conversational artificial intelligence-based application for their treatment. Thus, telehealth will rapidly and radically transform in-person care to remote consultation of patients. Because of this, it developed a Multilingual Conversational Bot based on Natural Language Processing (NLP) to provide free primary healthcare education, information, advice to chronic patients. The study introduces a novel computer application acting as a personal virtual doctor that has been opportunely designed and extensively trained to interact with patients like human beings. This application is based upon a serverless architecture and it aggregates the services of a doctor by providing preventive measures, homeremedies, interactive counseling sessions, healthcare tips, and symptoms covering the most prevalent diseases in rural India. The paper proposes a conversational bot “Aapka Chikitsak” on Google Cloud Platform (GCP) for delivering telehealth in India to increase the patient's access to healthcare knowledge and leverage the potentials of artificial intelligence to bridge the gap of demand and supply of human healthcare providers. This conversational application has resulted in reducing the barriers for access to healthcare facilities and procures intelligent consultations remotely to allow timely care and quality treatment, there by effectively assisting the society.","",""
0,"S. Sadeghi, M. Amiri, Farzaneh Mansoori Mooseloo","Artificial Intelligence and Its Application in Optimization under Uncertainty",2021,"","","","",95,"2022-07-13 09:20:03","","10.5772/intechopen.98628","","",,,,,0,0.00,0,3,1,"Nowadays, the increase in data acquisition and availability and complexity around optimization make it imperative to jointly use artificial intelligence (AI) and optimization for devising data-driven and intelligent decision support systems (DSS). A DSS can be successful if large amounts of interactive data proceed fast and robustly and extract useful information and knowledge to help decision-making. In this context, the data-driven approach has gained prominence due to its provision of insights for decision-making and easy implementation. The data-driven approach can discover various database patterns without relying on prior knowledge while also handling flexible objectives and multiple scenarios. This chapter reviews recent advances in data-driven optimization, highlighting the promise of data-driven optimization that integrates mathematical programming and machine learning (ML) for decision-making under uncertainty and identifies potential research opportunities. This chapter provides guidelines and implications for researchers, managers, and practitioners in operations research who want to advance their decision-making capabilities under uncertainty concerning data-driven optimization. Then, a comprehensive review and classification of the relevant publications on the data-driven stochastic program, data-driven robust optimization, and data-driven chance-constrained are presented. This chapter also identifies fertile avenues for future research that focus on deep-data-driven optimization, deep data-driven models, as well as online learning-based data-driven optimization. Perspectives on reinforcement learning (RL)-based data-driven optimization and deep RL for solving NP-hard problems are discussed. We investigate the application of data-driven optimization in different case studies to demonstrate improvements in operational performance over conventional optimization methodology. Finally, some managerial implications and some future directions are provided.","",""
81,"M. Coeckelbergh","Artificial Intelligence, Responsibility Attribution, and a Relational Justification of Explainability",2019,"","","","",96,"2022-07-13 09:20:03","","10.1007/s11948-019-00146-8","","",,,,,81,27.00,81,1,3,"","",""
27,"N. S. Saravana Kumar","IMPLEMENTATION OF ARTIFICIAL INTELLIGENCE IN IMPARTING EDUCATION AND EVALUATING STUDENT PERFORMANCE",2019,"","","","",97,"2022-07-13 09:20:03","","10.36548/jaicn.2019.1.001","","",,,,,27,9.00,27,1,3,"Simulation of human intelligence process is made possible with the help of artificial intelligence. The learning, reasoning and self-correction properties are made possible in computer systems. Along with AI, other technologies are combined effectively in order to create remarkable applications. We apply the changing role of AI and its techniques in new educational paradigms to create a personalised teaching-learning environment. Features like recognition, pattern matching, decision making, reasoning, problem solving and so on are applied along with knowledge based system and supervised machine learning for a complete learning and assessment process.","",""
103,"F. Schwendicke, W. Samek, J. Krois","Artificial Intelligence in Dentistry: Chances and Challenges",2020,"","","","",98,"2022-07-13 09:20:03","","10.1177/0022034520915714","","",,,,,103,51.50,34,3,2,"The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.","",""
27,"R. Abdullah, Bahjat Fakieh","Health Care Employees’ Perceptions of the Use of Artificial Intelligence Applications: Survey Study",2020,"","","","",99,"2022-07-13 09:20:03","","10.2196/17620","","",,,,,27,13.50,14,2,2,"Background The advancement of health care information technology and the emergence of artificial intelligence has yielded tools to improve the quality of various health care processes. Few studies have investigated employee perceptions of artificial intelligence implementation in Saudi Arabia and the Arabian world. In addition, limited studies investigated the effect of employee knowledge and job title on the perception of artificial intelligence implementation in the workplace. Objective The aim of this study was to explore health care employee perceptions and attitudes toward the implementation of artificial intelligence technologies in health care institutions in Saudi Arabia. Methods An online questionnaire was published, and responses were collected from 250 employees, including doctors, nurses, and technicians at 4 of the largest hospitals in Riyadh, Saudi Arabia. Results The results of this study showed that 3.11 of 4 respondents feared artificial intelligence would replace employees and had a general lack of knowledge regarding artificial intelligence. In addition, most respondents were unaware of the advantages and most common challenges to artificial intelligence applications in the health sector, indicating a need for training. The results also showed that technicians were the most frequently impacted by artificial intelligence applications due to the nature of their jobs, which do not require much direct human interaction. Conclusions The Saudi health care sector presents an advantageous market potential that should be attractive to researchers and developers of artificial intelligence solutions.","",""
19,"M. Komorowski","Clinical management of sepsis can be improved by artificial intelligence: yes",2019,"","","","",100,"2022-07-13 09:20:03","","10.1007/s00134-019-05898-2","","",,,,,19,6.33,19,1,3,"","",""
52,"R. Fjelland","Why general artificial intelligence will not be realized",2020,"","","","",101,"2022-07-13 09:20:03","","10.1057/s41599-020-0494-4","","",,,,,52,26.00,52,1,2,"","",""
45,"Tom Kamiel Magda Vercauteren, M. Unberath, N. Padoy, N. Navab","CAI4CAI: The Rise of Contextual Artificial Intelligence in Computer-Assisted Interventions",2019,"","","","",102,"2022-07-13 09:20:03","","10.1109/JPROC.2019.2946993","","",,,,,45,15.00,11,4,3,"Data-driven computational approaches have evolved to enable extraction of information from medical images with reliability, accuracy, and speed, which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theaters are extremely complex and typically rely on poorly integrated intraoperative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer-assisted interventions, we highlight the crucial need to take the context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer-assisted intervention (CAI4CAI) arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors, and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human–AI actor team; and how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision-making ultimately producing more precise and reliable interventions.","",""
38,"I. Stafford, M. Kellermann, E. Mossotto, R. M. Beattie, B. MacArthur, S. Ennis","A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases.",2020,"","","","",103,"2022-07-13 09:20:03","","10.1038/s41746-020-0229-3","","",,,,,38,19.00,6,6,2,"","",""
32,"André Renz, Romy Hilbig","Prerequisites for artificial intelligence in further education: identification of drivers, barriers, and business models of educational technology companies",2020,"","","","",104,"2022-07-13 09:20:03","","10.1186/s41239-020-00193-3","","",,,,,32,16.00,16,2,2,"","",""
3,"T. Kaur, Anirudra Diwakar, Kirandeep, Pranav Mirpuri, M. Tripathi, P. Chandra, T. Gandhi","Artificial Intelligence in Epilepsy",2021,"","","","",105,"2022-07-13 09:20:03","","10.4103/0028-3886.317233","","",,,,,3,3.00,0,7,1,"Background: The study of seizure patterns in electroencephalography (EEG) requires several years of intensive training. In addition, inadequate training and human error may lead to misinterpretation and incorrect diagnosis. Artificial intelligence (AI)-based automated seizure detection systems hold an exciting potential to create paradigms for proper diagnosis and interpretation. AI holds the promise to transform healthcare into a system where machines and humans can work together to provide an accurate, timely diagnosis, and treatment to the patients. Objective: This article presents a brief overview of research on the use of AI systems for pattern recognition in EEG for clinical diagnosis. Material and Methods: The article begins with the need for understanding nonstationary signals such as EEG and simplifying their complexity for accurate pattern recognition in medical diagnosis. It also explains the core concepts of AI, machine learning (ML), and deep learning (DL) methods. Results and Conclusions: In this present context of epilepsy diagnosis, AI may work in two ways; first by creating visual representations (e.g., color-coded paradigms), which allow persons with limited training to make a diagnosis. The second is by directly explaining a complete automated analysis, which of course requires more complex paradigms than the previous one. We also clarify that AI is not about replacing doctors and strongly emphasize the need for domain knowledge in building robust AI models that can work in real-time scenarios rendering good detection accuracy in a minimum amount of time.","",""
0,"Lauren M. Sanders, Jason H. Yang, Ryan T Scott, A. Qutub, H. Martín, D. Berrios, Jaden J. A. Hastings, J. Rask, Graham Mackintosh, A. Hoarfrost, Stuart Chalk, John Kalantari, K. Khezeli, E. Antonsen, Joel Babdor, Richard Barker, S. Baranzini, A. Beheshti, Guillermo M. Delgado-Aparicio, B. Glicksberg, C. Greene, M. Haendel, Arif A. Hamid, P. Heller, Daniel Jamieson, K. Jarvis, S. Komarova, M. Komorowski, P. Kothiyal, A. Mahabal, U. Manor, Christopher E. Mason, Mona Matar, G. Mias, Jack M. Miller, J. Myers, Charlotte A Nelson, Jonathan Oribello, Seung-min Park, P. Parsons-Wingerter, R. Prabhu, R. Reynolds, Amanda M. Saravia-Butler, S. Saria, A. Sawyer, N. Singh, Frank Soboczenski, Michael Snyder, Karthik Soman, C. Theriot","Beyond Low Earth Orbit: Biological Research, Artificial Intelligence, and Self-Driving Labs",2021,"","","","",106,"2022-07-13 09:20:03","","","","",,,,,0,0.00,0,50,1,"Space biology research aims to understand fundamental effects of spaceflight on organisms, develop foundational knowledge to support deep space exploration, and ultimately bioengineer spacecraft and habitats to stabilize the ecosystem of plants, crops, microbes, animals, and humans for sustained multi-planetary life. To advance these aims, the field leverages experiments, platforms, data, and model organisms from both spaceborne and ground-analog studies. As research is extended beyond low Earth orbit, experiments and platforms must be maximally autonomous, light, agile, and intelligent to expedite knowledge discovery. Here we present a summary of recommendations from a workshop organized by the National Aeronautics and Space Administration on artificial intelligence, machine learning, and modeling applications which offer key solutions toward these space biology challenges. In the next decade, the synthesis of artificial intelligence into the field of space biology will deepen the biological understanding of spaceflight effects, facilitate predictive modeling and analytics, support maximally autonomous and reproducible experiments, and efficiently manage spaceborne data and metadata, all with the goal to enable life to thrive in deep space.","",""
26,"Yingxu Wang, W. Kinsner, S. Kwong, Henry Leung, Jianhua Lu, Michael H. Smith, L. Trajković, E. Tunstel, K. Plataniotis, G. Yen","Brain-Inspired Systems: A Transdisciplinary Exploration on Cognitive Cybernetics, Humanity, and Systems Science Toward Autonomous Artificial Intelligence",2020,"","","","",107,"2022-07-13 09:20:03","","10.1109/MSMC.2018.2889502","","",,,,,26,13.00,3,10,2,"Brain-inspired cognitive systems (BCSs) are an emerging field of cybernetics, cognitive science, and system science. BCSs study not only the intelligence science foundations of artificial intelligence (AI) and cognitive systems, but also formal models of the brain embodied by computational intelligence. This article presents the brain and intelligence science foundations of BCS toward hybrid intelligent systems and the symbiotic intelligence of humanity. It explores the transdisciplinary theoretical foundations of system, brain, intelligence, knowledge, cybernetic, and cognitive sciences toward the next generation of knowledge processors beyond classic data processors for autonomous computing systems. A BCS provides an overarching platform for cognitive cybernetics, humanity, and systems to enable emerging hybrid societies shared by humans and intelligent machines.","",""
63,"S. Strohmeier, F. Piazza","Artificial Intelligence Techniques in Human Resource Management - A Conceptual Exploration",2015,"","","","",108,"2022-07-13 09:20:03","","10.1007/978-3-319-17906-3_7","","",,,,,63,9.00,32,2,7,"","",""
484,"M. Komorowski, L. Celi, O. Badawi, A. Gordon, Aldo A. Faisal","The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care",2018,"","","","",109,"2022-07-13 09:20:03","","10.1038/s41591-018-0213-5","","",,,,,484,121.00,97,5,4,"","",""
0,"Nigamanth Sridhar, Li Yang, J. Joshi, Victor P. Piotrowski","Cybersecurity Education in the Age of Artificial Intelligence",2021,"","","","",110,"2022-07-13 09:20:03","","10.1145/3408877.3439525","","",,,,,0,0.00,0,4,1,"The 2019 Federal Cybersecurity Research and Development Strategic Plan highlighted the mutual needs and benefits of artificial intelligence (AI) and cybersecurity. AI techniques are expected to enhance cybersecurity by assisting human system managers with automated monitoring, analysis, and responses to cybersecurity attacks. Conversely, it is essential to guard AI technologies from unintended uses and hostile exploitation by leveraging cybersecurity practices. Research results at the intersection of AI and cybersecurity can help us to be better equipped with tools and techniques to tackle the growing cybersecurity challenges, while also presenting an opportunity to devise fundamentally new ways to motivate and educate students about cybersecurity in the age of AI. Likewise, a June 2019 technical workshop on 'Artificial Intelligence and Cybersecurity: Opportunities and Challenges' noted how the interplay between AI, machine learning, and cybersecurity will continue to introduce new opportunities and challenges in the security of AI as well as AI for cybersecurity. Basic research at the intersection of AI, cybersecurity, and education has the potential to expand existing AI opportunities and resources in cybersecurity education and workforce development. Education efforts are needed to foster workforce knowledge and skills about applying AI expertise to cybersecurity as well as building robust and trustworthy AI. This BOF session will bring together researchers who are interested in these collaborative explorations.","",""
0,"","Neural Network Training Using Genetic Algorithms Series In Machine Perception And Artificial Intelligence",2021,"","","","",111,"2022-07-13 09:20:03","","","","",,,,,0,0.00,0,0,1,"Knowledge-Based Intelligent Information and Engineering Systems 2Nature-inspired Methods in Chemometrics: Genetic Algorithms and Artificial Neural NetworksParallel Implementations of Backpropagation Neural Networks on TransputersEvolutionary Algorithms and Neural NetworksTraining Neural Networks Using Hybrids with Genetic AlgorithmsNeural Network Training Using Genetic AlgorithmsGene Expression ProgrammingTraining a Neural Network with a Genetic AlgorithmMethods and Applications of Artificial IntelligenceClassification and Learning Using Genetic AlgorithmsPractical Computer Vision Applications Using Deep Learning with CNNsIntelligent Hybrid SystemsNeurogenetic LearningAdvances in Neural Networks ISNN 2007Hybrid Intelligent SystemsEncyclopedia of Computer Science and TechnologyMachine LearningUsing a Genetic Algorithm in Training an Artificial Neural Network to Implement the XOR FunctionGenetic and Evolutionary Computation — GECCO 2004Handbook of Fuzzy ComputationNeural Network Data Analysis Using SimulnetTMArtificial Neural Nets and Genetic AlgorithmsApplied Soft Computing Technologies: The Challenge of ComplexityGenetic Algorithm for Artificial Neural Network Training for the Purpose of Automated Part RecognitionNEURAL NETWORKS, FUZZY LOGIC AND GENETIC ALGORITHMAutomatic Generation of Neural Network Architecture Using Evolutionary ComputationPGANETThe Sixth International Symposium on Neural Networks (ISNN 2009)Evolutionary Machine Learning TechniquesModeling Decisions for Artificial IntelligenceEmpirical Studies on the Utility of Genetic Algorithms for Training and Designing of Neural NetworksTraining Neural Networks Using Genetic AlgorithmsTraining feedforward neural networks using genetic algorithmsMetaheuristic Procedures for Training Neural NetworksArtificial Neural Nets and Genetic AlgorithmsNature-Inspired Computing: Concepts, Methodologies, Tools, and ApplicationsApplications of Evolutionary ComputingArtificial Intelligence and CreativityArtificial Neural Nets and Genetic AlgorithmsDeep Learning Using Genetic Algorithms Creativity is one of the least understood aspects of intelligence and is often seen as `intuitive' and not susceptible to rational enquiry. Recently, however, there has been a resurgence of interest in the area, principally in artificial intelligence and cognitive science, but also in psychology, philosophy, computer science, logic, mathematics, sociology, and architecture and design. This volume brings this work together and provides an overview of this rapidly developing field. It addresses a range of issues. Can computers be creative? Can they help us to understand human creativity? How can artificial intelligence (AI) enhance human creativity? How, in particular, can it contribute to the `sciences of the artificial', such as design? Does the new wave of AI (connectionism, geneticism and artificial life) offer more promise in these areas than classical, symbol-handling AI? What would the implications be for AI and cognitive science if computers could not be creative? These issues are explored in five interrelated parts, each of which is introducted and explained by a leading figure in the field. Prologue (Margaret Boden) Part I: Foundational Issues (Terry Dartnall) Part II: Creativity and Cognition (Graeme S. Halford and Robert Levinson) Part III: Creativity and Connectionism (Chris Thornton) Part IV: Creativity and Design (John Gero) Part V: Human Creativity Enhancement (Ernest Edmonds) Epilogue (Douglas Hofstadter) For researchers in AI, cognitive science, computer science, philosophy, psychology, mathematics, logic, sociology, and architecture and design; and anyone interested in the rapidly growing field of artificial intelligence and creativity.From the contents: Neural networks – theory and applications: NNs (= neural networks) classifier on continuous data domains– quantum associative memory – a new class of neuron-like discrete filters to image processing – modular NNs for improving generalisation properties – presynaptic inhibition modelling for image processing application – NN recognition system for a curvature primal sketch – NN based nonlinear temporalspatial noise rejection system – relaxation rate for improving Hopfield network – Oja's NN and influence of the learning gain on its dynamics Genetic algorithms – theory and applications: transposition: a biological-inspired mechanism to use with GAs (= genetic algorithms) – GA for decision tree induction – optimising decision classifications using GAs – scheduling tasks with intertask communication onto multiprocessors by GAs – design of robust networks with GA – effect of degenerate coding on GAs – multiple traffic signal control using a GA – evolving musical harmonisation – niched-penalty approach for constraint handling in GAs – GA with dynamic population size – GA with dynamic niche clustering for multimodal function optimisation Soft computing and uncertainty: self-adaptation of evolutionary constructed decision trees by information spreading – evolutionary programming of near optimal NNsArtificial neural networks and genetic algorithms both are areas of research","",""
1,"L. Bori, M. Valera, D. Gilboa, R. Maor, I. Kottel, J. Remohi, D. Seidman, M. Meseguer","O-084 Computer vision can distinguish between euploid and aneuploid embryos. A novel artificial intelligence (AI) approach to measure cell division activity associated with chromosomal status",2021,"","","","",112,"2022-07-13 09:20:03","","10.1093/humrep/deab125.014","","",,,,,1,1.00,0,8,1,"      Can we distinguish between top-grade euploid and aneuploid embryos by AI measurement of cell edges in time-lapse videos?        Aneuploid embryos can be distinguished from euploid embryos by AI determination of a longer time to blastulation and higher cell activity.        Continuous monitoring of the embryo development has brought out morphokinetic parameters that are used to predict pre-implantation genetic testing (PGT) results. Previous publications showed that euploid embryos reach blastulation earlier than non-euploid embryos. However, time-lapse data are currently under-utilized in making predictions about embryo chromosomal content. AI and computer vision could take advantage of the massive amount of data embedded in the images of embryo development. This is the first attempt to distinguish between euploid and aneuploid embryos by computer vision in an objective and indirect way based on the measurement of cell edges as a proxy for cell activity.        We performed a retrospective analysis of 1,314 time-lapse videos from embryos cultured to the blastocyst stage with PGT results. This single-center study involved two phases; a comparison of the start time of blastulation between euploid (n = 544) and aneuploid embryos (n = 797). In phase two, we designed a novel methodology to examine whether precise measurement of cell edges over time could reflect cell activity differences in blastulation.        We assumed that the delay in blastulation is reflected by higher cell activity that could be determined accurately for the first time using computer vision and machine learning to measure the length of the edges (from t2 to t8). We compared computer vision based measurements of cell edges, reflecting cell number and size, in videos of 231 top-grade euploid (n = 111) and aneuploid (n = 120) embryos.        The mean and standard deviation of blastulation start time was 100.1±6.8 h for euploid embryos and 101.8±8.2 h for aneuploid embryos (p < 0.001). Regarding the measurement of cell activity, a computer vision algorithm identified the edges and provided a certainty score for each edge, higher when the algorithm is more certain that this is a cell edge (as opposed to noise in the images). A threshold was set to distinguish cell edges from noise using this score. The following results for top-grade embryos are shown as the sum of the edge lengths (µm) average of 160 pictures per embryo (frames between t2 and t8). The total length of the cell edges increased from two cells (420±85 µm) to eight cells (861±237 µm), in line with the mitosis events. Both the average total edge measured (450±162 µm for euploid embryos and 489±215 µm for aneuploid embryos, p < 0.01) and the average total of the difference between consecutive frames (135±47 µm for euploid embryos and 153±64 µm for aneuploid embryos, p < 0.01) were higher for aneuploid embryos than for euploid embryos. A regression model to differentiate between the two classes achieved 73% sensitivity and 73% specificity on this dataset.        The main limitation of this study is the difficulty to correlate our findings to other measure of cell activity. A more robust AI function (using not only cell edges lengths) would be required for future analysis to measure the cell activity in cell division up to the blastocyst stage.        Our results show for the first time that an AI based system can precisely measure microscopic cell edges in the dividing embryo. Using this novel method, we could distinguish between euploid and aneuploid embryos. This non-invasive method could further enhance our knowledge of the developing embryo.        Not Applicable ","",""
7,"David K. Spencer, Stephen Duncan, Adam Taliaferro","Operationalizing artificial intelligence for multi-domain operations: a first look",2019,"","","","",113,"2022-07-13 09:20:03","","10.1117/12.2524227","","",,,,,7,2.33,2,3,3,"Artificial Intelligence / Machine Learning (AI/ML) is a foundational requirement for Multi-Domain Operations (MDO). To solve some of MDO’s most critical problems, for example, penetrating and dis-integrating an adversary’s antiaccess/area denial (A2/AD) systems, the future force requires the ability to converge capabilities from across multiple domains at speeds and scales beyond human cognitive abilities. This requires robust, interoperable AI/ML that operates across multiple layers: from optimizing technologies and platforms, to fusing data from multiple sources, to transferring knowledge across joint functions to accomplish critical MDO tactical tasks. This paper provides an overview of ongoing work from the Unified Quest Future Study Plan and other events with the Army’s Futures and Concepts Center to operationalize AI/ML to address MDO problems with this layered approach. It includes insights and required AI/ML capabilities determined with subject matter experts from various organizations at these learning events over the past two years, as well as vignettes that illustrate how AI/ML can be operationalized to enable successful Multi-Domain Operations against a near peer adversary.","",""
7,"D. G. Harkut, K. Kasat","Introductory Chapter: Artificial Intelligence - Challenges and Applications",2019,"","","","",114,"2022-07-13 09:20:03","","10.5772/INTECHOPEN.84624","","",,,,,7,2.33,4,2,3,"Artificial intelligence (AI) is any task performed by program or machine, which otherwise human needs to apply intelligence to accomplish it. It is the science and engineering of making machines to demonstrate intelligence especially visual perception, speech recognition, decision-making, and translation between languages like human beings. AI is the simulation of human intelligence processes by machines, especially computer systems. This includes learning, reasoning, planning, self-correction, problem solving, knowledge representation, perception, motion, manipulation, and creativity. It is a science and a set of computational techniques that are inspired by the way in which human beings use their nervous system and their body to feel, learn, reason, and act. AI is related to machine learning and deep learning wherein machine learning makes use of algorithms to discover patterns and generate insights from the data they are working on. Deep learning is a subset of machine learning, one that brings AI closer to the goal of enabling machines to think and work as human as possible. AI is a debatable topic and is often represented in a negative way; some would call it a blessing in disguise for businesses, while for some it is a technology that endangers the mere existence of humankind as it is potentially capable of taking over and dominating human being, but in reality artificial intelligence has affected our lifestyle either directly or indirectly and shaping the future of tomorrow. AI has already become an intrinsic part of our daily life and has greatly impacted our lifestyle despite the imperative uses of digital assistants of mobile phones, driverassistance systems, the bots, texts and speech translators, and systems that assist in recommending products and services and customized learning. Every emerging technology is a source of both enthusiasm and skepticism. AI is a source of both advantages and disadvantages in different perspectives. However, we need to overcome certain challenges before we can realize the true potential and immense transformational capabilities of this emerging technology. Some of the challenges related to artificial intelligence are:","",""
0,"Yaxin Peng, S. Du, T. Zeng","Preface: Special Issue on Optimization Models and Algorithms in Artificial Intelligence",2019,"","","","",115,"2022-07-13 09:20:03","","10.1007/s40305-019-00278-5","","",,,,,0,0.00,0,3,3,"","",""
44,"Chiara Longoni, Luca Cian","Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect",2020,"","","","",116,"2022-07-13 09:20:03","","10.1177/0022242920957347","","",,,,,44,22.00,22,2,2,"Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).","",""
4,"T. Schmid","Deconstructing the Final Frontier of Artificial Intelligence: Five Theses for a Constructivist Machine Learning",2019,"","","","",117,"2022-07-13 09:20:03","","","","",,,,,4,1.33,4,1,3,"Ambiguity and diversity in human cognition can be regarded a final frontier in developing equivalent systems of artificial intelligence. Despite astonishing accomplishments, modern machine learning algorithms are still hardly more than adaptive systems. Deep neural networks, for example, represent complexity through complex connectivity but are not able to allow for abstraction and differentiation of interpretable knowledge, i.e., for key mechanisms of human cognition. Like support vector machines, random forests and other statistically motivated algorithms, they do neither reflect nor yield structures and strategies of human thinking. Therefore, we suggest to realign the use of existing machine learning tools with respect to the philosophical paradigm of constructivism, which currently is the key concept in human learning and professional teaching. Based on the idea that learning units like classifiers can be considered models with limited validity, we formulate five principles to guide a constructivist machine learning. We describe how to define such models and model limitations, how to relate them and how relationships allow to abstract and differentiate models. To this end, we propose the use of meta data for classifiers and other models. Moreover, we argue that such meta data-based machine learning results in a knowledge base that is both created by the means of automation and interpretable for humans. Over the last decade, it has become widely accepted to address computational systems intelligent. Not only journalists, but also scientists have adapted this habit in their publications. In fact, many classical engineering tasks like monitoring or regulating have profited from the employment of machine learning (Abellan-Nebot and Romero Subirón 2010; Mohanraj, Jayaraj, and Muraleedharan 2012). The same holds true for pattern recognition, most prominently in automated image and video analysis (Zafeiriou, Zhang, and Zhang 2015; Yang et al. 2011). And even though ultimate challenges like the infamous Turing test are left unsatisfied (You 2015), some exceptional results in specialized tasks like playing the game of go (Silver et al. 2016) make current learning machines look intelligent on a human level. Copyright held by the author(s). In A. Martin, K. Hinkelmann, A. Gerber, D. Lenat, F. van Harmelen, P. Clark (Eds.), Proceedings of the AAAI 2019 Spring Symposium on Combining Machine Learning with Knowledge Engineering (AAAI-MAKE 2019). Stanford University, Palo Alto, California, USA, March 25-27, 2019. A final frontier for learning systems, however, is the variety of alternative cognitive functions observable in a diverse set of individuals or from ambiguous stimuli (Kornmeier and Bach 2012). While philosophy has acknowledged and embraced the subjectivity and limitations of human cognition during the last decades (Prawat and Floden 1994), current learning systems regard cognition a complex, yet technical task to be solved. In particular, established algorithms do neither provide convincing answers to the challenges provided by an ambiguous environment; nor do they offer concepts that explicitly allow for contradictory judgements comparable to differences in social perception. The main reason for this shortcoming is that so far both algorithms and researchers have failed to incorporate a constructivist point of view. Constructivism implies not only cognition to be a highly individual phenomenon, but also humans to take an active role in their perception of the world – and that there is no such thing as a human-independent reality (Reich 2009). Yet algorithms and applications aiming to predict things other than laws of nature are implicitly founded on exactly this outdated asumption. In the following, we introduce axioms that allow machine learning to follow constructivist principles. Key features of this approach are the use of modern tools from empirical sciences, model-oriented learning, the ability to handle ambiguity, the ability to integrate supervised and unsupervised learning into a unified framework, the ability to create an individual knowledge base and the ability to abstract, differentiate or discard learned knowledge automatically. 1. The key component of cognitive functionality is a model. Since the introduction of artificial neural networks as a theoretical concept (McCulloch and Pitts 1943), many mathematicians and computer scientists have considered neurons the key component of learning systems. In education and psychology, however, cognitive functions are often seen as certain skills or abilities acquired and exposed by an individual human and described in terms like the concept of competence, which, e.g., is widely used in the modern European education system (Méhaut and Winch 2012). Functionalistic psychology explains cognitive functions of humans by the concept of mental models (Rouse and Morris 1986). Initially, mental models have been used to understand motor control, e.g., of hand movements (Veldhuyzen and Stassen 1977). In a more general sense, however, mental models are described as “hypothetical constructs” (Wickens 2000) that can be ordered hierarchically (Rasmussen 1979) and allow a human to make predictions about his physical and social environment (Oatley 1985). It has also been postulated that such models cannot be of static nature but rather underlay continuous modifications (Oatley 1985). Philosophers, too, consider models an important tool in human knowledge acquisition (Klaus 1967, p. 412) or even the only tool, respectively (Stachowiak 1973, p. 56). While varying and concurring theoretical definitions exist, most model concepts assume an image, an origin of the image and a relationship between them. This definition is, e.g., matched by the idea of mathematical modeling as proposed by Heinrich Hertz and others (Hertz 1894; Hamilton 1982). With the rise of robotics and artificial intelligence, engineers have adapted and extended this idea by postulating the concept of a cybernetic model, which involves a generalized subject and an object of the model (Rose 2009). Cybernetics, however, did neither reflect time-related aspects nor issues involved with individual model subjects. This matter was adressed by Herbert Stachowiak, who was influenced by cybernetics when developing his General Model Theory (Hof 2018). He postulated any model to be limited to specific subjects, specific temporal ranges and specific purposes (Stachowiak 1973, p. 133). Limitations, to this end, are considered a matter of fact rather than a matter of definition. Thus, such models circumvent ambiguity by viewing an otherwise ambiguous model with unknown validity limits as a number of models of limited validity. 2. Learning constitutes from constructing, reconstructing or deconstructing models. Modern education is dominated by the ideas of constructivism and constructivist learning (Fox 2001). At its heart, this approach is based on the assumption that humans acquire knowledge and competences actively and individually through processes called construction, reconstruction and deconstruction (Duffy and Jonassen 1992). Construction is associated with creation, innovation and production and implies searching for variations, combinations or transfers of knowledge (Reich 2004, p. 145). Analogously, reconstruction is associated with application, repetition or imitation and implies searching for order, patterns or models (Reich 2004, p. 145). Deconstruction is in the context of constructivism associated with reconsideration, doubt and modification and implies searching for omissions, additions and defective parts of acquired knowledge (Reich 2004, p. 145). Learning algorithms have been used for half a century to transform sample data into models in a mathematical sense, that is: into generalized mathematical relationships between image and origin. The two major approaches or objectives, known as supervised and unsupervised learning, either do or do not require a given target parameter. Artificial neural networks and their relatives are among the most popular and prominent algorithms for learning with a given target parameter (Singh, Thakur, and Sharma 2016), but statistically motivated approaches like support vector machines (Cristianini and Shawe-Taylor 2000) or random forests (Breiman 2001) are also widely used for supervised learning; a specialized field of supervised learning is reinforcement learning, which is popular in robotics (Kober and Peters 2012) and adaptive control (Lewis, Vrabie, and Vamvoudakis 2012). For unsupervised learning, too, biologically inspired approaches like self-organizing maps (Kohonen 2001) as well as statistically motivated approaches like k-means (Jain 2010) are employed. To some extent, machine learning parallels modern education concepts. A construction process in the constructivist sense may be matched by an unsupervised learning, i.e., identifying clusterings or dimensionality reduction, and can, e.g., be implemented with self-organizing maps, kmeans, autoencoders or feature clustering (Schmid 2018). A reconstruction process in the constructivist sense may be matched by a supervised learning, i.e., classification or regression tasks, and can, e.g., be implemented with artificial neural networks or random forests (Schmid 2018). Few researchers, however, have discussed a constructivist approach to machine learning (Drescher 1989; Quartz 1993), and even less how to design a deconstruction process. While domainspecific applications with manual re-engineering options exist (Herbst and Karagiannis 2000), to the best of our knowledge, there is currently only one working implementation of an algorithmic deconstruction process (Schmid 2018). 3. Deconstructing models computationally requires model-based meta data. In order to automate and implement a deconstruction process, successfully learned models must be held available for comparison or re-training. More over, possible matchings with n","",""
2,"Małgorzata Suchacka, N. Horáková","Towards Artificial Intelligence. Sociological Reflections on the Relationship Man - Organization - Device",2019,"","","","",118,"2022-07-13 09:20:03","","10.2478/czoto-2019-0116","","",,,,,2,0.67,1,2,3,"Abstract The main goal of the study will be to pay attention to technologization of the learning process and its social dimensions in the context of artificial intelligence. The reflection will mainly cover selected theories of learning and knowledge management in the organization and its broadly understood environment. Considering the sociological dimensions of these phenomena is supposed to lead to the emphasis on the importance of the security of the human-organization-device relationship. Due to the interdisciplinary nature of the issue, the article will include references to the concept of artificial intelligence and machine learning. Difficult questions will arise around the ideas and will become the conclusion of the considerations.","",""
4,"T. Moore, D. Cain, B.K.H. Sun, C. Dohner, M. Pereira","Artificial Intelligence: Human Expertise from Machines",1985,"","","","",119,"2022-07-13 09:20:03","","10.1109/mper.1985.5526420","","",,,,,4,0.11,1,5,37,"On-line expert systems can capture and emulate the knowledge, reasoning, and judgment of humans in highly specialized fields. The Electric Power Research Institute has launched a broad-based exploration of potential applications intended to augment the diagnostic and decision-making capabilities of utility personnel. Artificial Intelligence (AI) differs from traditional computer technologies in that it addresses problems of structuring large amounts of knowledge and representing relationships with rules and semantic frameworks rather than with mathematical equations. It is also convergent rather than divergent, generating large amounts of input to establish a few major conclusions. The four AI fields are robotics, natural language understanding, machine vision, and expert computer systems. Integration of these fields will produce the technology of the future. 5 references.","",""
0,"D. Mareschal, Sam Blakeman","Fast and Slow Learning in Human-Like Intelligence",2021,"","","","",120,"2022-07-13 09:20:03","","10.1093/oso/9780198862536.003.0016","","",,,,,0,0.00,0,2,1,"In this chapter we review the extent to which rapid one-short learning or fast-mapping exists in human learning. We find that it exists in both children and adults, but that it is almost always accompanied by slow consolidated learning in which new knowledge is integrated with existing knowledge-bases. Rapid learning is also present in a broad range of non-human species, particularly in the context of high reward values. We argue that reward prediction errors guide the extent to which fast or slow learning dominates, and present a Complementary Learning Systems neural network model (CTDL) of cortical/hippocampal learning that uses reward prediction errors to adjudicate between learning in the two systems. Developing human-like artificial intelligence will require implementing multiple learning and inference systems governed by a flexible control system with an equal capacity to that of human control systems.","",""
0,"E. Cambouropoulos, Maximos A. Kaliakatsos-Papakostas","Cognitive Musicology and Artificial Intelligence: Harmonic Analysis, Learning, and Generation",2021,"","","","",121,"2022-07-13 09:20:03","","10.1007/978-3-030-72116-9_10","","",,,,,0,0.00,0,2,1,"","",""
6,"Herut Uzan, Shira Sardi, A. Goldental, R. Vardi, I. Kanter","Biological learning curves outperform existing ones in artificial intelligence algorithms",2019,"","","","",122,"2022-07-13 09:20:03","","10.1038/s41598-019-48016-4","","",,,,,6,2.00,1,5,3,"","",""
2,"Á. Alberich-Bayarri, A. Pastor, Rafael López González, Fabio García Castro","How to Develop Artificial Intelligence Applications",2019,"","","","",123,"2022-07-13 09:20:03","","10.1007/978-3-319-94878-2_5","","",,,,,2,0.67,1,4,3,"","",""
75,"Qing Sun, Min Zhang, A. Mujumdar","Recent developments of artificial intelligence in drying of fresh food: A review",2019,"","","","",124,"2022-07-13 09:20:03","","10.1080/10408398.2018.1446900","","",,,,,75,25.00,25,3,3,"ABSTRACT Intellectualization is an important direction of drying development and artificial intelligence (AI) technologies have been widely used to solve problems of nonlinear function approximation, pattern detection, data interpretation, optimization, simulation, diagnosis, control, data sorting, clustering, and noise reduction in different food drying technologies due to the advantages of self-learning ability, adaptive ability, strong fault tolerance and high degree robustness to map the nonlinear structures of arbitrarily complex and dynamic phenomena. This article presents a comprehensive review on intelligent drying technologies and their applications. The paper starts with the introduction of basic theoretical knowledge of ANN, fuzzy logic and expert system. Then, we summarize the AI application of modeling, predicting, and optimization of heat and mass transfer, thermodynamic performance parameters, and quality indicators as well as physiochemical properties of dried products in artificial biomimetic technology (electronic nose, computer vision) and different conventional drying technologies. Furthermore, opportunities and limitations of AI technique in drying are also outlined to provide more ideas for researchers in this area.","",""
167,"Max Tegmark","Life 3.0: Being Human in the Age of Artificial Intelligence",2017,"","","","",125,"2022-07-13 09:20:03","","","","",,,,,167,33.40,167,1,5,"New York Times Best Seller How will Artificial Intelligence affect crime, war, justice, jobs, society and our very sense of being human? The rise of AI has the potential to transform our future more than any other technologyand theres nobody better qualified or situated to explore that future than Max Tegmark, an MIT professor whos helped mainstream research on how to keep AI beneficial. How can we grow our prosperity through automation without leaving people lacking income or purpose? What career advice should we give todays kids? How can we make future AI systems more robust, so that they do what we want without crashing, malfunctioning or getting hacked? Should we fear an arms race in lethal autonomous weapons? Will machines eventually outsmart us at all tasks, replacing humans on the job market and perhaps altogether? Will AI help life flourish like never before or give us more power than we can handle? What sort of future do you want? This book empowers you to join what may be the most important conversation of our time. It doesnt shy away from the full range of viewpoints or from the most controversial issuesfrom superintelligence to meaning, consciousness and the ultimate physical limits on life in the cosmos.","",""
1,"B. A.","Informational Linguistics: Computer, Internet, Artificial Intelligence and Language",2019,"","","","",126,"2022-07-13 09:20:03","","","","",,,,,1,0.33,1,1,3,"Modern technological progress is clearly mediated via the informational and communicational conceptualizations, which are first of all of language nature. Interdependence and syncretism of human cognitive activity create unlimited demand for knowledge interpretation of certain semantic format – information. Informational Linguistics is a discipline, dedicated to the interdisciplinary investigation of the specifics of communication contents.","",""
9,"K. Goodman, Diana Zandi, A. Reis, E. Vayena","Balancing risks and benefits of artificial intelligence in the health sector",2020,"","","","",127,"2022-07-13 09:20:03","","10.2471/blt.20.253823","","",,,,,9,4.50,2,4,2,"230 During the last decade, enhanced computing power and the availability of large amounts of data have prompted the practical use of artificial intelligence in health care. Health and medical journals now commonly include reports on machine learning and big data, and descriptions of the risks posed by, and the governance required to manage, this technology. Machine learning algorithms are used to make diagnoses, identify treatments and analyse public health threats, and these systems can learn and improve continuously in response to new data. The tension between risks and concerns on one hand versus potential and opportunity on the other has shaped this issue of the Bulletin of the World Health Organization on the new ethical challenges of artificial intelligence in public health. Data-driven discovery and analysis in health care can increase knowledge and efficiency as well as challenge social values related to privacy, data control and the monetization of personal information. In India, for example, the adoption of a system for assigning all citizens a unique identification number, linking it to individual health records and several health-related schemes, raises ethical, legal and social issues, and the need for an appropriate ethical framework and data governance.1 These issues might be particularly challenging in lowand middle-income countries. Trust is perhaps the overarching theme of the contributions to this issue, and it is indeed one of the central values in digital health. One article explores opportunities for a human-centric ethical and regulatory environment to support the evolution of trust-based artificial intelligence with special regard to health insurance.2 Likewise, trust plays a role along with empathy and compassion in the humane side of care, the importance of which must be preserved in exploring the kind of health care society ought to promote.3 Similarly, European Union guidance might be too context-specific and as such leaves too much room for local, contextualized discretion for it to foster trustworthy artificial intelligence globally.4 In the context of population health research, researchers propose a post-research review model for ethics governance of research using artificial intelligence.5 For mobile health research in behavioural science, machine learning tools pose novel challenges for transparency, privacy, consent and the management of adverse events, all of which point to the need for consensusbased guidelines.6 As use of artificial intelligence systems expands, accountability for harm to patients and responsibility for their safety entail the need for human control and understanding of these systems.7 Other safeguards will require deliberate investments in data quality, access to care and processes to minimize bias, all in the service of trustworthiness.8 Success in integrating artificial intelligence into everyday patient care, as for instance in the United Kingdom of Great Britain and Northern Ireland’s National Health Service, is dependent on transparency, accountability and trust.9 In addition to trust, the values of fairness, justice and equity are seen as posing challenges even if other ethical duties are met. If artificial intelligence systems can explicitly improve equity, it is also a requirement that they do not worsen inequity.10 Thus, the case of neglected tropical diseases in low-resource settings illustrates opportunities for improved public health, as well as new challenges.11 Globally, the potential to help address some shortages and unmet needs in public health and care services might be realized by artificial intelligence-controlled conversational agents or chatbots that give health advice. However, realizing this potential will require the collaborative establishment of best practices and international ethics guidelines for technologies that replace humans.12 The field of bioethics emerged and grew in response to the development of new technologies and, sometimes, related wrongdoing. Ensuring adequate education, governance and ongoing ethical scrutiny will be essential if we are to realize the benefits and minimize the risks of this new technology. Questions of artificial intelligence accountability, equity and inclusiveness remain. The field is quickly evolving, and more artificial intelligence-based applications and services are becoming available in high-income countries. Identifying better tools for benefit-sharing and, simultaneously, evidence-based safeguards and criteria for appropriate uses and users to benefit everyone, including those in middleand lowincome countries, is essential. The World Health Organization (WHO) has made a commitment to addressing ethics, governance and regulation of artificial intelligence for health. In late 2019, WHO established an expert group to help develop a global framework for ethics and governance in artificial intelligence. The goal of this initiative is to ensure that these technologies are aligned with the overarching aims of promoting fair and equitable global health, meeting human rights standards and supporting Member States’ commitments to achieve universal health coverage. ■ Balancing risks and benefits of artificial intelligence in the health sector Kenneth Goodman, Diana Zandi, Andreas Reis & Effy Vayena","",""
8,"Jun Zhu, Hang Su, Bo Zhang","Toward the third generation of artificial intelligence",2020,"","","","",128,"2022-07-13 09:20:03","","10.1360/ssi-2020-0204","","",,,,,8,4.00,3,3,2,"There have been two competing paradigms of artificial intelligence (AI) development since 1956, i.e., symbolism and connectionism (or subsymbolism). Both started at the same time, but symbolism had dominated AI development until the end of the 1980s. Connectionism began to develop in the 1990s and reached its climax at the beginning of this century, and it is likely to displace symbolism. Today, it seems that the two paradigms only simulate the human mind (or brain) in different ways and have their own advantages. True human intelligence cannot be achieved by relying on only one paradigm. Both are necessary to establish a new, explainable, and robust AI theory and method and develop safe, trustworthy, reliable, and extensible AI technology. To this end, it is imperative to combine the two paradigms, and the present article will illustrate this idea. For the sake of description, symbolism, connectionism, and the newly developed paradigm are termed as first-, second-, and third-generation AIs.","",""
0,"S. Cuddy","THE BENEFITS AND DANGERS OF USING ARTIFICIAL INTELLIGENCE IN PETROPHYSICS",2020,"","","","",129,"2022-07-13 09:20:03","","10.30632/spwla-5066","","",,,,,0,0.00,0,1,2,"Abstract Artificial Intelligence, or AI, is a method of data analysis that learns from data, identify patterns and makes predictions with the minimal human intervention. AI is bringing many benefits to petrophysical evaluation. Using case studies, this paper describes several successful applications. The future of AI has even more potential. However, if used carelessly there are potentially grave consequences. A complex Middle East Carbonate field needed a bespoke shaly water saturation equation. AI was used to ‘evolve’ an ideal equation, together with field specific saturation and cementation exponents. One UKCS gas field had an ‘oil problem’. Here, AI was used to unlock the hidden fluid information in the NMR T1 and T2 spectra and successfully differentiate oil and gas zones in real time. A North Sea field with 30 wells had shear velocity data (Vs) in only 4 wells. Vs was required for reservoir modelling and well bore stability prediction. AI was used to predict Vs in all 30 wells. Incorporating high vertical resolution data, the Vs predictions were even better than the recorded logs. As it is not economic to take core data on every well, AI is used to discover the relationships between logs, core, litho-facies and permeability in multi-dimensional data space. As a consequence, all wells in a field were populated with these data to build a robust reservoir model. In addition, the AI predicted data upscaled correctly unlike many conventional techniques. AI gives impressive results when automatically log quality controlling (LQC) and repairing electrical logs for bad hole and sections of missing data. AI doesn’t require prior knowledge of the petrophysical response equations and is self-calibrating. There are no parameters to pick or cross-plots to make. There is very little user intervention and AI avoids the problem of ‘garbage in, garbage out’ (GIGO), by ignoring noise and outliers. AI programs work with an unlimited number of electrical logs, core and gas chromatography data; and don’t ‘fall-over’ if some of those inputs are missing. AI programs currently being developed include ones where their machine code evolves using similar rules used by life’s DNA code. These AI programs pose considerable dangers far beyond the oil industry as described in this paper. A ‘risk assessment’ is essential on all AI programs so that all hazards and risk factors, that could cause harm, are identified and mitigated.","",""
0,"A. Paic","Policies for Artificial Intelligence in Science and Innovation",2020,"","","","",130,"2022-07-13 09:20:03","","10.22323/1.372.0045","","",,,,,0,0.00,0,1,2,"This contribution synthesizes the discussions of the special session on policies for Artificial Intelligence in Science and Innovation, organized by the OECD’s Directorate for Science, Technology and Innovation. The session was opened by Dr Judith Arrieta, Minister of the Foreign Service at the Chief of Staff’s Office of the Secretary of Foreign Affairs of Mexico, and the two panels included speakers from governments, industry and civil society from European countries, USA,Canada China and Australia. Participants discussed the disruptive nature of AI and the formidable challenges it poses. Most of the discussion focused under the umbrella title of ethics, but they span very different issues of human-centered values, fairness, transparency, explainability, and many more. Other challenges include employment, education, SME policy, enabling environment, access to data and computing technology. Responses by governments were also discussed with a particular focus on national strategies, whose main pillars are oriented toward knowledge creation through AI research, knowledge diffusion through linkages to the private sector, development of human capital which will underpin the development of the sector, and a strong values, ethical and regulatory framework to create the conditions for the development of trustworthy AI.  In a world of finite resources, discussants concluded that one cannot apply very stringent requirements to all AI decisions, and there is clearly a need to require more transparency, explainability and robustness from systems which have the greatest impact on human lives. Therefore an approach based on algorithmic impact assessment seems reasonable. Such an approach needs to be further developed and standardized.","",""
0,"Shivali Agarwal, Jayachandu Bandlamudi, Atri Mandal, Anupama Ray, G. Sridhara","Automated Assignment of Helpdesk Email Tickets: An Artificial Intelligence Life-Cycle Case Study",2020,"","","","",131,"2022-07-13 09:20:03","","","","",,,,,0,0.00,0,5,2,"Copyright © 2020, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 Fall 2020 45 The landscape of modern information technology service delivery is changing, with increased focus on automation and optimization. Most information technology vendors today have service platforms aimed toward end-to-end automation for carrying out mundane, repetitive labor-intensive tasks and even for tasks requiring human cognizance. One such task is ticket assignment and dispatch, where the service requests submitted by the end-users to the vendor in the form of tickets are reviewed by a centralized dispatch team and assigned to the appropriate service team and resolver group. The dispatch of a ticket to the correct group of practitioners is a critical step in the speedy resolution of a ticket. Incorrect dispatch decisions can significantly increase the total turnaround time for ticket resolution, as observed in a study of an actual production system (agarwal, Sindhgatta, and Sengupta 2012). When such delays occur, it causes customer dissatisfaction as well as monetary penalties for the vendor due to service-level-agreement breaches. Several factors make the dispatcher’s job challenging, namely the need for in-depth knowledge of the roles and responsibilities of various groups, the heterogeneous and informal nature of email text, and the high attrition rate in service delivery teams (Mandal et al. 2018). Given the fact that inefficiencies in dispatch have serious business consequences, there has been a lot of interest in automating the assignment process. a number of different approaches have been proposed for automating ticket dispatch (agarwal, Sindhgatta, and Sengupta 2012; Shao et al. 2008a, 2008b; Parvin, Bose, and Van Oyen 2009).  In this article, we present an endto-end automated helpdesk email ticket assignment system driven by high accuracy, coverage, business continuity, scalability, and optimal usage of computational resources. The primary objective of the system is to determine the problem mentioned in an incoming email ticket and then automatically dispatch it to an appropriate resolver group with high accuracy. While meeting this objective, it should also meet the objective of being able to operate at desired accuracy levels in the face of changing business needs by automatically adapting to the changes. The proposed system uses a system of classifiers with separate strategies for handling frequent and sparse resolver groups augmented with a semiautomatic rule engine and retraining strategies to ensure that it is accurate, robust, and adaptive to changing business needs. Our system has been deployed in the production of six major service providers in diverse service domains and currently assigns 100,000 emails per month, on an average, with an accuracy close to ninety percent and covering at least ninety percent of email tickets. This translates to achieving human-level accuracy and results in a net savings of more than 50,000 man-hours of effort per annum. To date, our deployed system has already served more than two million tickets in production. Automated Assignment of Helpdesk Email Tickets: An Artificial Intelligence Life-Cycle Case Study","",""
169,"D. Pinto dos Santos, D. Giese, S. Brodehl, S. Chon, W. Staab, R. Kleinert, D. Maintz, B. Baessler","Medical students' attitude towards artificial intelligence: a multicentre survey",2018,"","","","",132,"2022-07-13 09:20:03","","10.1007/s00330-018-5601-1","","",,,,,169,42.25,21,8,4,"","",""
19,"Thomas G. Dietterich","Robust artificial intelligence and robust human organizations",2018,"","","","",133,"2022-07-13 09:20:03","","10.1007/s11704-018-8900-4","","",,,,,19,4.75,19,1,4,"","",""
82,"T. Davenport","The AI Advantage: How to Put the Artificial Intelligence Revolution to Work",2018,"","","","",134,"2022-07-13 09:20:03","","","","",,,,,82,20.50,82,1,4,"Cutting through the hype, a practical guide to using artificial intelligence for business benefits and competitive advantage.In The AI Advantage, Thomas Davenport offers a guide to using artificial intelligence in business. He describes what technologies are available and how companies can use them for business benefits and competitive advantage. He cuts through the hype of the AI craze?remember when it seemed plausible that IBM's Watson could cure cancer??to explain how businesses can put artificial intelligence to work now, in the real world. His key recommendation: don't go for the ?moonshot? (curing cancer, or synthesizing all investment knowledge); look for the ?low-hanging fruit? to make your company more efficient.Davenport explains that the business value AI offers is solid rather than sexy or splashy. AI will improve products and processes and make decisions better informed?important but largely invisible tasks. AI technologies won't replace human workers but augment their capabilities, with smart machines to work alongside smart people. AI can automate structured and repetitive work; provide extensive analysis of data through machine learning (?analytics on steroids?), and engage with customers and employees via chatbots and intelligent agents. Companies should experiment with these technologies and develop their own expertise.Davenport describes the major AI technologies and explains how they are being used, reports on the AI work done by large commercial enterprises like Amazon and Google, and outlines strategies and steps to becoming a cognitive corporation. This book provides an invaluable guide to the real-world future of business AI.A book in the Management on the Cutting Edge series, published in cooperation with MIT Sloan Management Review.","",""
0,"A. Barrie, R. Smith, C. Hickman, I. Erlich, A. Campbell","P-287 An assessment of agreement between automated embryo annotation, through artificial intelligence, and manual embryo annotation",2022,"","","","",135,"2022-07-13 09:20:03","","10.1093/humrep/deac107.276","","",,,,,0,0.00,0,5,1,"      How strong is the agreement between embryo morphokinetic annotations performed by experienced embryologists compared to an automated embryo annotation system based on artificial intelligence (AI)?        Agreement between manual and automated annotation as determined by the interclass correlation coefficient (ICC) revealed strong or very strong agreement for all analysed morphokinetic variables.        Transitioning from time-lapse imaging to embryo selection for transfer, freezing or discard involves annotation; the action of converting images to numerical data. Numerical data can be used as input to selection models quantifying embryo viability. Currently, embryos are manually annotated by the embryologist which can be subjective and time-consuming. As such, clinics prioritise a manageable number of variables to annotate, leading to a range of clinic practices. There is the additional challenge of operator variation, despite the development of standardised definitions and quality assurance schemes. AI may help resolve these challenges.        Retrospective comparative analysis, including 2442 embryos from IVF and ICSI cycles, from four private fertility clinics belonging to the same group in the UK. All the embryos cultured in a time-lapse incubator (EmbryoScope,Vitrolife) between January 2016 and 2019 were included in the study. Manual annotations (MA) versus automated annotations (AA) were compared using a two-way, mixed interclass correlation coefficient (ICC), which produced five categories of agreement, very weak(0-0.20), weak(0.21-0.40), moderate(0.41-0.60), strong(0.61-0.80), very strong(0.81-1.00).        Videos were manually annotated by experienced embryologists from pronuclei fading (tPNf) to time of expanded blastocyst (tEB) with all cell stages annotated in between (time to two-cell (t2), three-cell (t3), four-cell (t4), five-cell (t5), six-cell (t6), seven-cell (t7), eight-cell (t8), nine-cell (t9), morula (tM), start of blastulation (tSB) and full blastocyst (tB)). Blind to human annotations, and without any training, the same videos were annotated by CHLOE (Fairtility) to produce automated annotation data.        Of the expected annotations, AA did not provide a result for 15.4% of the MA(3235/21008). Very strong agreement(0.81-1.00) between MA and AA was found for tPNf, t2, t3, t5, t6, tM, tSB, tB, tEB. Strong agreement(0.61-0.80) was found for t4, t7, t8 and t9+. Outliers in the AA data, defined as one standard deviation from the MA, were interrogated further for five key morphokinetic parameters; t2, t5, t8, tSB and tB. A total of 269 outliers were identified.  For t2 outliers(n = 14,6%), the average time difference was 5.97h(range;5.50-24.44h). All embryos with a t2 outlier were classed as either poor(PQ) or average quality(AQ).  The t5 outliers(n = 45,19%) had an average time difference of 2.84h(range;9.33-36.69h). 96%(n = 43) of these embryos were classed as PQ(n = 25,56%) or AQ(n = 18,40%).  Outliers for t8(138,58%) were, on average, 17.53h different between MA and AA(range;12.68-40.35h). 94%(n = 130)of these embryos were classed as PQ(n = 77,56%) or AQ(n = 53,38%).  The tSB outliers(n = 28,12%) had an average time difference of 3.58h(range;0.71-14.39h). 89%(n = 25) of these embryos were classed as PQ(n = 16,57%) or AQ(n = 9,32%).  Finally, outliers associated with tB(n = 44,18%) had an average time difference of 6.39h(range;0.02-33.67h). 95%(n = 42) of these embryos were classed as PQ(n = 38,86%) or AQ(n = 4,9%).  Almost 15%(n = 40) of the embryos had outliers in more than one of the five morphokinetic parameters.        The findings for this study reflect the capabilities of a specific AI-based annotation algorithm against the practice in multiple clinics in the same group and country. The automated annotation algorithm was not trained on this dataset prior to validation, which is encouraging for generalisability.        AI is ideally suited to resolve annotation challenges. This study demonstrates that where embryo quality is poor, annotation could be skewed both when performed manually and automatically. Once robustness is demonstrated, AI tools such as CHLOE, may allow clinics to process clinical data efficiently, objectively and consistently.        None ","",""
73,"Valentina Bellemo, Gilbert Lim, T. Rim, G. Tan, C. Cheung, S. Sadda, M. He, A. Tufail, M. Lee, W. Hsu, D. Ting","Artificial Intelligence Screening for Diabetic Retinopathy: the Real-World Emerging Application",2019,"","","","",136,"2022-07-13 09:20:03","","10.1007/s11892-019-1189-3","","",,,,,73,24.33,7,11,3,"","",""
14,"Tao Xie","The synergy of human and artificial intelligence in software engineering",2013,"","","","",137,"2022-07-13 09:20:03","","10.1109/RAISE.2013.6615197","","",,,,,14,1.56,14,1,9,"To reduce human efforts and burden on human intelligence in software-engineering activities, Artificial Intelligence (AI) techniques have been employed to assist or automate these activities. On the other hand, human's domain knowledge can serve as starting points for designing AI techniques. Furthermore, the results of AI techniques are often interpreted or verified by human users. Such user feedback could be incorporated to further improve the AI techniques, forming a continuous feedback loop. We recently proposed cooperative testing and analysis including human-tool cooperation (consisting of human-assisted computing and human-centric computing) and human-human cooperation. In this paper, we present example software-engineering problems with solutions that leverage the synergy of human and artificial intelligence, and illustrate how cooperative testing and analysis can help realize such synergy.","",""
125,"M. Matheny, D. Whicher, Sonoo Thadaney Israni","Artificial Intelligence in Health Care: A Report From the National Academy of Medicine.",2019,"","","","",138,"2022-07-13 09:20:03","","10.1001/jama.2019.21579","","",,,,,125,41.67,42,3,3,"The promise of artificial intelligence (AI) in health care offers substantial opportunities to improve patient and clinical team outcomes, reduce costs, and influence population health. Current data generation greatly exceeds human cognitive capacity to effectively manage information, and AI is likely to have an important and complementary role to human cognition to support delivery of personalized health care.1 For example, recent innovations in AI have shown high levels of accuracy in imaging and signal detection tasks and are considered among the most mature tools in this domain.2 However, there are challenges in realizing the potential for AI in health care. Disconnects between reality and expectations have led to prior precipitous declines in use of the technology, termed AI winters, and another such event is possible, especially in health care.3 Today, AI has outsized market expectations and technology sector investments. Current challenges include using biased data for AI model development, applying AI outside of populations represented in the training and validation data sets, disregarding the effects of possible unintended consequences on care or the patientclinician relationship, and limited data about actual effects on patient outcomes and cost of care. AI in Healthcare: The Hope, The Hype, The Promise, The Peril, a publication by the National Academy of Medicine (NAM), synthesizes current knowledge and offers a reference document for the responsible development, implementation, and maintenance of AI in the clinical enterprise.4 The publication outlines current and near-term AI solutions; highlights the challenges, limi-","",""
45,"L. Lynn","Artificial intelligence systems for complex decision-making in acute care medicine: a review",2019,"","","","",139,"2022-07-13 09:20:03","","10.1186/s13037-019-0188-2","","",,,,,45,15.00,45,1,3,"","",""
37,"C. Kulikowski","Beginnings of Artificial Intelligence in Medicine (AIM): Computational Artifice Assisting Scientific Inquiry and Clinical Art – with Reflections on Present AIM Challenges",2019,"","","","",140,"2022-07-13 09:20:03","","10.1055/s-0039-1677895","","",,,,,37,12.33,37,1,3,"Summary Background : The rise of biomedical expert heuristic knowledge-based approaches for computational modeling and problem solving, for scientific inquiry and medical decision-making, and for consultation in the 1970’s led to a major change in the paradigm that affected all of artificial intelligence (AI) research. Since then, AI has evolved, surviving several “winters”, as it has oscillated between relying on expensive and hard-to-validate knowledge-based approaches, and the alternative of using machine learning methods for inferring classification rules from labelled datasets. In the past couple of decades, we are seeing a gradual but progressive intertwining of the two. Objectives : To give an overview of early directions in AI in medicine and threads of some subsequent developments motivated by the very different goals of scientific inquiry for biomedical research, and for computational modeling of clinical reasoning and more general healthcare problem solving from the perspective of today’s “AI-Deep Learning Boom”. To show how, from the beginning, AI was central to Biomedical and Health Informatics (BMHI), as a field investigating how to understand intelligent thinking in dealing professionally with the practice for healthcare, developing mathematical models, technology, and software tools to aid human experts in biomedicine, despite many previous bouts of “exuberant optimism” about the methodologies deployed. Methods : An overview and commentary on some of the early research and publications in AI in biomedicine, emphasizing the different approaches to the modeling of problems involved in clinical practice in contrast to those of biomedical science. A concluding reflection of a few current challenges and pitfalls of AI in some biomedical applications. Conclusion : While biomedical knowledge-based systems played a critical role in influencing AI in its early days, 50 years later they have taken a back seat behind “Deep Learning” which promises to discover knowledge structures for inference and prediction, both in science and for clinical decision-support. Early work on AI for medical consultation turned out to be more useful for explanation and teaching than for clinical practice, as had been originally intended. Today, despite the many reported successes of deep learning, fundamental scientific challenges arise in drawing on models of brain science, cognition, and language, if AI is to augment and complement rather than replace human judgment and expertise in biomedicine while also incorporating these advances for translational medicine. Understanding clinical phenotypes and how they relate to precision and personalization of care requires not only scientific inquiry, but also humanistic models of treatment that respond to patient and practitioner narrative exchanges, since it is the stories and insights of human experts which encourage what Norbert Weiner termed the ethical “human use of human beings”, so central to adherence to the Hippocratic Oath","",""
1,"A. Admin, Dr.P Dr.P.Kavitha2, A. Akshaya, P. P.Shalin, R. R.Ramya","A Survey on Cyber Security Meets Artificial Intelligence: AI– Driven Cyber Security",2022,"","","","",141,"2022-07-13 09:20:03","","10.54216/jchci.020202","","",,,,,1,1.00,0,5,1,"The computerized version of human intelligence is Artificial Intelligence(AI). Artificial Intelligence systems combine large sets of data with intelligent and iterative processing algorithms in order to make predictions, based on patterns and features in the data that they analyse. With the booming technologies such as IOT and Cloud Computing, huge amounts of data are generated and collected that require cyber security protection today. There is a growing need for cyber security methods which are both robust and intelligent due to the ever-increasing complexity of cyber crimes. While data can be used to benefit business interests, it poses a number of challenges in terms of security and privacy protection. Artificial Intelligence (AI) based technologies, such as machine learning statistics, big data analysis, deep learning and so on, have been used to deal with cyber security threats. These technologies are used for intrusion detection systems, malicious software detection, and encrypted communications. In the rapidly growing field of AI driven security, scientists from multiple disciplines work together to combat cyber threats. AI models require unique cyber security defence and protection technologies. This survey provides various method, different datasets and methodologies that may be used for the proposed IA enabled cyber security technologies. This study aims to classify the AI-based cyber security solutions gathered and describe how they can help solve problems in the field of cyber security.","",""
0,"K. Sfakianoudis, E. Maziotis, S. Grigoriadis, A. Pantou, G. Kokkini, A. Trypidi, I. Angeli, T. Vaxevanoglou, K. Pantos, M. Simopoulou","O-122 Reporting on the value of Artificial Intelligence in predicting the optimal embryo for transfer: A systematic review and meta-analysis",2022,"","","","",142,"2022-07-13 09:20:03","","10.1093/humrep/deac105.022","","",,,,,0,0.00,0,10,1,"      Are Artificial Intelligence (AI) based models effective in robustly predicting in vitro fertilization (IVF) outcome by assessing embryo quality?        The majority of the AI-based models could provide an accurate prediction regarding live birth, clinical pregnancy, clinical pregnancy with fetal heartbeat and embryo ploidy status.        Precision and consistency in embryo quality evaluation are of paramount importance regarding the outcome of an IVF cycle. Numerous embryo grading and evaluation systems, employing morphological and morphokinetical assessment, have been proposed but without reaching a consensus yet. The main limitation of the aforementioned assessment systems is that they depend on human evaluation, which may be subject to subjectivity and interobserver variation. Thus, automated prediction models may be essential to optimize objectivity and reliability of embryo grading. Artificial neural network models may process microscopy images or time-lapse videos as input to predict the embryos’ potential competency.        A systematic review and meta-analysis including 18 published studies. The population consists of preimplantation embryos suitable for embryo transfer in IVF/ICSI cycles following employment of an AI-based prediction model. The outcome measures are prediction of live birth, clinical pregnancy, clinical pregnancy with heartbeat and ploidy status.        A systematic search of the literature was performed in the databases of Pubmed/Medline, Embase, and Cochrane Central Library limited to articles published in English up to August 2021. The initial search yielded a total of 694 studies with 97 of them being duplicates and other 579 being excluded on the grounds of not fulfilling inclusion criteria. Following full-text screening and citation mining a total of 18 studies were identified to be eligible for inclusion.        Four studies reported on prediction of live birth. The sensitivity was 70.6% (95%C.I.: 38.1-90.4%) and specificity was 90.6% (95%C.I.:79.3-96.1%).  The Area Under the Curve (AUC) of the Summary Receiver Operating Characteristics (SROC) curve was 0.905, while the partial AUC (pAUC) was 0.755. Employing the Bayesian approach, the total Observed:Expected ratio (O:E) was 1.12 (95%CI: 0.26–2.37; 95%PI:0.02-6.54). Ten studies reported on prediction of clinical pregnancy. The sensitivity and the specificity were 71% (95%C.I.: 58.1-81.2%) and 62.5% (95%C.I.: 47.4-75.5%) respectively. The AUC was 0.716, while pAUC was 0.693. Moreover, the total O:E ratio was 0.92 (95%CI: 0.61–1.28; 95%PI:0.13-2.43). Eight studies reported on prediction of clinical pregnancy with fetal heartbeat the sensitivity was 75.2% (95%C.I.: 66.8-82%) and the specificity was 55.3% (95%C.I.: 41.2-68.7%). The AUC was 0.722, while the pAUC was 0.774. The O:E ratio was 0.77 (95%CI: 0.54 – 1.05; 95%PI: 0.21-1.62). Four studies reported on the ploidy status of the embryo. The sensitivity and specificity were 59.4% (95%C.I.: 45.0-73.1%) and 79.2% (95%C.I.: 70.1-86.1%) respectively. The AUC was 0.751 and the pAUC was 0.585. The total O:E ratio was 0.86 (95%CI: 0.42 – 1.27; 95%PI: 0.03-1.83).        The limited number of studies fulfilling inclusion criteria, along with the different designs applied when developing AI models which may lead to increased heterogeneity, stand as limitations. Inclusion of women regardless of their age presents as another limitation, as advanced maternal age has been associated with diminished IVF outcomes.        Albeit, our findings support that AI is a highly promising tool in the era of personalized medicine providing precise predictions it does not appear to considerably surpass human prediction capabilities. More studies and more collaborations between the developers are of paramount importance prior to AI becoming the gold standard.        Not applicable ","",""
0,"I. Zvereva, K. Dmitry","P-256 The influence of artificial intelligence embryo scoring on male-female sex ratio",2022,"","","","",143,"2022-07-13 09:20:03","","10.1093/humrep/deac107.246","","",,,,,0,0.00,0,2,1,"      Are there correlations among male-female sex ratio, human blastocyst ploidy status and artificial intelligence (AI)-based morphokinetics embryo selection?        Embryo selection based on morphological evaluation by time-lapse system (TLS) with AI technology could lead to a female-biased sex ratio of resulting newborns.        As of now, there have been only limited attempts to evaluate how AI-based TLS embryo selection for priority transfer could affect male-to-female sex ratio in human population, and the results of different publications were contradicting. However, the morphokinetic assessment was made without calculating the embryos KID Score (Embryos with Known Implantation Data), which significantly improves and make faster the decision-making process.        This is a monocentric, retrospective study from October 2019 to December 2021 including 251 blastocysts with PGT-A results. Embryos were cultured in time-lapse incubator (EmbryoScope, Vitrolife) up to the time of trophectoderm biopsy. All embryos were evaluated based on the KIDscoreTM D5 algorithm (Vitrolife) under routine supervision by experienced embryologists. The PGT-A results were obtained by using next-generation sequencing (NGS) platform from Medical Genomics LLC laboratory (Illumina MiSeq, Illumina).        Sample size was 251 embryos from 101 women (mean female age was 36.0 ± 5.6 years). All embryos were divided in four groups in accordance with their final KID score: <2.5 ( n = 7), 2.6-5.0 ( n = 33), 5.1-7.5 ( n = 123) and >7.5 ( n = 88). The embryos with sex chromosome abnormalities were also included in research to assess the frequency of occurrence in embryos with low and high KID score.        As expected, the percentage of aneuploid blastocysts, as well as the rate of sex chromosome abnormalities, decreased with increasing the embryo KID score. The highest male-female sex ratio among all embryos was observed for the group with KID score <2.5 (1.33), and gradually decreased to values of 0.92 and 0.74 in groups with KID score 5.1-7.5 and >7.5, respectively. At the same time, the highest male-female sex ratio among euploid blastocysts was maximal in the group with KID score 2.6-5.0. The obtained data contradict results of some other studies, which reported faster development of male embryos (which should mean their higher KID score). However, the KID score was not evaluated in them, and thus these results cannot be directly compared to ours.        Most patients in this study had complicated reproductive history, with repeated failures in IVF programs, often with a stop in embryo development. Also, the present investigation is retrospective. A following multicenter researches with larger sample size and cross-centered validation of embryologist-performed annotation is considered in our future approach.        Obtained data doesn’t allow to establish the female-gender prevalence among embryos. Nevertheless, further accumulation of knowledge about relation between KID Embryo Score and embryo gender can be used for presumptive sex determination in special cases with sex-linked diseases, where poor embryo morphology doesn’t allow to perform biopsy for genetic analysis.        - ","",""
0,"F. Renna, Miguel L. Martins, Alexandre Neto, António Cunha, D. Libânio, M. Dinis-Ribeiro, M. Coimbra","Artificial Intelligence for Upper Gastrointestinal Endoscopy: A Roadmap from Technology Development to Clinical Practice",2022,"","","","",144,"2022-07-13 09:20:03","","10.3390/diagnostics12051278","","",,,,,0,0.00,0,7,1,"Stomach cancer is the third deadliest type of cancer in the world (0.86 million deaths in 2017). In 2035, a 20% increase will be observed both in incidence and mortality due to demographic effects if no interventions are foreseen. Upper GI endoscopy (UGIE) plays a paramount role in early diagnosis and, therefore, improved survival rates. On the other hand, human and technical factors can contribute to misdiagnosis while performing UGIE. In this scenario, artificial intelligence (AI) has recently shown its potential in compensating for the pitfalls of UGIE, by leveraging deep learning architectures able to efficiently recognize endoscopic patterns from UGIE video data. This work presents a review of the current state-of-the-art algorithms in the application of AI to gastroscopy. It focuses specifically on the threefold tasks of assuring exam completeness (i.e., detecting the presence of blind spots) and assisting in the detection and characterization of clinical findings, both gastric precancerous conditions and neoplastic lesion changes. Early and promising results have already been obtained using well-known deep learning architectures for computer vision, but many algorithmic challenges remain in achieving the vision of AI-assisted UGIE. Future challenges in the roadmap for the effective integration of AI tools within the UGIE clinical practice are discussed, namely the adoption of more robust deep learning architectures and methods able to embed domain knowledge into image/video classifiers as well as the availability of large, annotated datasets.","",""
0,"E. Nikitos, T. Triantafillou, K. Dimitropoulos, V. Kallergi, P. Psathas, I. Erlich, A. Ben-Meir, N. Bergelson","P-271 Challenges with comparing different commercially available Artificial Intelligence (AI) systems on the same data set of time-lapse selected euploid blastocysts",2022,"","","","",145,"2022-07-13 09:20:03","","10.1093/humrep/deac107.260","","",,,,,0,0.00,0,8,1,"      To identify challenges in choosing a robust AI following comparative validation with data already pre-selected with established embryos selection tools: blastulation, morphology, time-lapse, PGTA.        Challenges included: bias; assessment against outcomes AI models were not trained on; performance metrics prioritisation; statistical methodology; continuous data cutoffs for binary clinical decision making.        AI is commercially available to be incorporated into routine practice to support embryo selection decision-making. Different clinical practices and demographics are used to train AI models, potentially impacting the prediction efficacy of the same model when used in different clinics. Fertility professionals require robust methods of validation to responsibly implement AI-based tools. Unbiased and robust frameworks for comparing AI systems in the same dataset are needed. Validating AI in a dataset of time-lapse selected euploid blastocysts using all the current methods of embryo selection currently available is the toughest assessment possible and has not previously been performed.        This study uses a retrospectively timelapse dataset collected from 2018-2021 at a single private fertility clinic. The dataset included 915 blastocysts which underwent PGTA (913 results: 381 euploids, 528 aneuploids, 4 mosaics) and 46 euploids transferred with known bhcg and ongoing clinical outcome (of which 40 resulted to live birth).  Following a prospective, comparative, observational, cohort study design, blastocysts were blindly scored using the CHLOE(FAIRTILITY) and another commercially available AI system, referred to as ‘AI-2’.        Patients aged 24-47years (average 35.4). Blastocysts selected for biopsy and transfer based on morphology and KIDScore(Vitrolife). Both AI systems were tested in the data set blindly, without any training. Correlation Regression analysis assessed correlation with KIDSCORE and relative to each AI system. Efficacy of prediction (using metrics AUC, Accuracy, Sensitivity, Specificity and Informedness) of outcomes (ploidy, biochemical and clinical pregnancy) were assessed for both AI models (CHLOEvsAI-2) by two independent statisticians to establish significance.        Regression analysis demonstrated no correlation between KIDSCORE and AI-2(r2=0.3%,p=0.5) or between CHLOE(FAIRTILITY) and AI-2(r2=0.03%,p=0.9). CHLOE(FAIRTILITY) correlated with KIDSCORE(r2=29%,p<0.001).  AI-2 was not predictive of ploidy (Euploids vs Aneuploids+mosaic: AUC=0.5,p=0.6). CHLOE(Fairtility) was predictive of ploidy(AUC=0.66, p<0.001).  Neither AI-2 or CHLOE(Fairtility) predicted which embryo the human embryologist prioritised for transfer (AI-2 vs CHLOE:accuracy:0.31vs0.49, p<0.00001). Neither AI-2 nor CHLOE(Fairtility) predicted which embryo the human embryologist prioritised for transfer (AI-2 vs CHLOE:accuracy: 0.31vs0.49, p<0.00001). There was no difference detected in efficacy of prediction of biochemical (accuracy:0.52vs0.67,NS) and ongoing clinical pregnancy (accuracy:0.53 vs 0.78,NS) by AI-2 or CHLOE. This is partly due to the low number of euploid transfers assessed (n = 46), and partly due to the fact that neither of these algorithms are trained specifically on predicting outcome of euploid transfers.  CHLOE(Fairtility) was more specific than AI-2 for predicting selection for transfer(0.44/0.80vs0.17/0.93,p<0.05/NS) and ploidy(0.54/0.77vs0.23/0.87,p<0.05/NS), and they were equally as sensitive. CHLOE(Fairtility) was more sensitive, and less specific than AI-2 for predicting biochemical pregnancy(0.36/0.81vs0.86/0.38,p<0.05) and more sensitive but equally as specific for predicting clinical pregnancy(0.33/0.88vs0.83/0.46,NS/p<0.05).  Informedness was positive for both CHLOE(Fairtility) and AI-2 in predicting all outcomes assessed. Informedness was greater for AI-2 for predicting morphology(AI-2vsCHLOE:0.16vs0.31,p<0.05), transfer(0.11vs0.24,p<0.05), ploidy(0.10vs0.31,p<0.05) and equivalent for predicting biochemical (0.23vs0.17,NS) and clinical pregnancy(0.29vs0.22,NS).        In this single clinic study, both algorithms were assessed against outcomes (live birth following transfer of time-lapse cultured euploid blastocysts) for which they were not trained on: AI-2(designed for ploidy prediction) and CHLOE(FAIRTILITY, implantation prediction of non-PGTA embryos) and no clinic data was used for training.        The only way to decide which AI model is more useful is by a direct comparison of two or more models on the same dataset with same outcomes and metrics, as recommended by TRIPOD. To date, this is the first publication comparing multiple commercial AI models on the same dataset.        NA ","",""
21,"Li-Qi Shu, Yi-Kan Sun, L. Tan, Q. Shu, A. Chang","Application of artificial intelligence in pediatrics: past, present and future",2019,"","","","",146,"2022-07-13 09:20:03","","10.1007/s12519-019-00255-1","","",,,,,21,7.00,4,5,3,"","",""
23,"B. Chin-Yee, Ross E. G. Upshur","Three Problems with Big Data and Artificial Intelligence in Medicine",2019,"","","","",147,"2022-07-13 09:20:03","","10.1353/pbm.2019.0012","","",,,,,23,7.67,12,2,3,"ABSTRACT:The rise of big data and artificial intelligence (AI) in health care has engendered considerable excitement, claiming to improve approaches to diagnosis, prognosis, and treatment. Amidst the enthusiasm, the philosophical assumptions that underlie the big data and AI movement in medicine are rarely examined. This essay outlines three philosophical challenges faced by this movement: (1) the epistemological-ontological problem arising from the theory-ladenness of big data and measurement; (2) the epistemological-logical problem resulting from the inherent limitations of algorithms and attendant issues of reliability and interpretability; and (3) the phenomenological problem concerning the irreducibility of human experience to quantitative data. These philosophical issues demonstrate several important challenges for these technologies that must be considered prior to their integration into clinical care. Our article aims to initiate a critical dialogue on the impact of big data and AI in health care in order to allow for more robust evaluation of these technologies and to aid in the development of approaches to clinical care that better serve clinicians and their patients.","",""
11,"Wen-qian Sun, Xing Gao","The Construction of Undergraduate Machine Learning Course in the Artificial Intelligence Era",2018,"","","","",148,"2022-07-13 09:20:03","","10.1109/ICCSE.2018.8468758","","",,,,,11,2.75,6,2,4,"Machine learning technology has been greatly developed in the last decade, which makes artificial intelligence reach a revolutionary breakthrough and lets us really perceive the potential of artificial intelligence in changing human life. In order to improve the understanding and application ability of artificial intelligence, carrying out the corresponding machine learning course is of significance for the students during the undergraduate period. This paper probes into the teaching content, teaching form and other aspects of the undergraduate machine learning course based on this issue and proposes a teaching method driven by application scenarios to guide the undergraduate students to understand the development, current situation and frontier technology of machine learning. In the experimental design, the students' theoretical knowledge is fully considered, the practical questions are simplified, and the students' ability to think and solve problems is also raised, so as to lay a theoretical and practical basis for further study of machine learning.","",""
42,"George Gadanidis","Artificial intelligence, computational thinking, and mathematics education",2017,"","","","",149,"2022-07-13 09:20:03","","10.1108/IJILT-09-2016-0048","","",,,,,42,8.40,42,1,5,"Purpose          The purpose of this paper is to examine the intersection of artificial intelligence (AI), computational thinking (CT), and mathematics education (ME) for young students (K-8). Specifically, it focuses on three key elements that are common to AI, CT and ME: agency, modeling of phenomena and abstracting concepts beyond specific instances.          Design/methodology/approach          The theoretical framework of this paper adopts a sociocultural perspective where knowledge is constructed in interactions with others (Vygotsky, 1978). Others also refers to the multiplicity of technologies that surround us, including both the digital artefacts of our new media world, and the human methods and specialized processes acting in the world. Technology is not simply a tool for human intention. It is an actor in the cognitive ecology of immersive humans-with-technology environments (Levy, 1993, 1998) that supports but also disrupts and reorganizes human thinking (Borba and Villarreal, 2005).          Findings          There is fruitful overlap between AI, CT and ME that is of value to consider in mathematics education.          Originality/value          Seeing ME through the lenses of other disciplines and recognizing that there is a significant overlap of key elements reinforces the importance of agency, modeling and abstraction in ME and provides new contexts and tools for incorporating them in classroom practice.","",""
11,"C. E. Kahn","Artificial Intelligence, Real Radiology.",2019,"","","","",150,"2022-07-13 09:20:03","","10.1148/RYAI.2019184001","","",,,,,11,3.67,11,1,3,"Welcome to this inaugural issue of Radiology: Artificial Intelligence. Our journal’s mission is to publish highquality scientific work that advances our understanding of artificial intelligence (AI) in radiology. AI has become a topic of great interest—especially the application of machine learning techniques to medical images—but AI itself is not new. The term artificial intelligence was proposed in 1956 to describe efforts to understand, simulate, and improve upon human qualities such as reasoning, learning, solving problems, understanding verbal and written language, processing visual information, and playing games like chess and poker. What is new is a resurgence of interest in AI, particularly in the use of machine learning to recognize patterns in images. And, curiously, it is game playing that has opened this new frontier—but not the games of chess, checkers, or Go. Rather, think Xbox and PlayStation. Video games require a rapidly changing three-dimensional scene to be transformed into two-dimensional images shown in real time. The need to compute images efficiently spurred the development of highly parallelized graphics processing units. These specialized processors, in turn, have powered software for increasingly complex and sophisticated “deep” artificial neural network models. Whereas neural networks developed 10 years ago typically had three or four layers, today’s deep networks comprise hundreds of layers (1). Deep learning models have engendered both great excitement and a great deal of hyperbole. After all, if AI systems can pick out pictures of cats on the web, then surely such systems are ready to replace radiologists, right? Well, perhaps not, at least not right now. There is much work to be done to build and validate systems that can detect and characterize the thousands of imaging findings and their associated diseases that can be seen across a panoply of radiology studies. And that brings us to the quote from Shakespeare. Anyone can claim to build an AI system, but that doesn’t mean that the system will do their bidding as imagined. Our journal is here to assure that the science and applications of AI in radiology are built on thoughtful, innovative, and well-validated research. What sorts of topics will this journal publish? We will bring you the same high caliber of research that is found in RSNA’s flagship scientific journal, Radiology, but focused here on AI, machine learning, and data science in radiology. In particular, we seek to publish first-rate work that provides rigorous evaluation of AI’s applications to clinical problems in radiology. We invite manuscripts that show the impact of AI to extract information, diagnose and manage disease in patients, streamline radiology workflow, or improve health care outcomes. We’re interested in image segmentation, image reconstruction, automated detection of abnormalities, diagnostic reasoning, natural language processing, clinical workflow analysis, radiomics, and radiogenomics. We also invite manuscripts that demonstrate novel applications of AI in radiology or highlight innovative AI methodologies. Developers of publicly available sets of radiologic images, image annotations, radiology reports, or algorithms can present their work as a Data Resources report. AI and radiology do not exist in isolation: they are part of broad endeavors to advance knowledge and improve health. As such, this journal will feature articles on the ethical, legal, social, and economic implications of AI in radiology. AI is and must be a human—and humane—activity (2). We must engage in this work with an eye to how these technologies will help us care for our patients more effectively and humanely. Our goal is not to replace, but rather to extend our human abilities to provide medical care— and to improve the lives of those we are privileged to serve. All RSNA members receive access to this online bimonthly journal. We invite all readers (RSNA members or not) to sign up for our Editor’s Blog, The Vasty Deep (https://pubs.rsna.org/page/ai/blog) and to follow us on Twitter (@Radiology_AI). These social media platforms will augment the journal and offer innovative online features. Again, welcome!","",""
52,"Serge-Lopez Wamba-Taguimdje, S. Wamba, J. K. Kamdjoug, C. Wanko","Influence of artificial intelligence (AI) on firm performance: the business value of AI-based transformation projects",2020,"","","","",151,"2022-07-13 09:20:03","","10.1108/bpmj-10-2019-0411","","",,,,,52,26.00,13,4,2,"The main purpose of our study is to analyze the influence of Artificial Intelligence (AI) on firm performance, notably by building on the business value of AI-based transformation projects. This study was conducted using a four-step sequential approach: (1) analysis of AI and AI concepts/technologies; (2) in-depth exploration of case studies from a great number of industrial sectors; (3) data collection from the databases (websites) of AI-based solution providers; and (4) a review of AI literature to identify their impact on the performance of organizations while highlighting the business value of AI-enabled projects transformation within organizations.,This study has called on the theory of IT capabilities to seize the influence of AI business value on firm performance (at the organizational and process levels). The research process (responding to the research question, making discussions, interpretations and comparisons, and formulating recommendations) was based on a review of 500 case studies from IBM, AWS, Cloudera, Nvidia, Conversica, Universal Robots websites, etc. Studying the influence of AI on the performance of organizations, and more specifically, of the business value of such organizations’ AI-enabled transformation projects, required us to make an archival data analysis following the three steps, namely the conceptual phase, the refinement and development phase, and the assessment phase.,AI covers a wide range of technologies, including machine translation, chatbots and self-learning algorithms, all of which can allow individuals to better understand their environment and act accordingly. Organizations have been adopting AI technological innovations with a view to adapting to or disrupting their ecosystem while developing and optimizing their strategic and competitive advantages. AI fully expresses its potential through its ability to optimize existing processes and improve automation, information and transformation effects, but also to detect, predict and interact with humans. Thus, the results of our study have highlighted such AI benefits in organizations, and more specifically, its ability to improve on performance at both the organizational (financial, marketing and administrative) and process levels. By building on these AI attributes, organizations can, therefore, enhance the business value of their transformed projects. The same results also showed that organizations achieve performance through AI capabilities only when they use their features/technologies to reconfigure their processes.,AI obviously influences the way businesses are done today. Therefore, practitioners and researchers need to consider AI as a valuable support or even a pilot for a new business model. For the purpose of our study, we adopted a research framework geared toward a more inclusive and comprehensive approach so as to better account for the intangible benefits of AI within organizations. In terms of interest, this study nurtures a scientific interest, which aims at proposing a model for analyzing the influence of AI on the performance of organizations, and at the same time, filling the associated gap in the literature. As for the managerial interest, our study aims to provide managers with elements to be reconfigured or added in order to take advantage of the full benefits of AI, and therefore improve organizations’ performance, the profitability of their investments in AI transformation projects, and some competitive advantage. This study also allows managers to consider AI not as a single technology but as a set/combination of several different configurations of IT in the various company’s business areas because multiple key elements must be brought together to ensure the success of AI: data, talent mix, domain knowledge, key decisions, external partnerships and scalable infrastructure.,This article analyses case studies on the reuse of secondary data from AI deployment reports in organizations. The transformation of projects based on the use of AI focuses mainly on business process innovations and indirectly on those occurring at the organizational level. Thus, 500 case studies are being examined to provide significant and tangible evidence about the business value of AI-based projects and the impact of AI on firm performance. More specifically, this article, through these case studies, exposes the influence of AI at both the organizational and process performance levels, while considering it not as a single technology but as a set/combination of the several different configurations of IT in various industries.","",""
48,"L. Drukker, J. Noble, A. Papageorghiou","Introduction to artificial intelligence in ultrasound imaging in obstetrics and gynecology",2020,"","","","",152,"2022-07-13 09:20:03","","10.1002/uog.22122","","",,,,,48,24.00,16,3,2,"Artificial intelligence (AI) uses data and algorithms to aim to draw conclusions that are as good as, or even better than, those drawn by humans. AI is already part of our daily life; it is behind face recognition technology, speech recognition in virtual assistants (such as Amazon Alexa, Apple's Siri, Google Assistant and Microsoft Cortana) and self‐driving cars. AI software has been able to beat world champions in chess, Go and recently even Poker. Relevant to our community, it is a prominent source of innovation in healthcare, already helping to develop new drugs, support clinical decisions and provide quality assurance in radiology. The list of medical image‐analysis AI applications with USA Food and Drug Administration or European Union (soon to fall under European Union Medical Device Regulation) approval is growing rapidly and covers diverse clinical needs, such as detection of arrhythmia using a smartwatch or automatic triage of critical imaging studies to the top of the radiologist's worklist. Deep learning, a leading tool of AI, performs particularly well in image pattern recognition and, therefore, can be of great benefit to doctors who rely heavily on images, such as sonologists, radiographers and pathologists. Although obstetric and gynecological ultrasound are two of the most commonly performed imaging studies, AI has had little impact on this field so far. Nevertheless, there is huge potential for AI to assist in repetitive ultrasound tasks, such as automatically identifying good‐quality acquisitions and providing instant quality assurance. For this potential to thrive, interdisciplinary communication between AI developers and ultrasound professionals is necessary. In this article, we explore the fundamentals of medical imaging AI, from theory to applicability, and introduce some key terms to medical professionals in the field of ultrasound. We believe that wider knowledge of AI will help accelerate its integration into healthcare. © 2020 The Authors. Ultrasound in Obstetrics & Gynecology published by John Wiley & Sons Ltd on behalf of the International Society of Ultrasound in Obstetrics and Gynecology.","",""
0,"Jinyun Tang, W. Riley, Qing Zhu, T. Keenan","Using machine learning and artificial intelligence to improve model-data integrated earth system model predictions of water and carbon cycle extremes",2021,"","","","",153,"2022-07-13 09:20:03","","10.2172/1769794","","",,,,,0,0.00,0,4,1,"Jinyun Tang 1, William J Riley 1, Qing Zhu 1, Trevor Keenan 1, 2 1Lawrence Berkeley National Laboratory 2 University of California, Berkeley Focal Area(s) The research proposed here focuses on improving the predictive power of the land component of earth system models (ESMs) using (1) model-data fusion enabled by machine learning (ML) and artificial intelligence (AI), (2) predictive modeling through the combination of ML, AI, and big-data (comprising both model output and observations), and (3) insight of ESM structure and process mechanisms gleaned from complex data using ML and AI. Science Challenge Current efforts on water and carbon cycle benchmarking and improving ESM predictions focus on how well models capture (1) snapshots of climatology (e.g., the spatial pattern of land surface evapotranspiration), (2) time series of aggregated variables (e.g., interannual variability of net land carbon fluxes), and (3) one-vs-one variable correlations (e.g., the relationship between precipitation and evapotranspiration), all of which are less than three dimensional. However, ESM predictions are by nature of high dimension, beyond those spanned by space and time, when variables like vegetation diversity and human water use are considered. Moreover, in 10 years, with improved spatial-temporal resolution and the inclusion of more mechanistic processes, ESMs will very likely output more diverse data streams at much larger volume. Meanwhile, thanks to technological advancements, the amount of observations will also increase dramatically, in both type and spatial-temporal coverage. Current benchmark and model-data integration paradigms and methods are insufficient to address this big-data challenge. Further, current approaches are not of sufficient specificity or fidelity for evaluation at fine spatial resolutions (e.g., 1 km), nor do they provide comprehensive understanding of the casualty relationships that affect climate extremes. To address these challenges, research is proposed here to (1) make better uses of multiple scales of observations to concurrently analyze, evaluate, and reduce ESM uncertainty, and generate process knowledge of terrestrial processes, (2) achieve the ability to clearly integrate diverse observations, ML and AI, theory, and model predictive capabilities, (3) obtain robust quantification of multivariate functional relationships (e.g., net primary productivity to precipitation, temperature, and radiation) under a wide range of environmental conditions, and (4) provide high fidelity prediction and understanding of climate extreme events at fine spatial resolutions. Rationale ESM predictions are uncertain because (1) the earth system comprises many components, including atmosphere, land, ocean, biosphere, cryosphere, human activities, etc., each of which is insufficiently monitored and understood; (2) when the insufficient understanding of these earth system components are combined with the limited spatial-temporal resolution of ESMs,","",""
0,"C. Carpenter","Augmented Artificial Intelligence Improves Data Analytics in Heavy-Oil Reservoirs",2019,"","","","",154,"2022-07-13 09:20:03","","10.2118/0519-0068-JPT","","",,,,,0,0.00,0,1,3,"This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 193650, “Augmented-Artificial-Intelligence Solutions for Heavy-Oil Reservoirs: Innovative Work Flows That Build From Smart Analytics, Machine Learning, and Expert-Based Systems,” by David Castineira, Xiang Zhai, and Hamed Darabi, Quantum Reservoir Impact Group, prepared for the 2018 SPE International Heavy Oil Conference and Exhibition, Kuwait City, Kuwait, 10–12 December. The paper has not been peer reviewed.  Recently, many heavy-oil fields have seen exponentially higher volumes of data made available as a result of omnipresent connectivity. Existing data platforms have focused traditionally on solving the problem of data storage and access. The more-complex problem of true knowledge discovery and systematic value creation from the massive amount of data is less frequently addressed. The authors of this paper propose a novel work flow for the problem of building intelligent data analytics in heavy-oil fields.  Introduction  Optimal reservoir management for heavy-oil reservoirs requires systematic solutions that combine both engineering ability and advanced analytics. The authors believe that this requirement is addressed by what they call augmented artificial intelligence (AAI), a process inspired by the intelligence-amplification concept in which machine learning and human expertise are combined to improve solutions derived by systems that learn without any type of input from engineers or geoscientists. Practical deployment of AAI will involve automated work flows that use solid technical expertise and proven processes to transform field data into more-effective reservoir-management solutions.  Even with rapid data-preprocessing solutions in place, developing an optimal reservoir-management framework for heavy-oil assets is inherently complex. Identifying key recovery obstacles (KROs) and field-development plans (FDPs) typically takes many months, involving a large team of experts and the construction of sophisticated full-field simulation models. The recommendation is that automated work flows and AAI solutions are combined to identify those KROs rapidly and prepare robust FDPs that increase production and optimize current operations.  Perhaps the less-intuitive step in developing systematic solutions for heavy-oil fields is the process of developing a quantitative reservoir diagnostic framework. This process must build from big-data analytics platforms and an array of analytical, numerical, and empirical models combined to deliver a catalog of KROs affecting field performance. To this end, the entire historical set of well, field, and reservoir data must be processed and input into this diagnostics platform. Once the KROs are understood, the next step is to translate the diagnostics into detailed action plans in the field that can generate production, reserves, or capital-efficiency improvements.  This paper aims to offer an alternative approach to traditional work flows that identify recovery obstacles and development opportunities in heavy-oil fields by labor-intensive solutions. In contrast, the authors propose a systematic framework that provides three key advantages:  Execution time is fast, and an initial opportunity inventory can be generated.  The user can choose from multiple algorithms and methods to customize the technology to unique field/reservoir complexities.  The core algorithms are data-driven, integrate multidisciplinary data sets, and leave little room for the biases of the user, which allows for a consistent and repeatable analysis.","",""
25,"Feng Liu, Yong Shi, Y. Liu","Intelligence Quotient and Intelligence Grade of Artificial Intelligence",2017,"","","","",155,"2022-07-13 09:20:03","","10.1007/s40745-017-0109-0","","",,,,,25,5.00,8,3,5,"","",""
0,"Xiang Pan","Construction of Interdiscipline Curriculum Resource Library Based on Artificial Intelligence",2018,"","","","",156,"2022-07-13 09:20:03","","10.12783/dtssehs/ichae2018/25641","","",,,,,0,0.00,0,1,4,"In combination with the characteristics of artificial intelligence and separate subject course as well as the actual needs of teaching, this paper introduces the construction circumstance of the separate subject curriculum resource library based on artificial intelligence in respects of the construction of the separate subject curriculum resource library, the optimization of teaching resources and conditions, and analyzes the construction of the relevance resource database of the interdiscipline resource library in detail. By making use of the concrete application of machine learning algorithms in data mining, establishes the machine learning module and the expert system module to simulate the knowledge and experience of the human experts, and the decision-making support is provided for the solution steps and ways of problems.","",""
15,"Yun-he Pan","Special issue on artificial intelligence 2.0",2017,"","","","",157,"2022-07-13 09:20:03","","10.1631/FITEE.1710000","","",,,,,15,3.00,15,1,5,"With the ever-growing popularization of the Internet, universal existence of sensors, emergence of big data, development of e-commerce, rise of the information community, and interconnection and fusion of data and knowledge in human society, physical space, and cyberspace, the information environment surrounding artificial intelligence (AI) development has changed profoundly, leading to a new evolutionary stage: AI 2.0. The emergence of new technologies also promotes AI to a new stage (Pan, 2016). The next-generation AI, namely AI 2.0, is a more explainable, robust, open, and general AI with the following attractive merits: It effectively integrates data-driven machine learning approaches (bottom-up) with knowledge-guided methods (top-down). In addition, it can employ data with different modalities (e.g., visual, auditory, and natural language processing) to perform cross-media learning and inference. Furthermore, there will be a step from the pursuit of an intelligent machine to the hybridaugmented intelligence (i.e., high-level man-machine collaboration and fusion). AI 2.0 will also promote crowd-based intelligence and autonomous-intelligent systems. In the next decades, AI2.0 will probably achieve remarkable progress in aforementioned trends, and therefore significantly change our cities, products, services, economics, environments, even how we advance our society. This special issue aims at reporting recent re-thinking of AI 2.0 from aforementioned aspects as well as practical methodologies, efficient implementations, and applications of AI 2.0. The papers in this special issue can be categorized into two groups. The first group consists of six review papers and the second group five research papers. In the first group, Zhuang et al. (2017) reviewed recent emerging theoretical and technological advances of AI in big data settings. The authors concluded that integrating data-driven machine learning with human knowledge (common priors or implicit intuitions) can effectively lead to explainable, robust, and general AI. Li W et al. (2017) described the concepts of crowd intelligence, and explained its relationship to the existing related concepts, e.g., crowdsourcing and human computation. In addition, the authors introduced four categories of representative crowd intelligence platforms. Peng et al. (2017) presented approaches, advances, and future directions in cross-media analysis and reasoning. This paper covers cross-media representation, mining, reasoning, and cross-media knowledge evolution. Tian et al. (2017) reviewed the state-of-the-art research of the perception in terms of visual perception, auditory perception, and speech perception. It also covered perceptual information processing and learning engines. Zhang et al. (2017) introduced the trends in the development of intelligent unmanned autonomous systems. It covered unmanned vehicles, unmanned aerial vehicles, service robots, space robots, marine robots, and unmanned Editorial: Frontiers of Information Technology & Electronic Engineering www.zju.edu.cn/jzus; engineering.cae.cn; www.springerlink.com ISSN 2095-9184 (print); ISSN 2095-9230 (online) E-mail: jzus@zju.edu.cn","",""
8,"Amaan Anwar, S. Hassan","Applying Artificial Intelligence Techniques to Prevent Cyber Assaults",2017,"","","","",158,"2022-07-13 09:20:03","","","","",,,,,8,1.60,4,2,5,"Cyber security ostensibly is the discipline that could profit most from the introduction of Artificial Intelligence (AI). It is tough to make software for defending against the powerfully developing assaults in systems. It can be cured by applying techniques of artificial intelligence. Where conventional security systems may be slow and deficient, artificial intelligence techniques can enhance their overall security execution and give better security from an expanding number of complex cyber threats. Beside the great opportunities attributed to AI inside cyber security, its utilization has legitimized risks and concerns. To promote increment the development of cyber security, a holistic perspective of associations cyber environment is required in which AI is consolidated with human knowledge, since neither individuals nor AI alone has proven overall success in this sphere. In this manner, socially mindful utilization of AI techniques will be needed to further mitigate related risks and concerns.","",""
11,"K. Mudgal, Neelanjan Das","The ethical adoption of artificial intelligence in radiology",2019,"","","","",159,"2022-07-13 09:20:03","","10.1259/bjro.20190020","","",,,,,11,3.67,6,2,3,"Artificial intelligence (AI) is rapidly transforming healthcare—with radiology at the pioneering forefront. To be trustfully adopted, AI needs to be lawful, ethical and robust. This article covers the different aspects of a safe and sustainable deployment of AI in radiology during: training, integration and regulation. For training, data must be appropriately valued, and deals with AI companies must be centralized. Companies must clearly define anonymization and consent, and patients must be well-informed about their data usage. Data fed into algorithms must be made AI-ready by refining, purification, digitization and centralization. Finally, data must represent various demographics. AI needs to be safely integrated with radiologists-in-the-loop: guiding forming concepts of AI solutions and supervising training and feedback. To be well-regulated, AI systems must be approved by a health authority and agreements must be made upon liability for errors, roles of supervised and unsupervised AI and fair workforce distribution (between AI and radiologists), with a renewal of policy at regular intervals. Any errors made must have a root-cause analysis, with outcomes fedback to companies to close the loop—thus enabling a dynamic best prediction system. In the distant future, AI may act autonomously with little human supervision. Ethical training and integration can ensure a ""transparent"" technology that will allow insight: helping us reflect on our current understanding of imaging interpretation and fill knowledge gaps, eventually moulding radiological practice. This article proposes recommendations for ethical practise that can guide a nationalized framework to build a sustainable and transparent system.","",""
23,"J. C. Alvarado-Pérez, Diego Hernán Peluffo-Ordóñez, Roberto Therón","Bridging the gap between human knowledge and machine learning",2015,"","","","",160,"2022-07-13 09:20:03","","10.14201/ADCAIJ2015415464","","",,,,,23,3.29,8,3,7,"Nowadays, great amount of data is being created by several sources from academic, scientific, business and industrial activities. Such data intrinsically contains meaningful information allowing for developing techniques, and have scientific validity to explore the information thereof. In this connection, the aim of artificial intelligence (AI) is getting new knowledge to make decisions properly. AI has taken an important place in scientific and technology development communities, and recently develops computer-based processing devices for modern machines. Under the premise, the premise that the feedback provided by human reasoning -which is holistic, flexible and parallel- may enhance the data analysis, the need for the integration of natural and artificial intelligence has emerged. Such an integration makes the process of knowledge discovery more effective, providing the ability to easily find hidden trends and patterns belonging to the database predictive model. As well, allowing for new observations and considerations from beforehand known data by using both data analysis methods and knowledge and skills from human reasoning. In this work, we review main basics and recent works on artificial and natural intelligence integration in order to introduce users and researchers on this emergent field. As well, key aspects to conceptually compare them are provided.","",""
159,"V. Özdemir, N. Hekim","Birth of Industry 5.0: Making Sense of Big Data with Artificial Intelligence, ""The Internet of Things"" and Next-Generation Technology Policy.",2018,"","","","",161,"2022-07-13 09:20:03","","10.1089/omi.2017.0194","","",,,,,159,39.75,80,2,4,"Driverless cars with artificial intelligence (AI) and automated supermarkets run by collaborative robots (cobots) working without human supervision have sparked off new debates: what will be the impacts of extreme automation, turbocharged by the Internet of Things (IoT), AI, and the Industry 4.0, on Big Data and omics implementation science? The IoT builds on (1) broadband wireless internet connectivity, (2) miniaturized sensors embedded in animate and inanimate objects ranging from the house cat to the milk carton in your smart fridge, and (3) AI and cobots making sense of Big Data collected by sensors. Industry 4.0 is a high-tech strategy for manufacturing automation that employs the IoT, thus creating the Smart Factory. Extreme automation until ""everything is connected to everything else"" poses, however, vulnerabilities that have been little considered to date. First, highly integrated systems are vulnerable to systemic risks such as total network collapse in the event of failure of one of its parts, for example, by hacking or Internet viruses that can fully invade integrated systems. Second, extreme connectivity creates new social and political power structures. If left unchecked, they might lead to authoritarian governance by one person in total control of network power, directly or through her/his connected surrogates. We propose Industry 5.0 that can democratize knowledge coproduction from Big Data, building on the new concept of symmetrical innovation. Industry 5.0 utilizes IoT, but differs from predecessor automation systems by having three-dimensional (3D) symmetry in innovation ecosystem design: (1) a built-in safe exit strategy in case of demise of hyperconnected entrenched digital knowledge networks. Importantly, such safe exists are orthogonal-in that they allow ""digital detox"" by employing pathways unrelated/unaffected by automated networks, for example, electronic patient records versus material/article trails on vital medical information; (2) equal emphasis on both acceleration and deceleration of innovation if diminishing returns become apparent; and (3) next generation social science and humanities (SSH) research for global governance of emerging technologies: ""Post-ELSI Technology Evaluation Research"" (PETER). Importantly, PETER considers the technology opportunity costs, ethics, ethics-of-ethics, framings (epistemology), independence, and reflexivity of SSH research in technology policymaking. Industry 5.0 is poised to harness extreme automation and Big Data with safety, innovative technology policy, and responsible implementation science, enabled by 3D symmetry in innovation ecosystem design.","",""
7,"Arwin Datumaya Wahyudi Sumari, A. S. Ahmad, Cognitive Artificial","The application of cognitive artificial intelligence within C4ISR framework for national resilience",2017,"","","","",162,"2022-07-13 09:20:03","","10.1109/ACDTJ.2017.8259600","","",,,,,7,1.40,2,3,5,"Cognitive Artificial Intelligence (CAI) is a new perspective in Artificial Intelligence (AI) which is aimed to emulate how human brain works in generating knowledge. Human becomes intelligent because of knowledge which grows over time in his brain. With comprehensive knowledge, he can understand the world (environment) and is able to make decision and or action on it. On the other hand, strategic decision which impacts to the continuance of having a nation and having state is a critical and crucial matter, and it should be done in precise and quick manner especially in the case of contingency and faced to mutiple-data multiple-decision-alternative problems. The most precise decision has to be based on the knowledge from extracted comprehensive information. In this paper we show you the application of CAI for National Security with Knowledge-Growing System (KGS) as the engine of decision making system. We apply the CAI to a framework called Cognitive Command, Control, Communications, Computers, Intelligence, Surveillance and Reconnaissance (C4ISR) with examples taken from a simulated of real-life case in the Defense-Security domain.","",""
45,"Federico Pistono, Roman V Yampolskiy","Unethical Research: How to Create a Malevolent Artificial Intelligence",2016,"","","","",163,"2022-07-13 09:20:03","","","","",,,,,45,7.50,23,2,6,"Cybersecurity research involves publishing papers about malicious exploits as much as publishing information on how to design tools to protect cyber-infrastructure. It is this information exchange between ethical hackers and security experts, which results in a well-balanced cyber-ecosystem. In the blooming domain of AI Safety Engineering, hundreds of papers have been published on different proposals geared at the creation of a safe machine, yet nothing, to our knowledge, has been published on how to design a malevolent machine. Availability of such information would be of great value particularly to computer scientists, mathematicians, and others who have an interest in AI safety, and who are attempting to avoid the spontaneous emergence or the deliberate creation of a dangerous AI, which can negatively affect human activities and in the worst case cause the complete obliteration of the human species. This paper provides some general guidelines for the creation of a Malevolent Artificial Intelligence (MAI).","",""
96,"S. Wartman, C. Combs","Medical Education Must Move From the Information Age to the Age of Artificial Intelligence",2017,"","","","",164,"2022-07-13 09:20:03","","10.1097/ACM.0000000000002044","","",,,,,96,19.20,48,2,5,"Noteworthy changes coming to the practice of medicine require significant medical education reforms. While proposals for such reforms abound, they are insufficient because they do not adequately address the most fundamental change—the practice of medicine is rapidly transitioning from the information age to the age of artificial intelligence. Increasingly, future medical practice will be characterized by: the delivery of care wherever the patient happens to be; the provision of care by newly constituted health care teams; the use of a growing array of data from multiple sources and artificial intelligence applications; and the skillful management of the interface between medicine and machines. To be effective in this environment, physicians must work at the top of their license, have knowledge spanning the health professions and care continuum, effectively leverage data platforms, focus on analyzing outcomes and improving performance, and communicate the meaning of the probabilities generated by massive amounts of data to patients, given their unique human complexities. The authors believe that a “reboot” of medical education is required that makes better use of the findings of cognitive psychology and pays more attention to the alignment of humans and machines in education and practice. Medical education needs to move beyond the foundational biomedical and clinical sciences. Systematic curricular attention must focus on the organization of professional effort among health professionals, the use of intelligence tools involving large data sets, and machine learning and robots, all the while assuring the mastery of compassionate care.","",""
78,"H. Kitano","Artificial Intelligence to Win the Nobel Prize and Beyond: Creating the Engine for Scientific Discovery",2016,"","","","",165,"2022-07-13 09:20:03","","10.1609/aimag.v37i1.2642","","",,,,,78,13.00,78,1,6,"This article proposes a new grand challenge for AI reasearch: to develop AI system to make major scientific discoveries in biomedical sciences that worth Nobel Prize. There are a series of human cognitive limitations that prevents us from making accerlated scientific discoveries, particularity in biomedical sciences. As a result, scientific discoveries are left behind at the level of cottage industry. AI systems can transform scientific discoveries into highly efficient practice, thereby enable us to expand our knowledge in unprecedented way. Such system may out-compute all possible hypotheses and may redefine the nature of scientific intuition, hence scientific discovery process.","",""
0,"S. Bandyopadhyay, R. Mukherjee, S. Sarkar","A Report on the First Workshop on Software Engineering for Artificial Intelligence (SE4AI 2020)",2020,"","","","",166,"2022-07-13 09:20:03","","10.1145/3385032.3385055","","",,,,,0,0.00,0,3,2,"With advancement in technology-driven decision making, the software-intensive systems for decisions have become more robust, dynamic, adaptive, context-aware, dependable. Architectural designs of such systems crave for new approaches where the data-driven decision making has to be incorporated in the solution. Methods for recommendation mechanism, prediction of operation failures, dealing with unsafe conditions etc are going to be part of the solution itself. Integrating such features to conceive an intelligent system that will directly influence the business solution is mostly appreciated. This would not have been possible without the direct interference of Artificial Intelligence which has been a standard procedure of industrial repertoire since 1980s. The direct impact of AI on social and economic life has been been felt mostly in last decade (since 2007) with the advent of smart phone, which contribute largely to ""big data"". The era of ""big data"" has witnessed the efficacy of Machine Learning and there is a need of the hour to combine data-driven machine intelligence with human intelligence (insights and domain knowledge) to effectively make the software development (requirement, design, testing, deployment and operation management) intelligent. The research community has shown a keen interest in this emerging field. In this report, we present a pre-organization summary of the workshop to be held on February 27, 2020, at IIIT Jabbalpur (India), co-located with the 13th Innovations in Software Engineering Conference (ISEC 2020).","",""
0,"R. Brachman, David Gunning, Murray Burke","Integrated Artificial Intelligence Systems",2020,"","","","",167,"2022-07-13 09:20:03","","","","",,,,,0,0.00,0,3,2,"Copyright © 2020, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 66 AI MAGAZINE When one thinks about what it might take to build an intelligent system, it is evident that multiple capabilities will be required. Intelligence is generally considered to be reflected in the ability of a system to learn and understand the world around it, and to deal successfully with new or challenging situations. A closer look at what it might take to accomplish this reveals a surprisingly complex set of abilities that must work together. There are many variations on these themes, but roughly speaking, a robustly intelligent, autonomous agent embedded in the real world will need perceptual capabilities to sense and help interpret external signals and phenomena; a set of beliefs about the world, including itself and other agents, cause and effect, and a host of other things relevant to its survival and success in achieving its goals; a variety of reasoning capabilities to determine implications of its beliefs, understand its environment, plan ahead, solve problems, and so forth; a wide array of learning and adaptation capabilities; the ability to affect the world through action; and, some kind of rich communication mechanism along the lines of natural human language generation and understanding.  From Shakey the Robot to self-driving cars, from the personal computer to personal assistants on our phones, the Defense Advanced Research Projects Agency (DARPA) has led the development of integrated artificial intelligence (AI) systems for more than half a century. From the earliest days of AI, it was apparent that a robust, generally intelligent system should include a complete set of capabilities: perception, memory, reasoning, learning, planning, and action; and when DARPA initiated AI research in the 1960s, ambitious projects such as Shakey the Robot went after the complete package. As DARPA realized the challenges, they backed away from the ultimate goal of integrated AI and tried to make progress on the individual problems of image understanding, speech and language understanding, knowledge representation and reasoning, planning and decision aids, machine learning, and robotic manipulation. Yet, even as researchers struggled to make progress in these subdisciplines, DARPA periodically resurrected the challenge of integrated intelligent systems and pushed the community to try again. In the 1980s, DARPA’s Strategic Computing Initiative took on challenges of integrated AI projects such as the Autonomous Land Vehicle and the Pilot’s Associate. These did not succeed, but instead set the stage for the several decades of more siloed research that followed, until it was time to try again. In the 2000s, DARPA took on the integrated AI problem again with its Grand Challenges, which led to the first self-driving cars, and projects such as the Personalized Assistant that Learns, which produced Apple’s Siri. These efforts created complex, richlyintegrated systems that represented quantum leaps ahead in machine intelligence. The integration of sophisticated capabilities in a fundamental way is the key to general intelligence. This is the story of DARPA’s persistent long-term support for this essential premise of AI. Integrated Artificial Intelligence Systems","",""
90,"M. Alsharqi, W. Woodward, J. Mumith, D. C. Markham, R. Upton, P. Leeson","Artificial intelligence and echocardiography",2018,"","","","",168,"2022-07-13 09:20:03","","10.1530/ERP-18-0056","","",,,,,90,22.50,15,6,4,"Echocardiography plays a crucial role in the diagnosis and management of cardiovascular disease. However, interpretation remains largely reliant on the subjective expertise of the operator. As a result inter-operator variability and experience can lead to incorrect diagnoses. Artificial intelligence (AI) technologies provide new possibilities for echocardiography to generate accurate, consistent and automated interpretation of echocardiograms, thus potentially reducing the risk of human error. In this review, we discuss a subfield of AI relevant to image interpretation, called machine learning, and its potential to enhance the diagnostic performance of echocardiography. We discuss recent applications of these methods and future directions for AI-assisted interpretation of echocardiograms. The research suggests it is feasible to apply machine learning models to provide rapid, highly accurate and consistent assessment of echocardiograms, comparable to clinicians. These algorithms are capable of accurately quantifying a wide range of features, such as the severity of valvular heart disease or the ischaemic burden in patients with coronary artery disease. However, the applications and their use are still in their infancy within the field of echocardiography. Research to refine methods and validate their use for automation, quantification and diagnosis are in progress. Widespread adoption of robust AI tools in clinical echocardiography practice should follow and have the potential to deliver significant benefits for patient outcome.","",""
51,"M. Rigla, Gema García-Sáez, B. Pons, M. Hernando","Artificial Intelligence Methodologies and Their Application to Diabetes",2018,"","","","",169,"2022-07-13 09:20:03","","10.1177/1932296817710475","","",,,,,51,12.75,13,4,4,"In the past decade diabetes management has been transformed by the addition of continuous glucose monitoring and insulin pump data. More recently, a wide variety of functions and physiologic variables, such as heart rate, hours of sleep, number of steps walked and movement, have been available through wristbands or watches. New data, hydration, geolocation, and barometric pressure, among others, will be incorporated in the future. All these parameters, when analyzed, can be helpful for patients and doctors’ decision support. Similar new scenarios have appeared in most medical fields, in such a way that in recent years, there has been an increased interest in the development and application of the methods of artificial intelligence (AI) to decision support and knowledge acquisition. Multidisciplinary research teams integrated by computer engineers and doctors are more and more frequent, mirroring the need of cooperation in this new topic. AI, as a science, can be defined as the ability to make computers do things that would require intelligence if done by humans. Increasingly, diabetes-related journals have been incorporating publications focused on AI tools applied to diabetes. In summary, diabetes management scenarios have suffered a deep transformation that forces diabetologists to incorporate skills from new areas. This recently needed knowledge includes AI tools, which have become part of the diabetes health care. The aim of this article is to explain in an easy and plane way the most used AI methodologies to promote the implication of health care providers—doctors and nurses—in this field.","",""
38,"J. Yun, Doo Seok Lee, Heungju Ahn, Kyungbae Park, Tan Yigitcanlar","Not Deep Learning but Autonomous Learning of Open Innovation for Sustainable Artificial Intelligence",2016,"","","","",170,"2022-07-13 09:20:03","","10.3390/SU8080797","","",,,,,38,6.33,8,5,6,"What do we need for sustainable artificial intelligence that is not harmful but beneficial human life? This paper builds up the interaction model between direct and autonomous learning from the human’s cognitive learning process and firms’ open innovation process. It conceptually establishes a direct and autonomous learning interaction model. The key factor of this model is that the process to respond to entries from external environments through interactions between autonomous learning and direct learning as well as to rearrange internal knowledge is incessant. When autonomous learning happens, the units of knowledge determinations that arise from indirect learning are separated. They induce not only broad autonomous learning made through the horizontal combinations that surpass the combinations that occurred in direct learning but also in-depth autonomous learning made through vertical combinations that appear so that new knowledge is added. The core of the interaction model between direct and autonomous learning is the variability of the boundary between proven knowledge and hypothetical knowledge, limitations in knowledge accumulation, as well as complementarity and conflict between direct and autonomous learning. Therefore, these should be considered when introducing the interaction model between direct and autonomous learning into navigations, cleaning robots, search engines, etc. In addition, we should consider the relationship between direct learning and autonomous learning when building up open innovation strategies and policies.","",""
38,"H. Jaeger","Artificial intelligence: Deep neural reasoning",2016,"","","","",171,"2022-07-13 09:20:03","","10.1038/nature19477","","",,,,,38,6.33,38,1,6,"","",""
38,"D. Bruckner, H. Zeilinger, D. Dietrich","Cognitive Automation—Survey of Novel Artificial General Intelligence Methods for the Automation of Human Technical Environments",2012,"","","","",172,"2022-07-13 09:20:03","","10.1109/TII.2011.2176741","","",,,,,38,3.80,13,3,10,"Automation, the utilization of control and information technologies for reducing the need for human intervention in the production process is about to meet Cognition-the science concerned with human thinking-and related sciences. More and more processes require analysis and insights that allow controlling them beyond the mere execution of rules and beyond prefitted controllers in order to automatically keep them within the desired conditions. Automatic and flexible decision making based on challenging conditions such as increasing amounts of information, lacking prior knowledge of data, incomplete, missing or contradicting data, becomes the key challenges for future automation technologies.","",""
44,"A. Vellido","Societal Issues Concerning the Application of Artificial Intelligence in Medicine",2018,"","","","",173,"2022-07-13 09:20:03","","10.1159/000492428","","",,,,,44,11.00,44,1,4,"Background: Medicine is becoming an increasingly data-centred discipline and, beyond classical statistical approaches, artificial intelligence (AI) and, in particular, machine learning (ML) are attracting much interest for the analysis of medical data. It has been argued that AI is experiencing a fast process of commodification. This characterization correctly reflects the current process of industrialization of AI and its reach into society. Therefore, societal issues related to the use of AI and ML should not be ignored any longer and certainly not in the medical domain. These societal issues may take many forms, but they all entail the design of models from a human-centred perspective, incorporating human-relevant requirements and constraints. In this brief paper, we discuss a number of specific issues affecting the use of AI and ML in medicine, such as fairness, privacy and anonymity, explainability and interpretability, but also some broader societal issues, such as ethics and legislation. We reckon that all of these are relevant aspects to consider in order to achieve the objective of fostering acceptance of AI- and ML-based technologies, as well as to comply with an evolving legislation concerning the impact of digital technologies on ethically and privacy sensitive matters. Our specific goal here is to reflect on how all these topics affect medical applications of AI and ML. This paper includes some of the contents of the “2nd Meeting of Science and Dialysis: Artificial Intelligence,” organized in the Bellvitge University Hospital, Barcelona, Spain. Summary and Key Messages: AI and ML are attracting much interest from the medical community as key approaches to knowledge extraction from data. These approaches are increasingly colonizing ambits of social impact, such as medicine and healthcare. Issues of social relevance with an impact on medicine and healthcare include (although they are not limited to) fairness, explainability, privacy, ethics and legislation.","",""
0,"John Kalantari","A general purpose artificial intelligence framework for the analysis of complex biological systems",2017,"","","","",174,"2022-07-13 09:20:03","","10.17077/ETD.4ESKIJ3M","","",,,,,0,0.00,0,1,5,"This thesis encompasses research on Artificial Intelligence in support of automating scientific discovery in the fields of biology and medicine. At the core of this research is the ongoing development of a general-purpose artificial intelligence framework emulating various facets of human-level intelligence necessary for building cross-domain knowledge that may lead to new insights and discoveries. To learn and buildmodels in a data-drivenmanner, we develop a general-purpose learning framework called Syntactic Nonparametric Analysis of Complex Systems (SYNACX), which uses tools from Bayesian nonparametric inference to learn the statistical and syntactic properties of biological phenomena from sequence data. We show that the models learned by SYNACX offer performance comparable to that of standard neural network architectures. For complex biological systems or processes consisting of several heterogeneous components with spatio-temporal interdependencies across multiple scales, learning frameworks like SYNACX can become unwieldy due to the the resultant combinatorial complexity. Thus we also investigate ways to robustly reduce data dimensionality by introducing a new data abstraction. In particular, we extend traditional string and graph grammars in a new modeling formalism which we call Simplicial Grammar. This formalism integrates the topological properties of the simplicial complex with the expressive power of stochastic grammars in a computation abstraction with which we can decompose complex system behavior, into a finite set of modular grammar rules which parsimoniously describe the spatial/temporal structure and dynamics of patterns inferred from sequence data.","",""
0,"Jacob Pettigrew, Gideon Woo, Herbert H. Tsang","Computational Intelligence in Human Feature Analysis and Pose Selection",2020,"","","","",175,"2022-07-13 09:20:03","","10.1109/SSCI47803.2020.9308270","","",,,,,0,0.00,0,3,2,"Using computers to detect a human’s features is a difficult problem. The solution to this problem can be used in applications such as facial detection and gesture recognition. These applications require fast computation and high accuracy. In our research, we are trying to detect human poses by examining humans’ features such as the arms and legs. Artificial Neural Networks have been successfully used in feature analysis and are popular for use in human pose selection. In this paper, we present the results from our research comparing various computational intelligent approaches such as Convolutional Neural Networks (CNN), Multi-Layer Perceptrons (MLP), Support Vector Machines (SVM), and K-Nearest Neighbour (KNN). Among the four algorithms examined in this paper, we found that CNNs outperformed other algorithms in terms of prediction accuracy and calculation speed. Our main contribution is therefore a CNN specifically designed for learning a human’s body structure and limb articulation, producing high accuracy while being robust against different body types and variation in limb articulation.","",""
14,"A. Antonov","From Artificial Intelligence to Human Super- Intelligence",2011,"","","","",176,"2022-07-13 09:20:03","","","","",,,,,14,1.27,14,1,11,"The problem of artificial intelligence defined over 60 years ago aims at teaching computers to solve intellectual tasks without a human and instead of a human, i.e., basically, creates the preconditions for intellectual degradation of the mankind. Moreover, the definition of the artificial intelligence problem itself has now become obsolete and does not take account of the new knowledge on human intelligence. It sets the almost needless and intractable problem of solving complex multi-factor tasks, successfully solved by the human sub-consciousness, with the help of computer simulation of the low-factor human rational thinking. At the same time, the technological singularity concept reads that after the research of the artificial intelligence problem is successfully completed in early 21 st century the emergence of computer civilization is inevitable, followed by the possible extinction of the human civilization. Thus, we conclude that it is necessary to stop further research of the artificial intelligence problem. Instead, we suggest developing the problem of human super-intelligence, which will be able to solve successfully much more pressing multi-factor problems and will pose no threat to the humanity.","",""
27,"B. Markey-Towler","The Economics of Artificial Intelligence",2018,"","","","",177,"2022-07-13 09:20:03","","10.2139/ssrn.2907974","","",,,,,27,6.75,27,1,4,"This paper develops a model of Artificial Intelligence technology as an object in order to form an assessment of the future of labour, the future of production, and the future of the distribution of income, as well as policy responses which may ensure the liberty of individuals to live such a life as they deem fulfilling. The nature of modern Artificial Intelligence technology incorporating Machine Learning is considered to develop a model of Artificial Intelligence Machine Learning as an object which may be placed within a model of the economic system as a whole. This is then done. We find that in principle, Artificial Intelligence Machine Learning will replace all routine human labour. We find, further, that in principle, Artificial Intelligence Machine Learning creates an economy subject to production technology with massive scaling capability. We find, finally, that Artificial Intelligence Machine Learning facilitates a winner-take-all dynamic in the distribution of income where income resembles “rents” rather than remuneration for labour expenditure. We find that the liberty of individuals to live such a life as they deem fulfilling may be maintained by education policy which promotes the learning of generalised rather than specialised knowledge.","",""
47,"Chengjie Zheng, T. V. Johnson, Aakriti Garg, Michael V. Boland","Artificial intelligence in glaucoma",2019,"","","","",178,"2022-07-13 09:20:03","","10.1097/ICU.0000000000000552","","",,,,,47,15.67,12,4,3,"Purpose of review The use of computers has become increasingly relevant to medical decision-making, and artificial intelligence methods have recently demonstrated significant advances in medicine. We therefore provide an overview of current artificial intelligence methods and their applications, to help the practicing ophthalmologist understand their potential impact on glaucoma care. Recent findings Techniques used in artificial intelligence can successfully analyze and categorize data from visual fields, optic nerve structure [e.g., optical coherence tomography (OCT) and fundus photography], ocular biomechanical properties, and a combination thereof to identify disease severity, determine disease progression, and/or recommend referral for specialized care. Algorithms have become increasingly complex in recent years, utilizing both supervised and unsupervised methods of artificial intelligence. Impressive performance of these algorithms on previously unseen data has been reported, often outperforming standard global indices and expert observers. However, there remains no clearly defined gold standard for determining the presence and severity of glaucoma, which undermines the training of these algorithms. To improve upon existing methodologies, future work must employ more robust definitions of disease, optimize data inputs for artificial intelligence analysis, and improve methods of extracting knowledge from learned results. Summary Artificial intelligence has the potential to revolutionize the screening, diagnosis, and classification of glaucoma, both through the automated processing of large data sets, and by earlier detection of new disease patterns. In addition, artificial intelligence holds promise for fundamentally changing research aimed at understanding the development, progression, and treatment of glaucoma, by identifying novel risk factors and by evaluating the importance of existing ones.","",""
17,"Reid G. Hoffman","Using artificial intelligence to set information free",2016,"","","","",179,"2022-07-13 09:20:03","","10.7551/mitpress/11645.003.0007","","",,,,,17,2.83,17,1,6,"Artificial intelligence (AI) is about to transform management from an art into a combination of art and science. Specialized AI will allow us to apply data science to our human interactions at work in a way that earlier management theorists like Drucker could only imagine. These specialized forms of AI can process and manipulate enormous quantities of data at a rate our biological brains can't match. Therein lies the applicability to management: within the next five years, forward-thinking organizations will be using specialized forms of AI to build a complex and comprehensive corporate knowledge graph. Specialized AI will be ubiquitous throughout the organization, indexing every document, folder, and file. AI will also be sitting in the middle of the communication stream, collecting all of the work products, from emails to files shared to chat messages.","",""
36,"William R. Frey, D. Patton, M. Gaskell, K. McGregor","Artificial Intelligence and Inclusion: Formerly Gang-Involved Youth as Domain Experts for Analyzing Unstructured Twitter Data",2018,"","","","",180,"2022-07-13 09:20:03","","10.1177/0894439318788314","","",,,,,36,9.00,9,4,4,"Mining social media data for studying the human condition has created new and unique challenges. When analyzing social media data from marginalized communities, algorithms lack the ability to accurately interpret off-line context, which may lead to dangerous assumptions about and implications for marginalized communities. To combat this challenge, we hired formerly gang-involved young people as domain experts for contextualizing social media data in order to create inclusive, community-informed algorithms. Utilizing data from the Gang Intervention and Computer Science Project—a comprehensive analysis of Twitter data from gang-involved youth in Chicago—we describe the process of involving formerly gang-involved young people in developing a new part-of-speech tagger and content classifier for a prototype natural language processing system that detects aggression and loss in Twitter data. We argue that involving young people as domain experts leads to more robust understandings of context, including localized language, culture, and events. These insights could change how data scientists approach the development of corpora and algorithms that affect people in marginalized communities and who to involve in that process. We offer a contextually driven interdisciplinary approach between social work and data science that integrates domain insights into the training of qualitative annotators and the production of algorithms for positive social impact.","",""
168,"Steve Omohundro","Cryptocurrencies, smart contracts, and artificial intelligence",2014,"","","","",181,"2022-07-13 09:20:03","","10.1145/2685328.2685334","","",,,,,168,21.00,168,1,8,"Recent developments in ""cryptocurrencies"" and ""smart contracts"" are creating new opportunities for applying AI techniques. These economic technologies would benefit from greater real world knowledge and reasoning as they become integrated with everyday commerce. Cryptocurrencies and smart contracts may also provide an infrastructure for ensuring that AI systems follow specified legal and safety regulations as they become more integrated into human society.","",""
135,"Khalid Colchester, H. Hagras, D. Alghazzawi, G. Aldabbagh","A Survey of Artificial Intelligence Techniques Employed for Adaptive Educational Systems within E-Learning Platforms",2017,"","","","",182,"2022-07-13 09:20:03","","10.1515/jaiscr-2017-0004","","",,,,,135,27.00,34,4,5,"Abstract The adaptive educational systems within e-learning platforms are built in response to the fact that the learning process is different for each and every learner. In order to provide adaptive e-learning services and study materials that are tailor-made for adaptive learning, this type of educational approach seeks to combine the ability to comprehend and detect a person’s specific needs in the context of learning with the expertise required to use appropriate learning pedagogy and enhance the learning process. Thus, it is critical to create accurate student profiles and models based upon analysis of their affective states, knowledge level, and their individual personality traits and skills. The acquired data can then be efficiently used and exploited to develop an adaptive learning environment. Once acquired, these learner models can be used in two ways. The first is to inform the pedagogy proposed by the experts and designers of the adaptive educational system. The second is to give the system dynamic self-learning capabilities from the behaviors exhibited by the teachers and students to create the appropriate pedagogy and automatically adjust the e-learning environments to suit the pedagogies. In this respect, artificial intelligence techniques may be useful for several reasons, including their ability to develop and imitate human reasoning and decision-making processes (learning-teaching model) and minimize the sources of uncertainty to achieve an effective learning-teaching context. These learning capabilities ensure both learner and system improvement over the lifelong learning mechanism. In this paper, we present a survey of raised and related topics to the field of artificial intelligence techniques employed for adaptive educational systems within e-learning, their advantages and disadvantages, and a discussion of the importance of using those techniques to achieve more intelligent and adaptive e-learning environments.","",""
8,"Luteshna Bishnoi, Shailendra Narayan Singh","Artificial Intelligence Techniques Used In Medical Sciences: A Review",2018,"","","","",183,"2022-07-13 09:20:03","","10.1109/CONFLUENCE.2018.8442729","","",,,,,8,2.00,4,2,4,"Artificial intelligence(AI) is more augmented intelligence and that is a fundamental philosophical direction it took where it is complementing people's intelligence with machines that have enough aspects of the intelligent problem solving capabilities that together the person and machine can do better than one or the other. Medical diagnosis is hard for humans it actually takes a lot of time without the help of intelligent machines. AIM (Artificial Intelligence in Medical) systems have been used in health care system mainly for diagnosis tasks. AI is the branch of computer science which deals with the machine intelligence and maximising chances of success and accuracy. Medical is the field where technology is much needed. Our expanding desires of the highest quality of health care and the quick development of ever more detailed medical knowledge leave the doctor without sufficient time to give to each case and attempting to stay aware of the advancements in his field. The main aim of writing this paper is to review the effectiveness of AI techniques in medical sciences and compare them.","",""
1,"Arunaben Prahladbhai Gurjar, S. Patel","Fundamental Categories of Artificial Neural Networks",2021,"","","","",184,"2022-07-13 09:20:03","","10.4018/978-1-7998-4042-8.CH003","","",,,,,1,1.00,1,2,1,"The new era of the world uses artificial intelligence (AI) and machine learning. The combination of AI and machine learning is called artificial neural network (ANN). Artificial neural network can be used as hardware or software-based components. Different topology and learning algorithms are used in artificial neural networks. Artificial neural network works similarly to the functionality of the human nervous system. ANN is working as a nonlinear computing model based on activities performed by human brain such as classification, prediction, decision making, visualization just by considering previous experience. ANN is used to solve complex, hard-to-manage problems by accruing knowledge about the environment. There are different types of artificial neural networks available in machine learning. All types of artificial neural networks work based of mathematical operation and require a set of parameters to get results. This chapter gives overview on the various types of neural networks like feed forward, recurrent, feedback, classification-predication.","",""
3,"E.V. Blagodarny, A.A Vedyakhin, A.M. Raygorodsky","Development of Educational Projects on the Basis of Technological Platforms with Artificial Intelligence: The Experience of MIPT on the Use of High Vox-Platform",2018,"","","","",185,"2022-07-13 09:20:03","","10.1109/IC-AIAI.2018.8674452","","",,,,,3,0.75,1,3,4,"The MIPT School of Applied Mathematics and Computer Science conducts research on artificial intelligence and develops education in this field in Russia. Modern science and technology are developing so quickly that a person needs to constantly learn and acquire new skills. Therefore, MIPT develops educational courses at the school, academic and corporate levels. Employers and scientific laboratories are more interested in the practical skills of employees in AI, than just theoretical knowledge. For this reason, the MIPT School of Applied Mathematics and Computer Science conducts and develop new practice-oriented educational courses. Moreover, the Laboratory of Innovation at MIPT creates the HighVox platform, which will allow MIPT students to gain experience in solving real problems from Russian companies during their studies. The platform creates the digital trace of each student: competences, scientific interests, courses taken, completed projects, soft skills, etc. Based on the digital trace of each participant, the platform automatically creates recommendations for projects and teams most suitable for the student. In the future, HighVox will become a place where technical specialists search for work, get an education (lifelong learning), communicate with colleagues on specialized topics and offer their ideas for startups. As part of the creation of this platform, the laboratory of innovation conducts research in two directions: a model of human competence and the formation of effective teams based on hard & soft skills using artificial intelligence.","",""
189,"Lawrence B. Solum","Legal Personhood for Artificial Intelligences",2008,"","","","",186,"2022-07-13 09:20:03","","10.4324/9781003074991-37","","",,,,,189,13.50,189,1,14,"Could an artificial intelligence become a legal person? As of today, this question is only theoretical. No existing computer program currently possesses the sort of capacities that would justify serious judicial inquiry into the question of legal personhood. The question is nonetheless of some interest. Cognitive science begins with the assumption that the nature of human intelligence is computational, and therefore, that the human mind can, in principle, be modelled as a program that runs on a computer. Artificial intelligence (AI) research attempts to develop such models. But even as cognitive science has displaced behavioralism as the dominant paradigm for investigating the human mind, fundamental questions about the very possibility of artificial intelligence continue to be debated. This Essay explores those questions through a series of thought experiments that transform the theoretical question whether artificial intelligence is possible into legal questions such as, ""Could an artificial intelligence serve as a trustee?"" What is the relevance of these legal thought experiments for the debate over the possibility of artificial intelligence? A preliminary answer to this question has two parts. First, putting the AI debate in a concrete legal context acts as a pragmatic Occam's razor. By reexamining positions taken in cognitive science or the philosophy of artificial intelligence as legal arguments, we are forced to see them anew in a relentlessly pragmatic context. Philosophical claims that no program running on a digital computer could really be intelligent are put into a context that requires us to take a hard look at just what practical importance the missing reality could have for the way we speak and conduct our affairs. In other words, the legal context provides a way to ask for the ""cash value"" of the arguments. The hypothesis developed in this Essay is that only some of the claims made in the debate over the possibility of AI do make a pragmatic difference, and it is pragmatic differences that ought to be decisive. Second, and more controversially, we can view the legal system as a repository of knowledge-a formal accumulation of practical judgments. The law embodies core insights about the way the world works and how we evaluate it. Moreover, in common-law systems judges strive to decide particular cases in a way that best fits the legal landscape-the prior cases, the statutory law, and the constitution. Hence, transforming the abstract debate over the possibility of AI into an imagined hard case forces us to check our intuitions and arguments against the assumptions that underlie social decisions made in many other contexts. By using a thought experiment that explicitly focuses on wide coherence, we increase the chance that the positions we eventually adopt will be in reflective equilibrium with our views about related matters. In addition, the law embodies practical knowledge in a form that is subject to public examination and discussion. Legal materials are published and subject to widespread public scrutiny and discussion. Some of the insights gleaned in the law may clarify our approach to the artificial intelligence debate.","",""
69,"L. Saitta, Jean-Daniel Zucker","Abstraction in Artificial Intelligence and Complex Systems",2013,"","","","",187,"2022-07-13 09:20:03","","10.1007/978-1-4614-7052-6","","",,,,,69,7.67,35,2,9,"","",""
0,"","ACTIVITY REPORT Project-Team Models and Algorithms for Artiﬁcial Intelligence",2022,"","","","",188,"2022-07-13 09:20:03","","","","",,,,,0,0.00,0,0,1,"The expectation-maximization (EM) algorithm is a powerful computational technique for maximum likelihood estimation in incomplete data models. When the expectation step cannot be performed in closed form, a stochastic approximation of EM (SAEM) can be used. The convergence of the SAEM toward critical points of the observed likelihood has been proved and its numerical efﬁciency has been demonstrated. However, sampling from the posterior distribution may be intractable or have a high computational cost. Moreover, despite appealing features, the limit position of this algorithm can strongly depend on its starting one. To cope with this two issues, we propose in [11] new stochastic approximation version of the EM in which we do not sample from the exact distribution in the expectation phase of the procedure. We ﬁrst prove the convergence of this algorithm toward critical points of the observed likelihood. Then, we propose an instantiation of this general procedure to favor convergence toward global maxima. Experiments on synthetic and real data highlight the performance of this algorithm in comparison to the SAEM and the EM when feasible. of subject-speciﬁc weights characterizing partial membership across clusters. With this ﬂexibility come challenges in uniquely identifying, estimating, and interpreting the parameters. In [40], we propose a new class of Dimension-Grouped MMMs (Gro-M 3 s) for multivariate categorical data, which improve parsimony and interpretability. In Gro-M 3 s, observed variables are partitioned into groups such that the latent membership is constant for variables within a group but can differ across groups. Traditional latent class models are obtained when all variables are in one group, while traditional MMMs are obtained when each variable is in its own group. The new model corresponds to a novel decomposition of probability tensors. Theoretically, we derive transparent identiﬁability conditions for both the unknown grouping structure and model parameters in general settings. Methodologically, we propose a Bayesian approach for Dirichlet Gro-M3 s to inferring the variable grouping structure and estimating model parameters. Simulation results demonstrate good computational performance and empirically conﬁrm the identiﬁability results. We illustrate the new methodology through an application to a functional disability dataset. from this natural partition. In a Bayesian context, this is achieved by considering the Dirichlet cluster proportion prior parameter α as a regularisation term controlling the granularity of the clustering. This second step allows the exploration of the clustering at coarser scales and the ordering of the clusters an important output for the visual representations of the clustering results. The clustering results obtained with the proposed approach, on simulated as well as real settings, are compared with existing strategies and are shown to be particularly relevant. This work is implemented in the R package greed and Figure 2 illustrates the main idea of the method. In this applied work [19], we use the Fisher-EM algorithm for clustering for the unsupervised classiﬁcation of 702, 248 spectra of galaxies and quasars with resdshifts smaller than 0.25 that were retrieved from the Sloan Digital Sky Survey (SDSS) database, release 7. The spectra were ﬁrst corrected for the redshift, then wavelet-ﬁltered to reduce the noise, and ﬁnally binned to obtain about 1437 wavelengths per spectrum. Fisher-EM, an unsupervised clustering discriminative latent mixture model algorithm, was applied on these corrected spectra, considering the full set as well as several subsets of 100,000 and 300,000 spectra. The optimum number of classes given by a penalized likelihood criterion is 86 classes, the 37 most populated ones gathering 99% of the sample. These classes are established from a subset of 302144 spectra. Using several cross-validation techniques we ﬁnd that this classiﬁcation is in agreement with the results obtained on the other subsets with an average misclassiﬁcation error of about 15%. The large number of very small classes tends to increase this error rate. This is the ﬁrst time that an automatic, objective and robust unsupervised classiﬁcation is established on such a large amount of spectra of galaxies. The mean spectra of the classes can be used as templates for a large majority of galaxies in our Universe. Figure 7 illustrates the obtained results. Recurrent Neural Networks, Deep linguistic patterns the of a of of to the is this linguistic that becomes valuable for our descriptive approach through deep as it allows us to observe complex lexico-grammatical structures, that potentially associate several levels of text representation in the same structure. The convolutional model used until now must therefore be adapted to integrate this additional information in order to obtain an even ﬁner description of the textual salience of a corpus. the relevant features used by the CNN to perform the classiﬁcation task. We empirically demonstrate the efﬁciency of our approach on corpora from two different languages: English and French. On all datasets, wTDS automatically encodes complex linguistic objects based on co-occurrences and possibly on grammatical and syntax analysis. relationships between the concepts in the metadata by analyzing the contrast between the concepts similarities in the Joconde’s semantic model and other vocabularies and we tried to improve the model prediction scores based on the semantic relations. Our results show that cross-fertilization between symbolic AI and machine learning can indeed provide the tools to address the challenges of the museum curators work describing the artwork pieces and searching for the relevant images. that combines a geometric approach for decision rules with existing post hoc solutions for machine learning models to generate an intuitive feature ranking tailored to the end user. We show that established model-agnostic approaches produce poor results in this framework. Figure 13 illustrates this work. Algorithms involving Gaussian processes or determinantal point processes typically require computing the determinant of a kernel matrix. Frequently, the latter is computed from the Cholesky decomposition, an algorithm of cubic complexity in the size of the matrix. We show that, under mild assumptions, it is possible to estimate the determinant from only a sub-matrix, with probabilistic guarantee on the relative error. In [37], we present an augmentation of the Cholesky decomposition that stops under certain conditions before processing the whole matrix. Experiments demonstrate that this can save a considerable amount of time while having an overhead of less than 5% when not stopping early. More generally, we present a probabilistic stopping strategy for the approximation of a sum of known length where addends are revealed sequentially. We do not assume independence between addends, only that they are bounded from below and decrease in conditional expectation. of there is a signiﬁcant from combining and audio data in detecting active speakers. either of the modalities can potentially mislead audiovisual fusion by inducing unreliable or deceptive information. outlines active speaker detection as a multi-objective learning problem to leverage best of each modalities using a novel self-attention, uncertainty-based multimodal fusion scheme. Results obtained show that the proposed multi-objective learning architecture outperforms traditional approaches in improving both mAP and AUC scores. We further demonstrate that our fusion strategy surpasses, in active speaker detection, other modality fusion methods reported in various disciplines. We ﬁnally show that the proposed method signiﬁcantly improves the state-of-the-art on the AVA-ActiveSpeaker dataset. This paper explores the problem of summarizing professional soccer matches as automatically as possible using both the event-stream data collected from the ﬁeld and the content broadcasted on TV. We have designed an architecture, introducing ﬁrst (1) a Multiple Instance Learning method that takes into account the sequential dependency among events and then (2) a hierarchical multimodal attention layer that grasps the importance of each event in an action [31]. We evaluate our approach on matches from two professional European soccer leagues, showing its capability to identify the best actions for automatic summarization by comparing with real summaries made by human operators. Figure 18 illustrates the general schema of the approach. We a coherent framework for studying longitudinal manifold-valued data. We introduce a Bayesian mixed-effects model which allows estimating both a group-representative piecewise-geodesic creating clusters of similar sentences. The ideal practice is to obtain a cluster with only positive blocks and another with only negative ones. Comparing to the supervised approach (Bag of words + Logistic Regression Classiﬁer) with its f1-score as 0.8234 and f2-score as 0.8316, we found that both S-Bert [58] (with a f1-score of 0.6250 and f2-score of 0.6192) and BioBert [57] (f1-score as 0.7004 and f2 as 0.6955) can achieves relatively good results and latter even outperformed the former due to its domain speciﬁc knowledge. around 13 billion euros per year to European citizens [52]. In the ﬁeld of healthcare insurance, in France the compulsory scheme detected over 261.2 million euros of fraudulent services in 2018, mainly due to healthcare professionals and healthcare establishments [50]. In the United States, according to the FBI, medicare fraud costs insurance companies between 21 billion and 71 billion US dollars per year [55]. In a context where reducing management costs is a real issue for healthcare insurers, the ﬁght against fraud is a real expectation of the customers of professionals in the sector so that everyone receives a fair return for their contributions. This stud","",""
0,"S. Parente","Taxation of artificial intelligences and taxable cases",2021,"","","","",189,"2022-07-13 09:20:03","","10.13166/wsge//eajx8063","","",,,,,0,0.00,0,1,1,"In order to tax the facts emerging from the computer economy, it is necessary not only to verify whether artificial intelligence is endowed with an autonomous tax subjectivity, but also to ascertain its compatibility with the principle of ability to pay, the basis and limit of taxation. This twofold requirement applies in particular to machines with cognitive skills similar to those of human beings, capable of taking decisions independently and increasing their knowledge. In any event, it must be the case that the subjective suitability of the machine for assuming the tax obligation can be inferred. Within these limits, the provision of a robot tax that does not alter the structure of the tax system could privilege the compensatory view of the social damage caused by technological innovation, in order to take into account the negative externalities related to the automation of production processes in terms of employment and financing of public expenditure. The taxation of robotics would thus affect the production of technological companies, due to the negative externalities resulting from the adoption of automated processes, since these are activities that pursue economic growth objectives.","",""
45,"Ashok K. Goel","A 30-year case study and 15 principles: Implications of an artificial intelligence methodology for functional modeling",2013,"","","","",190,"2022-07-13 09:20:03","","10.1017/S0890060413000218","","",,,,,45,5.00,45,1,9,"Abstract Research on design and analysis of complex systems has led to many functional representations with several meanings of function. This work on conceptual design uses a family of representations called structure–behavior–function (SBF) models. The SBF family ranges from behavior–function models of abstract design patterns to drawing–shape–SBF models that couple SBF models with visuospatial knowledge of technological systems. Development of SBF modeling is an instance of cognitively oriented artificial intelligence research that seeks to understand human cognition and build intelligent agents for addressing complex tasks such as design. This paper first traces the development of SBF modeling as our perspective on design evolved from that of problem solving to that of memory and learning. Next, the development of SBF modeling as a case study is used to abstract some of the core principles of an artificial intelligence methodology for functional modeling. Finally, some implications of the artificial intelligence methodology for different meanings of function are examined.","",""
0,"Bushra Rasheed, M. Usama, Asmara Safdar","Robust Artificial Intelligence Approach to Stabilize and Control Propeller Driven Hybrid UGV",2022,"","","","",191,"2022-07-13 09:20:03","","10.1109/ICAI55435.2022.9773375","","",,,,,0,0.00,0,3,1,"Hybrid Unmanned Ground Vehicle (HUGV) can drive on any terrain including walls and fly as well, using the multi directional thrust force of propellers. In the era of industrial revolution, hybrid UGVs need to be autonomous with intelligent decision making capabilities. During wall climbing of hybrid UGVs, stability is essential and depends on real time feedback from multiple sensors. To increase stability and control, it is proposed that PID control loops should be replaced by AI based algorithms that reduce the decision time and mathematical complexity. For autonomous movement in any terrain using the proposed model, intelligent UGVs can map and localize simultaneously.They can make intelligent decisions about mode of movement i.e. driving on ground or wall, steering on ground or wall, flying and maneuvering by using real time sensor readings. Integration of the proposed AI models with HUGV can be applied to many areas which are hard for humans to access, for instance; inspection of large structures, bio & nuclear hazard environments, planetary exploration & magnetic fields detection.","",""
1,"Massoud Sokouti, B. Sokouti","Applying the Science of Systematic Review and Meta-Analysis to Retrospective Artificial Intelligence Based Studies: The importance of performance evaluation",2019,"","","","",192,"2022-07-13 09:20:03","","","","",,,,,1,0.33,1,2,3,"The rationale behind the meta-analysis goes back to the 17th century studies of astronomy which then Karl Pearson performed a study based on meta-analysis using the data for typhoid inoculation in 1904. After, William Cochran applied this type of analysis to medical researches by taking the advantage of multiple previous studies. For more information and details on the history, the readers are referred to. To emerge the important role of systematic and metaanalysis studies even in the area of artificial intelligence systems, it is an anticipated that more reliable results can be driven from previous research studies alongside a simple review of such studies from which most of them may be ignored or not included as a matter of their nonsystematical type of reviews. The meta-analysis technique uses various types of statistics tools and methodologies to commonly derive a predictive diagnostic or non-diagnostic performance result of their compared corresponding approaches on the target defined disorders using information included in different datasets of previous studies. Although, a meta-analysis study can be regarded as a review of previous studies, however, it thoroughly targets not only the achieving results of those studies but also determine the in-common and non-commonpatterns of those researches as well as biases of the performance results whether they have been inserted intentionally or unintentionally. The importance of meta-analysis has been vastly discussed in medical sciences and therefore, been conducted rigorously through various studies, mostly on clinical trial ones. However, this technique is one of those less valued tools imported in to biomedical engineering studies and hence, their related algorithms mostly on the performance of artificial intelligence approaches. One of those studies to mention is the one performed on classification algorithms for pattern recognition by So Young Sohn in 1999 based on some in-house implemented statistics tools without considering the meta-analysis software. Moreover, in 2015,Horn et al have conducted a systematic review on functional brain imaging studies on assessing the familiarity of artificial neural networks and discussed their pros and cons in terms of their experimental conflicting results based on a meta-analysis on 68 publishedarticles. In another recent study, the role of real-time biomedical systems has been evaluated by a meta-analysis approach on 134 real-times papers in terms of computational complexity, delay and speed up considering various types of algorithms and hardware implementation. Recently, two types of systematic review and analysis have been performed which shows the potential non-mature trends of this approach in artificial intelligence based researches.In the first one the authors studied the performance of different machine learning algorithms for heart disease diagnosis; however, the metaanalysis part was not performed due to the existence of heterogeneity in the final included studies through the PRISMA (Preferred reporting items for systematic reviews and meta-analyses) checklist. And in the second one, the performance of several DNA based encryption algorithms based according to the results obtained from previous publications has been proposed where, it has been found out that there were no improvements in the proposed algorithms and it has been suggested that a dataset of images should be available in order to test and evaluate the performance of methodologies. However, the methodologies should also be available for public use. Moreover, the analyses section can be carried out through a simple statistical student’s t test analysisor the metaanalysis procedure using available tools such as MetaDisc, MIX, and Meta-Analyst. While comparing the two environments (i.e., clinical and computational), there are in-common units for decision making in diagnosing symptoms which are human (brain system and some data) and computer (artificial intelligence systems and some data). This outstanding feature and the abovementioned examples makes the meta-analysis studies applicable to the researches performed based on artificial intelligence systems, too. This will open a new view on interactions between the results obtained from previous studies while considering their special algorithms, different datasets, and possible biases. One more thing to emphasize for the future research studies is on publicizing the datasets and the implemented algorithms in terms of web servers, Java, C++ and Matlab libraries or R packages to make the results re-generable using new datasets which make them more comparable with new designed methodologies to ease the metaanalysis robust studies. As, it is also clear, most of the webservers and datasets in the medical parts coupled with data derived from bioscience knowledge are publicly.","",""
0,"María del Rocío Soto Flores, Ingrid Yadibel Cuevas Zuñiga, Susana Garduño Román","The Formation of Human Capital and Its Relationship With the Knowledge Society in Mexico",2019,"","","","",193,"2022-07-13 09:20:03","","10.4018/978-1-5225-8461-2.CH003","","",,,,,0,0.00,0,3,3,"The processes of economic globalization and accelerating technological change have led to changes in economic and social life at a global level. New technologies, such as the TICs, systems of artificial intelligence, scanning, connectivity, nanotechnology, and biotechnology, among others, have transformed the national productive structures and human capital that require technologies disruptive today. In this context, education has become the main element of the knowledge society and training of human capital that demands a knowledge-based economy. The objective of the chapter is to analyze the relationship between human capital formations in the construction of a society of knowledge in Mexico. The structure is organized in three sections: 1) an analysis of the knowledge society, 2) the formation of human capital and the institutions of higher education in the knowledge society, and 3) human capital formation and its relationship in the construction of a society of knowledge in Mexico.","",""
93,"Nathan Ensmenger","Is chess the drosophila of artificial intelligence? A social history of an algorithm",2012,"","","","",194,"2022-07-13 09:20:03","","10.1177/0306312711424596","","",,,,,93,9.30,93,1,10,"Since the mid 1960s, researchers in computer science have famously referred to chess as the ‘drosophila’ of artificial intelligence (AI). What they seem to mean by this is that chess, like the common fruit fly, is an accessible, familiar, and relatively simple experimental technology that nonetheless can be used productively to produce valid knowledge about other, more complex systems. But for historians of science and technology, the analogy between chess and drosophila assumes a larger significance. As Robert Kohler has ably described, the decision to adopt drosophila as the organism of choice for genetics research had far-reaching implications for the development of 20th century biology. In a similar manner, the decision to focus on chess as the measure of both human and computer intelligence had important and unintended consequences for AI research. This paper explores the emergence of chess as an experimental technology, its significance in the developing research practices of the AI community, and the unique ways in which the decision to focus on chess shaped the program of AI research in the decade of the 1970s. More broadly, it attempts to open up the virtual black box of computer software – and of computer games in particular – to the scrutiny of historical and sociological analysis.","",""
95,"S. Dilek, Hüseyin Çakir, Mustafa Aydin","Applications of Artificial Intelligence Techniques to Combating Cyber Crimes: A Review",2015,"","","","",195,"2022-07-13 09:20:03","","10.5121/ijaia.2015.6102","","",,,,,95,13.57,32,3,7,"With the advances in information technology (IT) criminals are using cyberspace to commit numerous cyber crimes. Cyber infrastructures are highly vulnerable to intrusions and other threats. Physical devices and human intervention are not sufficient for monitoring and protection of these infrastructures; hence, there is a need for more sophisticated cyber defense systems that need to be flexible, adaptable and robust, and able to detect a wide variety of threats and make intelligent real-time decisions. Numerous bio-inspired computing methods of Artificial Intelligence have been increasingly playing an important role in cyber crime detection and prevention. The purpose of this study is to present advances made so far in the field of applying AI techniques for combating cyber crimes, to demonstrate how these techniques can be an effective tool for detection and prevention of cyber attacks, as well as to give the scope for future work.","",""
43,"Xiao-hui Li, Caleb Chen Cao, Yuhan Shi, Wei Bai, Han Gao, Luyu Qiu, Cong Wang, Yuanyuan Gao, Shenjia Zhang, Xun Xue, Lei Chen","A Survey of Data-Driven and Knowledge-Aware eXplainable AI",2020,"","","","",196,"2022-07-13 09:20:03","","10.1109/tkde.2020.2983930","","",,,,,43,21.50,4,11,2,"We are witnessing a fast development of Artificial Intelligence (AI), but it becomes dramatically challenging to explain AI models in the past decade. “Explanation” has a flexible philosophical concept of “satisfying the subjective curiosity for causal information”, driving a wide spectrum of methods being invented and/or adapted from many aspects and communities, including machine learning, visual analytics, human-computer interaction and so on. Nevertheless, from the view-point of data and knowledge engineering (DKE), a best explaining practice that is cost-effective in terms of extra intelligence acquisition should exploit the causal information and explaining scenarios which is hidden richly in the data itself. In the past several years, there are plenty of works contributing in this line but there is a lack of a clear taxonomy and systematic review of the current effort. To this end, we propose this survey, reviewing and taxonomizing existing efforts from the view-point of DKE, summarizing their contribution, technical essence and comparative characteristics. Specifically, we categorize methods into data-driven methods where explanation comes from the task-related data, and knowledge-aware methods where extraneous knowledge is incorporated. Furthermore, in the light of practice, we provide survey of state-of-art evaluation metrics and deployed explanation applications in industrial practice.","",""
35,"J. Hendler, A. Mulvehill","Social Machines: The Coming Collision of Artificial Intelligence, Social Networking, and Humanity",2016,"","","","",197,"2022-07-13 09:20:03","","","","",,,,,35,5.83,18,2,6,"Will your next doctor be a human beingor a machine? Will you have a choice? If you do, what should you know before making it? This book introduces the reader to the pitfalls and promises of artificial intelligence (AI) in its modern incarnation and the growing trend of systems to ""reach off the Web"" into the real world. The convergence of AI, social networking, and modern computing is creating an historic inflection point in the partnership between human beings and machines with potentially profound impacts on the future not only of computing but of our world and species. AI experts and researchers James Hendlerco-originator of the Semantic Web (Web 3.0)and Alice Mulvehilldeveloper of AI-based operational systems for DARPA, the Air Force, and NASAexplore the social implications of AI systems in the context of a close examination of the technologies that make them possible. The authors critically evaluate the utopian claims and dystopian counterclaims of AI prognosticators. Social Machines: The Coming Collision of Artificial Intelligence, Social Networking, and Humanity is your richly illustrated field guide to the future of your machine-mediated relationships with other human beings and with increasingly intelligent machines. What Readers Will Learn What the concept of a social machine is and how the activities of non-programmers are contributing to machine intelligence How modern artificial intelligence technologies, such as Watson, are evolving and how they process knowledge from both carefully produced information (such as Wikipedia and journal articles) and from big data collections The fundamentals of neuromorphic computing, knowledge graph search, and linked data, as well as the basic technology concepts that underlie networking applications such as Facebook and Twitter How the change in attitudes towards cooperative work on the Web, especially in the younger demographic, is critical to the future of Web applications Who This Book Is For General readers and technically engaged developers, entrepreneurs, and technologists interested in the threats and promises of the accelerating convergence of artificial intelligence with social networks and mobile web technologies.","",""
5,"Swati Sharma, Nitisha Payal, A. Kaushik, Nitin Goel","Blue Brain Technology: A Subway to Artificial Intelligence",2014,"","","","",198,"2022-07-13 09:20:03","","10.1109/CSNT.2014.226","","",,,,,5,0.63,1,4,8,"Blue Brain is the name of the world's first virtual brain. A Virtual machine is one that can function as, a very appropriate application of an Artificial Intelligence human brain. Reverse engineering is a foremost concept of implementing the human brain and recreate it at the cellular level inside a complete simulation. The four major motivations behind the Blue Brain Technology are treatment of brain disfunctioning, scientific curiosity about consciousness and human mind, a bottom up approach towards building thinking machine and databases of all neuroscientific research results and related past stories. There are three main steps 2to build the virtual brain are data acquisition, simulation and visualization of results. The mission is undertaking the Blue Brain technology is to gather all existing knowledge of the brain, raise the global research efficiency of reverse engineering and to build a complete theoretical framework.","",""
126,"W. Stead","Clinical Implications and Challenges of Artificial Intelligence and Deep Learning.",2018,"","","","",199,"2022-07-13 09:20:03","","10.1001/jama.2018.11029","","",,,,,126,31.50,126,1,4,"Artificial intelligence (AI) and deep learning are entering the mainstream of clinical medicine. For example, in December 2016, Gulshan et al1 reported development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. An accompanying editorial by Wong and Bressler2 pointed out limits of the study, the need for further validation of the algorithm in different populations, and unresolved challenges (eg, incorporating the algorithm into clinical work flows and convincing clinicians and patients to “trust a ‘black box’”). Sixteen months later, the Food and Drug Administration (FDA)3 permitted marketing of the first medical device to use AI to detect diabetic retinopathy. FDA reduced the risk of releasing the device by limiting the indication for use to screening adults who do not have visual symptoms for greater than mild retinopathy, to refer them to an eye care specialist. This issue of JAMA contains 2 Viewpoints on deep learning in health care. Hinton4 explains the technology underlying AI and deep learning, using clinical examples. AI is the general term for imitating human intelligence with computer systems. Early AI systems represented human reasoning with symbolic logic. As computer processing and storage became more powerful, researchers developed machine-learning techniques to imitate the way the human brain learns. The first machine learning continued to rely on human experts to label the data the system trained on (eg, the diagnosis) and to identify the significant features (eg, findings). Machine learning weighted the features from the data. With continued advances in computational power and with larger data sets, researchers began to develop deep learning techniques. The first deep learning algorithms were “supervised” in that human experts continued to label the training data, and the deep learning algorithms learned the features and weights directly from the data. The retinopathy screening algorithms are an example of supervised deep learning. Hinton4 describes continuing development of new deep learning techniques, including ones that are completely unsupervised. He also points out that it is not feasible to see the features learned by deep learning to explain how the system reaches a conclusion. Naylor5 identifies 7 factors driving adoption of AI and deep learning in health care: (1) the strengths of digital imaging over human interpretation; (2) the digitization of health-related records and data sharing; (3) the adaptability of deep learning to analysis of heterogeneous data sets; (4) the capacity of deep learning for hypothesis generation in research; (5) the promise of deep learning to streamline clinical workflows and empower patients; (6) the rapid-diffusion open-source and proprietary deep learning programs; and (7) of the adequacy of today’s basic deep learning technology to deliver improved performance as data sets get larger. Factors 3, 4, and 6 are specific to deep learning; the other factors apply to other AI techniques as well. Artificial intelligence is a family of technical techniques in the same way the radiologic imaging tool kit includes flat images, computed tomography scans, and functional imaging such as magnetic resonance imaging. Advances in computational technology, computer science, informatics, and statistics improve existing techniques and make new techniques possible. The addition of deep learning to the AI family of techniques represents an advance similar in magnitude to the addition of the computed tomography scanner to the radiology tool kit. Each AI technique has strengths and weaknesses. Symbolic logic is self-explaining but difficult to scale.6 For example, knowledge engineers extract the logic by interviewing or observing human experts. Statistical techniques such as supervised deep learning scale, but are subject to bias in the training data, and the reasoning cannot be explained. Since deep learning systems are trained on data from the past, they are not prepared to reason in the way humans do about conditions that have not been seen before. In the future, unsupervised deep learning may reduce this gap between human intelligence and AI. The potential applications of AI in health care present a range of computational difficulty. Narrow tasks, in which the context is predefined, are relatively easy. Imageprocessing tasks such as recognizing the border of an organ to suggest where to cut off a scan, or highlighting a suspicious area in an image for the radiologist or pathologist, are examples of narrow tasks. Image analysis and diagnostic prediction tasks such as the diabetic retinopathy example are broader and harder, but doable with today’s technology. Very broad data analysis and pattern prediction tasks such as analyzing heterogeneous data sets from diverse sources to suggest novel associations are feasible today because the purpose is limited to hypothesis generation. Thinking in the way humans do—reasoning, for example, from a few observations to suggest a novel scientific framework as Einstein did with the theory of relativity—is beyond technology on the horizon. Clinicians should view the output of AI programs or devices as statistical predictions. They should maintain an index of suspicion that the prediction may be wrong, just as they Viewpoint pages 1099 and 1101 Opinion","",""
92,"A. Annoni, P. Benczúr, P. Bertoldi, Blagoj Delipetrev, Giuditta De Prato, C. Feijóo, Enrique Fernández-Macías, E. Gutiérrez, M. Portela, H. Junklewitz, M. L. Cobo, B. Martens, Susana Nascimento, S. Nativi, Alexandre Pólvora, Jose Ignacio Sanchez Martin, Songuel Tolan, I. Tuomi, Lucia Vesnić Alujević","Artificial Intelligence: A European Perspective",2018,"","","","",200,"2022-07-13 09:20:03","","10.2760/11251","","",,,,,92,23.00,9,19,4,"We are only at the beginning of a rapid period of transformation of our economy and society due to the convergence of many digital technologies. Artificial Intelligence (AI) is central to this change and offers major opportunities to improve our lives. The recent developments in AI are the result of increased processing power, improvements in algorithms and the exponential growth in the volume and variety of digital data. Many applications of AI have started entering into our every-day lives, from machine translations, to image recognition, and music generation, and are increasingly deployed in industry, government, and commerce. Connected and autonomous vehicles, and AI-supported medical diagnostics are areas of application that will soon be commonplace. There is strong global competition on AI among the US, China, and Europe. The US leads for now but China is catching up fast and aims to lead by 2030. For the EU, it is not so much a question of winning or losing a race but of finding the way of embracing the opportunities offered by AI in a way that is human-centred, ethical, secure, and true to our core values. The EU Member States and the European Commission are developing coordinated national and European strategies, recognising that only together we can succeed. We can build on our areas of strength including excellent research, leadership in some industrial sectors like automotive and robotics, a solid legal and regulatory framework, and very rich cultural diversity also at regional and sub-regional levels. It is generally recognised that AI can flourish only if supported by a robust computing infrastructure and good quality data: â€¢ With respect to computing, we identified a window of opportunity for Europe to invest in the emerging new paradigm of computing distributed towards the edges of the network, in addition to centralised facilities. This will support also the future deployment of 5G and the Internet of Things. â€¢ With respect to data, we argue in favour of learning from successful Internet companies, opening access to data and developing interactivity with the users rather than just broadcasting data. In this way, we can develop ecosystems of public administrations, firms, and civil society enriching the data to make it fit for AI applications responding to European needs. We should embrace the opportunities afforded by AI but not uncritically. The black box characteristics of most leading AI techniques make them opaque even to specialists. AI systems are currently limited to narrow and well-defined tasks, and their technologies inherit imperfections from their human creators, such as the well-recognised bias effect present in data. We should challenge the shortcomings of AI and work towards strong evaluation strategies, transparent and reliable systems, and good human-AI interactions. Ethical and secure-by-design algorithms are crucial to build trust in this disruptive technology, but we also need a broader engagement of civil society on the values to be embedded in AI and the directions for future development. This social engagement should be part of the effort to strengthen our resilience at all levels from local, to national and European, across institutions, industry and civil society. Developing local ecosystems of skills, computing, data, and applications can foster the engagement of local communities, respond to their needs, harness local creativity and knowledge, and build a human-centred, diverse, and socially driven AI. We still know very little about how AI will impact the way we think, make decisions, relate to each other, and how it will affect our jobs. This uncertainty can be a source of concern but is also a sign of opportunity. The future is not yet written. We can shape it based on our collective vision of what future we would like to have. But we need to act together and act fast.","",""
