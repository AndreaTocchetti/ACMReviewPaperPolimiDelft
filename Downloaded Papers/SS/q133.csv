Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
0,"Aires da Conceição, S. Degadwala","Convolutional Neural Network Computation for Autonomous Vehicle",2020,"","","","",1,"2022-07-13 10:07:51","","10.32628/cseit2062112","","",,,,,0,0.00,0,2,2,"Self-driving vehicle is a vehicle that can drive by itself it means without human interaction. This system shows how the computer can learn and the over the art of driving using machine learning techniques. This technique includes line lane tracker, robust feature extraction and convolutional neural network.","",""
12,"Linqin Cai, Xiaoling Liu, Fuli Chen, Min Xiang","Robust human action recognition based on depth motion maps and improved convolutional neural network",2018,"","","","",2,"2022-07-13 10:07:51","","10.1117/1.JEI.27.5.051218","","",,,,,12,3.00,3,4,4,"Abstract. Human action recognition has been widely used in various fields of computer vision, pattern recognition, and human–computer interaction and has attracted substantial attention. Combining deep learning and depth information, this paper proposed a method of human action recognition based on improved convolutional neural networks (CNN). First, we use the depth motion maps to extract the depth sequence features and obtain three projected maps corresponding to front, side, and the top views. On this basis, an improved CNN is constructed to realize the recognition of human action, which uses three-dimensional (3-D) input and two-dimensional process identification to speed up the computation and reduce the complexity of recognition process. We evaluate our approach on two public 3-D action datasets: MSR Action3D dataset and UT-Kinect dataset, and our private CTP Action3D dataset built using Kinect to collect data. The experimental results show that the proposed methods of human action recognition achieve higher average recognition rate of 91.3% on MSR Action3D dataset, 97.98% on UT-Kinect dataset, and the average recognition rate is 93.8% on our CTP Action3D dataset. Furthermore, the trained model on one depth video sequence dataset can be easily generalized to different datasets without changing network parameters.","",""
11,"Hao Wang, Ruibin Feng, A. Leung, K. Tsang","Lagrange Programming Neural Network Approaches for Robust Time-of-Arrival Localization",2018,"","","","",3,"2022-07-13 10:07:51","","10.1007/s12559-017-9495-z","","",,,,,11,2.75,3,4,4,"","",""
1,"Á. F. Kungl","Robust learning algorithms for spiking and rate-based neural networks",2020,"","","","",4,"2022-07-13 10:07:51","","10.11588/HEIDOK.00028385","","",,,,,1,0.50,1,1,2,"Inspired by the remarkable properties of the human brain, the fields of machine learning, computational neuroscience and neuromorphic engineering have achieved significant synergistic progress in the last decade. Powerful neural network models rooted in machine learning have been proposed as models for neuroscience and for applications in neuromorphic engineering. However, the aspect of robustness is often neglected in these models. Both biological and engineered substrates show diverse imperfections that deteriorate the performance of computation models or even prohibit their implementation. This thesis describes three projects aiming at implementing robust learning with local plasticity rules in neural networks. First, we demonstrate the advantages of neuromorphic computations in a pilot study on a prototype chip. Thereby, we quantify the speed and energy consumption of the system compared to a software simulation and show how on-chip learning contributes to the robustness of learning. Second, we present an implementation of spike-based Bayesian inference on accelerated neuromorphic hardware. The model copes, via learning, with the disruptive effects of the imperfect substrate and benefits from the acceleration. Finally, we present a robust model of deep reinforcement learning using local learning rules. It shows how backpropagation combined with neuromodulation could be implemented in a biologically plausible framework. The results contribute to the pursuit of robust and powerful learning networks for biological and neuromorphic substrates.","",""
1,"A. Tellaeche, Ignacio Fidalgo Astorquia, Juan Ignacio Vázquez Gómez, S. Saikia","Gesture-Based Human Machine Interaction Using RCNNs in Limited Computation Power Devices",2021,"","","","",5,"2022-07-13 10:07:51","","10.3390/s21248202","","",,,,,1,1.00,0,4,1,"The use of gestures is one of the main forms of human machine interaction (HMI) in many fields, from advanced robotics industrial setups, to multimedia devices at home. Almost every gesture detection system uses computer vision as the fundamental technology, with the already well-known problems of image processing: changes in lighting conditions, partial occlusions, variations in color, among others. To solve all these potential issues, deep learning techniques have been proven to be very effective. This research proposes a hand gesture recognition system based on convolutional neural networks and color images that is robust against environmental variations, has a real time performance in embedded systems, and solves the principal problems presented in the previous paragraph. A new CNN network has been specifically designed with a small architecture in terms of number of layers and total number of neurons to be used in computationally limited devices. The obtained results achieve a percentage of success of 96.92% on average, a better score than those obtained by previous algorithms discussed in the state of the art.","",""
2,"S. Tan, Runpei Dong, Kaisheng Ma","Multi-Glimpse Network: A Robust and Efficient Classification Architecture based on Recurrent Downsampled Attention",2021,"","","","",6,"2022-07-13 10:07:51","","","","",,,,,2,2.00,1,3,1,"Most feedforward convolutional neural networks spend roughly the same efforts for each pixel. Yet human visual recognition is an interaction between eye movements and spatial attention, which we will have several glimpses of an object in different regions. Inspired by this observation, we propose an end-to-end trainable M ulti- G limpse N etwork ( MGNet ) which aims to tackle the challenges of high computation and the lack of robustness based on recurrent downsampled attention mechanism. Speciﬁcally, MGNet sequentially selects task-relevant regions of an image to focus on and then adaptively combines all collected information for the ﬁnal prediction. MGNet expresses higher resistance against adversarial attacks and common corruptions with less computation. Also, MGNet is inherently more interpretable as it explicitly informs us where it focuses during each iteration. Our experiments on ImageNet100 demonstrate the potential of recurrent downsampled attention mechanisms to improve a single feedforward manner. For example, MGNet improves 4.76% accuracy on average in common corruptions with only 36.9% computational cost. Moreover, while the baseline incurs an accuracy drop to 7.6%, MGNet manages to maintain 44.2% accuracy in the same PGD attack strength with ResNet-50 backbone. Our code is available at https: //github.com/siahuat0727/MGNet .","",""
2,"Jagadeesh Basavaiah, C. Patil","HUMAN ACTIVITY DETECTION AND ACTION RECOGNITION IN VIDEOS USING CONVOLUTIONAL NEURAL NETWORKS",2020,"","","","",7,"2022-07-13 10:07:51","","10.32890/JICT2020.19.2.1","","",,,,,2,1.00,1,2,2,"Human activity recognition from video scenes has become a significant area of research in the field of computer vision applications. Action recognition is one of the most challenging problems in the area of video analysis and it finds applications in human-computer interaction, anomalous activity detection, crowd monitoring and patient monitoring. Several approaches have been presented for human activity recognition using machine learning techniques. The main aim of this work is to detect and track human activity, and classify actions for two publicly available video databases. In this work, a novel approach of feature extraction from video sequence by combining Scale Invariant Feature Transform and optical flow computation are used where shape, gradient and orientation features are also incorporated for robust feature formulation. Tracking of human activity in the video is implemented using the Gaussian Mixture Model. Convolutional Neural Network based classification approach is used for database training and testing purposes. The activity recognition performance is evaluated for two public datasets namely Weizmann dataset and Kungliga Tekniska Hogskolan dataset with action recognition accuracy of 98.43% and 94.96%, respectively. Experimental and comparative studies have shown that the proposed approach outperformed state-of the art techniques.","",""
0,"Ibrahim Batuhan Akkaya, Kaan Karaman","A robust technique for real-time face verification with a generative network",2020,"","","","",8,"2022-07-13 10:07:51","","10.1117/12.2558526","","",,,,,0,0.00,0,2,2,"Real-time face verification from a live stream is still an open question, although it is quite popular in recent years. In order to overcome this problem, several bio-metric techniques are widely used for authentication purposes, both in military and civilian areas. It is merely the task of detecting and comparing a candidate face with the other faces in the database to validate whether they are the same person or not. Typically, A face verification pipeline is composed of four stages: Face detection, alignment, recognition, and matching. The faces in the frames of the live stream are detected via a deep neural network (DNN) in the first part. Then, the detected faces are aligned, and another DNN extracts the face features. The feature vector of each face is used for matching with other vectors in the database to validate the identity. New developments in deep learning lead to achieving human-level performance on the aforementioned tasks. However, the networks used in the stages require high computation power. In order to achieve real-time performance in resource-limited devices, lightweight networks should be preferred. Unfortunately, usage of these kinds of networks decreases the detection and recognition performance dramatically in some frames of a live stream. Therefore, the set of feature vectors for an individual, collected from the live stream, contains outliers that complicate obtaining a robust reference feature vector, which is essential for achieving high confidence in verification tasks. In this work, a conditional generative network is utilized for generating these vectors for the given candidate. We conduct the experiments on a real-life scenario for showing the incrementation of performance that is caused by our proposed generative network.","",""
0,"J. Conradt","Bayesian Computation in Recurrent Neural Circuits-a Neuro-Control Approach",2017,"","","","",9,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,1,5,"Over the past years, Bayesian probability theory became a popular tool for modeling human sensorimotor control and behavior. It is commonly used in the creation of optimal controllers due to its robustness in the presence of uncertainty and noise which is also a pervasive property within human sensory measurements and dynamic control tasks like saccadic eye motion. The Hidden Markov Model is an explicit example for a dynamic Bayesian network. The idea of this work is to implement the latter as a state estimator of velocity, location, and movement direction of a moving object within a robotic controller in order to recreate human-like saccadic motion. The visual input is given by a neuromorphic dynamic-vision sensor (DVS). This is fed into the Bayesian network which is built as a recurrent neural model inspired by visual processing in the brain. The motion detection network proves to be robust against noise, detects multiple stimuli, and is able to react to motion perturbations. A winner-takes-all policy, with additional movement cost considerations, then generates output initiating a saccadic movement in order to center the moving stimulus in the foveal area of the visual field. Moreover, further model extensions are addressed which could enable the controller to produce smooth pursuit behavior a more complex eye movement for continuous tracking.","",""
1,"Neziha Jaouedi, Noureddine Boujnah, M. Bouhlel","A novel recurrent neural networks architecture for behavior analysis",2021,"","","","",10,"2022-07-13 10:07:51","","10.34028/iajit/18/2/1","","",,,,,1,1.00,0,3,1,"Behavior analysis is an important yet challenging task on computer vision area. However, human behavior is still a necessity in differents sectors. In fact, in the increase of crimes, everyone needs video surveillance to keep their belongings safe and to automatically detect events by collecting important information for the assistance of security guards. Moreover, the surveillance of human behavior is recently used in medicine fields to quickly detect physical and mental health problems of patients. The complex and the variety presentation of human features in video sequence encourage researches to find the effective presentation. An effective presentation is the most challenging part. It must be invariant to changes of point of view, robust to noise and efficient with a low computation time. In this paper, we propose new model for human behavior analysis which combine transfer learning model and Recurrent Neural Network (RNN). Our model can extract human features from frames using the pre-trained model of Convolutional Neural Network (CNN) the Inception V3. The human features obtained are trained using RNN with Gated Recurrent Unit (GRU). The performance of our proposed architecture is evaluated by three different dataset for human action, UCF Sport, UCF101 and KTH, and achieved good classification accuracy","",""
3,"Vasco Lopes, Saeid Alirezazadeh, L. A. Alexandre","EPE-NAS: Efficient Performance Estimation Without Training for Neural Architecture Search",2021,"","","","",11,"2022-07-13 10:07:51","","10.1007/978-3-030-86383-8_44","","",,,,,3,3.00,1,3,1,"","",""
2,"Taro Kiritani, Koji Ono","Recurrent Attention Model with Log-Polar Mapping is Robust against Adversarial Attacks",2020,"","","","",12,"2022-07-13 10:07:51","","","","",,,,,2,1.00,1,2,2,"Convolutional neural networks are vulnerable to small $\ell^p$ adversarial attacks, while the human visual system is not. Inspired by neural networks in the eye and the brain, we developed a novel artificial neural network model that recurrently collects data with a log-polar field of view that is controlled by attention. We demonstrate the effectiveness of this design as a defense against SPSA and PGD adversarial attacks. It also has beneficial properties observed in the animal visual system, such as reflex-like pathways for low-latency inference, fixed amount of computation independent of image size, and rotation and scale invariance. The code for experiments is available at this https URL.","",""
6,"R. Hamad, Masashi Kimura, Longzhi Yang, W. L. Woo, Bo Wei","Dilated causal convolution with multi-head self attention for sensor human activity recognition",2021,"","","","",13,"2022-07-13 10:07:51","","10.1007/S00521-021-06007-5","","",,,,,6,6.00,1,5,1,"","",""
0,"Yaou Zhao, Yuehui Chen, M. Jiang","A novel ensemble of probabilistic neural network for predicting protein-protein interactions",2012,"","","","",14,"2022-07-13 10:07:51","","10.1109/BMEI.2012.6513055","","",,,,,0,0.00,0,3,10,"The knowledge of protein-protein interactions (PPIs) in cells is indispensable for deep understanding the biological process. Although many computational methods have been developed for identification of PPIs, there are still many difficulties due to high computation complexity and noisy data. In this paper, we proposed an ensemble of probabilistic neural network (PNN) to predict PPIs from primary sequence which achieved promising results. The key advantage of the algorithm is that it combines variety of physicochemical property features to construct diverse individual classifiers for ensemble prediction. What makes the method much more attractive is that it not only generated much more diverse and robust individual classifiers, but also contains different interaction physicochemical information which dictated the structure and the function of proteins. Moreover, the PNN is robust to noise and trained easily, it is suitable for dealing with the large scale noisy PPIs data. Experiment results on H. pylori and Human datasets show that our proposed method performs at least 8% higher accuracy than the best of other related works.","",""
74,"D. Krotov, J. Hopfield","Dense Associative Memory Is Robust to Adversarial Inputs",2017,"","","","",15,"2022-07-13 10:07:51","","10.1162/neco_a_01143","","",,,,,74,14.80,37,2,5,"Deep neural networks (DNNs) trained in a supervised way suffer from two known problems. First, the minima of the objective function used in learning correspond to data points (also known as rubbish examples or fooling images) that lack semantic similarity with the training data. Second, a clean input can be changed by a small, and often imperceptible for human vision, perturbation so that the resulting deformed input is misclassified by the network. These findings emphasize the differences between the ways DNNs and humans classify patterns and raise a question of designing learning algorithms that more accurately mimic human perception compared to the existing methods. Our article examines these questions within the framework of dense associative memory (DAM) models. These models are defined by the energy function, with higher-order (higher than quadratic) interactions between the neurons. We show that in the limit when the power of the interaction vertex in the energy function is sufficiently large, these models have the following three properties. First, the minima of the objective function are free from rubbish images, so that each minimum is a semantically meaningful pattern. Second, artificial patterns poised precisely at the decision boundary look ambiguous to human subjects and share aspects of both classes that are separated by that decision boundary. Third, adversarial images constructed by models with small power of the interaction vertex, which are equivalent to DNN with rectified linear units, fail to transfer to and fool the models with higher-order interactions. This opens up the possibility of using higher-order models for detecting and stopping malicious adversarial attacks. The results we present suggest that DAMs with higher-order energy functions are more robust to adversarial and rubbish inputs than DNNs with rectified linear units.","",""
0,"Rassa Ghavami Modegh, Ahmadali Salimi, H. Rabiee","LAP: An Attention-Based Module for Faithful Interpretation and Knowledge Injection in Convolutional Neural Networks",2022,"","","","",16,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,3,1,"Despite the state-of-the-art performance of deep convolutional neural networks, they are susceptible to bias and malfunction in unseen situations. The complex computation behind their reasoning is not sufficiently human-understandable to develop trust. External explainer methods have tried to interpret the network decisions in a human-understandable way, but they are accused of fallacies due to their assumptions and simplifications. On the other side, the inherent self-interpretability of models, while being more robust to the mentioned fallacies, cannot be applied to the already trained models. In this work, we propose a new attention-based pooling layer, called Local Attention Pooling (LAP), that accomplishes self-interpretability and the possibility for knowledge injection while improving the model’s performance. Moreover, several weakly-supervised knowledge injection methodologies are provided to enhance the process of training. We verified our claims by evaluating several LAP-extended models on three different datasets, including Imagenet. The proposed framework offers more valid humanunderstandable and more faithful-to-the-model interpretations than the commonly used white-box explainer methods.","",""
19,"He Xu, Chang Shu-juan","An Adaptive Image Watermarking Algorithm Based on Neural Network",2011,"","","","",17,"2022-07-13 10:07:51","","10.1109/ICICTA.2011.386","","",,,,,19,1.73,10,2,11,"According to the characteristic of Human Visual System and the association memory ability of neural network, an adaptive image watermarking algorithm based on neural network is proposed. The watermarking signal is embedded in higher frequency, which is in the lower frequency of original image by DWT joined with DCT. The ability of attracting is improved by pretreatment and re-treatment of image scrambling and Hop field network. Experimental results show that the embedded digital watermarking is invisible and robust enough against common image processing such as JPEG compression, noises, filter and shearing.","",""
16,"Tamas Jantvik, L. Gustafsson, A. Paplinski","A Self-Organized Artificial Neural Network Architecture for Sensory Integration with Applications to Letter-Phoneme Integration",2011,"","","","",18,"2022-07-13 10:07:51","","10.1162/NECO_a_00149","","",,,,,16,1.45,5,3,11,"The multimodal self-organizing network (MMSON), an artificial neural network architecture carrying out sensory integration, is presented here. The architecture is designed using neurophysiological findings and imaging studies that pertain to sensory integration and consists of interconnected lattices of artificial neurons. In this artificial neural architecture, the degree of recognition of stimuli, that is, the perceived reliability of stimuli in the various subnetworks, is included in the computation. The MMSON's behavior is compared to aspects of brain function that deal with sensory integration. According to human behavioral studies, integration of signals from sensory receptors of different modalities enhances perception of objects and events and also reduces time to detection. In neocortex, integration takes place in bimodal and multimodal association areas and result, not only in feedback-mediated enhanced unimodal perception and shortened reaction time, but also in robust bimodal or multimodal percepts. Simulation data from the presented artificial neural network architecture show that it replicates these important psychological and neuroscientific characteristics of sensory integration.","",""
0,"M. D. Putro, Duy-Linh Nguyen, K. Jo","A Fast Real-time Facial Expression Classifier Deep Learning-based for Human-robot Interaction",2021,"","","","",19,"2022-07-13 10:07:51","","10.23919/ICCAS52745.2021.9650034","","",,,,,0,0.00,0,3,1,"Human-robot interaction drives the need for vision technology to recognize user expressions. Convolutional Neural Networks (CNN) has been introduced as a robust facial feature extractor and can overcome classification task. However, it is not supported by efficient computation for real-time applications. The work proposes an efficient CNN architecture to recognize human facial expressions that consist of five stages containing a combination of lightweight convolution operations. It introduces the efficient contextual extractor with a partial transfer module to suppress computational compression. This technique is applied to the mid and high-level features by separating the channel-based input features into two parts. Then it applies sequential convolution to only one part and combines it with the previous separated part. A shuffle channel group is used to exchange the information extracted. The structure of the entire network generates less than a million parameters. The CK+ and KDEF datasets are used as training and test sets to evaluate the performance of the proposed architecture. As a result, the proposed classifier obtains an accuracy that is competitive with other methods. In addition, the efficiency of the classifier has strongly suitable for implementation to edge devices by achieving 43 FPS on a Jetson Nano.","",""
28,"Gabriel L. Oliveira, Claas Bollen, W. Burgard, T. Brox","Efficient and robust deep networks for semantic segmentation",2018,"","","","",20,"2022-07-13 10:07:51","","10.1177/0278364917710542","","",,,,,28,7.00,7,4,4,"This paper explores and investigates deep convolutional neural network architectures to increase the efficiency and robustness of semantic segmentation tasks. The proposed solutions are based on up-convolutional networks. We introduce three different architectures in this work. The first architecture, called Part-Net, is designed to tackle the specific problem of human body part segmentation and to provide robustness to overfitting and body part occlusion. The second network, called Fast-Net, is a network specifically designed to provide the smallest computation load without losing representation power. Such an architecture is capable of being run on mobile GPUs. The last architecture, called M-Net, aims to maximize the robustness characteristics of deep semantic segmentation approaches through multiresolution fusion. The networks achieve state-of-the-art performance on the PASCAL Parts dataset and competitive results on the KITTI dataset for road and lane segmentation. Moreover, we introduce a new part segmentation dataset, the Freiburg City dataset, which is designed to bring semantic segmentation to highly realistic robotics scenarios. Additionally, we present results obtained with a ground robot and an unmanned aerial vehicle and a full system to explore the capabilities of human body part segmentation in the context of human–robot interaction.","",""
0,"Ziwei Chen, Yiye Wang, Wankou Yang","Video Based Fall Detection Using Human Poses",2021,"","","","",21,"2022-07-13 10:07:51","","10.1007/978-981-16-9709-8_19","","",,,,,0,0.00,0,3,1,"","",""
1,"Shekhar Nayak, D. Shashank, Saurabhchand Bhati, Koilakuntla Bramhendra, K. S. R. Murty","Instantaneous Frequency Features for Noise Robust Speech Recognition",2019,"","","","",22,"2022-07-13 10:07:51","","10.1109/NCC.2019.8732216","","",,,,,1,0.33,0,5,3,"Analytic phase of the speech signal plays an important role in human speech perception, specially in the presence of noise. Generally, phase information is ignored in most of the recent speech recognition systems. In this paper, we illustrate the importance of analytic phase of the speech signal for noise robust automatic speech recognition. To avoid phase wrapping problem involved in the computation of analytic phase, features are extracted from instantaneous frequency (IF) which is time derivative of analytic phase. Deep neural network (DNN) based acoustic models are trained on clean speech using features extracted from the IF of speech signals. Robustness of IF features in combination with mel-frequency cepstral coefficients (MFCCs) was evaluated in varied noisy conditions. System combination using minimum Bayes risk decoding of IF features with MFCCs delivered absolute improvements of upto 13% over MFCC features alone for DNN based systems under noisy conditions. The impact of the system combination of magnitude and phase based features on different phonetic classes was studied under noisy conditions and was found to model both voiced and unvoiced phonetic classes efficiently.","",""
11,"Nan Tian, Benjamin Kuo, X. Ren, Michael Yu, Robert Zhang, Bill Huang, Ken Goldberg, S. Sojoudi","A Cloud-Based Robust Semaphore Mirroring System for Social Robots",2018,"","","","",23,"2022-07-13 10:07:51","","10.1109/COASE.2018.8560553","","",,,,,11,2.75,1,8,4,"We present a cloud-based human-robot interaction system that automatically controls a humanoid robot to mirror a human demonstrator performing flag semaphores. We use a cloud-based framework called Human Augmented Robotic Intelligence (HARI) to perform gesture recognition of the human demonstrator and gesture control of a local humanoid robot, named Pepper. To ensure that the system is real-time, we design a system to maximize cloud computation contribution to the deep-neural-network-based gesture recognition system, OpenPose, and to minimize communication costs between the cloud and the robot. A hybrid control system is used to hide latency caused by either routing or physical distances. We conducted real-time semaphore mirroring experiments in which both the robots and the demonstrator were located in Tokyo, Japan, whereas the cloud server was deployed in the United States. The total latency was 400ms for the video streaming to the cloud and 108ms for the robot commanding from the cloud. Further, we measured the reliability of our gesture-based semaphore recognition system with two human subjects, and were able to achieve 90% and 76.7% recognition accuracy, respectively, for the two subjects with open-loop when the subjects were not allowed to see the recognition results. We could achieve 100% recognition accuracy when both subjects were allowed to adapt to the recognition system under a closed-loop setting. Lastly, we showed that we can support two humanoid robots with a single server at the same time. With this real-time cloud-based HRI system, we illustrate that we can deploy gesture-based human-robot globally and at scale.","",""
8,"Li Ji, Xiao-dong Wang, S. Luo, Liang Qin, Xv-jie Yang, Shushen Liu, Lian-Sheng Wang","QSAR study on estrogenic activity of structurally diverse compounds using generalized regression neural network",2008,"","","","",24,"2022-07-13 10:07:51","","10.1007/S11426-008-0070-Z","","",,,,,8,0.57,1,7,14,"","",""
4,"Li Jing, Fenlin Liu","Applying General Regression Neural Network in Digital Image Watermarking",2008,"","","","",25,"2022-07-13 10:07:51","","10.1109/ICNC.2008.175","","",,,,,4,0.29,2,2,14,"Combing the human visual system with the image local correlation properties, a robust digital image watermarking method based on general regression neural network (GRNN) and fuzzy c-mean clustering algorithm (FCM) is proposed. It uses FCM to adaptively identify watermarking embedding location and strength and uses trained GRNN to help watermarking embedding and extracting. In watermark extracting it does not need original image. Experimental results show that the proposed method has better performance than the similar method in countering common image process, such as JPEG compression, noise adding, filtering and so on.","",""
85,"Xinyu Zhang, Q. Wang, Jian Zhang, Zhaobai Zhong","Adversarial AutoAugment",2019,"","","","",26,"2022-07-13 10:07:51","","","","",,,,,85,28.33,21,4,3,"Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. Through finding the best policy in well-designed search space of data augmentation, AutoAugment can significantly improve validation accuracy on image classification tasks. However, this approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object and augmentation policy search loss. The augmentation policy network attempts to increase the training loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we reuse the computation in target network training for policy evaluation, and dispense with the retraining of the target network. Compared to AutoAugment, this leads to about 12x reduction in computing cost and 11x shortening in time overhead on ImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100, ImageNet, and demonstrate significant performance improvements over state-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36%, which is the currently best performing single model. On ImageNet, we achieve a leading performance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D without extra data.","",""
10,"Asanka G. Perera, Yee Wei Law, J. Chahl","Human Pose and Path Estimation from Aerial Video Using Dynamic Classifier Selection",2018,"","","","",27,"2022-07-13 10:07:51","","10.1007/s12559-018-9577-6","","",,,,,10,2.50,3,3,4,"","",""
4,"R. Gentili, Hyuk Oh, Javier Molina, J. Contreras-Vidal","Cortical network modeling for inverse kinematic computation of an anthropomorphic finger",2011,"","","","",28,"2022-07-13 10:07:51","","10.1109/IEMBS.2011.6092034","","",,,,,4,0.36,1,4,11,"The performance of reaching movements to visual targets requires complex kinematic mechanisms such as redundant, multijointed, anthropomorphic actuators and thus is a difficult problem since the relationship between sensory and motor coordinates is highly nonlinear. In this article, we present a neural model able to learn the inverse kinematics of a simulated anthropomorphic robot finger (ShadowHand™ finger) having four degrees of freedom while performing 3D reaching movements. The results revealed that this neural model was able to control accurately and robustly the finger when performing single 3D reaching movements as well as more complex patterns of motion while generating kinematics comparable to those observed in human. The long term goal of this research is to design a bio-mimetic controller providing adaptive, robust and flexible control of dexterous robotic/prosthetics hands.","",""
11,"Wei Jiang, Wei Wang","Face detection and recognition for home service robots with end-to-end deep neural networks",2017,"","","","",29,"2022-07-13 10:07:51","","10.1109/ICASSP.2017.7952553","","",,,,,11,2.20,6,2,5,"This paper proposes an effective end-to-end face detection and recognition framework based on deep convolutional neural networks for home service robots. We combine the state-of-the-art region proposal based deep detection network with the deep face embedding network into an end-to-end system, so that the detection and recognition networks can share the same deep convolutional layers, enabling significant reduction of computation through sharing convolutional features. The detection network is robust to large occlusion, and scale, pose, and lighting variations. The recognition network does not require explicit face alignment, which enables an effective training strategy to generate a unified network. A practical robot system is also developed based on the proposed framework, where the system automatically asks for a minimum level of human supervision when needed, and no complicated region-level face annotation is required. Experiments are conducted over WIDER and LFW benchmarks, as well as a personalized dataset collected from an office setting, which demonstrate state-of-the-art performance of our system.","",""
23,"W. Wen, Feng Yan, H. Li","AutoGrow: Automatic Layer Growing in Deep Convolutional Networks",2019,"","","","",30,"2022-07-13 10:07:51","","10.1145/3394486.3403126","","",,,,,23,7.67,8,3,3,"Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We proposeAutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture,AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures,AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off,AutoGrow discovers a better depth combination in \resnets than human experts. OurAutoGrow is efficient. It discovers depth within similar time of training a single DNN. Our code is available at \urlhttps://github.com/wenwei202/autogrow.","",""
32,"Salim Arslan, Sarah Parisot, D. Rueckert","Joint Spectral Decomposition for the Parcellation of the Human Cerebral Cortex Using Resting-State fMRI",2015,"","","","",31,"2022-07-13 10:07:51","","10.1007/978-3-319-19992-4_7","","",,,,,32,4.57,11,3,7,"","",""
0,"Sivaji Satrasupalli, Ebenezer Daniel, Sitaramanjaneya Reddy Gunturu","Single image haze removal using CNN based encoder-decoder architecture",2022,"","","","",32,"2022-07-13 10:07:51","","10.1109/IC3IOT53935.2022.9767867","","",,,,,0,0.00,0,3,1,"Haze is a natural phenomenon, which severely obscure the visibility of the distant objects makes it difficult for both the autonomous and human driving vehicles to take appropriate decision and may cause an accident. An efficient and robust solution in removing haze will help in reducing accidents. Recently, many prior based dehazing algorithms were proposed and doing fairly good but computationally intensive. In this contribution, we have proposed a computationally efficient encoder-decoder model based on up sampling and down sampling was used in convolutional neural network (CNN). The model was trained to update weight matrix using different datasets such as RESIDE and FRIDA for getting model to experience wide variety of data. Maxpooling and dropout layers were used to get advantage in both computation and for better generalization on new data. Objective analysis has shown that the proposed architecture doing relatively better in terms of SSIM & PSNR compared with the recent methods.","",""
0,"Adversarial Autoaugment, Xinyu Zhang, Qian Wang","A DVERSARIAL A UTO A UGMENT",2022,"","","","",33,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,3,1,"Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. Through finding the best policy in well-designed search space of data augmentation, AutoAugment (Cubuk et al., 2018) can significantly improve validation accuracy on image classification tasks. However, this approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object and augmentation policy search loss. The augmentation policy network attempts to increase the training loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we reuse the computation in target network training for policy evaluation, and dispense with the retraining of the target network. Compared to AutoAugment, this leads to about 12× reduction in computing cost and 11× shortening in time overhead on ImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100, ImageNet, and demonstrate significant performance improvements over state-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36%, which is the currently best performing single model. On ImageNet, we achieve a leading performance of top-1 accuracy 79.40% on ResNet-50 and 80.00% on ResNet-50-D without extra data.","",""
11,"Benjamin Busam, Hyunjun Jung, N. Navab","I Like to Move It: 6D Pose Estimation as an Action Decision Process",2020,"","","","",34,"2022-07-13 10:07:51","","","","",,,,,11,5.50,4,3,2,"Object pose estimation is an integral part of robot vision and augmented reality. Robust and accurate pose prediction of both object rotation and translation is a crucial element to enable precise and safe human-machine interactions and to allow visualization in mixed reality. Previous 6D pose estimation methods treat the problem either as a regression task or discretize the pose space to classify. We reformulate the problem as an action decision process where an initial pose is updated in incremental discrete steps that sequentially move a virtual 3D rendering towards the correct solution. A neural network estimates likely moves from a single RGB image iteratively and determines so an acceptable final pose. In comparison to previous approaches that learn an object-specific pose embedding, a decision process allows for a lightweight architecture while it naturally generalizes to unseen objects. Moreover, the coherent action for process termination enables dynamic reduction of the computation cost if there are insignificant changes in a video sequence. While other methods only provide a static inference time, we can thereby automatically increase the runtime depending on the object motion. We evaluate robustness and accuracy of our action decision network on video scenes with known and unknown objects and show how this can improve the state-of-the-art on YCB videos significantly.","",""
4,"Daniel Seichter, Benjamin Lewandowski, Dominik Höchemer, Tim Wengefeld, H. Groß","Multi-Task Deep Learning for Depth-based Person Perception in Mobile Robotics",2020,"","","","",35,"2022-07-13 10:07:51","","10.1109/IROS45743.2020.9340870","","",,,,,4,2.00,1,5,2,"Efficient and robust person perception is one of the most basic skills a mobile robot must have to ensure intuitive human-machine interaction. In addition to person detection, this also includes estimating various attributes, like posture or body orientation, in order to achieve user-adaptive behavior. However, given limited computing and battery capabilities on a mobile robot, it is inefficient to solve all perception tasks separately, especially when using computationally expensive deep neural networks. Therefore, we propose a multi-task system for person perception, comprising of a fast, depth- based region proposal and an efficient, lightweight deep neural network. Using a single network forward pass, the system simultaneously detects persons, classifies their body postures, and estimates the upper body orientations while retaining almost the same computation time as a single-task network. We describe how to handle a real-world multi-task scenario and conduct an extensive series of experiments in order to compare various network architectures and task weightings. We further show that multi-task learning improves the networks’ performance compared to their single-task baselines. For training and evaluation, we combine an existing dataset for orientation estimation and a new, self-recorded dataset, consisting of more than 235,000 depth patches that is made publicly available to the research community.","",""
5,"Rohini Goel, Avinash Sharma, Rajiv Kapoor","State-of-the-Art Object Recognition Techniques: A Comparative Study",2020,"","","","",36,"2022-07-13 10:07:51","","10.1007/978-981-15-0751-9_85","","",,,,,5,2.50,2,3,2,"","",""
2,"Will X. Y. Li, K. Cui, Wei Zhang","A memory efficient implementation scheme of Gauss error function in a Laguerre-Volterra network for neuroprosthetic devices.",2017,"","","","",37,"2022-07-13 10:07:51","","10.1063/1.4980058","","",,,,,2,0.40,1,3,5,"Cognitive neural prosthesis is a manmade device which can be used to restore or compensate for lost human cognitive modalities. The generalized Laguerre-Volterra (GLV) network serves as a robust mathematical underpinning for the development of such prosthetic instrument. In this paper, a hardware implementation scheme of Gauss error function for the GLV network targeting reconfigurable platforms is reported. Numerical approximations are formulated which transform the computation of nonelementary function into combinational operations of elementary functions, and memory-intensive look-up table (LUT) based approaches can therefore be circumvented. The computational precision can be made adjustable with the utilization of an error compensation scheme, which is proposed based on the experimental observation of the mathematical characteristics of the error trajectory. The precision can be further customizable by exploiting the run-time characteristics of the reconfigurable system. Compared to the polynomial expansion based implementation scheme, the utilization of slice LUTs, occupied slices, and DSP48E1s on a Xilinx XC6VLX240T field-programmable gate array has decreased by 94.2%, 94.1%, and 90.0%, respectively. While compared to the look-up table based scheme, 1.0×1017 bits of storage can be spared under the maximum allowable error of 1.0×10-3. The proposed implementation scheme can be employed in the study of large-scale neural ensemble activity and in the design and development of neural prosthetic device.","",""
11,"P. Poudel, A. Illanes, E. Ataide, N. Esmaeili, Sathish Balakrishnan, M. Friebe","Thyroid Ultrasound Texture Classification Using Autoregressive Features in Conjunction With Machine Learning Approaches",2019,"","","","",38,"2022-07-13 10:07:51","","10.1109/ACCESS.2019.2923547","","",,,,,11,3.67,2,6,3,"The thyroid is one of the largest endocrine glands in the human body, which is involved in several body mechanisms like controlling protein synthesis, use of energy sources, and controlling the body’s sensitivity to other hormones. Thyroid segmentation and volume reconstruction are hence essential to diagnose thyroid related diseases as most of these diseases involve a change in the shape and size of the thyroid over time. Classification of thyroid texture is the first step toward the segmentation of the thyroid. The classification of texture in thyroid Ultrasound (US) images is not an easy task as it suffers from low image contrast, presence of speckle noise, and non-homogeneous texture distribution inside the thyroid region. Hence, a robust algorithmic approach is required to accurately classify thyroid texture. In this paper, we propose three machine learning based approaches: Support Vector Machine; Artificial Neural Network; and Random Forest Classifier to classify thyroid texture. The computation of features for training these classifiers is based on a novel approach recently proposed by our team, where autoregressive modeling was applied on a signal version of the 2D thyroid US images to compute 30 spectral energy-based features for classifying the thyroid and non-thyroid textures. Our approach differs from the methods proposed in the literature as they use image-based features to characterize thyroid tissues. We obtained an accuracy of around 90% with all the three methods.","",""
24,"Zhixin Yang, Lulu Tang, Kun Zhang, P. Wong","Multi-View CNN Feature Aggregation with ELM Auto-Encoder for 3D Shape Recognition",2018,"","","","",39,"2022-07-13 10:07:51","","10.1007/s12559-018-9598-1","","",,,,,24,6.00,6,4,4,"","",""
1,"C. Wan","Electric-Double-Layer Coupled Oxide-Based Neuromorphic Transistors Studies",2018,"","","","",40,"2022-07-13 10:07:51","","10.1007/978-981-13-3314-9","","",,,,,1,0.25,1,1,4,"","",""
2,"C. Jin","Adaptive Digital Watermark System Using Soft Computation",2010,"","","","",41,"2022-07-13 10:07:51","","10.2316/Journal.202.2010.3.202-2846","","",,,,,2,0.17,2,1,12,"Abstract Digital watermark has been proposed as an effective method to protect the copyright of multimedia data. An adaptive watermark system of still image in discrete cosine transform (DCT) domain is proposed. By exploiting the luminance and texture characteristics of human visual system (HVS) and considering properties of DCT block coefficient, a soft computing framework is designed. adopts the feature vectors composed the luminance and texture information of each DCT block, the fuzzy clustering (FC) for more reliable classifying to DCT blocks, and neural network (NN) for determining adaptively the best watermark embedding strength of each DCT block are used. The experimental results show that the proposed system are robust against attacks most commonly used image processing algorithms and provides very good results in term of image imperceptibility.","",""
44,"R. T. Santos, J. C. Nievola, A. Freitas","Extracting comprehensible rules from neural networks via genetic algorithms",2000,"","","","",42,"2022-07-13 10:07:51","","10.1109/ECNN.2000.886228","","",,,,,44,2.00,15,3,22,"A common problem in KDD (Knowledge Discovery in Databases) is the presence of noise in the data being mined. Neural networks are robust and have a good tolerance to noise, which makes them suitable for mining very noisy data. However, they have the well-known disadvantage of not discovering any high-level rule that can be used as a support for human decision making. In this work we present a method for extracting accurate, comprehensible rules from neural networks. The proposed method uses a genetic algorithm to find a good neural network topology. This topology is then passed to a rule extraction algorithm, and the quality of the extracted rules is then fed back to the genetic algorithm. The proposed system is evaluated on three public-domain data sets and the results show that the approach is valid.","",""
2,"Albert Gong, Qiang Qiu, G. Sapiro","Virtual CNN Branching: Efficient Feature Ensemble for Person Re-Identification",2018,"","","","",43,"2022-07-13 10:07:51","","","","",,,,,2,0.50,1,3,4,"In this paper we introduce an ensemble method for convolutional neural network (CNN), called ""virtual branching,"" which can be implemented with nearly no additional parameters and computation on top of standard CNNs. We propose our method in the context of person re-identification (re-ID). Our CNN model consists of shared bottom layers, followed by ""virtual"" branches, where neurons from a block of regular convolutional and fully-connected layers are partitioned into multiple sets. Each virtual branch is trained with different data to specialize in different aspects, e.g., a specific body region or pose orientation. In this way, robust ensemble representations are obtained against human body misalignment, deformations, or variations in viewing angles, at nearly no any additional cost. The proposed method achieves competitive performance on multiple person re-ID benchmark datasets, including Market-1501, CUHK03, and DukeMTMC-reID.","",""
89,"J. Suri","Two-dimensional fast magnetic resonance brain segmentation",2001,"","","","",44,"2022-07-13 10:07:51","","10.1109/51.940054","","",,,,,89,4.24,89,1,21,"Region-based level-set snakes are a very powerful technique for segmenting white matter/grey matter in MR slices of human brain. We showed how one can apply the region-based level-set technique for segmenting the brain using fast techniques. The system used the fuzzy clustering method for computing the fuzzy membership values, which were used in the regional speed computation. Recently, the authors have developed a mathematical morphology-based speed control function that acts as a regularizer for making the propagation more robust and leak free. It would also be worth exploring how either the neural network or learning models would do in terms of the performance evaluation if clustering was to be replaced.","",""
18,"Jong-Min Kim, Myunga Kang","A Study of Face Recognition Using the PCA and Error Back-Propagation",2010,"","","","",45,"2022-07-13 10:07:51","","10.1109/IHMSC.2010.160","","",,,,,18,1.50,9,2,12,"In this paper the real-time face region was detected by suggesting the rectangular feature-based classifier and the robust detection algorithm that satisfied the efficiency of computation and detection performance was suggested. By using the detected face region as a recognition input image, in this paper the face recognition method combined with PCA and the multi-layer network which is one of the intelligent classification was suggested and its performance was evaluated. As a pre-processing algorithm of input face image, this method computes the eigenface through PCA and expresses the training images with it as a fundamental vector. Each image takes the set of weights for the fundamental vector as a feature vector and it reduces the dimension of image at the same time, and then the face recognition is performed by inputting the multi-layer neural network. As a result of comparing with existing methods, Euclidean and Mahananobis method, the suggested method showed the improved recognition performance with the incorrect matching or matching failure. In addition, by studying the changes of recognition rate according to the learning rate in various environments, the most optimum value of learning rate was calculated.","",""
0,"Mugdha Tripathi","Face Recognition Revisited on Pose, Alignment, Color, Illumination and Expression-PyTen",2016,"","","","",46,"2022-07-13 10:07:51","","10.21275/v5i3.nov161995","","",,,,,0,0.00,0,1,6,"Growing interest in intelligent human computer interactions has motivated a recent surge in research on problems such as pose estimation, illumination variation, color differences, alignment distinction and expression variation. Human faces are highly nonrigid objects with high degree of variability in pose, color, expression, alignment angles and illumination conditions and most face recognition algorithms (not all), are designed to work best with well aligned, well illuminated, and frontal pose face images. An optimal face representation should be discriminative, robust, compact and easy to implement. The conventional pipeline of face representation consists of image pre-processing, extraction, alignment, representation and classification. Our approach is based on feature sharing structure of deep network called Pyramid CNN (Pyramid Convolutional Neural Network) which has known to adopt a greedy filter and down sampling approach for a fast and computation efficient training procedure. CNN learns representation of the face utilized by recognition algorithms in later stages. The color values of face images are normalized to RGB color space to reduce the lightning effect in normalization process. We use Field proposed Log Gabor filters for feature extraction which allows more information to be captured in high frequency domains with desirable high-pass characteristics. Using feature sharing Pyramid CNN we are able to achieve competitive accuracy on LFW database","",""
0,"Nethra Chandrasekaran Sashikar-necsashi, Prashanth Kumar Murali-prmurali, Robert J Henderson-rojahend","FEBEI-Face Expression Based Emoticon Identification CSB 657 Computer Vision",2016,"","","","",47,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,3,6,"The Face Expression Based Emoticon Identification (FEBEI) system is an open source extension to the Tracker.js framework which converts a human facial expression to the best matching emoticon. The contribution of this project was to build this robust classifier which can identify facial expression in real time without any reliance on an external server or computation node. An entirely client-side JavaScript implementation has clear privacy benefits as well as the avoidance of any lag inherent in uploading and downloading images. We accomplished this by utilizing several computationally efficient methods. Tracking.js provided a Viola Jones based face detector which we used to pass facial images to our own implementation of an eigenemotion detection system which was trained to distinguish between happy and angry faces. We have implemented a similar eigenface classifier in python and have trained a Convoluted Neural Network (CNN) to classify emotions to provide a comparative view of its advantages. We aim to make FEBEI easily extendable so that the developer community will be able to add classifiers for more emoticon.","",""
2,"Zeke Xu, Zhenhao Huang, Zhuoxiong Zhao, Zhiyuan Li, Pengsen Huang","Sparse Representation for Kinect Based Hand Gesture Recognition System",2013,"","","","",48,"2022-07-13 10:07:51","","10.2991/ICAICTE.2013.125","","",,,,,2,0.22,0,5,9,"Hand gesture recognition that has proven a significant factor to directly influence the nonverbal communication between human and computer is becoming a challenging topic in the field of machine vision. This paper aims to propose a novel hand gesture recognition system which applies sparse representation to the Kinect to improve the efficiency of Kinect-based human-computer interaction. Auto-encoder neural network computation is also utilized to achieve better result. The sparse auto-encoder neural network is versatile and robust in complex features learning and computational efficient. Finally, results indicate that our algorithm greatly facilitates the gesture recognition rate up to 95%.","",""
1,"Poonam Sharma, K. V. Arya, R. Yadav","Face recognition using generalized self organising map based on PCNN features",2015,"","","","",49,"2022-07-13 10:07:51","","10.1109/SPIN.2015.7095441","","",,,,,1,0.14,0,3,7,"This paper presents an efficient face recognition method invariant to slight variation in pose, illumination and background where pulse coupled neural network (PCNN) is used to compute time signature used for feature extraction and generalized self organizing map is being used for classification. The efficiency has been significant improvement by combining PCNN which better resemble human brain neurons and possess better computation power in terms of performance than other neural networks. It is coupled with generalized self organizing map for recognition. The proposed method is robust as it provided better result for slight variation of background and pose and recognition time is greatly reduced.","",""
1,"G. Martin","From Image to Word: a Computational Model of Word Recognition in Reading",1997,"","","","",50,"2022-07-13 10:07:51","","","","",,,,,1,0.04,1,1,25,"This paper describes a working, computational model of word recognition that combines a letter classiication component with a component that segments the string of classiied letters into words and uses a dynamic programming method for matching the words against a lexicon of over 2,800 words. The letter classiication component is a neural network trained to classify, in parallel , inputs corresponding to 20x188 pixel array images of letter sequences, 14 or more letters long. Consistent with human capabilities, the system can classify all 14 letters at a level above chance, and on average, clas-siies the rst 7 or 8 letters in the sequence correctly. Dictionary lookup improves classiication accuracy by 1 character per image. The model is robust, having been trained and tested on the entire text of the book The Wonderful Wizard of Oz, printed in multiple fonts and in both mixed and upper-case letters. It provides a computation-level understanding of word recognition capabilities, in which errors are attributable to the theoretically inevitable diiculties associated with learning to classify large input patterns. The model mimics human capabilities for circumventing some of these diicul-ties by imposing constraints on xation positions that reduce image variability.","",""
3,"Carla Sendra-Balcells, R. Salvador, J. Pedro, M. Biagi, Charlène Aubinet, B. Manor, A. Thibaut, S. Laureys, K. Lekadir, G. Ruffini","Convolutional neural network MRI segmentation for fast and robust optimization of transcranial electrical current stimulation of the human brain",2020,"","","","",51,"2022-07-13 10:07:51","","10.1101/2020.01.29.924985","","",,,,,3,1.50,0,10,2,"The segmentation of structural MRI data is an essential step for deriving geometrical information about brain tissues. One important application is in transcranial direct current stimulation (e.g., tDCS), a non-invasive neuromodulatory technique where head modeling is required to determine the electric field (E-field) generated in the cortex to predict and optimize its effects. Here we propose a deep learning-based model (StarNEt) to automatize white matter (WM) and gray matter (GM) segmentation and compare its performance with FreeSurfer, an established tool. Since good definition of sulci and gyri in the cortical surface is an important requirement for E-field calculation, StarNEt is specifically designed to output masks at a higher resolution than that of the original input T1w-MRI. StarNEt uses a residual network as the encoder (ResNet) and a fully convolutional neural network with U-net skip connections as the decoder to segment an MRI slice by slice. Slice vertical location is provided as an extra input. The model was trained on scans from 425 patients in the open-access ADNI+IXI datasets, and using FreeSurfer segmentation as ground truth. Model performance was evaluated using the Dice Coefficient (DC) in a separate subset (N=105) of ADNI+IXI and in two extra testing sets not involved in training. In addition, FreeSurfer and StarNEt were compared to manual segmentations of the MRBrainS18 dataset, also unseen by the model. To study performance in real use cases, first, we created electrical head models derived from the FreeSurfer and StarNEt segmentations and used them for montage optimization with a common target region using a standard algorithm (Stimweaver) and second, we used StarNEt to successfully segment the brains of minimally conscious state (MCS) patients having suffered from brain trauma, a scenario where FreeSurfer typically fails. Our results indicate that StarNEt matches FreeSurfer performance on the trained tasks while reducing computation time from several hours to a few seconds, and with the potential to evolve into an effective technique even when patients present large brain abnormalities.","",""
12,"Deepak Baby, Arthur Van Den Broucke, S. Verhulst","A convolutional neural-network model of human cochlear mechanics and filter tuning for real-time applications",2020,"","","","",52,"2022-07-13 10:07:51","","10.1038/s42256-020-00286-8","","",,,,,12,6.00,4,3,2,"","",""
248,"Lukas Schott, Jonas Rauber, M. Bethge, Wieland Brendel","Towards the first adversarially robust neural network model on MNIST",2018,"","","","",53,"2022-07-13 10:07:51","","","","",,,,,248,62.00,62,4,4,"Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful defense by Madry et al. (1) overfits on the L-infinity metric (it's highly susceptible to L2 and L0 perturbations), (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-infinity perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.","",""
1,"Mohammad Mehdi Yadollahi, Farzaneh Shoeleh, S. Dadkhah, A. Ghorbani","Robust Black-box Watermarking for Deep Neural Network using Inverse Document Frequency",2021,"","","","",54,"2022-07-13 10:07:51","","10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00100","","",,,,,1,1.00,0,4,1,"Recently, Deep Neural Networks (DNNs), presented exceptional achievement in implementing human-level capabilities for various predicaments, such as Natural Language Processing (NLP), voice recognition, and image processing, etc. Training these models is expensive in terms of computational power and the existence of enough labelled data. Thus, ML-based models such as DNNs establish genuine business value and intellectual property (IP) for their owners. Therefore the trained models need to be protected from any adversary attacks such as illegal redistribution, reproducing, and derivation. Watermarking can be considered as an effective technique for securing a DNN model. However, so far, most of the watermarking algorithms focus on watermarking the DNN by adding noise to an image. To this end, we propose a framework for watermarking a DNN model designed for textual domain. The watermark generation scheme provides a secure watermarking method by combining Term Frequency (TF) and Inverse Document Frequency (IDF) of a particular word. The proposed embedding procedure takes place in the model's training stage, which makes the watermark verification stage straightforward by sending the watermarked document to the trained model. The experimental results show that watermarked models have the same accuracy as the original one, and the proposed framework accurately verifies the ownership of all surrogate models without impairing the performance. The proposed algorithm is robust against well-known attacks such as parameter pruning and brute force attack.","",""
0,"S. Farooq, Simranjit Kaur, Satnam Singh Dub","Analysis of Genetic Algorithm Optimized Neural Network System for Human Gait Recognition",2021,"","","","",55,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,3,1,"This paper explores a gait recognition method with binary silhouette-based input images and Genetic Algorithm Optimized Artificial Neural Network (GA-NN) classification. The performance of the recognition method depends significantly on the quality of the extracted binary silhouettes. A fuzzy correlogram based method is employed for background subtraction and Frame Difference Energy Image (FDEI) reconstruction is performed to make the recognition method robust to partial incompleteness of silhouettes. Feature extraction process uses extracted features directly for classification while the indirect method maps the higher-dimensional features into a lower-dimensional space by means of a Frame-to-Exemplar-Distance (FED) vector. The FED uses the distance measure between pre-determined exemplars and the feature vectors of the current frame as an identification criterion. The extracted features are fed to the GA optimizer for feature fitness evaluation and NN-based classifier which models human gait to compute similarity scores and carry out identification. This work achieves an overall accuracy for all database datasets with difference in recognition time as the dataset sample bulk increases and the GA optimization and ANN trains and learns itself from increased number of samples. As expected, the results using this approach are better, owing to a basic trade-off between complexity and efficiency. The approach is computationally more extensive with a better overall performance, while the indirect approach tends to be computationally less intensive at the cost of recognition performance. Also this approach is less vulnerable to noise and distortion compared to the indirect approach. Surprisingly, the results obtained by using the FED vector show that in this case, the overall performance is actually slightly lesser in the case of FDEI reconstruction as compared to the use of direct binary silhouettes. It is also seen that the recognition performance depends to a certain extent on the specific gait sequences. The algorithms used in this work produce relatively better results in the case of CASIA Dataset-A.","",""
1,"M. Mohammadi, Arman Garousi","A Fast and Simple Face Detection Algorithm Using Neural Network and Its Implementation on FPGA",2021,"","","","",56,"2022-07-13 10:07:51","","10.54380/ijrdetv10i102","","",,,,,1,1.00,1,2,1,"Face recognition is one of the interesting types of biometric which determines the presence or absence of human faces in the picture. In this paper, a face recognition system is presented that benefits from an optimized architecture based on the MLP neural network. The proposed method considerably improves the speed and the accuracy of detection compared to traditional architectures of neural network. To reduce the overall computation, neural network is organized so that to be able to rule out the majority of the non-image areas located in the image’s background before applying the main algorithm. An important advantage of this new architecture is its homogeneous structure that makes it suitable for optimized implementation on a hardware platform. In this work, FPGA is used as the platform for implementation of the proposed algorithms. The implementation was done considering Taylor expansion of the activation functions. The performance of the proposed method and the implemented system was evaluated on the BioID dataset. Accomplishment of the proposed method is high precision while reducing training time and total calculations, together with appropriate robustness. Finally, a comparison with other face recognition methods has been done to show the performance of the presented system. The comparison result shows that the proposed system outperforms the other mentioned methods.","",""
0,"Timothy R. Newhouse, Pengpeng Zhang, Jungmin Eun, Masha Elkin, Yizhou Zhao, Rachel Cantrell","A Neural Network Model Informs Total Synthesis of Clovane Sesquiterpenoids",2021,"","","","",57,"2022-07-13 10:07:51","","10.33774/chemrxiv-2021-41d5z","","",,,,,0,0.00,0,6,1,"Efficient syntheses of complex small molecules often involve speculative experimental approaches. The central challenge of such plans is that experimental evaluation of high-risk strategies is resource intensive, as it entails iterative attempts at unsuccessful strategies. Herein, we report a complementary strategy that combines creative human-generated synthetic plans with robust computational prediction of the feasibility of key steps in the proposed synthesis. A neural network model was developed to predict the outcome of a generally disfavored transformation, the 6-endo-trig radical cyclization, and applied to synthetic planning of clovan-2,9-dione, resulting in a 5-step total synthesis that improves on a prior 15-step approach. This work establishes how machine learning can guide multistep syntheses that employ innovative and high-risk human-generated plans.","",""
0,"J. Qiu, Xinlei Yan, W. Wang, Wei Wei, Kai Fang","Skeleton-Based Abnormal Behavior Detection Using Secure Partitioned Convolutional Neural Network Model.",2021,"","","","",58,"2022-07-13 10:07:51","","10.1109/JBHI.2021.3137334","","",,,,,0,0.00,0,5,1,"The abnormal behavior detection is the vital for evaluation of daily-life health status of the patient with cognitive impairment. Previous studies about abnormal behavior detection indicate that convolution neural network (CNN)-based computer vision owns the high robustness and accuracy for detection. However, executing CNN model on the cloud possible incurs a privacy disclosure problem during data transmission, and the high computation overhead makes difficult to execute the model on edge-end IoT devices with a well real-time performance. In this paper, we realize a skeleton-based abnormal behavior detection, and propose a secure partitioned CNN model (SP-CNN) to extract human skeleton keypoints and achieve safely collaborative computing by deploying different CNN model layers on the cloud and the IoT device. Because, the data outputted from the IoT device are processed by the several CNN layers instead of transmitting the sensitive video data, objectively it reduces the risk of privacy disclosure. Moreover, we also design an encryption method based on channel state information (CSI) to guarantee the sensitive data security. At last, we apply SP-CNN in abnormal behavior detection to evaluate its effectiveness. The experiment results illustrate that the efficiency of the abnormal behavior detection based on SP-CNN is at least 33.2% higher than the state-of-the-art methods, and its detection accuracy arrives to 97.54%.","",""
0,"Sang-kyu Bahn, Bo-Yeong Kang, Chany Lee","A computational study on the optimization of transcranial temporal interfering stimulation with high-definition electrode using unsupervised neural network",2021,"","","","",59,"2022-07-13 10:07:51","","10.1101/2021.11.14.467844","","",,,,,0,0.00,0,3,1,"Transcranial temporal interfering stimulation (tTIS) can focally stimulate deep parts of the brain, which are related to specific functions, by using beats at two high AC frequencies that do not affect the human brain. However, it has limitations in terms of calculation time and precision for optimization because of its complexity and non-linearity. We aimed to propose a method using an unsupervised neural network (USNN) for tTIS to optimize quickly the interfering current value of high-definition electrodes, which can finely stimulate the deep part of the brain, and analyze the performance and characteristics of tTIS. A computational study was conducted using 16 realistic head models. This method generated the strongest stimulation on the target, even when targeting deep areas or multi-target stimulation. The tTIS was robust with target depth compared with transcranial alternating current stimulation, and mis-stimulation could be reduced compared with the case of using two-pair inferential stimulation. Optimization of a target could be performed in 3 min. By proposing the USNN for tTIS, we showed that the electrode currents of tTIS can be optimized quickly and accurately, and the possibility of stimulating the deep part of the brain precisely with transcranial electrical stimulation was confirmed.","",""
0,"Apostol T. Vassilev, Munawar Hasan, Honglan Jin","Can You Tell? SSNet - A Biologically-Inspired Neural Network Framework for Sentiment Classifiers",2020,"","","","",60,"2022-07-13 10:07:51","","10.1007/978-3-030-95467-3_27","","",,,,,0,0.00,0,3,2,"","",""
1,"B. Soni, Prachi Mathur, Angshuman Bora","In Depth Analysis, Applications and Future Issues of Artificial Neural Network",2020,"","","","",61,"2022-07-13 10:07:51","","10.1007/978-3-030-52067-0_7","","",,,,,1,0.50,0,3,2,"","",""
0,"Pedro Vinícius A. B. De Venâncio, T. M. Rezende, A. C. Lisboa, A. V. Barbosa","Fire Detection based on a Two-Dimensional Convolutional Neural Network and Temporal Analysis",2021,"","","","",62,"2022-07-13 10:07:51","","10.1109/LA-CCI48322.2021.9769824","","",,,,,0,0.00,0,4,1,"In the last few years there has been a substantial increase in the success of deep learning, especially with regard to convolutional neural networks for computer vision tasks. These architectures are being widely used in emergency situations, where a fast and accurate response is needed. In environmental monitoring, several works have focused on fire detection, since fires have been increasingly associated with negative consequences such as respiratory diseases, economical losses and the destruction of natural resources. The automatic detection of smoke and fire, however, poses a particularly difficult challenge to computer vision systems, since the variability in the shape, color and texture of these objects makes the process of learning how to detect them much more complicated than for other ordinary objects. As a consequence, the number of false positives may grow high, which is especially problematic for a real-time application that mobilizes human efforts to fight fire. This work presents a robust fire detection tool based on a 2D deep convolutional network capable of suppressing false alarms from clouds, fogs, car lights and other objects that are easily confused with fire and smoke. Our approach integrates an object detector with an object tracker; this makes it possible to analyze the temporal behavior of the object and use that information in the decision process. We also present D-Fire, a public and labeled dataset containing more than 21,000 images, which is used to train and test the proposed system. The experimental results show that the detector reached an mAP@0.50 = 75.91% and that the incorporation of the temporal context resulted in a 60% reduction in the false positive rate at the cost of a 2.86% reduction in true positive rate. In addition, the computational cost added by the proposed approach to the fire detector is negligible, so that real-time detection is still completely feasible.","",""
54,"N. Ahmadi, G. Akbarizadeh","Hybrid robust iris recognition approach using iris image pre-processing, two-dimensional gabor features and multi-layer perceptron neural network/PSO",2017,"","","","",63,"2022-07-13 10:07:51","","10.1049/iet-bmt.2017.0041","","",,,,,54,10.80,27,2,5,"Computational intelligence is employed to solve factual and complicated global problems, though neural networks (NNs) and evolutionary computing have also affected these issues. Biometric traits are applicable for detecting crime in security systems because they offer attractive features such as stability and uniqueness. Although various methods have been proposed for this objective, feature shortcomings such as computational complexity, long run times, and high memory consumption remain. The current study proposes a novel human iris recognition approach based on a multi-layer perceptron NN and particle swarm optimisation (PSO) algorithms to train the network in order to increase generalisation performance. A combination of these algorithms was used as a classifier. A pre-processing step was performed on the iris images to improve the results and two-dimensional gabor kernel feature extraction was applied. The data was normalised, trained, and tested using the proposed method. A PSO algorithm was applied to train the NN for data classification. The experimental results show that the proposed method performs better than many other well-known techniques. The benchmark Chinese Academy of Science and Institute of Automation (CASIA)-iris V3 and Center for Machine Learning and Intelligent Systems at the University of California, Irvine (UCI) machine learning repository datasets were used for testing and comparison.","",""
0,"I. T. Toudjeu, J. Tapamo","A 2D Convolutional Neural Network Approach for Human Action Recognition",2019,"","","","",64,"2022-07-13 10:07:51","","10.1109/AFRICON46755.2019.9133840","","",,,,,0,0.00,0,2,3,"Nowadays, deep neural networks are widely used for human action recognition (HAR) due to their ability to operate directly on the raw video inputs by extracting both the spatial and temporal information. Although the 3D convolutional neural networks as deep models have achieved superior performance, they remain computational expensive. In this paper we propose a 2D-CNN approach that learns robust feature representation from temporal information embedded into the motion history images of action videos. The proposed approach is simple and reduces the computational complexity imposed by the 3D-CNN approaches. The KTH database is used to validate our approach and the achieved results are compared favorably against the handcrafted state-of-the-art methods.","",""
0,"Yasmine Benchaib","IMPROVED ARTIFICIAL NEURAL NETWORK FOR EPILEPTIC SEIZURES DETECTION",2021,"","","","",65,"2022-07-13 10:07:51","","10.1142/s0219519421500457","","",,,,,0,0.00,0,1,1,"Electroencephalogram (EEG) is a fundamental and unique tool for exploring human brain activity in general and epileptic mechanism in particular. It offers significant information about epileptic seizures source known as epileptogenic area. However, it is often complicated to detect critical changes in EEG signals by visual examination, since this signal aspect of epileptic persons seems to be normal out of the seizure. Thus, the challenge is to design such a robust and automatic system to detect these unseen changes and use them for diagnosis. In this research, we apply the Artificial Metaplasticity Multi-Layer Perceptron (AMMLP) together with discrete wavelet transform (DWT) to Bonn EEG signals for seizure detection goal. Significant features were then extracted from the well-known EEG brainwaves. Aiming to decrease the computational time and improve classification accuracy, we performed a features ranking and selection employing the Relief algorithm. The obtained AMMLP classification accuracy of 98.97% proved the effctiveness of the applied approach. Our results were compared to recent researches results on the same database, proving to be superior or at least an interesting alternative for seizures detection within EEG signals.","",""
1,"Subash C. Pakhrin, Kiyoko F. Aoki-Kinoshita, Doina Caragea, Dukka B Kc","DeepNGlyPred: A Deep Neural Network-Based Approach for Human N-Linked Glycosylation Site Prediction",2021,"","","","",66,"2022-07-13 10:07:51","","10.3390/molecules26237314","","",,,,,1,1.00,0,4,1,"Protein N-linked glycosylation is a post-translational modification that plays an important role in a myriad of biological processes. Computational prediction approaches serve as complementary methods for the characterization of glycosylation sites. Most of the existing predictors for N-linked glycosylation utilize the information that the glycosylation site occurs at the N-X-[S/T] sequon, where X is any amino acid except proline. Not all N-X-[S/T] sequons are glycosylated, thus the N-X-[S/T] sequon is a necessary but not sufficient determinant for protein glycosylation. In that regard, computational prediction of N-linked glycosylation sites confined to N-X-[S/T] sequons is an important problem. Here, we report DeepNGlyPred a deep learning-based approach that encodes the positive and negative sequences in the human proteome dataset (extracted from N-GlycositeAtlas) using sequence-based features (gapped-dipeptide), predicted structural features, and evolutionary information. DeepNGlyPred produces SN, SP, MCC, and ACC of 88.62%, 73.92%, 0.60, and 79.41%, respectively on N-GlyDE independent test set, which is better than the compared approaches. These results demonstrate that DeepNGlyPred is a robust computational technique to predict N-Linked glycosylation sites confined to N-X-[S/T] sequon. DeepNGlyPred will be a useful resource for the glycobiology community.","",""
15,"F. Ma, Jingyao Zhang, Wei Liang, Jingyu Xue","Automated Classification of Atrial Fibrillation Using Artificial Neural Network for Wearable Devices",2020,"","","","",67,"2022-07-13 10:07:51","","10.1155/2020/9159158","","",,,,,15,7.50,4,4,2,"Atrial fibrillation (AF), as one of the most common arrhythmia diseases in clinic, is a malignant threat to human health. However, AF is difficult to monitor in real time due to its intermittent nature. Wearable electrocardiogram (ECG) monitoring equipment has flourished in the context of telemedicine due to its real-time monitoring and simple operation in recent years, providing new ideas and methods for the detection of AF. In this paper, we propose a low computational cost classification model for robust detection of AF episodes in ECG signals, using RR intervals of the ECG signals and feeding them into artificial neural network (ANN) for classification, to compensate the defect of the computational complexity in traditional wearable ECG monitoring devices. In addition, we compared our proposed classifier with other popular classifiers. The model was trained and tested on the AF Termination Challenge Database and MIT-BIH Arrhythmia Database. Experimental results achieve the highest sensitivity of 99.3%, specificity of 97.4%, and accuracy of 98.3%, outperforming most of the others in the recent literature. Accordingly, we observe that ANN using RR intervals as an input feature can be a suitable candidate for automatic classification of AF.","",""
46,"Wen Qi, Hang Su, Chenguang Yang, G. Ferrigno, E. Momi, A. Aliverti","A Fast and Robust Deep Convolutional Neural Networks for Complex Human Activity Recognition Using Smartphone",2019,"","","","",68,"2022-07-13 10:07:51","","10.3390/s19173731","","",,,,,46,15.33,8,6,3,"As a significant role in healthcare and sports applications, human activity recognition (HAR) techniques are capable of monitoring humans’ daily behavior. It has spurred the demand for intelligent sensors and has been giving rise to the explosive growth of wearable and mobile devices. They provide the most availability of human activity data (big data). Powerful algorithms are required to analyze these heterogeneous and high-dimension streaming data efficiently. This paper proposes a novel fast and robust deep convolutional neural network structure (FR-DCNN) for human activity recognition (HAR) using a smartphone. It enhances the effectiveness and extends the information of the collected raw data from the inertial measurement unit (IMU) sensors by integrating a series of signal processing algorithms and a signal selection module. It enables a fast computational method for building the DCNN classifier by adding a data compression module. Experimental results on the sampled 12 complex activities dataset show that the proposed FR-DCNN model is the best method for fast computation and high accuracy recognition. The FR-DCNN model only needs 0.0029 s to predict activity in an online way with 95.27% accuracy. Meanwhile, it only takes 88 s (average) to establish the DCNN classifier on the compressed dataset with less precision loss 94.18%.","",""
7,"Kudakwashe Zvarevashe, O. Olugbara","Recognition of speech emotion using custom 2D-convolution neural network deep learning algorithm",2020,"","","","",69,"2022-07-13 10:07:51","","10.3233/IDA-194747","","",,,,,7,3.50,4,2,2,"Speech emotion recognition has become the heart of most human computer interaction applications in the modern world. The growing need to develop emotionally intelligent devices has opened up a lot of research opportunities. Most researchers in this field have applied the use of handcrafted features and machine learning techniques in recognising speech emotion. However, these techniques require extra processing steps and handcrafted features are usually not robust. They are computationally intensive because the curse of dimensionality results in low discriminating power. Research has shown that deep learning algorithms are effective for extracting robust and salient features in dataset. In this study, we have developed a custom 2D-convolution neural network that performs both feature extraction and classification of vocal utterances. The neural network has been evaluated against deep multilayer perceptron neural network and deep radial basis function neural network using the Berlin database of emotional speech, Ryerson audio-visual emotional speech database and Surrey audio-visual expressed emotion corpus. The described deep learning algorithm achieves the highest precision, recall and F1-scores when compared to other existing algorithms. It is observed that there may be need to develop customized solutions for different language settings depending on the area of applications.","",""
6,"Gang Liu, Jing Wang","A Polynomial Neural Network with Controllable Precision and Human-Readable Topology for Prediction and System Identification",2020,"","","","",70,"2022-07-13 10:07:51","","","","",,,,,6,3.00,3,2,2,"Although artificial neural networks (ANNs) are successful, there is still a concern among many over their ""black box"" nature. Why do they work? Could we design a ""transparent"" network? This paper presents a controllable and readable polynomial neural network (CR-PNN) for approximation, prediction, and system identification. CR-PNN is simple enough to be described as one ""small"" formula, so that we can control the approximation precision and explain the internal structure of the network. CR-PNN, in fact, essentially is the fascinating Taylor expansion in the form of network. The number of layers represents precision. Derivatives in Taylor expansion are exactly imitated by error back-propagation algorithm. Firstly, we demonstrated that CR-PNN shows excellent analysis performance to the ""black box"" system through ten synthetic data with noise. Also, the results were compared with synthetic data to substantiate its search easily towards the global optimum. Secondly, it was verified, by ten real-world applications, that CR-PNN brought better generalization capability relative to the typical ANNs that approximate depended on the nonlinear activation function. Finally, 200,000 repeated experiments, with 4898 samples, demonstrated that CR-PNN is five times more efficient than typical ANN for one epoch and ten times more efficient than typical ANN for one forward-propagation. In short, compared with the traditional neural networks, the novelties and advantages of CR-PNN include readability of the internal structure, easy to find global optimal solution, lower computational complexity, and likely better robustness to real-world approximation. (We're strong believers in Open Source, and provide CR-PNN code for others. GitHub: this https URL)","",""
0,"Apostol T. Vassilev, Munawar Hasan","Can you tell? SSNet - a Sagittal Stratum-inspired Neural Network Framework for Sentiment Analysis",2020,"","","","",71,"2022-07-13 10:07:51","","10.2139/ssrn.3641938","","",,,,,0,0.00,0,2,2,"When people try to understand nuanced language they typically process multiple input sensor modalities to complete this cognitive task. It turns out the human brain has even a specialized neuron formation, called sagittal stratum, to help us understand sarcasm. We use this biological formation as the inspiration for designing a neural network architecture that combines predictions of different models on the same text to construct a robust, accurate and computationally efficient classifier for sentiment analysis. Experimental results on representative benchmark datasets and comparisons to other methods1show the advantages of the new network architecture.","",""
0,"Yinqian Sun, Yi Zeng, Tielin Zhang","Quantum Superposition Spiking Neural Network",2020,"","","","",72,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,3,2,"Quantum brain as a novel hypothesis states that some non-trivial mechanisms in quantum computation, such as superposition and entanglement, may have important influence for the formation of brain functions. Inspired by this idea, we propose Quantum Superposition Spiking Neural Network (QS-SNN), which introduce quantum superposition to spiking neural network models to handel challenges which are hard for other state-of-the-art machine learning models. For human brain, grasping the main information no matter how the background changes is necessary to interact efficiently with diverse environments. As an example, it is easy for human to recognize the digits whether it is white character with black background or inversely black character with white background. While if the current machine learning models are trained with one of the cases (e.g. white character with black background), it will be nearly impossible for them to recognize the color inverted version. To handel this challenge, we propose two-compartment spiking neural network with superposition states encoding, which is inspired by quantum information theory and spatial-temporal spiking property from neuron information encoding in the brain. Typical network structures like fully-connected ANN, VGG, ResNet and DenseNet are challenged with the same task. We train these networks on original image dataset and then invert the background color to test their generalization. Result shows that artificial neural network can not deal with this condition while the quantum superposition spiking neural network(QS-SNN) which we proposed in this paper recognizes the color-inverse image successfully. Further the QS-SNN shows its robustness when noises are added on inputs.","",""
1,"Muhammad Abdullah Hanif, M. Shafique","Dependable Deep Learning: Towards Cost-Efficient Resilience of Deep Neural Network Accelerators against Soft Errors and Permanent Faults",2020,"","","","",73,"2022-07-13 10:07:51","","10.1109/IOLTS50870.2020.9159734","","",,,,,1,0.50,1,2,2,"Deep Learning has enabled machines to learn computational models (i.e., Deep Neural Networks – DNNs) that can perform certain complex tasks with claims to be close to human-level precision. This state-of-the-art performance offered by DNNs in many Artificial Intelligence (AI) applications has paved their way to being used in several safety-critical applications where even a single failure can lead to catastrophic results. Therefore, improving the robustness of these models to hardware-induced faults (such as soft errors, aging, and manufacturing defects) is of significant importance to avoid any disastrous event. Traditional redundancy-based fault mitigation techniques cannot be employed in a wide of applications due to their high overheads, which, when coupled with the compute-intensive nature of DNNs, lead to undesirable resource consumption. In this article, we present an overview of different low-cost fault-mitigation techniques that exploit the intrinsic characteristics of DNNs to limit their overheads. We discuss how each technique can contribute to the overall resilience of a DNN-based system, and how they can be integrated together to offer resilience against multiple diverse hardware-induced reliability threats. Towards the end, we highlight several key future directions that are envisioned to help in achieving highly dependable DL-based systems.","",""
1,"Zerui Chen, Yan Huang, Hongyuan Yu, Liang Wang","Learning a Robust Part-Aware Monocular 3D Human Pose Estimator via Neural Architecture Search",2021,"","","","",74,"2022-07-13 10:07:51","","10.1007/s11263-021-01525-0","","",,,,,1,1.00,0,4,1,"","",""
16,"Xiao Qi, L. G. Brown, D. Foran, J. Nosher, I. Hacihaliloglu","Chest X-ray image phase features for improved diagnosis of COVID-19 using convolutional neural network",2020,"","","","",75,"2022-07-13 10:07:51","","10.1007/s11548-020-02305-w","","",,,,,16,8.00,3,5,2,"","",""
1,"Joshua Emoto, Y. Hirata","Lightweight Convolutional Neural Network for Image Processing Method for Gaze Estimation and Eye Movement Event Detection",2020,"","","","",76,"2022-07-13 10:07:51","","10.2197/ipsjtbio.13.7","","",,,,,1,0.50,1,2,2,": Advancements in technology have recently made it possible to obtain various types of biometric informa- tion from humans, enabling studies on estimation of human conditions in medicine, automobile safety, marketing, and other areas. These studies have particularly pointed to eye movement as an e ﬀ ective indicator of human conditions, and research on its applications is actively being pursued. The devices now widely used for measuring eye movements are based on the video-oculography (VOG) method, wherein the direction of gaze is estimated by processing eye images obtained through a camera. Applying convolutional neural networks (ConvNet) to the processing of eye images has been shown to enable accurate and robust gaze estimation. Conventional image processing, however, is premised on execution using a personal computer, making it di ﬃ cult to carry out real-time gaze estimation using ConvNet, which involves the use of a large number of parameters, in a small arithmetic unit. Also, detecting eye movement events, such as blinking and saccadic movements, from the inferred gaze direction sequence for particular purposes requires the use of a separate algorithm. We therefore propose a new eye image processing method that batch-processes gaze estimation and event detection from end to end using an independently designed lightweight ConvNet. This paper discusses the structure of the proposed lightweight ConvNet, the methods for learning and evaluation used, and the proposed method’s ability to simultaneously detect gaze direction and event occurrence using a smaller memory and at lower computational complexity than conventional methods.","",""
1,"Jung-Yoon Kim, J. Hwang, E. Park, Hyeon-Uk Nam, Songhee Cheon","Flat-Feet Prediction Based on a Designed Wearable Sensing Shoe and a PCA-Based Deep Neural Network Model",2020,"","","","",77,"2022-07-13 10:07:51","","10.1109/ACCESS.2020.3033826","","",,,,,1,0.50,0,5,2,"Gait is a significant factor that affects human health, and monitoring a person’s gait with sensing devices during daily life can detect abnormal gait events that affect numerous physical health problems. In particular, flat feet can cause changes in alignment conditions of the foot, ankle, leg, pelvis and spine. The primary problem with previous studies of wearable devices for measuring gait have focused on quantitatively monitoring the degree of gait rather than the limited gait ability. The existing method of feeding back the degree of gait or activity does not consider the severity of the subject and is insufficient for qualitative evaluation or training of gait. The significance of this study is development of convenient detecting and long-term tracking tools that can be used by both patients and clinicians for prescreening flat feet and monitoring the progress of flat feet treatment. For wearable devices for flatfoot detection to be most effective, detection systems and algorithms must be accurate, robust, reliable and computationally-efficient. In this paper, we developed an integrated smart wearable gait-monitoring device comprised of three sensors: front force, rear force, and an ankle flex sensor. We propose a new flat feet detection methodology based on a dynamic sensing window and a deep neural network with scaled principal component analysis (PCA). We tested 24 subjects, including both those with healthy gait and flat-feet-affected gait. Our study shows that the proposed sensing devices could be worn comfortably. The proposed deep neural network (DNN) model outperformed the other five classifier algorithms considered, and the area under the curve (AUC) value of the method was 87.1%. This wearable device can thus be easily and simply used both by patients and doctors to monitor the progress of flat feet and prescreen for possible gait problems in daily life.","",""
268,"Xuanqing Liu, Minhao Cheng, Huan Zhang, Cho-Jui Hsieh","Towards Robust Neural Networks via Random Self-ensemble",2017,"","","","",78,"2022-07-13 10:07:51","","10.1007/978-3-030-01234-2_23","","",,,,,268,53.60,67,4,5,"","",""
0,"Ioannis E. Polykretis, Guangzhi Tang, Praveenram Balachandar, K. Michmizos","A Spiking Neural Network Mimics the Oculomotor System to Control a Biomimetic Robotic Head Without Learning on a Neuromorphic Hardware",2022,"","","","",79,"2022-07-13 10:07:51","","10.1109/TMRB.2022.3155278","","",,,,,0,0.00,0,4,1,"Facilitated by the emergence of neuromorphic hardware, neuromorphic algorithms mimic the brain’s asynchronous computation to improve energy efficiency, low latency, and robustness, which are crucial for a wide variety of real-time robotic applications. However, the limited on-chip learning abilities hinder the applicability of neuromorphic computing to real-world robotic tasks. Biomimetism can overcome this limitation by complementing or replacing training with the knowledge of the brain’s connectome associated with the targeted behavior. By drawing inspiration from the human oculomotor network, we designed a spiking neural network (SNN) that tracked visual targets in real-time. We deployed the biomimetic controller on Intel’s Loihi neuromorphic processor to control an in-house robotic head. The robot’s behavior resembled the smooth pursuit and saccadic eye movements observed in humans, while the SNN on Loihi exhibited similar performance to a CPU-run PID controller. Interestingly, this behavior emerged from the SNN without training, which places the biomimetic design as an alternative to the energy- and data-greedy learning-based methods. This work reinforces our on-going efforts to devise energy-efficient autonomous robots that mimic the robustness and versatility of their biological counterparts.","",""
0,"Xiangyu Fu","GomokuPro: An Implementation of Enhanced Machine Learning Algorithm Utilizing Convolutional Neural Network in Gomoku Strategy and Predictions Model",2022,"","","","",80,"2022-07-13 10:07:51","","10.1109/ICSP54964.2022.9778476","","",,,,,0,0.00,0,1,1,"In this paper, the researcher proposes a machine learning Gomoku model, GomokuPro, which is based on CNN (Convolutional Neural Network) as well as Monte Carlo tree search. The purpose of this research aggregation is to solve the previous problem of artificial intelligence algorithms taking too long to compute in board games. The researchers turned a more multidimensional fractional model into an adaptive model based on a single machine learning strategy previously developed by the researchers and capable of making judgments and predictions based on the following human algorithms. Due to the limitations of the hardware used for testing, the researchers found that the results were not applicable to some specific cases. Although this algorithm significantly reduces the computational effort, it still does not provide robust performance in some cases. The computational effort of convolutional neural networks is also influenced by the type of data. However, most of the time, machine learning models are still able to make master predictions.","",""
0,"P. Panda","Learning and Design Methodologies for Efficient, Robust Neural Networks",2019,"","","","",81,"2022-07-13 10:07:51","","10.25394/PGS.8256749.V1","","",,,,,0,0.00,0,1,3,"""Can machines think?"", the question brought up by Alan Turing, has led to the development of the eld of brain-inspired computing, wherein researchers have put substantial effort in building smarter devices and technology that have the potential of human-like understanding. However, there still remains a large (several orders-of-magnitude) power efficiency gap between the human brain and computers that attempt to emulate some facets of its functionality. In this thesis, we present design techniques that exploit the inherent variability in the difficulty of input data and the correlation of characteristic semantic information among inputs to scale down the computational requirements of a neural network with minimal impact on output quality. While large-scale artificial neural networks have achieved considerable success in a range of applications, there is growing interest in more biologically realistic models, such as, Spiking Neural Networks (SNNs), due to their energy-efficient spike based processing capability. We investigate neuroscienti fic principles to develop novel learning algorithms that can enable SNNs to conduct on-line learning. We developed an auto-encoder based unsupervised learning rule for training deep spiking convolutional networks that yields state-of-the-art results with computationally efficient learning. Further, we propose a novel ""learning to forget"" rule that addresses the catastrophic forgetting issue predominant with traditional neural computing paradigm and offers a promising solution for real-time lifelong learning without the expensive re-training procedure. Finally, while artificial intelligence grows in this digital age bringing large-scale social disruption, there is a growing security concern in the research community about the vulnerabilities of neural networks towards adversarial attacks. To that end, we describe discretization-based solutions, that are traditionally used for reducing the resource utilization of deep neural networks, for adversarial robustness. We also propose a novel noise-learning training strategy as an adversarial defense method. We show that implicit generative modeling of random noise with the same loss function used during posterior maximization, improves a model's understanding of the data manifold, furthering adversarial robustness. We evaluated and analyzed the behavior of the noise modeling technique using principal component analysis that yields metrics which can be generalized to all adversarial defenses.","",""
49,"Jibin Wu, Yansong Chua, Malu Zhang, Haizhou Li, K. Tan","A Spiking Neural Network Framework for Robust Sound Classification",2018,"","","","",82,"2022-07-13 10:07:51","","10.3389/fnins.2018.00836","","",,,,,49,12.25,10,5,4,"Environmental sounds form part of our daily life. With the advancement of deep learning models and the abundance of training data, the performance of automatic sound classification (ASC) systems has improved significantly in recent years. However, the high computational cost, hence high power consumption, remains a major hurdle for large-scale implementation of ASC systems on mobile and wearable devices. Motivated by the observations that humans are highly effective and consume little power whilst analyzing complex audio scenes, we propose a biologically plausible ASC framework, namely SOM-SNN. This framework uses the unsupervised self-organizing map (SOM) for representing frequency contents embedded within the acoustic signals, followed by an event-based spiking neural network (SNN) for spatiotemporal spiking pattern classification. We report experimental results on the RWCP environmental sound and TIDIGITS spoken digits datasets, which demonstrate competitive classification accuracies over other deep learning and SNN-based models. The SOM-SNN framework is also shown to be highly robust to corrupting noise after multi-condition training, whereby the model is trained with noise-corrupted sound samples. Moreover, we discover the early decision making capability of the proposed framework: an accurate classification can be made with an only partial presentation of the input.","",""
0,"","Neural Network Training Using Genetic Algorithms Series In Machine Perception And Artificial Intelligence",2021,"","","","",83,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,0,1,"Knowledge-Based Intelligent Information and Engineering Systems 2Nature-inspired Methods in Chemometrics: Genetic Algorithms and Artificial Neural NetworksParallel Implementations of Backpropagation Neural Networks on TransputersEvolutionary Algorithms and Neural NetworksTraining Neural Networks Using Hybrids with Genetic AlgorithmsNeural Network Training Using Genetic AlgorithmsGene Expression ProgrammingTraining a Neural Network with a Genetic AlgorithmMethods and Applications of Artificial IntelligenceClassification and Learning Using Genetic AlgorithmsPractical Computer Vision Applications Using Deep Learning with CNNsIntelligent Hybrid SystemsNeurogenetic LearningAdvances in Neural Networks ISNN 2007Hybrid Intelligent SystemsEncyclopedia of Computer Science and TechnologyMachine LearningUsing a Genetic Algorithm in Training an Artificial Neural Network to Implement the XOR FunctionGenetic and Evolutionary Computation — GECCO 2004Handbook of Fuzzy ComputationNeural Network Data Analysis Using SimulnetTMArtificial Neural Nets and Genetic AlgorithmsApplied Soft Computing Technologies: The Challenge of ComplexityGenetic Algorithm for Artificial Neural Network Training for the Purpose of Automated Part RecognitionNEURAL NETWORKS, FUZZY LOGIC AND GENETIC ALGORITHMAutomatic Generation of Neural Network Architecture Using Evolutionary ComputationPGANETThe Sixth International Symposium on Neural Networks (ISNN 2009)Evolutionary Machine Learning TechniquesModeling Decisions for Artificial IntelligenceEmpirical Studies on the Utility of Genetic Algorithms for Training and Designing of Neural NetworksTraining Neural Networks Using Genetic AlgorithmsTraining feedforward neural networks using genetic algorithmsMetaheuristic Procedures for Training Neural NetworksArtificial Neural Nets and Genetic AlgorithmsNature-Inspired Computing: Concepts, Methodologies, Tools, and ApplicationsApplications of Evolutionary ComputingArtificial Intelligence and CreativityArtificial Neural Nets and Genetic AlgorithmsDeep Learning Using Genetic Algorithms Creativity is one of the least understood aspects of intelligence and is often seen as `intuitive' and not susceptible to rational enquiry. Recently, however, there has been a resurgence of interest in the area, principally in artificial intelligence and cognitive science, but also in psychology, philosophy, computer science, logic, mathematics, sociology, and architecture and design. This volume brings this work together and provides an overview of this rapidly developing field. It addresses a range of issues. Can computers be creative? Can they help us to understand human creativity? How can artificial intelligence (AI) enhance human creativity? How, in particular, can it contribute to the `sciences of the artificial', such as design? Does the new wave of AI (connectionism, geneticism and artificial life) offer more promise in these areas than classical, symbol-handling AI? What would the implications be for AI and cognitive science if computers could not be creative? These issues are explored in five interrelated parts, each of which is introducted and explained by a leading figure in the field. Prologue (Margaret Boden) Part I: Foundational Issues (Terry Dartnall) Part II: Creativity and Cognition (Graeme S. Halford and Robert Levinson) Part III: Creativity and Connectionism (Chris Thornton) Part IV: Creativity and Design (John Gero) Part V: Human Creativity Enhancement (Ernest Edmonds) Epilogue (Douglas Hofstadter) For researchers in AI, cognitive science, computer science, philosophy, psychology, mathematics, logic, sociology, and architecture and design; and anyone interested in the rapidly growing field of artificial intelligence and creativity.From the contents: Neural networks – theory and applications: NNs (= neural networks) classifier on continuous data domains– quantum associative memory – a new class of neuron-like discrete filters to image processing – modular NNs for improving generalisation properties – presynaptic inhibition modelling for image processing application – NN recognition system for a curvature primal sketch – NN based nonlinear temporalspatial noise rejection system – relaxation rate for improving Hopfield network – Oja's NN and influence of the learning gain on its dynamics Genetic algorithms – theory and applications: transposition: a biological-inspired mechanism to use with GAs (= genetic algorithms) – GA for decision tree induction – optimising decision classifications using GAs – scheduling tasks with intertask communication onto multiprocessors by GAs – design of robust networks with GA – effect of degenerate coding on GAs – multiple traffic signal control using a GA – evolving musical harmonisation – niched-penalty approach for constraint handling in GAs – GA with dynamic population size – GA with dynamic niche clustering for multimodal function optimisation Soft computing and uncertainty: self-adaptation of evolutionary constructed decision trees by information spreading – evolutionary programming of near optimal NNsArtificial neural networks and genetic algorithms both are areas of research","",""
1,"J. Constantin, A. Bigand, I. Constantin","Pooling spike neural network for fast rendering in global illumination",2019,"","","","",84,"2022-07-13 10:07:51","","10.1007/s00521-018-3941-z","","",,,,,1,0.33,0,3,3,"","",""
2,"Philip Sperl, Konstantin Böttinger","Optimizing Information Loss Towards Robust Neural Networks",2020,"","","","",85,"2022-07-13 10:07:51","","","","",,,,,2,1.00,1,2,2,"Neural Networks (NNs) are vulnerable to adversarial examples. Such inputs differ only slightly from their benign counterparts yet provoke misclassifications of the attacked NNs. The required perturbations to craft the examples are often negligible and even human imperceptible. To protect deep learning-based systems from such attacks, several countermeasures have been proposed with adversarial training still being considered the most effective. Here, NNs are iteratively retrained using adversarial examples forming a computational expensive and time consuming process often leading to a performance decrease. To overcome the downsides of adversarial training while still providing a high level of security, we present a new training approach we call \textit{entropic retraining}. Based on an information-theoretic-inspired analysis, entropic retraining mimics the effects of adversarial training without the need of the laborious generation of adversarial examples. We empirically show that entropic retraining leads to a significant increase in NNs' security and robustness while only relying on the given original data. With our prototype implementation we validate and show the effectiveness of our approach for various NN architectures and data sets.","",""
0,"Edwin Kwadwo Tenagyei, Zongbo Hao, Kwadwo Kusi, K. Sarpong","Robust Real-Time Human Action Detection through the Fusion of 3D and 2D CNN",2021,"","","","",86,"2022-07-13 10:07:51","","10.1109/PRML52754.2021.9520696","","",,,,,0,0.00,0,4,1,"Recent approaches for human action detection often rely on appearance and optical flow networks for frame-level detections before linking them to form action tubes. However, they achieve unsatisfactory performance in real-time due to their huge computational complexity and large parameter usage during training. In this paper, we design and implement a unified end-to-end convolutional neural network (CNN) architecture that consists of two branches, extracting both spatial and temporal information concurrently before predicting bounding boxes and action probabilities from video clips. We also design a novel mechanism that exploits the inter-channel dependencies for an effective fusion of features from the branches. Specifically, we propose a Channel Fusion and Relation-Global Attention (CFRGA) module to aggregate the two features smoothly and model their inter-channel dependencies by considering their global scope structural relation information when inferring attention. We conduct experiments on the untrimmed video dataset, UCF101-24, and achieved impressive results in frame-mAP and video-mAP. The experimental results show that our channel fusion and relation-global attention module contributes to its good performance.","",""
3,"C. Ananth, M. Karthikeyan, N. Mohananthini","Discrete Wavelet Transform Based Multiple Watermarking for Digital Images Using Back-Propagation Neural Network",2019,"","","","",87,"2022-07-13 10:07:51","","10.1007/978-3-030-33846-6_49","","",,,,,3,1.00,1,3,3,"","",""
1,"Gustav Müller-Franzes, T. Nolte, Malin Ciba, Justus Schock, Firas Khader, A. Prescher, L. M. Wilms, C. Kuhl, S. Nebelung, D. Truhn","Fast, Accurate, and Robust T2 Mapping of Articular Cartilage by Neural Networks",2022,"","","","",88,"2022-07-13 10:07:51","","10.3390/diagnostics12030688","","",,,,,1,1.00,0,10,1,"For T2 mapping, the underlying mono-exponential signal decay is traditionally quantified by non-linear Least-Squares Estimation (LSE) curve fitting, which is prone to outliers and computationally expensive. This study aimed to validate a fully connected neural network (NN) to estimate T2 relaxation times and to assess its performance versus LSE fitting methods. To this end, the NN was trained and tested in silico on a synthetic dataset of 75 million signal decays. Its quantification error was comparatively evaluated against three LSE methods, i.e., traditional methods without any modification, with an offset, and one with noise correction. Following in-situ acquisition of T2 maps in seven human cadaveric knee joint specimens at high and low signal-to-noise ratios, the NN and LSE methods were used to estimate the T2 relaxation times of the manually segmented patellofemoral cartilage. In-silico modeling at low signal-to-noise ratio indicated significantly lower quantification error for the NN (by medians of 6–33%) than for the LSE methods (p < 0.001). These results were confirmed by the in-situ measurements (medians of 10–35%). T2 quantification by the NN took only 4 s, which was faster than the LSE methods (28–43 s). In conclusion, NNs provide fast, accurate, and robust quantification of T2 relaxation times.","",""
0,"Qin Cheng, Ziliang Ren, Jun Cheng, Qieshi Zhang, Hao Yan, Jianming Liu","Skeleton-based Action Recognition with Multi-scale Spatial-temporal Convolutional Neural Network",2021,"","","","",89,"2022-07-13 10:07:51","","10.1109/RCAR52367.2021.9517665","","",,,,,0,0.00,0,6,1,"The skeleton data convey significant information for human action recognition since they can robustly accommodate cluttered background and illumination variation. Early convolutional neural networks (CNN) based method mainly structure the skeleton sequence into pseudo-image and feed it into image classification neural network such as Resnet, which can not capture comprehensive spatial-temporal feature. Recently, graph convolutional networks (GCNs) have obtained superior performance. However, the computational complexity of GCN-based methods is quite high, some works even reach 100 GFLOPs for one action sample. This is contrary to the highly condensed attributes of skeleton data. In this paper, a Multi-scale Spatial-temporal Convolution Neural Network (MSST-Net) is proposed for skeleton-based action recognition. Our MSST-Net abandons complex graph convolutions and takes the implicit complementary advantages across different scales of spatial-temporal representations, which are often ignored in the previous work. On two datasets for action recognition, MSST-Net achieves impressive recognition accuracy with a small amount of calculation.","",""
0,"Calvin Chi","HLA Allele Imputation with Multitask Deep Convolutional Neural Network",2021,"","","","",90,"2022-07-13 10:07:51","","10.1101/2021.06.03.447012","","",,,,,0,0.00,0,1,1,"Motivation The Human leukgocyte antigen (HLA) system is a highly polymorphic gene complex encoding the major histocompatibility complex proteins in humans. HLA alleles are of strong epidemiological interest for their large effect sizes in associations with autoimmune diseases, infectious diseases, severe drug reactions, and transplant medicine. Since HLA genotyping can be time-consuming and cost-prohibitive, methods to impute HLA alleles from SNP genotype data have been developed, including HLA Genotype Imputation with Attribute Bagging (HIBAG), HLA*IMP:02, and SNP2HLA. However, limitations of these imputation programs include imputation accuracy, computational runtime, and ability to impute HLA allele haplotypes. Results We present a deep learning framework for HLA allele imputation using a multitask convolutional neural network (CNN) architecture. In this approach, we use phased SNP genotype data flanking ±250 kb from each HLA locus to simultaneously impute HLA allele haplotyes across loci HLA-A, -B, -C, -DQA1, -DQB1, -DPA1, -DPB1, and -DRB1. We start by tokenizing phased genotype sequences into k-mers that serve as input to the model. The CNN architecture starts with a shared embedding layer for learning low-dimensional representations of k-mers, shared convolutional layers for detecting genotype motifs, and branches off into separate densely-connected layers for imputing each HLA loci. We present evidence that the CNN used information from known tag SNPs to impute HLA alleles, and demonstrate the architecture is robust against a selection of hyperparameters. On the T1DGC dataset, our model achieved 97.6% imputation accuracy, which was superior to SNP2HLA’s performance and comparable to HIBAG’s performance. However, unlike HIBAG, our method can impute an entire HLA haplotype sequence instead of imputing one locus at a time. Additionally, by separating the training and inference steps, our imputation program provides user flexibility to reduce usage time. Availability The source code is available at https://github.com/CalvinTChi/HLA_imputation Contact calvin.chi@berkeley.edu","",""
6,"Hyunkyu Park, Hyosang Lee, Kyungseo Park, Sangwoo Mo, Jung Kim","Deep Neural Network Approach in Electrical Impedance Tomography-based Real-time Soft Tactile Sensor",2019,"","","","",91,"2022-07-13 10:07:51","","10.1109/IROS40897.2019.8968532","","",,,,,6,2.00,1,5,3,"Recently, a whole-body tactile sensing have emerged in robotics for safe human-robot interaction. A key issue in the whole-body tactile sensing is ensuring large-area manufacturability and high durability. To fulfill these requirements, a reconstruction method called electrical impedance tomography (EIT) was adopted in large-area tactile sensing. This method maps voltage measurements to conductivity distribution using only a few number of measurement electrodes. A common approach for the mapping is using a linearized model derived from the Maxwell’s equation. This linearized model shows fast computation time and moderate robustness against measurement noise but reconstruction accuracy is limited. In this paper, we propose a novel nonlinear EIT algorithm through Deep Neural Network (DNN) approach to improve the reconstruction accuracy of EIT-based tactile sensors. The neural network architecture with rectified linear unit (ReLU) function ensured extremely low computational time (0.002 seconds) and nonlinear network structure which provides superior measurement accuracy. The DNN model was trained with dataset synthesized in simulation environment. To achieve the robustness against measurement noise, the training proceeded with additive Gaussian noise that estimated through actual measurement noise. For real sensor application, the trained DNN model was transferred to a conductive fabric-based soft tactile sensor. For validation, the reconstruction error and noise robustness were mainly compared using conventional linearized model and proposed approach in simulation environment. As a demonstration, the tactile sensor equipped with the trained DNN model is presented for a contact force estimation.","",""
3,"A. Giannakidis, K. Kamnitsas, V. Spadotto, J. Keegan, Gillian Smith, B. Glocker, D. Rueckert, S. Ernst, M. Gatzoulis, D. Pennell, S. Babu-Narayan, D. Firmin","Fast Fully Automatic Segmentation of the Severely Abnormal Human Right Ventricle from Cardiovascular Magnetic Resonance Images Using a Multi-Scale 3D Convolutional Neural Network",2016,"","","","",92,"2022-07-13 10:07:51","","10.1109/SITIS.2016.16","","",,,,,3,0.50,0,12,6,"Cardiac magnetic resonance (CMR) is regarded as the reference examination for cardiac morphology in tetralogy of Fallot (ToF) patients allowing images of high spatial resolution and high contrast. The detailed knowledge of the right ventricular anatomy is critical in ToF management. The segmentation of the right ventricle (RV) in CMR images from ToF patients is a challenging task due to the high shape and image quality variability. In this paper we propose a fully automatic deep learning-based framework to segment the RV from CMR anatomical images of the whole heart. We adopt a 3D multi-scale deep convolutional neural network to identify pixels that belong to the RV. Our robust segmentation framework was tested on 26 ToF patients achieving a Dice similarity coefficient of 0.8281±0.1010 with reference to manual annotations performed by expert cardiologists. The proposed technique is also computationally efficient, which may further facilitate its adoption in the clinical routine.","",""
7,"Eric Minor, Stian D. Howard, Adam A S Green, M. Glaser, C. Park, N. Clark","End-to-end machine learning for experimental physics: using simulated data to train a neural network for object detection in video microscopy.",2019,"","","","",93,"2022-07-13 10:07:51","","10.1039/c9sm01979k","","",,,,,7,2.33,1,6,3,"We demonstrate a method for training a convolutional neural network with simulated images for usage on real-world experimental data. Modern machine learning methods require large, robust training data sets to generate accurate predictions. Generating these large training sets requires a significant up-front time investment that is often impractical for small-scale applications. Here we demonstrate a 'full-stack' computational solution, where the training data set is generated on-the-fly using a noise injection process to produce simulated data characteristic of the experimental system. We demonstrate the power of this full-stack approach by applying it to the study of topological defect annihilation in systems of liquid crystal freely-suspended films. This specific experimental system requires accurate observations of both the spatial distribution of the defects and the total number of defects, making it an ideal system for testing the robustness of the trained network. The fully trained network was found to be comparable in accuracy to human hand-annotation, with four-orders of magnitude improvement in time efficiency.","",""
0,"Jiamin Zhou","Spectrome-AI: a Neural Network Framework for Inferring MEG Spectra",2019,"","","","",94,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,1,3,"Author(s): Zhou, Jiamin | Advisor(s): Raj, Ashish | Abstract: Computational modeling is a tool that allows for biological systems involving large networks to be studied, such as in studying the correlations between structural connectivity and functional connectivity in the human brain. Raj et al. proposed the spectral graph model in 2019 as a linear, low-dimensional alternative to conventional neural field and mass models that are more computationally expensive, especially when optimizing parameters, which is necessary in order to obtain quantitative and qualitative information about functional neural activity. The initial method used for inferring the spectral graph model parameters was Markov chain Monte Carlo (MCMC) sampling, which provided a robust way to estimate what the target parameter distributions were most likely to be. However, MCMC methods are still slow and computationally expensive. In this study, we trained a fully connected neural network on MCMC-simulated magnetoencephalography (MEG) data to perform parameter estimation for the spectral graph model in an accelerated manner. We found that the neural network was able to predict most parameters of interest without much loss in precision while generating the parameters in less than a second. This approach puts us closer to obtaining real time neurophysiological information from functional neuroimaging data for applications in diagnosis, prognosis, and characterization of various neurological diseases.","",""
1,"Muhammad Altaf Hussain, U. Rehman, S. Islam, M. F. Sheikh, Amber Javaid","Detection and Classification of Retinal Red Lesions via Regional Spatial Transformations and Neural Network",2019,"","","","",95,"2022-07-13 10:07:51","","10.1145/3330482.3330486","","",,,,,1,0.33,0,5,3,"The worldwide loss in human vision is primarily associated with Diabetic Retinopathy (DR). It occurs due to accelerated levels of blood sugar thereby causing perforation, bulging and leakage of retinal blood vessels (BVs). DR commences with the emergence of small blood spots on the retinal surface known as Microaneurysms (MAs) that are subsequently transformed into heavy blood deposits called Hemorrhages (HGs). This paper proposes an optimized and computationally inexpensive digital image processing (DIP) technique for detection and classification of 'Retinal Red Lesions' (RRLs) i.e. MAs and HGs using green channel of the digital fundus images. The basic essence of the proposed technique revolves around regional spatial transformations detection performed through region based spatial filtering, matching features and neural networks classification. The proposed technique comprises of five main stages i.e. Pre-processing, Regional Spatial Transformations, Optimization, Features extraction and Classification. Speed Up Robust Features (SURF) algorithm has been used for features selection & extraction while Feed-forward Back-propagation Artificial Neural Network (FFBP ANN) has been used for classification. The proposed technique has been successfully applied on commercially available digital fundus image data-set and has yielded 98.4% 'Sensitivity' (SE), 94% 'Specificity' (SP) and 98% 'Accuracy' (AC). The SE, SP and AC have also been compared with other RRLs detection methods and has shown highly promising and encouraging results.","",""
2,"F. Khatami, M. Escabí","Spiking network optimized for noise robust word recognition approaches human-level performance and predicts auditory system hierarchy",2018,"","","","",96,"2022-07-13 10:07:51","","10.1101/243915","","",,,,,2,0.50,1,2,4,"The auditory neural code is resilient to acoustic variability and capable of recognizing sounds amongst competing sound sources, yet, the transformations enabling noise robust abilities are largely unknown. We report that a hierarchical spiking neural network (HSNN) trained to maximize word recognition accuracy in noise and multiple talkers approaches human-level performance. Intriguingly, comparisons with data from auditory nerve, midbrain, thalamus and cortex reveals that the organization and nonlinear transformations of the optimal network predict several properties of the ascending auditory pathway including a sequential loss of temporal resolution, increasing sparseness and selectivity. The optimal organizational scheme is critical for noise robustness since an identical network arranged to enable high information transfer does not predict auditory pathway organization and has substantially poorer performance. Furthermore, conventional linear and nonlinear receptive field-based models fail to achieve similar noise robust performance. The findings suggest that the auditory pathway hierarchy and its sequential nonlinear feature extraction computations may form a near optimal code capable of efficiently detecting sounds in noise impoverished conditions. Significance Statement The brain’s ability to recognize sounds in the presence of competing sounds or background noise is essential for everyday hearing tasks. How the brain accomplishes noise resiliency, however, is poorly understood. Using neural recording from the ascending auditory pathway and an auditory spiking network model trained for optimal sound recognition in noise we explore the computational strategies that enable noise robustness. Our results suggest that the hierarchical organization of the auditory pathway and the resulting nonlinear transformations may form a near optimal strategy that is essential for sound recognition in the presence of noise.","",""
23,"F. Glang, A. Deshmane, S. Prokudin, F. Martin, K. Herz, T. Lindig, B. Bender, K. Scheffler, M. Zaiss","DeepCEST 3T: Robust MRI parameter determination and uncertainty quantification with neural networks—application to CEST imaging of the human brain at 3T",2019,"","","","",97,"2022-07-13 10:07:51","","10.1002/mrm.28117","","",,,,,23,7.67,3,9,3,"Calculation of sophisticated MR contrasts often requires complex mathematical modeling. Data evaluation is computationally expensive, vulnerable to artifacts, and often sensitive to fit algorithm parameters. In this work, we investigate whether neural networks can provide not only fast model fitting results, but also a quality metric for the predicted values, so called uncertainty quantification, investigated here in the context of multi‐pool Lorentzian fitting of CEST MRI spectra at 3T.","",""
0,"Takuya Sakuma, Hiroki Matsutani","An Area-Efficient Recurrent Neural Network Core for Unsupervised Time-Series Anomaly Detection",2021,"","","","",98,"2022-07-13 10:07:51","","10.1587/transele.2020lhp0003","","",,,,,0,0.00,0,2,1,"Since most sensor data depend on each other, time-series anomaly detection is one of practical applications of IoT devices. Such tasks are handled by Recurrent Neural Networks (RNNs) with a feedback structure, such as Long Short Term Memory. However, their learning phase based on Stochastic Gradient Descent (SGD) is computationally expensive for such edge devices. This issue is addressed by executing their learning on high-performance server machines, but it introduces a communication overhead and additional power consumption. On the other hand, Recursive Least-Squares Echo State Network (RLS-ESN) is a simple RNN that can be trained at low cost using the least-squares method rather than SGD. In this paper, we propose its area-efficient hardware implementation for edge devices and adapt it to human activity anomaly detection as an example of interdependent time-series sensor data. The model is implemented in Verilog HDL, synthesized with a 45 nm process technology, and evaluated in terms of the anomaly capability, hardware amount, and performance. The evaluation results demonstrate that the RLS-ESN core with a feedback structure is more robust to hyper parameters than an existing Online Sequential Extreme Learning Machine (OS-ELM) core. It consumes only 1.25 times larger hardware amount and 1.11 times longer latency than the existing OS-ELM core. key words: on-device learning, machine learning, anomaly detection","",""
14,"L. Ngo, J. Cha, Jae‐Ho Han","Deep Neural Network Regression for Automated Retinal Layer Segmentation in Optical Coherence Tomography Images",2020,"","","","",99,"2022-07-13 10:07:51","","10.1109/TIP.2019.2931461","","",,,,,14,7.00,5,3,2,"Segmenting the retinal layers in optical coherence tomography (OCT) images helps to quantify the layer information in early diagnosis of retinal diseases, which are the main cause of permanent blindness. Thus, the segmentation process plays a critical role in preventing vision impairment. However, because there is a lack of practical automated techniques, expert ophthalmologists still have to manually segment the retinal layers. In this paper, we propose an automated segmentation method for OCT images based on a feature-learning regression network without human bias. The proposed deep neural network regression takes the intensity, gradient, and adaptive normalized intensity score (ANIS) of an image segment as features for learning, and then predicts the corresponding retinal boundary pixel. Reformulating the segmentation as a regression problem obviates the need for a huge dataset and reduces the complexity significantly, as shown in the analysis of computational complexity given here. In addition, assisted by ANIS, the method operates robustly on OCT images containing intensity variances, low-contrast regions, speckle noise, and blood vessels, yet remains accurate and time-efficient. In the evaluation of the method conducted using 114 images, the processing time was approximately 10.596 s per image for identifying eight boundaries, and the training phase for each boundary line took only 30 s. Further, the Dice similarity coefficient used for assessing accuracy gave a computed value of approximately 0.966. The absolute pixel distance of manual and automatic segmentation using the proposed scheme was 0.612, which is less than a one-pixel difference, on average.","",""
9,"Xiang Zhai, Kui Liu, W. Nash, D. Castineira","Smart Autopilot Drone System for Surface Surveillance and Anomaly Detection via Customizable Deep Neural Network",2020,"","","","",100,"2022-07-13 10:07:51","","10.2523/iptc-20111-ms","","",,,,,9,4.50,2,4,2,"  Copter-based unmanned aerial vehicle (drone) systems are being utilized for surveillance, inspection and security purposes for well sites, gathering centers, pipelines, refineries, and other surface facilities. However, most of the practices largely rely on humans, including drone operation, data transfer, image analysis, etc. In this paper, we present a comprehensive, cloud-enabled, human-free autopilot drone system and its application in field surveillance and anomaly detection powered by customizable deep neural network and computer vision models.  The proposed system consists of customized quadcopter drones equipped with high-definition cameras, thermal imaging and gas sensing devices, autopiloted by cloud-connected onboard computers. A series of advanced algorithms are developed and deployed onboard and over the Cloud for processing and diagnosing the image/thermal/gas sensing data collected by the drones in real-time or near real-time, including accurate 2D geospatial aerial mapping, anomaly detection and classification for events like oil leak, gas leak, facility failure, human activities, etc. Object detection deep learning models are customized and parallelized for low-profile multi-core single board computers.  In our case study, a pre-configured drone flew along the same path twice at a 6-month gap. A robust, iterative image registering algorithm is developed to precisely align and overlay images taken at different days at the same or similar GPS locations, even with significant changes to the environment due to season shift, human activities, camera angles or height variations. Local changes are filtered and selected based on their sizes and magnitudes in the residual images by subtracting pairs of perfectly overlaid scenes. Pre-trained Residual Convolutional Neural network (He et al. 2015) is rapidly re-trained to further classify the type of changes using the techniques of transfer learning and data augmentation. An ROC of 99% was achieved in the multi-task binary classification, wherein the detected changes are divided into positive anomalies (such as oil/gas leak, facility failures, unauthored human activities) and negative (natural/insignificant) signals. Comparing against a support vector machine baseline with a ROC=92%, the ResNet model demonstrates significant, more promising detection accuracy at a faster training time.  This innovative integrated platform is presented that combines physical drone, onboard imaging/sensing devices, cloud connectivity, onboard and back-end control system, deep learning and computer vision architecture for situational awareness of oil & gas fields and the mining industry. It achieves full automation of mass surveillance, data acquisition and storage, diagnostics and asset situational understanding. The system architecture, especially the onboard and cloud computation engines, can be readily transferred and applied to other common drone platforms.","",""
0,"Hirak J. Kashyap","Brain Inspired Neural Network Models of Visual Motion Perception and Tracking in Dynamic Scenes",2020,"","","","",101,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,1,2,"Author(s): Kashyap, Hirak Jyoti | Advisor(s): Krichmar, Jeffrey L | Abstract: For self-driving vehicles, aerial drones, and autonomous robots to be successfully deployed in the real-world, they must be able to navigate complex environments and track objects. While Artificial Intelligence and Machine Vision have made significant progress in dynamic scene understanding, they are not yet as robust and computationally efficient as humans or other primates in these tasks. For example, the current state-of-the-art visual tracking methods become inaccurate when applied to random test videos. We suggest that ideas from cortical visual processing can inspire real world solutions for motion perception and tracking that are robust and efficient. In this context, the following contributions are made in this dissertation. First, a method for estimating 6DoF ego-motion and pixel-wise object motion is introduced, based on a learned overcomplete motion field basis set. The method uses motion field constraints for training and a novel differentiable sparsity regularizer to achieve state-of-the-art ego and object-motion performances on benchmark datasets. Second, a Convolutional Neural Network (CNN) that learns hidden neural representations analogous to the response characteristics of dorsal Medial Superior Temporal area (MSTd) neurons for optic flow and object motion is presented. The findings suggest that goal driven training of CNNs might automatically result in the MSTd-like response properties of model neurons. Third, a recurrent neural network model of predictive smooth pursuit eye movements is presented that generates similar pursuit initiation and predictive pursuit behaviors as observed in humans. The model provides the computational mechanisms of formation and rapid update of an internal model of target velocity, commonly attributed to zero lag tracking and smooth pursuit of occluded objects. Finally, a spike based stereo depth algorithm is presented that reconstructs dynamic visual scenes at 400 frames-per-second with one watt of power consumption when implemented using the IBM TrueNorth processor. Taken together, the presented models and implementations provide the computations for motion perception in the dorsal visual pathway in the brain and inform ideas for efficient computational vision systems.","",""
7,"Hossein Mehnatkesh, A. Alasty, M. Boroushaki, M. Khodsiani, M. Hasheminasab, M. Kermani","Estimation of Water Coverage Ratio in Low Temperature PEM-Fuel Cell Using Deep Neural Network",2020,"","","","",102,"2022-07-13 10:07:51","","10.1109/JSEN.2020.2993181","","",,,,,7,3.50,1,6,2,"Proton exchange membrane fuel cell (PEMFC) is a rich source of renewable energy. A non-destructive prediction method is needed to determine the content of water in the PEMFC. In the gas channel of a transparent PEMFC, water is detected with image processing. This method has a high computational cost and is sensitive to the initial position of the camera and ambient lighting. In this paper, the deep neural network (DNN) has been trained to learn the transparent PEMFC’s labeled images as a way to determine the content of water, limit human interference and employed in a real-time process. This DNN model is a virtual sensor for measuring the water coverage ratio. To produce the label of images, all data are divided into 6 classes based on the percentage of water coverage ratio. Through analyzing the number of each class, the imbalance data is unfolded. To overcome this problem, random oversampling and undersampling techniques are used. The images and the classes are considered as the input and output of the DNN, respectively. Also, the region of water accumulation in the gas channel can be recognized with robustness to the environmental conditions. Final results of 4-41-64-120-99-6 nodes for DNN layers were derived due to GA optimization. Accuracy of 96.77% and 94.23% in train and test data have been achieved. This DNN, processes the images up to 10 times faster than image processing. Also, the region of water accumulation in the gas channel can be recognized.","",""
3,"Hussein Hassan-Harrirou, Ce Zhang, T. Lemmin","RosENet: Improving binding affinity prediction by leveraging molecular mechanics energies with a 3D Convolutional Neural Network",2020,"","","","",103,"2022-07-13 10:07:51","","10.1101/2020.05.12.090191","","",,,,,3,1.50,1,3,2,"The worldwide increase and proliferation of drug resistant microbes, coupled with the lag in new drug development represents a major threat to human health. In order to reduce the time and cost for exploring the chemical search space, drug discovery increasingly relies on computational biology approaches. One key step in these approaches is the need for the rapid and accurate prediction of the binding affinity for potential leads. Here, we present RosENet (Rosetta Energy Neural Network), a three-dimensional (3D) Convolutional Neural Network (CNN), which combines voxelized molecular mechanics energies and molecular descriptors for predicting the absolute binding affinity of protein – ligand complexes. By leveraging the physico-chemical properties captured by the molecular force field, our model achieved a Root Mean Square Error (RMSE) of 1.26 on the PDBBind v2016 core set. We also explored some limitations and the robustness of the PDBBind dataset and our approach, on nearly 500 structures, including structures determined by Nuclear Magnetic Resonance and virtual screening experiments. Our study demonstrated that molecular mechanics energies can be voxelized and used to help improve the predictive power of the CNNs. In the future, our framework can be extended to features extracted from other biophysical and biochemical models, such as molecular dynamics simulations. Availability https://github.com/DS3Lab/RosENet","",""
68,"Qiang Yu, Rui Yan, Huajin Tang, K. Tan, Haizhou Li","A Spiking Neural Network System for Robust Sequence Recognition",2016,"","","","",104,"2022-07-13 10:07:51","","10.1109/TNNLS.2015.2416771","","",,,,,68,11.33,14,5,6,"This paper proposes a biologically plausible network architecture with spiking neurons for sequence recognition. This architecture is a unified and consistent system with functional parts of sensory encoding, learning, and decoding. This is the first systematic model attempting to reveal the neural mechanisms considering both the upstream and the downstream neurons together. The whole system is a consistent temporal framework, where the precise timing of spikes is employed for information processing and cognitive computing. Experimental results show that the system is competent to perform the sequence recognition, being robust to noisy sensory inputs and invariant to changes in the intervals between input stimuli within a certain range. The classification ability of the temporal learning rule used in the system is investigated through two benchmark tasks that outperform the other two widely used learning rules for classification. The results also demonstrate the computational power of spiking neurons over perceptrons for processing spatiotemporal patterns. In summary, the system provides a general way with spiking neurons to encode external stimuli into spatiotemporal spikes, to learn the encoded spike patterns with temporal learning rules, and to decode the sequence order with downstream neurons. The system structure would be beneficial for developments in both hardware and software.","",""
22,"Haihua Liu, Na Shu, Q. Tang, Wensheng Zhang","Computational Model Based on Neural Network of Visual Cortex for Human Action Recognition",2018,"","","","",105,"2022-07-13 10:07:51","","10.1109/TNNLS.2017.2669522","","",,,,,22,5.50,6,4,4,"In this paper, we propose a bioinspired model for human action recognition through modeling neural mechanisms of information processing in two visual cortical areas: the primary visual cortex (V1) and the middle temporal cortex (MT) dedicated to motion. This model, named V1-MT, is composed of V1 and MT models (layers) corresponding to their cortical areas, which are built with layered spiking neural networks (SNNs). Some neuron properties in V1 and MT, such as direction and speed selectivity, spatiotemporal inseparability, and center surround suppression, are integrated into SNNs. Based on speed and direction selectivity, V1 and MT models contain multiple SNN channels, each of which processes motion information in sequences with spatiotemporal tunings of neurons at a certain speed and different directions. Therefore, we propose two operations, input signal perceiving with 3-D Gabor filters and surround inhibition processing with 3-D differences of Gaussian functions, to perform this task according to the spatiotemporal inseparability and center surround suppression of neurons. Then, neurons are modeled with our simplified integrate-and-fire model and motion information is transformed into spike trains. Afterward, we define a new feature vector: a mean motion map computed from spike trains in all channels to represent human actions. Finally, a support vector machine is trained to classify actions represented by the feature vectors. We conducted extensive experiments on public action databases, and the results show that our model outperforms other bioinspired models and rivals the state-of-the-art approaches.","",""
12,"T. D’orazio, G. Attolico, G. Cicirelli, C. Guaragnella","A Neural Network Approach for Human Gesture Recognition with a Kinect Sensor",2014,"","","","",106,"2022-07-13 10:07:51","","10.5220/0004919307410746","","",,,,,12,1.50,3,4,8,"Service robots are expected to be used in many household    in the near future, provided that proper interfaces are developed    for the human robot interaction. Gesture recognition has been    recognized as a natural way for the communication especially for    elder or impaired people. With the developments of new    technologies and the large availability of inexpensive depth    sensors, real time gesture recognition has been faced by using    depth information and avoiding the limitations due to complex    background and lighting situations. In this paper the Kinect    Depth Camera, and the OpenNI framework have been used to obtain    real time tracking of human skeleton. Then, robust and significant    features have been selected to get rid of unrelated features and    decrease the computational costs. These features are fed to a set    of Neural Network Classifiers that recognize ten different    gestures. Several experiments demonstrate that the proposed method    works effectively. Real time tests prove the robustness of    the method for realization of human robot interfaces.","",""
9,"R. Blything, Valerio Biscione, Ivan I. Vankov, Casimir J H Ludwig, J. Bowers","The human visual system and CNNs can both support robust online translation tolerance following extreme displacements",2020,"","","","",107,"2022-07-13 10:07:51","","10.1167/jov.21.2.9","","",,,,,9,4.50,2,5,2,"Visual translation tolerance refers to our capacity to recognize objects over a wide range of different retinal locations. Although translation is perhaps the simplest spatial transform that the visual system needs to cope with, the extent to which the human visual system can identify objects at previously unseen locations is unclear, with some studies reporting near complete invariance over 10 degrees and other reporting zero invariance at 4 degrees of visual angle. Similarly, there is confusion regarding the extent of translation tolerance in computational models of vision, as well as the degree of match between human and model performance. Here, we report a series of eye-tracking studies (total N = 70) demonstrating that novel objects trained at one retinal location can be recognized at high accuracy rates following translations up to 18 degrees. We also show that standard deep convolutional neural networks (DCNNs) support our findings when pretrained to classify another set of stimuli across a range of locations, or when a global average pooling (GAP) layer is added to produce larger receptive fields. Our findings provide a strong constraint for theories of human vision and help explain inconsistent findings previously reported with convolutional neural networks (CNNs).","",""
7,"Mengyuan Gong, Taosheng Liu","Biased Neural Representation of Feature-Based Attention in the Human Frontoparietal Network",2020,"","","","",108,"2022-07-13 10:07:51","","10.1523/JNEUROSCI.0690-20.2020","","",,,,,7,3.50,4,2,2,"Selective attention is a core cognitive function for efficient processing of information. Although it is well known that attention can modulate neural responses in many brain areas, the computational principles underlying attentional modulation remain unclear. Contrary to the prevailing view of a high-dimensional, distributed neural representation, here we show a surprisingly simple, biased neural representation for feature-based attention in a large dataset including five human fMRI studies. Selective attention is a core cognitive function for efficient processing of information. Although it is well known that attention can modulate neural responses in many brain areas, the computational principles underlying attentional modulation remain unclear. Contrary to the prevailing view of a high-dimensional, distributed neural representation, here we show a surprisingly simple, biased neural representation for feature-based attention in a large dataset including five human fMRI studies. We found that when human participants (both sexes) selected one feature from a compound stimulus, voxels in many cortical areas responded consistently higher to one attended feature over the other. This univariate bias was consistent across brain areas within individual subjects. Importantly, this univariate bias showed a progressively stronger magnitude along the cortical hierarchy. In frontoparietal areas, the bias was strongest and contributed largely to pattern-based decoding, whereas early visual areas lacked such a bias. These findings suggest a gradual transition from a more analog to a more abstract representation of attentional priority along the cortical hierarchy. Biased neural responses in high-level areas likely reflect a low-dimensional neural code that can facilitate a robust representation and simple readout of cognitive variables. SIGNIFICANCE STATEMENT It is typically assumed that cognitive variables are represented by distributed population activities. Although this view is rooted in decades of work in the sensory system, it has not been rigorously tested at different levels of cortical hierarchy. Here we show a novel, low-dimensional coding scheme that dominated the representation of feature-based attention in frontoparietal areas. The simplicity of such a biased code may confer a robust representation of cognitive variables, such as attentional selection, working memory, and decision-making.","",""
1,"Masood Ul Hassan, R. Veerabhadrappa, James Zhang, A. Bhatti","Robust Optimal Parameter Estimation (OPE) for Unsupervised Clustering of Spikes Using Neural Networks",2020,"","","","",109,"2022-07-13 10:07:51","","10.1109/SMC42975.2020.9283347","","",,,,,1,0.50,0,4,2,"Spike sorting of electrophysiological data plays an important role in deciphering useful information from the brain. Unsupervised clustering of brain data relative to respective neurons is important to understand single cell and networks dynamics. A large number of clustering techniques exist in the literature; however, the dependency of these clustering algorithms on the selection of appropriate parameters, such as, bandwidth or threshold window size is critical. Iterative methods are generally employed to estimate optimal parameters, however, significant computational time and associated large number of iterations make the clustering inefficient to implement. To address this issue, we introduce a robust Optimal Parameter Estimation (OPE) Algorithm that can estimate the optimized parameters in a fast and efficient way. The performance of the OPE algorithm is tested on MeanShift and DBSCAN clustering algorithms. Three different extracellular recorded datasets including two simulated and one single human cell, as well as two feature sets including PCA and Haar Wavelets are used for validation purposes.","",""
2,"O. Oyedotun, Abd El Rahman Shabayek, Djamila Aouada, B. Ottersten","Improved Highway Network Block for Training Very Deep Neural Networks",2020,"","","","",110,"2022-07-13 10:07:51","","10.1109/ACCESS.2020.3026423","","",,,,,2,1.00,1,4,2,"Very deep networks are successful in various tasks with reported results surpassing human performance. However, training such very deep networks is not trivial. Typically, the problems of learning the identity function and feature reuse can work together to plague optimization of very deep networks. In this paper, we propose a highway network with gate constraints that addresses the aforementioned problems, and thus alleviates the difficulty of training. Namely, we propose two variants of highway network, HWGC and HWCC, employing feature summation and concatenation respectively. The proposed highway networks, besides being more computationally efficient, are shown to have more interesting learning characteristics such as natural learning of hierarchical and robust representations due to a more effective usage of model depth, fewer gates for successful learning, better generalization capacity and faster convergence than the original highway network. Experimental results show that our models outperform the original highway network and many state-of-the-art models. Importantly, we observe that our second model with feature concatenation and compression consistently outperforms our model with feature summation of similar depth, the original highway network, many state-of-the-art models and even ResNets on four benchmarking datasets which are CIFAR-10, CIFAR-100, Fashion-MNIST, SVHN and imagenet-2012 (ILSVRC) datasets. Furthermore, the second proposed model is more computationally efficient than the state-of-the-art in view of training, inference time and GPU memory resource, which strongly supports real-time applications. Using a similar number of model parameters for the CIFAR-10, CIFAR-100, Fashion-MNIST and SVHN datasets, the significantly shallower proposed model can surpass the performance of ResNet-110 and ResNet-164 that are roughly 6 and 8 times deeper, respectively. Similarly, for the imagenet dataset, the proposed models surpass the performance of ResNet-101 and ResNet-152 that are roughly three times deeper.","",""
67,"Keisuke Sakaguchi, Kevin Duh, Matt Post, Benjamin Van Durme","Robsut Wrod Reocginiton via Semi-Character Recurrent Neural Network",2016,"","","","",111,"2022-07-13 10:07:51","","10.1609/aaai.v31i1.10970","","",,,,,67,11.17,17,4,6,"    Language processing mechanism by humans is generally more robust than computers. The Cmabrigde Uinervtisy (Cambridge University) effect from the psycholinguistics literature has demonstrated such a robust word processing mechanism, where jumbled words (e.g. Cmabrigde / Cambridge) are recognized with little cost. On the other hand, computational models for word recognition (e.g. spelling checkers) perform poorly on data with such noise. Inspired by the findings from the Cmabrigde Uinervtisy effect, we propose a word recognition model based on a semi-character level recurrent neural network (scRNN). In our experiments, we demonstrate that scRNN has significantly more robust performance in word spelling correction (i.e. word recognition) compared to existing spelling checkers and character-based convolutional neural network. Furthermore, we demonstrate that the model is cognitively plausible by replicating a psycholinguistics experiment about human reading difficulty using our model.   ","",""
0,"Mark Ikechukwu Ogbodo, K. Dang, Abderazek Ben Abdallah","Study of a Multi-modal Neurorobotic Prosthetic Arm Control System based on Recurrent Spiking Neural Network",2022,"","","","",112,"2022-07-13 10:07:51","","10.1051/shsconf/202213903019","","",,,,,0,0.00,0,3,1,"The use of robotic arms in various fields of human endeavor has increased over the years, and with recent advancements in artificial intelligence enabled by deep learning, they are increasingly being employed in medical applications like assistive robots for paralyzed patients with neurological disorders, welfare robots for the elderly, and prosthesis for amputees. However, robot arms tailored towards such applications are resource-constrained. As a result, deep learning with conventional artificial neural network (ANN) which is often run on GPU with high computational complexity and high power consumption cannot be handled by them. Neuromorphic processors, on the other hand, leverage spiking neural network (SNN) which has been shown to be less computationally complex and consume less power, making them suitable for such applications. Also, most robot arms unlike living agents that combine different sensory data to accurately perform a complex task, use uni-modal data which affects their accuracy. Conversely, multi-modal sensory data has been demonstrated to reach high accuracy and can be employed to achieve high accuracy in such robot arms. This paper presents the study of a multi-modal neurorobotic prosthetic arm control system based on recurrent spiking neural network. The robot arm control system uses multi-modal sensory data from visual (camera) and electromyography sensors, together with spike-based data processing on our previously proposed R-NASH neuromorphic processor to achieve robust accurate control of a robot arm with low power. The evaluation result using both uni-modal and multi-modal input data show that the multi-modal input achieves a more robust performance at 87%, compared to the uni-modal.","",""
0,"Jing Yuan, P. Barmpoutis, T. Stathaki","Pedestrian Detection Using Integrated Aggregate Channel Features and Multitask Cascaded Convolutional Neural-Network-Based Face Detectors",2022,"","","","",113,"2022-07-13 10:07:51","","10.3390/s22093568","","",,,,,0,0.00,0,3,1,"Pedestrian detection is a challenging task, mainly owing to the numerous appearances of human bodies. Modern detectors extract representative features via the deep neural network; however, they usually require a large training set and high-performance GPUs. For these cases, we propose a novel human detection approach that integrates a pretrained face detector based on multitask cascaded convolutional neural networks and a traditional pedestrian detector based on aggregate channel features via a score combination module. The proposed detector is a promising approach that can be used to handle pedestrian detection with limited datasets and computational resources. The proposed detector is investigated comprehensively in terms of parameter choices to optimize its performance. The robustness of the proposed detector in terms of the training set, test set, and threshold is observed via tests and cross dataset validations on various pedestrian datasets, including the INRIA, part of the ETHZ, and the Caltech and Citypersons datasets. Experiments have proved that this integrated detector yields a significant increase in recall and a decrease in the log average miss rate compared with sole use of the traditional pedestrian detector. At the same time, the proposed method achieves a comparable performance to FRCNN on the INRIA test set compared with sole use of the Aggregated Channel Features detector.","",""
0,"Minghua Zhao, Min Yuan, Yaning Yang, Steven Xu","CPGL: Prediction of compound-protein interaction by integrating graph attention network with long short-term memory neural network",2022,"","","","",114,"2022-07-13 10:07:51","","10.1101/2022.04.19.488691","","",,,,,0,0.00,0,4,1,"Recent advancements of artificial intelligence based on deep learning algorithms have made it possible to computationally predict compound-protein interaction (CPI) without conducting laboratory experiments. In this manuscript, we integrated a graph attention network (GAT) for compounds and a long short-term memory neural network (LSTM) for proteins, used end-to-end representation learning for both compounds and proteins, and proposed a deep learning algorithm, CPGL (CPI with GAT and LSTM) to optimize the feature extraction from compounds and proteins and to improve the model robustness and generalizability. CPGL demonstrated an excellent predictive performance and outperforms recently reported deep learning models. Based on 3 public CPI datasets, C.elegans, Human and BindingDB, CPGL represented 1 - 5% improvement compared to existing deep-learning models. Our method also achieves excellent results on datasets with imbalanced positive and negative proportions constructed based on the above two datasets. More importantly, using 2 label reversal datasets, GPCR and Kinase, CPGL showed superior performance compared to other existing deep learning models. The AUC were substantially improved by 15% to 50% on the Kinase dataset, indicative of the robustness and generalizability of CPGL.","",""
0,"U. Cnrs","Neural Network and Wavelet Multiresolution System for Human Being Detection",2017,"","","","",115,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,1,5,"ABSTRACT Many applications, in robotics, require identification of human being. Using complex methods, based on modelmatching are too computationally expensive and not always justified. We propose a fast and simple method for identification of human being. This method takes profit of the learning capabilities of a neural network. The idea is to train a neural network on some images of persons. In order to reduce the amount of this data (images), we use waveletmultiresolution propriety analysis that allows to bring significant information content of image. This one thus ischaracterised by its approximation at a given resolution. After the training phase, the generalization capabilities of the network allow it to identify no-learned images.We describe here the proposed method, and we present experimental results obtained on a data base of 437 images.Key words: Segmentation - Neural Network -Identification-Image Processing-Wavelet Multiresolution. 1. INTRODUCTION Human is able to localise and identify a human being very quickly, in different situations and with a good reliability.This capability is very robust: this one resists to important image changes due to modification of point of view or lightingconditions, etc....That why, the visual analysis by the humans has fascinated a lot of scientifics like Aristote or Darwin,since centuries. The automatic detection systems are interesting by theory knowledge that they will can bring to us aboutthe visual human system. But they have a lot of practical applications like: perception of autonomous vehicles, controlaccess (banks,..), etc.The connexionist models (neural networks) give a panoply of methods for classification, event detection and signal","",""
2,"Rachel Sterneck, Abhishek Moitra, P. Panda","Noise Sensitivity-Based Energy Efficient and Robust Adversary Detection in Neural Networks",2021,"","","","",116,"2022-07-13 10:07:51","","10.1109/TCAD.2021.3091436","","",,,,,2,2.00,1,3,1,"Neural have achieved remarkable performance in computer vision, however, they are vulnerable to adversarial examples. Adversarial examples are inputs that have been carefully perturbed to fool classifier networks, while appearing unchanged to humans. Based on prior works on detecting adversaries, we propose a structured methodology of augmenting a deep neural network (DNN) with a detector subnetwork. We use adversarial noise sensitivity (ANS), a novel metric for measuring the adversarial gradient contribution of different intermediate layers of a network. Based on the ANS value, we append a detector to the most sensitive layer. In prior works, more complex detectors were added to a DNN, increasing the inference computational cost of the model. In contrast, our structured and strategic addition of a detector to a DNN reduces the complexity of the model while making the overall network adversarially resilient. Through comprehensive white-box and black-box experiments on MNIST, CIFAR-10, and CIFAR-100, we show that our method improves state-of-the-art detector robustness against adversarial examples. Furthermore, we validate the energy efficiency of our proposed adversarial detection methodology through an extensive energy analysis on various hardware scalable CMOS accelerator platforms. We also demonstrate the effects of quantization on our detector-appended networks.","",""
0,"Kian Long Tan, C. Lee, K. Anbananthen, K. Lim","RoBERTa-LSTM: A Hybrid Model for Sentiment Analysis With Transformer and Recurrent Neural Network",2022,"","","","",117,"2022-07-13 10:07:51","","10.1109/access.2022.3152828","","",,,,,0,0.00,0,4,1,"Due to the rapid development of technology, social media has become more and more common in human daily life. Social media is a platform for people to express their feelings, feedback, and opinions. To understand the sentiment context of the text, sentiment analysis plays the role to determine whether the sentiment of the text is positive, negative, neutral or any other personal feeling. Sentiment analysis is prominent from the perspective of business or politics where it highly impacts the strategic decision making. The challenges of sentiment analysis are attributable to the lexical diversity, imbalanced dataset and long-distance dependencies of the texts. In view of this, a data augmentation technique with GloVe word embedding is leveraged to synthesize more lexically diverse samples by similar word vector replacements. The data augmentation also focuses on the oversampling of the minority classes to mitigate the imbalanced dataset problems. Apart from that, the existing sentiment analysis mostly leverages sequence models to encode the long-distance dependencies. Nevertheless, the sequence models require a longer execution time as the processing is done sequentially. On the other hand, the Transformer models require less computation time with parallelized processing. To that end, this paper proposes a hybrid deep learning method that combines the strengths of sequence model and Transformer model while suppressing the limitations of sequence model. Specifically, the proposed model integrates Robustly optimized BERT approach and Long Short-Term Memory for sentiment analysis. The Robustly optimized BERT approach maps the words into a compact meaningful word embedding space while the Long Short-Term Memory model captures the long-distance contextual semantics effectively. The experimental results demonstrate that the proposed hybrid model outshines the state-of-the-art methods by achieving F1-scores of 93%, 91%, and 90% on IMDb dataset, Twitter US Airline Sentiment dataset, and Sentiment140 dataset, respectively.","",""
3,"Kaushalya Kumarasinghe, Mahonri Owen, Denise Taylor, N. Kasabov, C. Kit","FaNeuRobot: A Framework for Robot and Prosthetics Control Using the NeuCube Spiking Neural Network Architecture and Finite Automata Theory",2018,"","","","",118,"2022-07-13 10:07:51","","10.1109/ICRA.2018.8460197","","",,,,,3,0.75,1,5,4,"Limb amputation is a global problem. Prosthetic limbs can enhance the quality of life of amputees. To this end, anthropomorphic design and intuitive manipulation are two essential requirements. This paper presents a motor control framework for prosthetic control through Brain-Machine Interface (BMI) using Finite Automata Theory, and NeuCube Evolving Spiking Neural Network (SNN) architecture. Voluntary control of prosthetics requires decoding motor commands from the Central Nervous System of the amputee. Selection of the most suitable biomedical signal depends on many parameters such as level of amputation and muscle atrophy. Non-invasive BMI's have the possibility of supporting a wider range of amputees as it extracts the motor commands from the brain. In this paper, we present a proof of concept study on whether a cognitive computational model that is inspired by the motor control of the human body through muscle synergies combined with an anthropomorphic mechanical design, can result in accurate and robust prosthetic control through a noninvasive BMI. In future, learning of a complex Finite Automata that reflects complex upper limb motor behaviours will be investigated.","",""
27,"M. Sreenivasa, K. Ayusawa, Yoshihiko Nakamura","Modeling and Identification of a Realistic Spiking Neural Network and Musculoskeletal Model of the Human Arm, and an Application to the Stretch Reflex",2016,"","","","",119,"2022-07-13 10:07:51","","10.1109/TNSRE.2015.2478858","","",,,,,27,4.50,9,3,6,"This study develops a multi-level neuromuscular model consisting of topological pools of spiking motor, sensory and interneurons controlling a bi-muscular model of the human arm. The spiking output of motor neuron pools were used to drive muscle actions and skeletal movement via neuromuscular junctions. Feedback information from muscle spindles were relayed via monosynaptic excitatory and disynaptic inhibitory connections, to simulate spinal afferent pathways. Subject-specific model parameters were identified from human experiments by using inverse dynamics computations and optimization methods. The identified neuromuscular model was used to simulate the biceps stretch reflex and the results were compared to an independent dataset. The proposed model was able to track the recorded data and produce dynamically consistent neural spiking patterns, muscle forces and movement kinematics under varying conditions of external forces and co-contraction levels. This additional layer of detail in neuromuscular models has important relevance to the research communities of rehabilitation and clinical movement analysis by providing a mathematical approach to studying neuromuscular pathology.","",""
0,"Jiahui Yu, Hongwei Gao, Qing Gao, Dalin Zhou, Zhaojie Ju","Skeleton-based Human Activity Analysis Using Deep Neural Networks with Adaptive Representation Transformation",2021,"","","","",120,"2022-07-13 10:07:51","","10.1109/ICARM52023.2021.9536067","","",,,,,0,0.00,0,5,1,"Compared with RGB-D-based human action analysis, skeleton-based works reach higher robustness and better performance, which are widely applied in the real world. However, the diversity of action observation perspectives hinders the improvement of recognition accuracy. Most of the existing works solve this problem by increasing the amount of training data, which brings a huge computational cost and cannot improve the robustness of the models. This paper proposes an adaptive model to obtain high-performance representations to improve human action recognition accuracy. First, a skeleton representation transfer scheme is proposed to transform the input skeleton-based body model to the best perspective, in which all parameters can be adaptively learned. This is more robust and cost-effective than hand-crafted features. Next, a re-designed backbone is proposed to train the model with a small computational cost based on the 3D-CNN. In the training process, a data enhancement method is also introduced to enhance robustness. Finally, extensive experimental evaluations are conducted on two benchmarks. The results show that this deep model can effectively and adaptively obtain high-performance skeleton representation and its performance is better than other state-of-the-art methods.","",""
24,"Omar Costilla-Reyes, R. Vera-Rodríguez, Patricia J. Scully, K. Ozanyan","Analysis of Spatio-Temporal Representations for Robust Footstep Recognition with Deep Residual Neural Networks",2019,"","","","",121,"2022-07-13 10:07:51","","10.1109/TPAMI.2018.2799847","","",,,,,24,8.00,6,4,3,"Human footsteps can provide a unique behavioural pattern for robust biometric systems. We propose spatio-temporal footstep representations from floor-only sensor data in advanced computational models for automatic biometric verification. Our models deliver an artificial intelligence capable of effectively differentiating the fine-grained variability of footsteps between legitimate users (clients) and impostor users of the biometric system. The methodology is validated in the largest to date footstep database, containing nearly 20,000 footstep signals from more than 120 users. The database is organized by considering a large cohort of impostors and a small set of clients to verify the reliability of biometric systems. We provide experimental results in 3 critical data-driven security scenarios, according to the amount of footstep data made available for model training: at airports security checkpoints (smallest training set), workspace environments (medium training set) and home environments (largest training set). We report state-of-the-art footstep recognition rates with an optimal equal false acceptance and false rejection rate (equal error rate) of 0.7 percent an improvement ratio of 371 percent compared to previous state-of-the-art. We perform a feature analysis of deep residual neural networks showing effective clustering of client's footstep data and to provide insights of the feature learning process.","",""
0,"Hai Jiang, Jing Liu, H. Cheng","Short-Term TLE Uncertainty Estimation Using an Artificial Neural Network Model",2018,"","","","",122,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,3,4,"A growing number of space activities have created an orbital debris environment that poses increasing impact risks to existing space systems and human space flight. Accurate knowledge of orbit propagation errors of space debris is essential for many types of analyses, such as space surveillance network tasking, conjunction analysis etc. Unfortunately, for two-line elements (TLEs) this is not available. In this paper, a new short-term TLE uncertainty estimation method based on an artificial neural network model is proposed. Object properties, orbit type, space environment and prediction time-span are considered as the input of the network, the propagation errors in the direction of downrange, normal and conormald are as the output of the network. In order to assure the chosen orbit for training is not for an object using station keeping, only debris and R/B are used. The network’s efficiency is demonstrated with some objects with high ephemeris data. Overall, the method proves accurate, computationally fast, and robust, and is applicable to any object in the satellite catalogue, especially for those newly launched objects.","",""
2,"V. Bahrami, A. Kalhor, M. T. Masouleh","Dynamic model estimating and designing controller for the 2-DoF planar robot in interaction with cable-driven robot based on adaptive neural network",2021,"","","","",123,"2022-07-13 10:07:51","","10.3233/JIFS-210180","","",,,,,2,2.00,1,3,1,"This study intends to investigate the dynamic model estimation and the design of an adaptive neural network based controller for a passive planar robot, performing 2-DoF motion pattern which is in interaction with an actuated cable-driven robot. In fact, the main goal of applying this structure is to use a number of light cables to drive serial robot links and track the desired reference model by the robot’s end-effector. The under study system can be used as a rehabilitation setup which is helpful for those with arm disability. In this way, upon applying sliding mode error dynamics, it is necessary to determine a vector that contains the matrices related to the robot dynamics. However, finding these matrices requires the use of computational approaches such as Newton-Euler or Lagrange. In addition, since the purpose of this paper is to express comprehensive methods, so with increasing the number of links and degrees of freedom of the robot, finding the dynamics of the robot becomes more difficult. Therefore, the Adaptive Neural Network (ANN) with specific inputs has been used for estimation unknown matrices of the system and the controller design has been performed based on it. So, the main idea in using an adaptive controller is the fact there is no pre-knowledge for the dynamic modeling of the system since the human arm could have different dynamic properties. Hence, the controller is formed by an ANN and robust term. In this way, the adaptation laws of the parameters are extracted by Lyapunov approach, and as a result, as aforementioned, the asymptotic stability of the whole of the system is guaranteed. Simulation results certify the efficiency of the proposed method. Finally, using the Roots Mean Square Error (RMSE) criteria, it has been revealed that, in the presence of bounded disturbance with different amplitude, adding the robust term to the controller leads to improve the tracking error about 34% and 62%, respectively.","",""
2,"Hanxiao Xu, Da Xu, Naiqian Zhang, Yusen Zhang, Rui Gao","Protein-Protein Interaction Prediction Based on Spectral Radius and General Regression Neural Network.",2021,"","","","",124,"2022-07-13 10:07:51","","10.1021/acs.jproteome.0c00871","","",,,,,2,2.00,0,5,1,"Protein-protein interaction (PPI) not only plays a critical role in cell life activities, but also plays an important role in discovering the mechanism of biological activity, protein function, and disease states. Developing computational methods is of great significance for PPIs prediction since experimental methods are time-consuming and laborious. In this paper, we proposed a PPI prediction algorithm called GRNN-PPI only using the amino acid sequence information based on general regression neural network and two feature extraction methods. Specifically, we designed a new feature extraction method named Mutation Spectral Radius (MSR) to extract evolutionary information by the BLOSUM62 matrix. Meanwhile, we integrated another feature extraction method, autocorrelation description, which can completely extract information on physicochemical properties and protein sequences. The principal component analysis was applied to eliminate noise, and the general regression neural network was adopted as a classifier. The prediction accuracy of the yeast, human, and Helicobacter pylori1 (H. pylori1) data sets were 97.47%, 99.63%, and 99.97%, respectively. In addition, we also conducted experiments on two important PPI networks and six independent data sets. All results were significantly higher than some state-of-the-art methods used for comparison, showing that our method is feasible and robust.","",""
1,"V. Papageorgiou","Brain Tumor Detection Based on Features Extracted and Classified Using a Low-Complexity Neural Network",2021,"","","","",125,"2022-07-13 10:07:51","","10.18280/ts.380302","","",,,,,1,1.00,1,1,1,"Brain tumor detection or brain tumor classification is one of the most challenging problems in modern medicine, where patients suffering from benign or malignant brain tumors are usually characterized by low life expectancy making the necessity of a punctual and accurate diagnosis mandatory. However, even today, this kind of diagnosis is based on manual classification of magnetic resonance imaging (MRI), culminating in inaccurate conclusions especially when they derive from inexperienced doctors. Hence, trusted, automatic classification schemes are essential for the reduction of humans’ death rate due to this major chronic disease. In this article, we propose an automatic classification tool, using a computationally economic convolutional neural network (CNN), for the purposes of a binary problem concerning MRI images depicting the existence or the absence of brain tumors. The proposed model is based on a dataset containing real MRI images of both classes with nearly perfect validation-testing accuracy and low computational complexity, resulting a very fast and reliable training-validation process. During our analysis we compare the diagnostic capacity of three alternative loss functions, validating the appropriateness of cross entropy function, while underlining the capability of an alternative loss function named Jensen-Shannon divergence since our model accomplished nearly excellent testing accuracy, as with cross-entropy. The multiple validation tests applied, enhancing the robustness of the produced results, render this low-complexity CNN structure as an ideal and trustworthy medical aid for the classification of small datasets.","",""
0,"Deep LearningTensorFlow","Deep Convolutional Neural Network Based Approach For",2021,"","","","",126,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,1,1,"Deep Learning for Data AnalyticsDeep Learning for Chest RadiographsBrain Tumor MRI Image Segmentation Using Deep Learning TechniquesEfficient Processing of Deep Neural NetworksMachine Learning and Deep Learning in Real-Time ApplicationsState of the Art in Neural Networks and Their ApplicationsDatabase Systems for Advanced ApplicationsMedical Image Computing and Computer-Assisted Intervention – MICCAI 2016A Guide to Convolutional Neural Networks for Computer VisionEffect of Enhancement on Convolutional Neural Network Based Multi-view Object ClassificationAdvancing Cardiovascular MRI Acquisition Through Deep Convolutional Neural Network-Based LocalizationModelling and Analysis of Active Biopotential Signals in Healthcare, Volume 2Deep Learning-Based Approaches for Sentiment AnalysisIntroduction to Graph Neural NetworksPractical Convolutional Neural Networks2021 International Conference on Digital Futures and Transformative Technologies (ICoDT2)From Natural to Artificial IntelligenceNeural Networks and Deep LearningMultimodal Behavior Analysis in the WildMultimodal Scene UnderstandingAdvances in Machine Learning/Deep Learning-Based TechnologiesDeep LearningDeep Learning Applications with Practical Measured Results in Electronics IndustriesFundamentals of Brain Network AnalysisComputer Vision ECCV 2014 WorkshopsDeep Learning and Convolutional Neural Networks for Medical Image ComputingDeep Learning and Convolutional Neural Networks for Medical Imaging and Clinical InformaticsAdvanced Applied Deep LearningHands-On Convolutional Neural Networks with TensorflowComputer Vision – ECCV 2016A Convolutional Neural Network-based Approach to Personalized 3D Modeling of the Human Body and Its ClassificationDeep Learning in Computer VisionMulti-faceted Deep LearningDeep Learning in Computer Vision2019 IEEE National Aerospace and Electronics Conference (NAECON)Deep Learning Neural NetworksDeep Convolutional Neural Network Architecture for Effective Image AnalysisHybrid Intelligent SystemsNeural Networks: Tricks of the TradeThoracic Image Analysis This book reviews the state of the art in deep learning approaches to high-performance robust disease detection, robust and accurate organ segmentation in medical image computing (radiological and pathological imaging modalities), and the construction and mining of large-scale radiology databases. It particularly focuses on the application of convolutional neural networks, and on recurrent neural networks like LSTM, using numerous practical examples to complement the theory. The book’s chief features are as follows: It highlights how deep neural networks can be used to address new questions and protocols, and to tackle current challenges in medical image computing; presents a comprehensive review of the latest research and literature; and describes a range of different methods that employ deep learning for object or landmark detection tasks in 2D and 3D medical imaging. In addition, the book examines a broad selection of techniques for semantic segmentation using deep learning principles in medical imaging; introduces a novel approach to text and image deep embedding for a large-scale chest xray image database; and discusses how deep learning relational graphs can be used to organize a sizable collection of radiology findings from real clinical practice, allowing semantic similarity-based retrieval. The intended reader of this edited book is a professional engineer, scientist or a graduate student who is able to comprehend general concepts of image processing, computer vision and medical image analysis. They can apply computer science and mathematical principles into problem solving practices. It may be necessary to have a certain level of familiarity with a number of more advanced subjects: image formation and enhancement, image understanding, visual recognition in medical applications, statistical learning, deep neural networks, structured prediction and image segmentation.The eight-volume set comprising LNCS volumes 9905-9912 constitutes the refereed proceedings of the 14th European Conference on Computer Vision, ECCV 2016, held in Amsterdam, The Netherlands, in October 2016. The 415 revised papers presented were carefully reviewed and selected from 1480 submissions. The papers cover all aspects of computer vision and pattern recognition such as 3D computer vision; computational photography, sensing and display; face and gesture; low-level vision and image processing; motion and tracking; optimization methods; physicsbased vision, photometry and shape-from-X; recognition: detection, categorization, indexing, matching; segmentation, grouping and shape representation; statistical methods and learning; video: events, activities and surveillance; applications. They are organized in topical sections on detection, recognition and retrieval; scene understanding; optimization; image and video processing; learning; action activity and tracking; 3D; and 9 poster sessions.This book collects 14 articles from the Special Issue entitled “Deep Learning Applications with Practical Measured Results in Electronics Industries” of Electronics. Topics covered in this Issue include four main parts: (1) environmental information analyses and predictions, (2) unmanned aerial vehicle (UAV) and object tracking applications, (3) measurement and denoising techniques, and (4) recommendation systems and education systems. These authors used and improved deep learning techniques (e.g., ResNet (deep residual network), Faster-RCNN (faster regions with convolutional neural network), LSTM (long short term memory), ConvLSTM (convolutional LSTM), GAN (generative adversarial network), etc.) to analyze and denoise measured data in a variety of applications and services (e.g., wind speed prediction, air quality prediction, underground mine applications, neural audio caption, etc.). Several practical experiments were conducted, and the results indicate that the performance of the presented deep learning methods is improved compared with the performance of conventional machine learning methods.Deep Learning Neural Networks is the fastest growing field in machine learning. It serves as a powerful computational tool for solving prediction, decision, diagnosis, detection and decision problems based on a well-defined computational architecture. It has been successfully applied to a broad field of applications ranging from computer security, speech recognition, image and video recognition to industrial fault detection, medical diagnostics and finance. This comprehensive textbook is the first in the new emerging field. Numerous case studies are succinctly demonstrated in the text. It is intended for use as a one-semester graduate-level university text and as a textbook for research and development establishments in industry, medicine and financial research.In this thesis, we introduce an integrated method to build personalized full body 3D models of people given frontal and profile silhouette images. Several deep convolutional neural network (CNN) architectures have been designed and trained to accurately estimate the positions of a set of anthropometric set of ordered control points on the frontal and profile silhouette images. For the prediction of key points on the frontal silhouette image, the output from four different convolutional neural networks have been fused together to generate the final coordinates. A global CNN is first designed to predict those control points on all parts of the body. This has been reinforced with local deep CNN architectures focused on the prediction of control points on localized areas of the body to improve on the accuracy of predictions. Fusing the global and local predictions yielded an estimate of the coordinates of 56 control points on the frontal image and 26 control points on the side view image of a person. The controlled points are then regularized to reside on the silhouette of the frontal and profile images using a combination of Canny edge detector and shortest distance mapping. The set of regularized control points are then fed into a model-based 3D reconstruction algorithm [1] to yield the corresponding high-resolution 3D model of the person. A database of 800 models from the Caesar dataset were studied, of which 100 were used to train and the other 700 were used for testing and classification of 3D models. Our method achieves an accuracy of 99.7 % in prediction of control points and 3D reconstruction using those points. We also present a classification scheme to allocate a test surface to one of competing base surfaces. The classification is based on computing the error between salient points with identical anthropometric meaning that reside on a nested set of boundaries in the frontal and profile projection image spaces. The method can have a variety of applications ranging from medical imaging, to 3D modeling for recognition, virtual reality, generation of video games, 3D animation, etc.It is our belief that researchers and practitioners acquire, through experience and word-of-mouth, techniques and heuristics that help them successfully apply neural networks to di cult real world problems. Often these \tricks"" are theotically well motivated. Sometimes they are the result of trial and error. However, their most common link is that they are usually hidden in people’s heads or in the back pages of spaceconstrained conference papers. As a result newcomers to the eld waste much time wondering why their networks train so slowly and perform so poorly. This book is an outgrowth of a 1996 NIPS workshop called Tricks of the Trade whose goal was to begin the process of gathering and documenting these tricks. The interest that the workshop generated motivated us to expand our collection and compile it into this book. Although we have no doubt that there are many tricks we have missed, we hope that what we have included will prove to be useful, particularly to those who are relatively new t","",""
17,"Yi-Shan Lin, Wen-Chuan Lee, Z. B. Celik","What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors",2020,"","","","",127,"2022-07-13 10:07:51","","10.1145/3447548.3467213","","",,,,,17,8.50,6,3,2,"EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the input parts deemed important to arrive at a decision for a specific target. However, it remains challenging to quantify the correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for the systematic evaluation of explanations that an XAI method generates. We evaluate seven state-of-the-art model-free and model-specific post-hoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We found six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region. We made our code available at https://github.com/yslin013/evalxai.","",""
15,"Junfeng Jing, Amei Dong, Pengfei Li, Kaibing Zhang","Yarn-dyed fabric defect classification based on convolutional neural network",2017,"","","","",128,"2022-07-13 10:07:51","","10.1117/1.OE.56.9.093104","","",,,,,15,3.00,4,4,5,"Abstract. Considering that manual inspection of the yarn-dyed fabric can be time consuming and inefficient, we propose a yarn-dyed fabric defect classification method by using a convolutional neural network (CNN) based on a modified AlexNet. CNN shows powerful ability in performing feature extraction and fusion by simulating the learning mechanism of human brain. The local response normalization layers in AlexNet are replaced by the batch normalization layers, which can enhance both the computational efficiency and classification accuracy. In the training process of the network, the characteristics of the defect are extracted step by step and the essential features of the image can be obtained from the fusion of the edge details with several convolution operations. Then the max-pooling layers, the dropout layers, and the fully connected layers are employed in the classification model to reduce the computation cost and extract more precise features of the defective fabric. Finally, the results of the defect classification are predicted by the softmax function. The experimental results show promising performance with an acceptable average classification rate and strong robustness on yarn-dyed fabric defect classification.","",""
0,"J. Constantin, A. Bigand, I. Constantin","Pooling Spike Neural Network for Acceleration of Global Illumination Rendering",2017,"","","","",129,"2022-07-13 10:07:51","","10.1007/978-3-319-59153-7_18","","",,,,,0,0.00,0,3,5,"","",""
0,"Sanjit Bhat","Towards E cient Methods for Training Robust Deep Neural Networks",2019,"","","","",130,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,1,3,"In recent years, it has been shown that neural networks are vulnerable to adversarial examples, i.e., specially crafted inputs that look visually similar to humans yet cause machine learning models to make incorrect predictions. A lot of research has been focused on training robust models models immune to adversarial examples. One such method is Adversarial Training, in which the model continuously trains on adversarially perturbed inputs. However, since these inputs require signi cant computation time to create, Adversarial Training is often much slower than vanilla training. In this work, we explore two approaches to increase the e ciency of Adversarial Training. First, we study whether faster yet less accurate methods for generating adversarially perturbed inputs su ce to train a robust model. Second, we devise a method for asynchronous parallel Adversarial Training and analyze a phenomenon of independent interest that arises staleness. Taken together, these two techniques enable comparable robustness on the MNIST dataset to prior art with a 26× reduction in training time from 4 hours to just 9 minutes.","",""
27,"Zeke Xie, Fengxiang He, Shaopeng Fu, Issei Sato, D. Tao, M. Sugiyama","Artificial Neural Variability for Deep Learning: On Overfitting, Noise Memorization, and Catastrophic Forgetting",2020,"","","","",131,"2022-07-13 10:07:51","","10.1162/neco_a_01403","","",,,,,27,13.50,5,6,2,"Deep learning is often criticized by two serious issues that rarely exist in natural nervous systems: overfitting and catastrophic forgetting. It can even memorize randomly labeled data, which has little knowledge behind the instance-label pairs. When a deep network continually learns over time by accommodating new tasks, it usually quickly overwrites the knowledge learned from previous tasks. Referred to as the neural variability, it is well known in neuroscience that human brain reactions exhibit substantial variability even in response to the same stimulus. This mechanism balances accuracy and plasticity/flexibility in the motor learning of natural nervous systems. Thus, it motivates us to design a similar mechanism, named artificial neural variability (ANV), that helps artificial neural networks learn some advantages from “natural” neural networks. We rigorously prove that ANV plays as an implicit regularizer of the mutual information between the training data and the learned model. This result theoretically guarantees ANV a strictly improved generalizability, robustness to label noise, and robustness to catastrophic forgetting. We then devise a neural variable risk minimization (NVRM) framework and neural variable optimizers to achieve ANV for conventional network architectures in practice. The empirical studies demonstrate that NVRM can effectively relieve overfitting, label noise memorization, and catastrophic forgetting at negligible costs.","",""
12,"Junfeng Jing, Amei Dong, Pengfei Li","Yarn-dyed fabric defect classification based on convolutional neural network",2017,"","","","",132,"2022-07-13 10:07:51","","10.1117/12.2281978","","",,,,,12,2.40,4,3,5,"Considering that the manual inspection of the yarn-dyed fabric can be time consuming and less efficient, a convolutional neural network (CNN) solution based on the modified AlexNet structure for the classification of the yarn-dyed fabric defect is proposed. CNN has powerful ability of feature extraction and feature fusion which can simulate the learning mechanism of the human brain. In order to enhance computational efficiency and detection accuracy, the local response normalization (LRN) layers in AlexNet are replaced by the batch normalization (BN) layers. In the process of the network training, through several convolution operations, the characteristics of the image are extracted step by step, and the essential features of the image can be obtained from the edge features. And the max pooling layers, the dropout layers, the fully connected layers are also employed in the classification model to reduce the computation cost and acquire more precise features of fabric defect. Finally, the results of the defect classification are predicted by the softmax function. The experimental results show the capability of defect classification via the modified Alexnet model and indicate its robustness.","",""
8,"Lorena Guachi, R. Guachi, F. Bini, F. Marinozzi","Automatic Colorectal Segmentation with Convolutional Neural Network",2018,"","","","",133,"2022-07-13 10:07:51","","10.14733/CADCONFP.2018.312-316","","",,,,,8,2.00,2,4,4,"Introduction: In the recent years, modern medicine uses image processing technique, such as image segmentation in Computer Aided Diagnosis System (CAD) in order to reduce the dependence of diagnosis by doctors’ knowledge and experience, as well as to locate the prior tissue lesions timely and effectively [6]. Medical image segmentation uses several imaging modalities (MRI, Computed Tomography (CT), Positron Emission Tomography (PET), X-RAY, Ultrasound). However, it is a challenge yet, due to added noise, artifacts, limitations, and unclear edges [8]. In this way, colon tissues segmentation in human abdominal CT images is the base of analysis and identification of cancer nidus, providing powerful information in a CAD, such as early polyps detection, which can reduce the incidence of colon cancer [3],[6],[14]. Colon segmentation techniques can also be used in colorectal tissues simulations to make preoperative plans and simulations of surgery [4]. Some colon segmentation algorithms are introduced in literature, each one having its own model, computational complexity, and overall quality. Such as Local region based active contours [6], which is based on local statistics of tissue of interest and background, instead of global statistics. In [15], an isotropic volume reconstructed from the CT images is used to extract a thick region encompassing the entire colon, where mean curvature, dimensionless ratio sphericity and minimum polyp size are used as parameters to filter anomalies and reduce false positives. Classifications of multispectral colorectal cancer tissues [5], classify tissues samples using convolutional neural network (CNN) and uses active contours technique to extract colorectal regions corresponding to pathological tissues. Although some works presented in literature have demonstrated how CNN provides effective results to analyze colon images, those works are based on the segmentation image regions containing pathological colorectal tissues [5],[7], and glandular colon structure [10]. On the contrary, the analysis of colon tissues as pre-processing task for applications, as tissues simulations, is our motivation to challenge the use of pixel-wise segmentation with CNN. In order to overcome the problem of misclassifying colon tissue pixels, in this paper, we propose a method for automatic colon tissues segmentation based on spatial features learned with CNN. The proposed method has been compared to three state-of-the-art methods. Preliminary experimental results demonstrate the proposed method achieves a higher robustness in terms of sensitivity and similarity, and reduces the number of misclassified colon tissue pixels.","",""
29,"C. Corbane, V. Syrris, F. Sabo, P. Politis, M. Melchiorri, M. Pesaresi, P. Soille, T. Kemper","Convolutional Neural Networks for Global Human Settlements Mapping from Sentinel-2 Satellite Imagery",2020,"","","","",134,"2022-07-13 10:07:51","","10.1007/S00521-020-05449-7","","",,,,,29,14.50,4,8,2,"","",""
0,"","Arrhythmia detection and classification using convolutional neural network",2021,"","","","",135,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,0,1,"Healthcare and Life Sciences are among severely researched domains. With the introduction of computing paradigms and possibility of leveraging computational techniques have opened new research avenues. Heart is one among the most researched organs and the reason being trivial. With the advent of computational paradigms researchers explored ways to measure the electrical activity of the heartbeat which is known as electrocardiogram (ECG). Since then ECG has become one of the most important as well as primary tests to diagnose any irregularities in the functioning of the heart. The availability of the ECG data and possibility of employing deep learning models and their robustness has made researchers venture into leveraging them to elevate the accuracy of the ECG Analysis. To date there exists no end-to-end evaluation model based on deep learning techniques. In our present work, we identify different classes of cardiac rhythm by leveraging single-lead ECG. Our results show a significant improvement in ROC (approx. 0.97). Furthermore, our results demonstrate that we can classify a wide spectrum of arrhythmias using Deep Learning Techniques which are comparable to that of cardiologists. This approach can be used to minimize the component of human misdiagnose and help improve the quality of cardiac care.","",""
0,"Srungeer Simha, J. Goudswaard, P. Devarakota, P. Somawanshi","Neural Network Assisted Seismic Velocity Editing",2019,"","","","",136,"2022-07-13 10:07:51","","10.2118/197919-ms","","",,,,,0,0.00,0,4,3,"  Normal Move-Out (NMO) velocity pick editing is the segregation of good and bad picks from an unsupervised auto-picking algorithm. As not all these picks are correct, manual velocity editing is required. This is time consuming, repetitive and typically requires a seismic expert for days to weeks. Automating it would require an algorithm that mimics the domain knowledge and expertise of a seismic processor; a deterministic approach would therefore likely fail. Alternatively, we propose a machine learning algorithm to identify valid time-velocity picks.  The proposed approach is a supervised classification approach which utilizes human interpreted velocity picks (1-5% of all picks) as training data. The algorithm learns to recognize the features of a valid velocity pick from metadata such as semblance energy, depth, areal location etc. and utilizes said understanding to segregate valid picks from invalid ones (multiples etc.) amongst the remaining velocity picks. The algorithm has been trained using synthetic NMO picks created by finite-difference forward modelling CMP data, including multiples, in the Marmousi model and auto-picking the move-out. The ground-truth NMO picks were created directly from the velocity model.  The trained classification neural network shows a very high > 97% accuracy on segregation of valid and invalid NMO velocity picks based on a 5% input data set. Further reduction of the training data set to 1% of velocity picks reduces test accuracy only by an additional 2 percentage points. Training and execution time of the neural network on a dataset of ~ 40000 velocity picks are also extremely fast (< 5 mins). Initial results on RMO picks also show a very similar performance characteristic.  The metadata for all valid picks spans a multi-dimensional feature space, from which the neural network constructs a non-linear selection criterion. A human can either manually QC each pick or perform attribute-based selection using only lower dimensional linear selection criteria. The robustness and speed of the neural network outperforms the manual editing while also reducing cycle time; the resulting velocity models will be superior, leading to improved signal processing and imaging results further in the processing sequence.  Automating velocity picking and editing has been a research objective for many years now, but only since the availability of modern computation and optimization algorithms can we properly deploy this to augment the high-quality modern velocity picking software and significantly decrease turn-around time by automating the picking and QC process.","",""
6,"Nikolaos Livathinos, Cesar Berrospi, M. Lysak, Viktor Kuropiatnyk, Ahmed Nassar, A. Carvalho, Michele Dolfi, Christoph Auer, K. Dinkla, P. Staar","Robust PDF Document Conversion Using Recurrent Neural Networks",2021,"","","","",137,"2022-07-13 10:07:51","","","","",,,,,6,6.00,1,10,1,"The number of published PDF documents in both the academic and commercial world has increased exponentially in recent decades. There is a growing need to make their rich content discoverable to information retrieval tools. Achieving high-quality semantic searches demands that a document's structural components such as title, section headers, paragraphs, (nested) lists, tables and figures (including their captions) are properly identified. Unfortunately, the PDF format is known to not conserve such structural information because it simply represents a document as a stream of low-level printing commands, in which one or more characters are placed in a bounding box with a particular styling. In this paper, we present a novel approach to document structure recovery in PDF using recurrent neural networks to process the low-level PDF data representation directly, instead of relying on a visual re-interpretation of the rendered PDF page, as has been proposed in previous literature. We demonstrate how a sequence of PDF printing commands can be used as input into a neural network and how the network can learn to classify each printing command according to its structural function in the page. This approach has three advantages: First, it can distinguish among more fine-grained labels (typically 10-20 labels as opposed to 1-5 with visual methods), which results in a more accurate and detailed document structure resolution. Second, it can take into account the text flow across pages more naturally compared to visual methods because it can concatenate the printing commands of sequential pages. Last, our proposed method needs less memory and it is computationally less expensive than visual methods. This allows us to deploy such models in production environments at a much lower cost. Through extensive architectural search in combination with advanced feature engineering, we were able to implement a model that yields a weighted average F-1 score of 97% across 17 distinct structural labels. The best model we achieved is currently served in production environments on our Corpus Conversion Service (CCS), which was presented at KDD18. This model enhances the capabilities of CCS significantly, as it eliminates the need for human annotated label ground-truth for every unseen document layout. This proved particularly useful when applied to a huge corpus of PDF articles related to COVID-19.","",""
8,"Tanmay Vilas Samak, Chinmay Vilas Samak, Sivanathan Kandhasamy","Robust Behavioral Cloning for Autonomous Vehicles using End-to-End Imitation Learning",2020,"","","","",138,"2022-07-13 10:07:51","","10.4271/12-04-03-0023","","",,,,,8,4.00,3,3,2,"In this work, we present a robust pipeline for cloning driving behavior of a human using end-to-end imitation learning. The proposed pipeline was employed to train and deploy three distinct driving behavior models onto a simulated vehicle. The training phase comprised of data collection, balancing, augmentation, preprocessing and training a neural network, following which, the trained model was deployed onto the ego vehicle to predict steering commands based on the feed from an onboard camera. A novel coupled control law was formulated to generate longitudinal control commands on-the-go based on the predicted steering angle and other parameters such as actual speed of the ego vehicle and the prescribed constraints for speed and steering. We analyzed computational efficiency of the pipeline and evaluated robustness of the trained models through exhaustive experimentation. Even a relatively shallow convolutional neural network model was able to learn key driving behaviors from sparsely labelled datasets and was tolerant to environmental variations during deployment of the said driving behaviors.","",""
0,"Byeongkeun Kang","Learning Robust Representations in Random Forest and Deep Neural Networks for Semantic Segmentation",2018,"","","","",139,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,1,4,"Author(s): Kang, Byeongkeun | Advisor(s): Nguyen, Truong Q. | Abstract: As semantic segmentation provides the class and the location of objects in a captured scene, it has been one of the core algorithms in many computer vision applications including autonomous driving, robot navigation, surveillance camera system, and human-machine interaction. Most of these applications demand high accuracy, robustness, and efficiency to understand a captured scene accurately in a timely manner in order to avoid accidents, to provide a meaningful warning, and to communicate naturally. We address this needs by using two popular approaches: random forest and deep neural network.We start by introducing a cascaded random forest for binary class segmentation. The framework first detects regions of interest and then segments foreground in the regions. Since the detection reduces the regions for the segmentation forest, the cascaded scheme improves efficiency and accuracy. We then explore learning more robust representations in a random forest. Since predetermined constraints in typical feature extractors restrict learning and extracting optimal features, we present a random forest framework that learns the weights, shapes, and sparsities of feature extractors. We propose an unconstrained filter, an iterative optimization algorithm for learning, a processing pipeline for inference. Experimental results demonstrate that the proposed method achieves real-time semantic segmentation using limited computational and memory resources.Moreover, we present a method to learn/extract depth-adaptive features in a deep neural network. It accomplishes a step toward depth-invariant feature learning and extracting. Since typical neural networks receive inputs from predetermined locations regardless of the distance from the camera, it is challenging to generalize the features of objects at various distances. Hence, we propose the depth-adaptive multiscale convolution layer consisting of the adaptive perception neuron and the in-layer multiscale neuron. The adaptive neuron is to adjust the receptive field at each spatial location using the depth information. The multiscale neuron is to learn features at multiple scales. Experimental results show that the proposed method outperforms the state-of-the-art methods without any additional layers or pre/post-processing.Lastly, we present applications of segmentation including sign language fingerspelling recognition and hand articulation tracking. We also present a potential data augmentation method using generative adversarial networks.","",""
27,"Sheng Zhang, W. Zheng","Recursive Adaptive Sparse Exponential Functional Link Neural Network for Nonlinear AEC in Impulsive Noise Environment",2018,"","","","",140,"2022-07-13 10:07:51","","10.1109/TNNLS.2017.2761259","","",,,,,27,6.75,14,2,4,"Recently, an adaptive exponential trigonometric functional link neural network (AETFLN) architecture has been introduced to enhance the nonlinear processing capability of the trigonometric functional link neural network (TFLN). However, it suffers from slow convergence speed, heavy computational burden, and poor robustness to noise in nonlinear acoustic echo cancellation, especially in the double-talk scenario. To reduce its computational complexity and improve its robustness against impulsive noise, this paper develops a recursive adaptive sparse exponential TFLN (RASETFLN). Based on sparse representations of functional links, the robust proportionate adaptive algorithm is deduced from the robust cost function over the RASETFLN in impulsive noise environments. Theoretical analysis shows that the proposed RASETFLN is stable under certain conditions. Finally, computer simulations illustrate that the proposed RASETFLN achieves much improved performance over the AETFLN in several nonlinear scenarios in terms of convergence rate, steady-state error, and robustness against noise.","",""
0,"G. I. Parisi","Multimodal Learning of Actions with Deep Neural Network Self-Organization",2017,"","","","",141,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,1,5,"Perceiving the actions of other people is one of the most important social skills of human beings. We are able to reliably discern a variety of socially relevant information from people’s body motion such as intentions, identity, gender, and affective states. This ability is supported by highly developed visual skills and the integration of additional modalities that in concert contribute to providing a robust perceptual experience. Multimodal integration is a fundamental feature of the brain that together with widely studied biological mechanisms for action perception has served as inspiration for the development of artificial systems. However, computational mechanisms for processing and integrating knowledge reliably from multiple perceptual modalities are still to be fully investigated.  The goal of this thesis is to study and develop artificial learning architectures for action perception. In light of a wide understanding of the brain areas and underlying neural mechanisms for processing biological motion patterns, we propose a series of neural network models for learning multimodal action representations. Consistent with neurophysiological studies evidencing a hierarchy of cortical layers driven by the distribution of the input, we demonstrate how computational models of input-driven self-organization can account for the learning of action features with increasing complexity of representation. For this purpose, we introduce a novel model of recurrent self-organization for learning action features with increasingly large spatiotemporal receptive fields. Visual representations obtained through unsupervised learning are incrementally associated to symbolic action labels for the purpose of action classification.  From a multimodal perspective, we propose a model in which multimodal action representations can develop from neural network organization in terms of associative connectivity patterns between unimodal representations. We report a set of experiments showing that deep self-organizing hierarchies allow to learn statistically significant features of actions, with multimodal representations emerging from co-occurring audiovisual stimuli. We evaluated our neural network architectures on the tasks of human action recognition, body motion assessment, and the detection of abnormal behavior. Finally, we conducted two robot experiments that provide quantitative evidence for the advantages of multimodal integration for triggering sensory-driven motor behavior. The first scenario consists of an assistive task for the detection of falls, whereas in the second experiment we propose audiovisual integration in an interactive reinforcement learning scenario. Together, our results demonstrate that deep neural self-organization can account for robust action perception, yielding state-of-the-art performance also in the presence of sensory uncertainty and conflict.  The research presented in this thesis comprises interdisciplinary aspects of action perception and multimodal integration for the development of efficient neurocognitive architectures. While the brain mechanisms for multimodal perception are still to be fully understood, the proposed neural network architectures may be seen as a basis for modeling higher-level cognitive functions.","",""
38,"G. Lai, Zhi Liu, Yun Zhang, C. L. Chen","Adaptive Position/Attitude Tracking Control of Aerial Robot With Unknown Inertial Matrix Based on a New Robust Neural Identifier",2016,"","","","",142,"2022-07-13 10:07:51","","10.1109/TNNLS.2015.2406812","","",,,,,38,6.33,10,4,6,"This paper presents a novel adaptive controller for controlling an autonomous helicopter with unknown inertial matrix to asymptotically track the desired trajectory. To identify the unknown inertial matrix included in the attitude dynamic model, this paper proposes a new structural identifier that differs from those previously proposed in that it additionally contains a neural networks (NNs) mechanism and a robust adaptive mechanism, respectively. Using the NNs to compensate the unknown aerodynamic forces online and the robust adaptive mechanism to cancel the combination of the overlarge NNs compensation error and the external disturbances, the new robust neural identifier exhibits a better identification performance in the complex flight environment. Moreover, an optimized algorithm is included in the NNs mechanism to alleviate the burdensome online computation. By the strict Lyapunov argument, the asymptotic convergence of the inertial matrix identification error, position tracking error, and attitude tracking error to arbitrarily small neighborhood of the origin is proved. The simulation and implementation results are provided to evaluate the performance of the proposed controller.","",""
39,"Sitong Wu, Z. Gao, Zhi Liu, Jianwen Luo, Heye Zhang, S. Li","Direct Reconstruction of Ultrasound Elastography Using an End-to-End Deep Neural Network",2018,"","","","",143,"2022-07-13 10:07:51","","10.1007/978-3-030-00928-1_43","","",,,,,39,9.75,7,6,4,"","",""
207,"I. Mcloughlin, Haomin Zhang, Zhipeng Xie, Yan Song, Wei Xiao","Robust Sound Event Classification Using Deep Neural Networks",2015,"","","","",144,"2022-07-13 10:07:51","","10.1109/TASLP.2015.2389618","","",,,,,207,29.57,41,5,7,"The automatic recognition of sound events by computers is an important aspect of emerging applications such as automated surveillance, machine hearing and auditory scene understanding. Recent advances in machine learning, as well as in computational models of the human auditory system, have contributed to advances in this increasingly popular research field. Robust sound event classification, the ability to recognise sounds under real-world noisy conditions, is an especially challenging task. Classification methods translated from the speech recognition domain, using features such as mel-frequency cepstral coefficients, have been shown to perform reasonably well for the sound event classification task, although spectrogram-based or auditory image analysis techniques reportedly achieve superior performance in noise. This paper outlines a sound event classification framework that compares auditory image front end features with spectrogram image-based front end features, using support vector machine and deep neural network classifiers. Performance is evaluated on a standard robust classification task in different levels of corrupting noise, and with several system enhancements, and shown to compare very well with current state-of-the-art classification techniques.","",""
37,"Daniel Zoran, Mike Chrzanowski, Po-Sen Huang, Sven Gowal, A. Mott, Pushmeet Kohli","Towards Robust Image Classification Using Sequential Attention Models",2019,"","","","",145,"2022-07-13 10:07:51","","10.1109/cvpr42600.2020.00950","","",,,,,37,12.33,6,6,3,"In this paper we propose to augment a modern neural-network architecture with an attention model inspired by human perception. Specifically, we adversarially train and analyze a neural model incorporating a human inspired, visual attention component that is guided by a recurrent top-down sequential process. Our experimental evaluation uncovers several notable findings about the robustness and behavior of this new model. First, introducing attention to the model significantly improves adversarial robustness resulting in state-of-the-art ImageNet accuracies under a wide range of random targeted attack strengths. Second, we show that by varying the number of attention steps (glances/fixations) for which the model is unrolled, we are able to make its defense capabilities stronger, even in light of stronger attacks --- resulting in a ``computational race'' between the attacker and the defender. Finally, we show that some of the adversarial examples generated by attacking our model are quite different from conventional adversarial examples --- they contain global, salient and \emph{spatially coherent} structures coming from the target class that would be recognizable even to a human, and work by distracting the attention of the model away from the main object in the original image.","",""
2,"Andreas Huemer, M. Gongora, D. Elizondo","A robust reinforcement based self constructing neural network",2010,"","","","",146,"2022-07-13 10:07:51","","10.1109/IJCNN.2010.5596762","","",,,,,2,0.17,1,3,12,"Usually, many high-skilled human resources are required to create sophisticated control systems. Automatic generation of control systems can overcome these requirements. Because of their versatility and flexibility neural networks gained an important role for this task. While evolutionary methods have been relatively successful in generating neural networks, they have some limitations, in addition to being computationally expensive, because they rely on adapting populations instead of individuals. Reinforcement methods on the other hand can improve and adapt the behaviour of an individual; the reinforcement methods that are presented in this paper can grow a neural network during operation. We show that neural networks can be created for various domains without changing any parameters. Additionally, our neural network can learn the action selection policy and the value function locally within the neurons. These features make our neural network highly flexible and distinguish it from other reinforcement based constructive neural networks.","",""
2,"Jyun-Yu Jiang, Yichao Zhou, Xiusi Chen, Yan-Ru Jhou, Liqi Zhao, Sabrina Liu, Po-Chun Yang, Jule Ahmar, Wei Wang","COVID-19 Surveiller: toward a robust and effective pandemic surveillance system based on social media mining",2021,"","","","",147,"2022-07-13 10:07:51","","10.1098/rsta.2021.0125","","",,,,,2,2.00,0,9,1,"The outbreak of the novel coronavirus, COVID-19, has become one of the most severe pandemics in human history. In this paper, we propose to leverage social media users as social sensors to simultaneously predict the pandemic trends and suggest potential risk factors for public health experts to understand spread situations and recommend proper interventions. More precisely, we develop novel deep learning models to recognize important entities and their relations over time, thereby establishing dynamic heterogeneous graphs to describe the observations of social media users. A dynamic graph neural network model can then forecast the trends (e.g. newly diagnosed cases and death rates) and identify high-risk events from social media. Based on the proposed computational method, we also develop a web-based system for domain experts without any computer science background to easily interact with. We conduct extensive experiments on large-scale datasets of COVID-19 related tweets provided by Twitter, which show that our method can precisely predict the new cases and death rates. We also demonstrate the robustness of our web-based pandemic surveillance system and its ability to retrieve essential knowledge and derive accurate predictions across a variety of circumstances. Our system is also available at http://scaiweb.cs.ucla.edu/covidsurveiller/. This article is part of the theme issue ‘Data science approachs to infectious disease surveillance’.","",""
1,"M. S. Eltokhy","Robust multimodal biometric authentication algorithms using fingerprint, iris and voice features fusion",2021,"","","","",148,"2022-07-13 10:07:51","","10.3233/jifs-200425","","",,,,,1,1.00,1,1,1,"Development of a robust triple multimodal biometric approach for human authentication using fingerprint, iris and voice biometric is the main objective of this manuscript. Accordingly, three essential algorithms for biometric authentication are presented. The extracted features from these multimodals are combined via feature fusion center (FFC) and feature scores. These features are trained through artificial neural network (ANN) and support vector machine (SVM) classifiers. The first algorithm depends on boundary energy method (BEM) extracted features from fingerprint, normalized combinational features from iris and dimensionality reduction methods (DRM) from voice using sum/average FFC. The second proposed algorithm uses extracted features from zoning method of fingerprint, SIFT of iris and higher order statistics (HOS) of voice signals. The third proposed algorithm consists of extracted features from zoning method for fingerprint, SIFT from iris and DRM from voice signals. Classification accuracy of implemented algorithms is estimated. Comparison between proposed algorithms is introduced in terms of equal error rate (EER) and ROC curves. The experimental results confirm superiority of second proposed algorithm which achieves a classification rate of 100% using SVM classifier and sum FFC. From computational point of view, the first algorithm consumes the lowest time using SVM classifier. On other hand, the lowest EER is achieved by first proposed algorithm for extracted features from Karhunen-Loeve transform (KLT) method of DRM. Additionally, the lowest ROC curves are accomplished respectively for extracted features from multidimensional scaling (MDS), generated ARMA synthesis and Isomap features. Their accuracy is improved with SVM. Also, the sum FFC introduces efficient results compared to average FFC. These algorithms have the advantages of robustness and the strength of selecting unimodal, double and triple biometric authentication. The obtained results accomplish a remarkable accuracy for authentication and security within multi practical applications.","",""
1,"A. Duggento, A. Conti, M. Guerrisi, N. Toschi","A novel multi-branch architecture for state of the art robust detection of pathological phonocardiograms",2021,"","","","",149,"2022-07-13 10:07:51","","10.1098/rsta.2020.0264","","",,,,,1,1.00,0,4,1,"Heart auscultation is an inexpensive and fundamental technique to effectively diagnose cardiovascular disease. However, due to relatively high human error rates even when auscultation is performed by an experienced physician, and due to the not universal availability of qualified personnel, e.g. in developing countries, many efforts are made worldwide to propose computational tools for detecting abnormalities in heart sounds. The large heterogeneity of achievable data quality and devices, the variety of possible heart pathologies, and a generally poor signal-to-noise ratio make this problem very challenging. We present an accurate classification strategy for diagnosing heart sounds based on (1) automatic heart phase segmentation, (2) state-of-the art filters drawn from the field of speech synthesis (mel-frequency cepstral representation) and (3) an ad hoc multi-branch, multi-instance artificial neural network based on convolutional layers and fully connected neuronal ensembles which separately learns from each heart phase hence implicitly leveraging their different physiological significance. We demonstrate that it is possible to train our architecture to reach very high performances, e.g. an area under the curve of 0.87 or a sensitivity of 0.97. Our machine-learning-based tool could be employed for heartsound classification, especially as a screening tool in a variety of situations including telemedicine applications. This article is part of the theme issue ‘Advanced computation in cardiovascular physiology: new challenges and opportunities’.","",""
8,"Wenjin Zhang, Jiacun Wang, Fangping Lan","Dynamic hand gesture recognition based on short-term sampling neural networks",2021,"","","","",150,"2022-07-13 10:07:51","","10.1109/JAS.2020.1003465","","",,,,,8,8.00,3,3,1,"Hand gestures are a natural way for human-robot interaction. Vision based dynamic hand gesture recognition has become a hot research topic due to its various applications. This paper presents a novel deep learning network for hand gesture recognition. The network integrates several well-proved modules together to learn both short-term and long-term features from video inputs and meanwhile avoid intensive computation. To learn short-term features, each video input is segmented into a fixed number of frame groups. A frame is randomly selected from each group and represented as an RGB image as well as an optical flow snapshot. These two entities are fused and fed into a convolutional neural network ( ConvNet ) for feature extraction. The ConvNets for all groups share parameters. To learn long-term features, outputs from all ConvNets are fed into a long short-term memory ( LSTM ) network, by which a final classification result is predicted. The new model has been tested with two popular hand gesture datasets, namely the Jester dataset and Nvidia dataset. Comparing with other models, our model produced very competitive results. The robustness of the new model has also been proved with an augmented dataset with enhanced diversity of hand gestures.","",""
0,"Justin A. Goodwin, Olivia M. Brown, Victoria Helus","Fast Training of Deep Neural Networks Robust to Adversarial Perturbations",2020,"","","","",151,"2022-07-13 10:07:51","","10.1109/HPEC43674.2020.9286256","","",,,,,0,0.00,0,3,2,"Despite their promising performance, deep neural networks have shown sensitivities to perturbations of their inputs (e.g., adversarial examples) and their learned feature representations are often difficult to interpret, raising concerns about their true capability and trustworthiness. Recent work in adversarial training, a form of robust optimization in which the model is optimized against adversarial examples, demonstrates the ability to improve performance sensitivities to perturbations and yield feature representations that are more interpretable. Adversarial training, however, comes with an increased computational cost over that of standard (i.e., nonrobust) training, rendering it impractical for use in large-scale problems. Recent work suggests that a fast approximation to adversarial training shows promise for reducing training time and maintaining robustness in the presence of perturbations bounded by the infinity norm. In this work, we demonstrate that this approach extends to the Euclidean norm and preserves the human-aligned feature representations that are common for robust models. Additionally, we show that using a distributed training scheme can further reduce the time to train robust deep networks. Fast adversarial training is a promising approach that will provide increased security and explainability in machine learning applications for which robust optimization was previously thought to be impractical.","",""
15,"J. Hopfield","Understanding Emergent Dynamics: Using a Collective Activity Coordinate of a Neural Network to Recognize Time-Varying Patterns",2015,"","","","",152,"2022-07-13 10:07:51","","10.1162/NECO_a_00768","","",,,,,15,2.14,15,1,7,"Abstract In higher animals, complex and robust behaviors are produced by the microscopic details of large structured ensembles of neurons. I describe how the emergent computational dynamics of a biologically based neural network generates a robust natural solution to the problem of categorizing time-varying stimulus patterns such as spoken words or animal stereotypical behaviors. The recognition of these patterns is made difficult by their substantial variation in cadence and duration. The neural circuit behaviors used are similar to those associated with brain neural integrators. In the larger context described here, this kind of circuit becomes a building block of an entirely different computational algorithm for solving complex problems. While the network behavior is simulated in detail, a collective view is essential to understanding the results. A closed equation of motion for the collective variable describes an algorithm that quantitatively accounts for many aspects of the emergent network computation. The feedback connections and ongoing activity in the network shape the collective dynamics onto a reduced dimensionality manifold of activity space, which defines the algorithm and computation actually performed. The external inputs are weak and are not the dominant drivers of network activity.","",""
2,"Jun-Ho Seo, J. Park, Jung-Hyun Lee, Kyung-Joong Kim","Designing Robust Robotic Car Controllers based on Artificial Neural Network",2010,"","","","",153,"2022-07-13 10:07:51","","","","",,,,,2,0.17,1,4,12,"There are several factors to be considered in the design of controllers for simulated car racing competition: Speed, safety and adaptation. In this paper, we propose an experience-based design method for safe driving which improves its driving skills from failures. This imitates human's learning mechanisms which exploit a huge number of failures to get stable performance. In the context of simulated car racing competition, the failures mean that car has accidents or is stuck. Our system provides with methods to design controllers for safe driving by analyzing, memorizing, predicting failures from its experience. At first, our analysis shows that the accidents occur frequently when the car enters a corner with high speed. Based on this analysis, we decide to recognize the corner before entering it and reduce speed for safe turn. Secondly, our system remembers the position where the car fails to drive successfully. In the next lap, the car reduces its speed at the position by recalling memory. Although this is a useful approach to minimize failures incrementally but there is no guarantee that the failure will occur in the same place. Finally, we use a computational intelligence method (neural network) to predict the failures from sensory information. Experimental results show that the combination of the three strategies can improve the design of controllers for simulated car racing.","",""
8,"Yang Lou, Yaodong He, Lin Wang, Guanrong Chen","Predicting Network Controllability Robustness: A Convolutional Neural Network Approach",2019,"","","","",154,"2022-07-13 10:07:51","","10.1109/TCYB.2020.3013251","","",,,,,8,2.67,2,4,3,"Network controllability measures how well a networked system can be controlled to a target state, and its robustness reflects how well the system can maintain the controllability against malicious attacks by means of node removals or edge removals. The measure of network controllability is quantified by the number of external control inputs needed to recover or to retain the controllability after the occurrence of an unexpected attack. The measure of the network controllability robustness, on the other hand, is quantified by a sequence of values that record the remaining controllability of the network after a sequence of attacks. Traditionally, the controllability robustness is determined by attack simulations, which is computationally time consuming. In this article, a method to predict the controllability robustness based on machine learning using a convolutional neural network (CNN) is proposed, motivated by the observations that: 1) there is no clear correlation between the topological features and the controllability robustness of a general network; 2) the adjacency matrix of a network can be regarded as a grayscale image; and 3) the CNN technique has proved successful in image processing without human intervention. Under the new framework, a fairly large number of training data generated by simulations are used to train a CNN for predicting the controllability robustness according to the input network-adjacency matrices, without performing conventional attack simulations. Extensive experimental studies were carried out, which demonstrate that the proposed framework for predicting controllability robustness of different network configurations is accurate and reliable with very low overheads.","",""
1,"Y. Diao, I. Jelescu","Parameter estimation for WMTI-Watson model of white matter using encoder-decoder recurrent neural network",2022,"","","","",155,"2022-07-13 10:07:51","","10.48550/arXiv.2203.00595","","",,,,,1,1.00,1,2,1,"Biophysical modelling of the diffusion MRI signal provides estimates of specific microstructural tissue properties. Model parameters estimates can be obtained by fitting the model to the measured signal. Although nonlinear optimization such as non-linear least squares (NLLS) is the most widespread method for model estimation, it suffers from local minima, high computational cost and uncertain accuracy. Deep Learning approaches are steadily replacing NL fitting, but come with the limitation that the model needs to be retrained for each acquisition protocol and noise level. The White Matter Tract Integrity (WMTI)Watson model was proposed as an implementation of the Standard Model of diffusion in white matter that estimates model parameters from the diffusion and kurtosis tensors (DKI), thereby overcoming fitting the model signal equation. Here we proposed a deep learning approach based on the encoder-decoder recurrent neural network (RNN) to increase the robustness and accelerate the parameter estimation of WMTI-Watson. We use an embedding approach to render the model insensitive to potential differences in distributions between training data and experimental data. This RNN-based solver thus has the advantage of being highly efficient in computation and more readily translatable to other datasets, irrespective of acquisition protocol and underlying parameter distributions as long as diffusion and kurtosis tensors (or their typical derived scalars) were pre-computed from the data. In this study, we evaluated the performance of NLLS, the RNN-based method and a baseline DL architecture based on multilayer perceptron (MLP) on synthetic and in vivo datasets of rat and human brain. We showed that the proposed RNN-based fitting approach had the advantage of highly reduced computation time over NLLS (from hours to seconds), with similar accuracy and precision but improved robustness, and superior translatability to new datasets over MLP, irrespective of acquisition protocol or species being rat or human.","",""
2,"Xuechun Wang, Wendong Chen, Yuan Ji, F. Ran","Gesture recognition based on parallel hardware neural network implemented with stochastic logics",2016,"","","","",156,"2022-07-13 10:07:51","","10.1109/ICALIP.2016.7846555","","",,,,,2,0.33,1,4,6,"A new method based on neural network using stochastic computing is presented for the recognition of human gesture. In the current gesture recognition study, most of the technologies require high hardware resources and power consumption. Considering gesture recognition algorithms, the power limitations of their complex systems have encouraged designers toward searching for a reconfigurable architecture, stochastic computing. For different neural networks with complex arithmetic operations, computation on stochastic bit streams costs fewer resources and performs very efficient in operation. The experimental results demonstrate that the stochastic neural network could recognize different hand gesture effectively and take less hardware area. Even more, it has good robustness to the different environments.","",""
15,"Ruihao Li, Dongbing Gu, Qiang Liu, Zhiqiang Long, Huosheng Hu","Semantic Scene Mapping with Spatio-temporal Deep Neural Network for Robotic Applications",2018,"","","","",157,"2022-07-13 10:07:51","","10.1007/s12559-017-9526-9","","",,,,,15,3.75,3,5,4,"","",""
17,"Robert Grupp, M. Unberath, Cong Gao, Rachel A. Hegeman, R. Murphy, C. Alexander, Y. Otake, B. McArthur, M. Armand, R. Taylor","Automatic annotation of hip anatomy in fluoroscopy for robust and efficient 2D/3D registration",2019,"","","","",158,"2022-07-13 10:07:51","","10.1007/s11548-020-02162-7","","",,,,,17,5.67,2,10,3,"","",""
84,"T. Kline, P. Korfiatis, Marie E. Edwards, J. Blais, F. Czerwiec, P. Harris, B. King, V. Torres, B. Erickson","Performance of an Artificial Multi-observer Deep Neural Network for Fully Automated Segmentation of Polycystic Kidneys",2017,"","","","",159,"2022-07-13 10:07:51","","10.1007/s10278-017-9978-1","","",,,,,84,16.80,9,9,5,"","",""
58,"Zhan Li, M. Hayashibe, C. Fattal, D. Guiraud","Muscle Fatigue Tracking with Evoked EMG via Recurrent Neural Network: Toward Personalized Neuroprosthetics",2014,"","","","",160,"2022-07-13 10:07:51","","10.1109/MCI.2014.2307224","","",,,,,58,7.25,15,4,8,"One of the challenging issues in computational rehabilitation is that there is a large variety of patient situations depending on the type of neurological disorder. Human characteristics are basically subject specific and time variant; for instance, neuromuscular dynamics may vary due to muscle fatigue. To tackle such patient specificity and time-varying characteristics, a robust bio-signal processing and a precise model-based control which can manage the nonlinearity and time variance of the system, would bring break-through and new modality toward computational intelligence (CI) based rehabilitation technology and personalized neuroprosthetics. Functional electrical stimulation (FES) is a useful technique to assist restoring motor capability of spinal cord injured (SCI) patients by delivering electrical pulses to paralyzed muscles. However, muscle fatigue constraints the application of FES as it results in the time-variant muscle response. To perform adaptive closedloop FES control with actual muscle response feedback taken into account, muscular torque is essential to be estimated accurately. However, inadequacy of the implantable torque sensor limits the direct measurement of the time-variant torque at the joint. This motivates the development of methods to estimate muscle torque from bio-signals that can be measured. Evoked electromyogram (eEMG) has been found to be highly correlated with FES-induced torque under various muscle conditions, indicating that it can be used for torque/force prediction. A nonlinear ARX (NARX) type model is preferred to track the relationship between eEMG and stimulated muscular torque. This paper presents a NARX recurrent neural network (NARX-RNN) model for identification/prediction of FES-induced muscular dynamics with eEMG. The NARX-RNN model may possess novelty of robust prediction performance. Due to the difficulty of choosing a proper forgetting factor of Kalman filter for predicting time-variant torque with eEMG, the presented NARX-RNN could be considered as an alternative muscular torque predictor. Data collected from five SCI patients is used to evaluate the proposed NARX-RNN model, and the results show promising estimation performances. In addition, the general importance regarding CI-based motor function modeling is introduced along with its potential impact in the rehabilitation domain. The issue toward personalized neuroprosthetics is discussed in detail with the potential role of CI-based identification and the benefit for motor-impaired patient community.","",""
0,"Ze Fu, Xiaosha Wang, Xiaoying Wang, Huichao Yang, Jiahuan Wang, Tao Wei, Xuhong Liao, Zhiyuan Liu, Huimin Chen, Y. Bi","Computational mechanisms for neural representation of words",2021,"","","","",161,"2022-07-13 10:07:51","","10.1101/2021.06.27.450093","","",,,,,0,0.00,0,10,1,"A critical way for humans to acquire, represent and communicate information is through language, yet the underlying computation mechanisms through which language contributes to our word meaning representations are poorly understood. We compared three major types of word computation mechanisms from large language corpus (simple co-occurrence, graph-space relations and neural-network-vector-embedding relations) in terms of the association of words’ brain activity patterns, measured by two functional magnetic resonance imaging (fMRI) experiments. Word relations derived from a graph-space representation, and not neural-network-vector-embedding, had unique explanatory power for the neural activity patterns in brain regions that have been shown to be particularly sensitive to language processes, including the anterior temporal lobe (capturing graph-common-neighbors), inferior frontal gyrus, and posterior middle/inferior temporal gyrus (capturing graph-shortest-path). These results were robust across different window sizes and graph sizes and were relatively specific to language inputs. These findings highlight the role of cumulative language inputs in organizing word meaning neural representations and provide a mathematical model to explain how different brain regions capture different types of language-derived information.","",""
0,"Christian David M'arton, L'eo Gagnon, Guillaume Lajoie, Kanaka Rajan","Efficient and robust multi-task learning in the brain with modular latent primitives",2021,"","","","",162,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,4,1,"Biological agents do not have inﬁnite resources to learn new things. For this reason, a central aspect of human learning is the ability to recycle previously acquired knowledge in a way that allows for faster, less resource-intensive acquisition of new skills. In spite of that, how neural networks in the brain leverage existing knowledge to learn new computations is not well understood. In this work, we study this question in artiﬁcial recurrent neural networks (RNNs) trained on a corpus of commonly used neuroscience tasks. Combining brain-inspired inductive biases we call functional and structural, we propose a system that learns new tasks by building on top of pre-trained latent dynamics organised into separate recurrent modules. These modules, acting as prior knowledge acquired previously through evolution or development, are pre-trained on the statistics of the full corpus of tasks so as to be independent and maximally informative. The resulting model, we call a Modular Latent Primitives (MoLaP) network, allows for learning multiple tasks while keeping parameter counts, and updates, low. We also show that the skills acquired with our approach are more robust to a broad range of perturbations compared to those acquired with other multi-task learning strategies, and that generalisation to new tasks is facilitated. This work offers a new perspective on achieving efﬁcient multi-task learning in the brain, illustrating the beneﬁts of leveraging pre-trained latent dynamical primitives. models of biologically plausible multi-task learning. Finally, we show that generalization to new tasks that share basic structure with the corpus is also facilitated. This draws a path towards cheaper multitask solutions, and offers new hypotheses for how continual learning might be implemented in the biological brain. which are used to pre-train separate network modules. B. During task training, recurrent weights remain frozen and only input and readout weights are adjusted. Task-speciﬁc responses can be obtained via separate readout weights. C. Parameter overview for the various multi-task training methods. Parameters trained (or updated) and total network size is compared across the three methods. MoLaP makes ∼ 2 orders of magnitude fewer parameter updates than DNI, at a relatively moderate increase of total parameter count.","",""
619,"Mou Chen, S. Ge, B. How","Robust Adaptive Neural Network Control for a Class of Uncertain MIMO Nonlinear Systems With Input Nonlinearities",2010,"","","","",163,"2022-07-13 10:07:51","","10.1109/TNN.2010.2042611","","",,,,,619,51.58,206,3,12,"In this paper, robust adaptive neural network (NN) control is investigated for a general class of uncertain multiple-input-multiple-output (MIMO) nonlinear systems with unknown control coefficient matrices and input nonlinearities. For nonsymmetric input nonlinearities of saturation and deadzone, variable structure control (VSC) in combination with backstepping and Lyapunov synthesis is proposed for adaptive NN control design with guaranteed stability. In the proposed adaptive NN control, the usual assumption on nonsingularity of NN approximation for unknown control coefficient matrices and boundary assumption between NN approximation error and control input have been eliminated. Command filters are presented to implement physical constraints on the virtual control laws, then the tedious analytic computations of time derivatives of virtual control laws are canceled. It is proved that the proposed robust backstepping control is able to guarantee semiglobal uniform ultimate boundedness of all signals in the closed-loop system. Finally, simulation results are presented to illustrate the effectiveness of the proposed adaptive NN control.","",""
29,"N. Amoroso, R. Errico, S. Bruno, A. Chincarini, E. Garuccio, F. Sensi, S. Tangaro, A. Tateo, R. Bellotti","Hippocampal unified multi-atlas network (HUMAN): protocol and scale validation of a novel segmentation tool.",2015,"","","","",164,"2022-07-13 10:07:51","","10.1088/0031-9155/60/22/8851","","",,,,,29,4.14,3,9,7,"In this study we present a novel fully automated Hippocampal Unified Multi-Atlas-Networks (HUMAN) algorithm for the segmentation of the hippocampus in structural magnetic resonance imaging. In multi-atlas approaches atlas selection is of crucial importance for the accuracy of the segmentation. Here we present an optimized method based on the definition of a small peri-hippocampal region to target the atlas learning with linear and non-linear embedded manifolds. All atlases were co-registered to a data driven template resulting in a computationally efficient method that requires only one test registration. The optimal atlases identified were used to train dedicated artificial neural networks whose labels were then propagated and fused to obtain the final segmentation. To quantify data heterogeneity and protocol inherent effects, HUMAN was tested on two independent data sets provided by the Alzheimer's Disease Neuroimaging Initiative and the Open Access Series of Imaging Studies. HUMAN is accurate and achieves state-of-the-art performance (Dice[Formula: see text] and Dice[Formula: see text]). It is also a robust method that remains stable when applied to the whole hippocampus or to sub-regions (patches). HUMAN also compares favorably with a basic multi-atlas approach and a benchmark segmentation tool such as FreeSurfer.","",""
0,"B. Mukunthan","A NEURAL NETWORK APPROACH FOR THE PRECISE PATTERN RECOGNITION OF HUMAN DNA",2012,"","","","",165,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,1,10,"The primary goal of bio informatics and neural networks solely is to increase our understanding of biological processes and focus on developing and applying computationally intensive techniques (e.g., pattern recognition, data mining, machine learning algorithms, and visualization) to achieve this goal. The neural networks exhibit characteristics such as mapping capabilities or pattern association, generalization, robustness, fault tolerance, parallel and high speed information processing. Neural networks learn by examples they can therefore be trained with known examples of a problem to ‘acquire’ knowledge about it. Once appropriately trained, the network can be put to effective use in solving ‘unknown’ or ‘untrained’ instances of the problem. The perfect blend made of bioinformatics and neural networks results in efficient pattern analysis techniques. The conventional techniques and algorithms employed by forensic scientists to assist in the identification of individuals on the basis of their respective DNA profiles involves more computational steps and mathematical formulas that leads to more time and space complexity resulting in complicated and less efficient algorithms which can be shorted out by emerging Artificial Neural Network approach.","",""
0,"Junxiao Xue, Junjin Cheng, Qibin Zhang, Yibo Guo, Aiguo Lu, Jian Li, Xi Wan, Jing Xu","Improved Efficient Convolutional Neural Networks for Complex Scene Mask-Wearing Detection",2021,"","","","",166,"2022-07-13 10:07:51","","10.3724/sp.j.1089.2021.18635","","",,,,,0,0.00,0,8,1,": To solve the problem about low accuracy of mask wear detection under complex lighting and face lean conditions, a method of mask wear detection under intricate environment using efficient convolutional neural network is proposed, which uses pre-training such as hard negative mining to learn more samples of face feature, utilize multi-task convolutional neural networks (MTCNN) to estimate the possibility of face information, and get accurate face location. With attention mechanism in feature pyramid network, enhanc-ing the weight of key points on human face, employing efficient neural network detection will be wore on mask-wearing detection as a simple binary classification problem. Under the environment of TensorFlow platform, not only data training, data preprocessing, but also the contrast experiment with AIZOO method are completed. A data set containing with 816 images is collected, marked and trained. During the data pre-processing, images are set as fixed size to reduce the amount of computation and promote the detection speed. Then, image enhancement algorithm is used to conduct distortion processing to improve the robust-ness of this model. On this basis, MTCNN is used to detect the face information in pictures, modify and normalize all data, then put them into neural network and the trained model to detection. The experimental results show that under complex conditions such as complex lighting and face tilt, the accuracy can reach 83% and 91% respectively, which means can accurately detect whether wearing a mask.","",""
0,"Hafez Ghaemi, Erfan Mirzaei, Mahbod Nouri, S. R. Kheradpisheh","BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks",2021,"","","","",167,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,4,1,". Brain-inspired computation and information processing alongside compatibility with neuromorphic hardware have made spiking neural networks (SNN) a promising method for solving learning tasks in machine learning (ML). Spiking neurons are only one of the requirements for building a bio-plausible learning model. Network architecture and learning rules are other important factors to consider when developing such artiﬁcial agents. In this work, inspired by the human visual pathway and the role of dopamine in learning, we propose a reward-modulated locally connected spiking neural network, BioLCNet, for visual learning tasks. To extract visual features from Poisson-distributed spike trains, we used local ﬁlters that are more analogous to the biological visual system compared to convolutional ﬁlters with weight sharing. In the decoding layer, we applied a spike population-based voting scheme to determine the decision of the network. We employed Spike-timing-dependent plasticity (STDP) for learning the visual features, and its reward-modulated variant (R-STDP) for training the decoder based on the reward or punishment feedback signal. For evaluation, we ﬁrst assessed the robustness of our rewarding mechanism to varying target responses in a classical conditioning experiment. Afterwards, we evaluated the performance of our network on image classiﬁcation tasks of MNIST and XOR MNIST datasets.","",""
0,"M. Swarna, N. Sudhakar, N. Vadaparthi","An effective tropical cyclone intensity estimation model using Convolutional Neural Networks",2021,"","","","",168,"2022-07-13 10:07:51","","10.54302/mausam.v72i2.616","","",,,,,0,0.00,0,3,1,"The tropical cyclones in India is a common natural disaster happening every year. As per the statistics, about three cyclones hit India's east coast in the Bay of Bengal, which damaged human lives, crops and property. It is essential to predict the cyclones in advance to prevent and reduce huge damage. The techniques used are based on numerical models that require vast expertise and higher skill sets to achieve better prediction accuracy. The usage of Convolutional Neural Networks shall overcome various issues like domain knowledge, the scope for human errors. Hence, in this paper, we attempted to predict cyclone intensity using Convolutional Neural Networks by proposing a simple and robust architecture for Tropical Cyclone intensity estimation. The results yielded better performance than the state-of-the-art techniques with reduced computation time.","",""
3,"Dalai Tang, Tiong Yew Tang, János Botzheim, N. Kubota, Toru Yamaguchi","Fuzzy Spiking Neural Network for Abnormality Detection in Cognitive Robot Life Supporting System",2015,"","","","",169,"2022-07-13 10:07:51","","10.1109/SSCI.2015.29","","",,,,,3,0.43,1,5,7,"In aging nation such as Japan, elderly people belong to the vulnerable group that constantly need health care and monitoring for their well-being. Therefore, an early warning system for detecting abnormality in their daily activities could save their life (e.g. Heart attack, stroke and etc.). However, such early warning system must not trigger any false warning signals in order to robustly operate in real world applications. Robot interactions with human are useful to prevent false warning signals from sending out to healthcare worker. Next, the system should be able to detect short-term abnormal and also long-term abnormal behaviors of the elderly people within their normal daily life routine. Therefore, it is important to integrate information ally structured space with cognitive robot to confirm the elderly's abnormal situation with human-robot interactions before sending out warning signals to healthcare workers. In this work, we proposed an evolutionary computation based approach to optimize fuzzy spiking neural network for detecting abnormal activities in the elderly people's daily activities.","",""
2,"Yue-Huan Wang, Zenan Li, Jingwei Xu, Ping Yu, Xiaoxing Ma","Fast Robustness Prediction for Deep Neural Network",2019,"","","","",170,"2022-07-13 10:07:51","","10.1145/3361242.3361243","","",,,,,2,0.67,0,5,3,"Deep neural networks (DNNs) have achieved impressive performance in many difficult tasks. However, DNN models are essentially uninterpretable to humans, and unfortunately prone to adversarial attacks, which hinders their adoption in security and safety-critical scenarios. The robustness of a DNN model, which measures its stableness against adversarial attacks, becomes an important topic in both the machine learning and the software engineering communities. Analytical evaluation of DNN robustness is difficult due to the high-dimensionality of inputs, the huge amount of parameters, and the nonlinear network structure. In practice, the degree of robustness of DNNs is empirically approximated with adversarial searching, which is computationally expensive and cannot be applied in resource constrained settings such as embedded computing. In this paper, we propose to predict the robustness of a DNN model for each input with another DNN model, which takes the output of neurons of the former model as input. We train a regression model to encode the connections between output of the penultimate layer of a DNN model and its robustness. With this trained model, the robustness for an input can be predicted instantaneously. Experiments with MNIST and CIFAR10 datasets and LeNet, VGG and ResNet DNN models were conducted to evaluate the efficacy of the proposed approach. The results indicated that our approach achieved 0.05-0.21 mean absolute errors and significantly outperformed confidence and surprise adequacy-based approaches.","",""
0,"Gautam Chakraborty, Mridusmita Sharma, Navajit Saikia, K. K. Sarma","Soft-computation based speech recognition system for Sylheti language",2022,"","","","",171,"2022-07-13 10:07:51","","10.1007/s10772-022-09976-7","","",,,,,0,0.00,0,4,1,"","",""
6,"Yang Yu, M. Rashidi, B. Samali, Masoud Mohammadi, T. N. Nguyen, Xinxiu Zhou","Crack detection of concrete structures using deep convolutional neural networks optimized by enhanced chicken swarm algorithm",2022,"","","","",172,"2022-07-13 10:07:51","","10.1177/14759217211053546","","",,,,,6,6.00,1,6,1,"With the rapid increase of ageing infrastructures worldwide, effective and robust inspection techniques are highly demanding to evaluate structural conditions and residual lifetime. The damages on structural surfaces, for example, spalling, crack, rebar buckling and exposure, are important indicators to assess the structural condition. In fact, several state-of-the-art automated inspection techniques using these indicators have been developed to reduce human-conducted onsite inspection activities. However, the efficiency of these techniques is still required to be improved in terms of accuracy and computational cost. In this study, a vision-based crack diagnosis method is developed using deep convolutional neural network (DCNN) and enhanced chicken swarm algorithm (ECSA). A DCNN model is designed with a deep architecture, consisting of six convolutional layers, two pooling layers and three fully connected layers. To enhance the generalisation capacity of trained model, ECSA is introduced to optimize meta-parameters of the DCNN model. The model is trained and tested using image patches cropped from raw images obtained from damaged concrete samples. Finally, a comparative study on different crack detection techniques is conducted to evaluate performance of the proposed method via a group of statistical evaluation indicators.","",""
9,"Zhensong Wei, Chao Wang, Peng Hao, M. Barth","Vision-Based Lane-Changing Behavior Detection Using Deep Residual Neural Network",2019,"","","","",173,"2022-07-13 10:07:51","","10.1109/ITSC.2019.8917158","","",,,,,9,3.00,2,4,3,"Accurate lane localization and lane change detection are crucial in advanced driver assistance systems and autonomous driving systems for safer and more efficient trajectory planning. Conventional localization devices such as Global Positioning System only provide road-level resolution for car navigation, which is incompetent to assist in lane-level decision making. The state of art technique for lane localization is to use Light Detection and Ranging sensors to correct the global localization error and achieve centimeter-level accuracy, but the real-time implementation and popularization for LiDAR is still limited by its computational burden and current cost. As a cost-effective alternative, vision-based lane change detection has been highly regarded for affordable autonomous vehicles to support lane-level localization. A deep learning based computer vision system is developed to detect the lane change behavior using the images captured by a front-view camera mounted on the vehicle and data from the inertial measurement unit for highway driving. Testing results on real-world driving data have shown that the proposed method is robust with real-time working ability and could achieve around 87% lane change detection accuracy. Compared to the average human reaction to visual stimuli, the proposed computer vision system works 9 times faster, which makes it capable of helping make life-saving decisions in time.","",""
1,"Manh-Quan Bui, Viet-Hang Duong, Tzu-Chiang Tai, Jia-Ching Wang","Depth Human Action Recognition Based on Convolution Neural Networks and Principal Component Analysis",2018,"","","","",174,"2022-07-13 10:07:51","","10.1109/ICIP.2018.8451232","","",,,,,1,0.25,0,4,4,"In this work, we address human action recognition problem under viewpoint variation. The proposed model is formulated by wisely combining convolution neural network (CNN) model with principle component analysis (PCA). In this context, we pass real depth videos through a CNN model in a frame-wise manner. The view invariant features are extracted by employing convolution layers as mid-outputs and considered as 3D nonnegative tensors. The PCA algorithm is separately imposed on view-invariant high-level space of image and video groups to seek both local and holistic hidden dynamics information. To deal with noisy data and temporal misalignment, we utilize the Fourier temporal pyramid to encode temporal and obtain the final descriptors. Our proposed framework supplies a robust discriminative representation with low dimension and computational requirements. We evaluate the proposed method on two standard multiview depth video datasets. The experimental results show that our method has superior performance compared to other approaches.","",""
4,"Jay Roberts, Theodoros Tsiligkaridis","Controllably Sparse Perturbations of Robust Classifiers for Explaining Predictions and Probing Learned Concepts",2021,"","","","",175,"2022-07-13 10:07:51","","","","",,,,,4,4.00,2,2,1,"Explaining the predictions of a deep neural network (DNN) in image classification is an active area of research. Many methods focus on localizing pixels, or groups of pixels, which maximize a relevance metric for the prediction. Others aim at creating local ""proxy"" explainers which aim to account for an individual prediction of a model. We aim to explore ""why"" a model made a prediction by perturbing inputs to robust classifiers and interpreting the semantically meaningful results. For such an explanation to be useful for humans it is desirable for it to be sparse; however, generating sparse perturbations can computationally expensive and infeasible on high resolution data. Here we introduce controllably sparse explanations that can be efficiently generated on higher resolution data to provide improved counter-factual explanations. Further we use these controllably sparse explanations to probe what the robust classifier has learned. These explanations could provide insight for model developers as well as assist in detecting dataset bias. CCS Concepts • Computing methodologies → Machine learning; Artificial intelligence;","",""
2,"A. Opeyemi, R. Gbenga","Symptomatic and Climatic Based Malaria Threat Detection Using Multilevel Thresholding FeedForward Neural Network",2017,"","","","",176,"2022-07-13 10:07:51","","10.5815/ijitcs.2017.08.05","","",,,,,2,0.40,1,2,5,"Recent worldwide medical research is focusing on new intelligence approaches for diagnosis of various infections. The sporadic occurrence of malaria diseases in human has pushed the need to develop computational approaches for its diagnoses. Most existing conventional malaria models for classification problems examine the dynamics of asymptomatic and morphological characteristics of malaria parasite in the thick blood smear, but this study examine the symptomatic characteristics of malaria parasite combined with effects of climatic factor which is a great determinant of malaria severity. The need to predict the occurrence of malaria disease and its outbreak will be helpful to take appropriate actions by individuals, World Health Organizations and Government Agencies and its devastating impact will be reduced. This paper proposed Feed-Forward Back-Propagation (FF_BP) Neural Network model to determine the rate of malaria transmission. Monthly averages of climatic factors; rainfall, temperature and relative humidity with monthly malaria incidences were used as input variables. An optimum threshold value of 0.7100 with classification accuracy 87.56%, sensitivity 96.67% and specificity 76.67% and mean square error of 0.100 were achieved. While, the model malaria threat detection rate was 87.56%, positive predictive value was 89.23%, negative predictive value was 92.00% and the standard deviation is 2.533. The statistical analysis of Feed-Forward BackPropagation Neural Network model was conducted and its results were compared with other existing models to check its robustness and viability.","",""
2,"Mohammed Alawad, Mingjie Lin","Stochastic-based multi-stage streaming realization of deep convolutional neural network",2017,"","","","",177,"2022-07-13 10:07:51","","10.1109/ISQED.2017.7918285","","",,,,,2,0.40,1,2,5,"Large-scale convolutional neural network (CNN), conceptually mimicking the operational principle of visual perception in human brain, has been widely applied to tackle many challenging computer vision and artificial intelligence applications. Unfortunately, despite of its simple architecture, a typically-sized CNN is well known to be computationally intensive. This work presents a novel stochastic-based and scalable hardware architecture and circuit design that computes a large-scale CNN with FPGA. The key idea is to implement all key components of a deep learning CNN, including multi-dimensional convolution, activation, and pooling layers, completely in the probabilistic computing domain in order to achieve high computing robustness, high performance, and low hardware usage. Most importantly, through both theoretical analysis and FPGA hardware implementation, we demonstrate that stochastic-based deep CNN can achieve superior hardware scalability when compared with its conventional deterministic-based FPGA implementation by allowing a stream computing mode and adopting efficient random sample manipulations. Overall, being highly scalable and energy efficient, our stochastic-based convolutional neural network architecture is well-suited for a modular vision engine with the goal of performing real-time detection, recognition and segmentation of mega-pixel images, especially those perception-based computing tasks that are inherently fault-tolerant, while still requiring high energy efficiency.","",""
12,"Á. Martínez-González, M. Villamizar, Olivier Can'evet, J. Odobez","Efficient Convolutional Neural Networks for Depth-Based Multi-Person Pose Estimation",2019,"","","","",178,"2022-07-13 10:07:51","","10.1109/TCSVT.2019.2952779","","",,,,,12,4.00,3,4,3,"Achieving robust multi-person 2D body landmark localization and pose estimation is essential for human behavior and interaction understanding as encountered for instance in HRI settings. Accurate methods have been proposed recently, but they usually rely on rather deep Convolutional Neural Network (CNN) architecture, thus requiring large computational and training resources. In this paper, we investigate different architectures and methodologies to address these issues and achieve fast and accurate multi-person 2D pose estimation. To foster speed, we propose to work with depth images, whose structure contains sufficient information about body landmarks while being simpler than textured color images and thus potentially requiring less complex CNNs for processing. In this context, we make the following contributions. i) we study several CNN architecture designs combining pose machines relying on the cascade of detectors concept with lightweight and efficient CNN structures; ii) to address the need for large training datasets with high variability, we rely on semi-synthetic data combining multi-person synthetic depth data with real sensor backgrounds; iii) we explore domain adaptation techniques to address the performance gap introduced by testing on real depth images; iv) to increase the accuracy of our fast lightweight CNN models, we investigate knowledge distillation at several architecture levels which effectively enhance performance. Experiments and results on synthetic and real data highlight the impact of our design choices, providing insights into methods addressing standard issues normally faced in practical applications, and resulting in architectures effectively matching our goal in both performance and speed.","",""
12454,"Kaiming He, X. Zhang, Shaoqing Ren, Jian Sun","Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",2015,"","","","",179,"2022-07-13 10:07:51","","10.1109/ICCV.2015.123","","",,,,,12454,1779.14,3114,4,7,"Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.","",""
0,"Meenal Suryakant Vatsaraj, D. Bade","Anomalous Behaviour Detection in Crowded Environments Using Classifiers Artificial Neural Network and Support Vector Machine",2017,"","","","",180,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,2,5,"Our proposed method focuses to detect and localize anomalous behavior in videos of the crowded area means different scenario from the dominant pattern. Proposed method consist motion and appearance information, therefore, different kinds of anomalies can be robustly identified in a wide range of situations. Histogram of oriented gradients can easily capture the varying dynamic of the crowded environment. Histogram of oriented gradients can also effectively recognize and characterize each frame of each scene. Our method of detecting anomalies using artificial neural network and support vector machine consist both appearance and motion features which extract this features within the spatiotemporal domain of moving pixels that ensures robustness to local noise and thus increases accuracy in detection of a local anomaly with low computational cost. UCSD dataset which will be used and which consist various situations with varying human crowds as well as traffic data with occlusions when feed to our proposed method can achieve significantly higher accuracy probably more for pixel level events detection as compared to any other methods.","",""
0,"Mohammed Alawad, Mingjie Lin","Stochastic-Based Multi-stage Streaming Realization of a Deep Convolutional Neural Network (Abstract Only)",2017,"","","","",181,"2022-07-13 10:07:51","","10.1145/3020078.3021788","","",,,,,0,0.00,0,2,5,"Large-scale convolutional neural network (CNN), conceptually mimicking the operational principle of visual perception in human brain, has been widely applied to tackle many challenging computer vision and artificial intelligence applications. Unfortunately, despite of its simple architecture, a typically sized CNN is well known to be computationally intensive. This work presents a novel stochastic-based and scalable hardware architecture and circuit design that computes a large-scale CNN with FPGA. The key idea is to implement all key components of a deep learning CNN, including multi-dimensional convolution, activation, and pooling layers, completely in the probabilistic computing domain in order to achieve high computing robustness, high performance, and low hardware usage. Most importantly, through both theoretical analysis and FPGA hardware implementation, we demonstrate that stochastic-based deep CNN can achieve superior hardware scalability when compared with its conventional deterministic-based FPGA implementation by allowing a stream computing mode and adopting efficient random sample manipulations. Overall, being highly scalable and energy efficient, our stochastic-based convolutional neural network architecture is well-suited for a modular vision engine with the goal of performing real-time detection, recognition and segmentation of mega-pixel images, especially those perception-based computing tasks that are inherently fault-tolerant, while still requiring high energy efficiency.","",""
0,"Meenal Suryakant Vatsaraj, D. Bade","Anomalous Behavior Detection in Crowded Environments Using Classifiers Artificial Neural Network and Support Vector Machine",2017,"","","","",182,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,2,5,"Our propose method focuses to detect and localize anomalous behavior in videos of crowded area means different scenario from dominant pattern. Proposed method consist motion and appearance information therefore different kinds of anomalies can be robustly identified in a wide range of situations. Histogram of oriented gradients can easily captures varying dynamic of crowded environment. Histogram of oriented gradients can also effectively recognize and characterize each frame of each scene. Our method of detecting anomalies using artificial neural network and support vector machine consist both appearance and motion features which extracts this features within spatio temporal domain of moving pixels that ensures robustness to local noise and thus increases accuracy in detection of local anomaly with low computational cost. UCSD dataset which will be used and which consist various situations with varying human crowds as well as traffic data with occlusions when feed to our propose method can achieve significantly higher accuracy probably more for pixel level events detection as compared to any other methods.","",""
0,"Zheyuan Wang, M. Gombolay","Stochastic Resource Optimization over Heterogeneous Graph Neural Networks for Failure-Predictive Maintenance Scheduling",2022,"","","","",183,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,2,1,"Resource optimization for predictive maintenance is a challenging computational problem that requires inferring and reasoning over stochastic failure models and dynamically allocating repair resources. Predictive maintenance scheduling is typically performed with a combination of ad hoc, handcrafted heuristics with manual scheduling corrections by human domain experts, which is a labor-intensive process that is hard to scale. In this paper, we develop an innovative heterogeneous graph neural network to automatically learn an end-to-end resource scheduling policy. Our approach is fully graph-based with the addition of state summary and decision value nodes that provides a computationally lightweight and nonparametric means to perform dynamic scheduling. We augment our policy optimization procedure to enable robust learning in highly stochastic environments for which typical actor-critic reinforcement learning methods are ill-suited. In consultation with aerospace industry partners, we develop a virtual predictive-maintenance environment for a heterogeneous fleet of aircraft, called AirME. Our approach sets a new state-of-the-art by outperforming conventional, hand-crafted heuristics and baseline learning methods across problem sizes and various objective functions.","",""
0,"Zixi Liu, Yang Feng, Yining Yin, Zhenyu Chen","DeepState: Selecting Test Suites to Enhance the Robustness of Recurrent Neural Networks",2022,"","","","",184,"2022-07-13 10:07:51","","10.1145/3510003.3510231","","",,,,,0,0.00,0,4,1,"Deep Neural Networks (DNN) have achieved tremendous success in various software applications. However, accompanied by outstanding effectiveness, DNN-driven software systems could also exhibit incorrect behaviors and result in some critical accidents and losses. The testing and optimization of DNN-driven software systems rely on a large number of labeled data that often require many human efforts, resulting in high test costs and low efficiency. Although plenty of coverage-based criteria have been proposed to assist in the data selection of convolutional neural networks, it is difficult to apply them on Recurrent Neural Network (RNN) models due to the difference between the working nature. In this paper, we propose a test suite selection tool DeepState towards the particular neural network structures of RNN models for reducing the data labeling and computation cost. DeepState selects data based on a stateful perspective of RNN, which identifies the possibly misclassified test by capturing the state changes of neurons in RNN models. We further design a test selection method to enable testers to obtain a test suite with strong fault detection and model improvement capability from a large dataset. To evaluate DeepState, we conduct an extensive empirical study on popular datasets and prevalent RNN models containing image and text processing tasks. The experimental results demonstrate that DeepState outperforms existing coverage-based techniques in selecting tests regarding effectiveness and the inclusiveness of bug cases. Meanwhile, we observe that the selected data can improve the robustness of RNN models effectively.","",""
33,"V. Nguyen, J. Starzyk, Wooi-Boon Goh, Daniel Jachyra","Neural Network Structure for Spatio-Temporal Long-Term Memory",2012,"","","","",185,"2022-07-13 10:07:51","","10.1109/TNNLS.2012.2191419","","",,,,,33,3.30,8,4,10,"This paper proposes a neural network structure for spatio-temporal learning and recognition inspired by the long-term memory (LTM) model of the human cortex. Our structure is able to process real-valued and multidimensional sequences. This capability is attained by addressing three critical problems in sequential learning, namely the error tolerance, the significance of sequence elements and memory forgetting. We demonstrate the potential of the framework with a series of synthetic simulations and the Australian sign language (ASL) dataset. Results show that our LTM model is robust to different types of distortions. Second, our LTM model outperforms other sequential processing models in a classification task for the ASL dataset.","",""
6,"Mozammel Chowdhury, Junbin Gao, R. Islam","Robust human detection and localization in security applications",2017,"","","","",186,"2022-07-13 10:07:51","","10.1002/cpe.3977","","",,,,,6,1.20,2,3,5,"Human detection and localization has attracted much attention in security applications because of the increasing demand of safety and security in different environments, including surveillance systems, secure access control, person recognition, border monitoring, preventing criminal acts, intrusion detection, alarm monitoring, and so on. This article proposes a robust approach for human detection and localization by analyzing and matching corresponding facial features extracted from video sequences. The proposed technique captures the video scenes using a stereo system consisting of two cameras: left and right cameras with similar intrinsic parameters. The system first tracks the human by detecting the face area from the video scenes using an efficient fuzzy face detection algorithm. To localize the human position, the depth information is computed from the extracted face images by using a robust stereo matching algorithm. A neural network is used to match the correspondence pixels between the left and the right face images. Experimental evaluation demonstrates the competence and robustness of the proposed method. The low computation time required for the detection and localization of human objects compared with other methods raises its suitability toward its use in real‐time applications. Copyright © 2016 John Wiley & Sons, Ltd.","",""
0,"U. Jaramillo-Avila, J. Aitken, K. Gurney, S. Anderson","Robust top-down and bottom-up visual saliency for mobile robots using bio-inspired design principles",2021,"","","","",187,"2022-07-13 10:07:51","","10.1109/IROS51168.2021.9636800","","",,,,,0,0.00,0,4,1,"Modern camera systems in robotics tend to pro-duce overwhelming amounts of visual information due to their high resolutions and high frame rates. This raises a fundamental question of how robots should focus attention on a region of the visual scene, and how they should process information in the periphery. This is particularly an issue for mobile robots, where the computational resources of low-power embedded computing boards tend to be much less than for workstations. In this paper, we look to biological design in the primate brain for inspiration on how to solve this problem. We develop a novel computational fusion of bottom-up and top-down visual saliency information. The bottom-up saliency is produced using standard colour, intensity, and motion image processing methods. The top-down saliency is produced using a deep convolutional neural network for object detection and recognition, with foveated images for computational efficiency. Regions of attention are obtained using a computational model of the basal ganglia, thought to be involved in optimal decision making. The model of the basal ganglia is based on the multi-hypothesis sequential probability ratio test (MSPRT). The visual saliency scheme is evaluated on omnidirectional video feed highlighting a proximity to human behaviour.","",""
0,"T. P. Moreira, M. C. Santana, L. A. Passos, J. Papa, K. Costa","An End-to-End Approach for Seam Carving Detection using Deep Neural Networks",2022,"","","","",188,"2022-07-13 10:07:51","","10.48550/arXiv.2203.02728","","",,,,,0,0.00,0,5,1,"Seam carving is a computational method capable of resizing images for both reduction and expansion based on its content, instead of the image geometry. Although the technique is mostly employed to deal with redundant information, i.e., regions composed of pixels with similar intensity, it can also be used for tampering images by inserting or removing relevant objects. Therefore, detecting such a process is of extreme importance regarding the image security domain. However, recognizing seam-carved images does not represent a straightforward task even for human eyes, and robust computation tools capable of identifying such alterations are very desirable. In this paper, we propose an end-to-end approach to cope with the problem of automatic seam carving detection that can obtain state-of-the-art results. Experiments conducted over public and private datasets with several tampering configurations evidence the suitability of the proposed model.","",""
0,"S. Cawley","Hardware spiking neural network and remote FPGA lab",2013,"","","","",189,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,1,9,"College of Engineering and Informatics Electrical & Electronic Engineering Doctor of Philosophy Hardware Spiking Neural Network and Remote FPGA Lab by Seamus Cawley The automatic design of intelligent systems has been inspired by biology, specifically the operation of the human brain. Researchers hope to exploit and replicate the brain’s ability to adapt and self repair in order to develop robust and error tolerant embedded hardware computational devices. Spiking Neural Networks (SNNs) emulate neural behaviour observed in biology. This thesis describes the successful development of a Networkon-Chip based hardware SNN(EMBRACE-FPGA) and the supporting GA-based SNN training and application implementation tools (SNNDevSys). Hardware SNNs can be configured for multiple applications through programming of neuron spike firing threshold potentials, synaptic weights and the hardware SNN interconnection topology. This thesis describes the hardware SNN architecture and prototype and its application to a range of benchmark control and classification problems. This work has contributed to ongoing EMBRACE-FPGA architecture development within the Bio-Inspired and Reconfigurable Computing research group to improve practical hardware SNN scalability. Phase II of this thesis describes the development of the Remote FPGA Laboratory (RFL). In recent years there has been a growing interest in the development of webbased e-learning systems. The RFL is a web-based distance learning application which enables the teaching of digital systems design using real FPGA devices through a standard web browser. The RFL allows users to configure and interact with FPGA devices via the Internet as part of a combined training and evaluation framework. The system has been designed as an interactive learning tool, which aims to increase student interaction and understanding through a learn-by-doing approach. The system enables better understanding of the operation of digital systems through animation of internal signals, real-time timing diagrams and single-stepping of hardware circuits.","",""
5,"Kshitij Dwivedi, Radoslaw Martin Cichy, G. Roig","Unraveling Representations in Scene-selective Brain Regions Using Scene-Parsing Deep Neural Networks",2020,"","","","",190,"2022-07-13 10:07:51","","10.1162/jocn_a_01624","","",,,,,5,2.50,2,3,2,"Abstract Visual scene perception is mediated by a set of cortical regions that respond preferentially to images of scenes, including the occipital place area (OPA) and parahippocampal place area (PPA). However, the differential contribution of OPA and PPA to scene perception remains an open research question. In this study, we take a deep neural network (DNN)-based computational approach to investigate the differences in OPA and PPA function. In a first step, we search for a computational model that predicts fMRI responses to scenes in OPA and PPA well. We find that DNNs trained to predict scene components (e.g., wall, ceiling, floor) explain higher variance uniquely in OPA and PPA than a DNN trained to predict scene category (e.g., bathroom, kitchen, office). This result is robust across several DNN architectures. On this basis, we then determine whether particular scene components predicted by DNNs differentially account for unique variance in OPA and PPA. We find that variance in OPA responses uniquely explained by the navigation-related floor component is higher compared to the variance explained by the wall and ceiling components. In contrast, PPA responses are better explained by the combination of wall and floor, that is, scene components that together contain the structure and texture of the scene. This differential sensitivity to scene components suggests differential functions of OPA and PPA in scene processing. Moreover, our results further highlight the potential of the proposed computational approach as a general tool in the investigation of the neural basis of human scene perception.","",""
1,"Zihan Pan, Malu Zhang, Jibin Wu, Jiadong Wang, Haizhou Li","Multi-Tone Phase Coding of Interaural Time Difference for Sound Source Localization With Spiking Neural Networks",2021,"","","","",191,"2022-07-13 10:07:51","","10.1109/TASLP.2021.3100684","","",,,,,1,1.00,0,5,1,"Mammals exhibit remarkable capability of detecting and localizing sound sources in complex acoustic environments by using binaural cues in the spiking manner. Emulating the auditory process for sound source localization (SSL) by mammals, we propose a computational model for accurate and robust SSL under the neuromorphic spiking neural network (SNN) framework. The center of this model is a Multi-Tone Phase Coding (MTPC) scheme, which encodes the interaural time difference (ITD) between binaural pure tones into discriminative spike patterns that can be directly classified by SNNs. As such, SSL can be implemented as an event-driven task on highly efficient, neuromorphic parallel processors. We evaluate the proposed computational model on a directional audio dataset recorded from a microphone array in a realistic acoustic environment with background noise, obstruction, reflection, and other interferences. We report superior localization capability with a mean absolute error (MAE) of $1.02^\circ$ or 100% classification accuracy with an angle resolution of $5^\circ$, which surpasses other SNN-based biologically plausible neuromorphic approaches by a relatively large margin and on par with human performance in similar tasks. This study opens up many application opportunities in human-robot interaction where energy efficiency is crucial. As a case study, we successfully deploy the proposed SSL system in a robotic platform to track the speaker and orient the robot's attention.","",""
0,"Shome S. Das","A data-set and a method for pointing direction estimation from depth images for human-robot interaction and VR applications",2021,"","","","",192,"2022-07-13 10:07:51","","10.1109/ICRA48506.2021.9561143","","",,,,,0,0.00,0,1,1,"3D pointing devices are indispensable in virtual reality (hereafter VR) and human-robot interaction scenarios. Existing devices are cumbersome or non-immersive or have a limited volume of operation. Hand gesture-based interfaces do not suffer from these problems and can be used for 3D pointing purposes. However, there is a lack of robust, accurate hand gesture-based pointing techniques which can be attributed to the non-existence of large and accurate data-set for the same. To overcome this barrier, we propose a data-set consisting of depth images with a large number (107000) of samples collected from 11 subjects, with accurate ground-truth and adequate variation in the orientation and distance of the hand w.r.t. the camera. We propose a 3D convolutional neural network based technique that works on the proposed data-set and achieves an accuracy of 94.49% for an angle error threshold of 10 degrees. The proposed data-set may be used for developing more accurate, robust, less computationally expensive methods.","",""
0,"M. Younis, Masood Salik, S. T. Gul, Abdul Aleem","Automatic Human Facial Affect Classification Using Computational Intelligence Techniques",2021,"","","","",193,"2022-07-13 10:07:51","","10.1109/MAJICC53071.2021.9526237","","",,,,,0,0.00,0,4,1,"Automatic and robust emotion recognition is of prime concern while designing affect-sensitive machines for human-computer interaction to fully understand the emotional context implicated. This paper presents the development of a Decision Tree (DT) based scheme for the automatic classification of elementary human facial emotions. In order to implement affect classification algorithms, this research employs the Cohn-Kanade dataset based on facial features. Action units are mapped to emotion labels using the Facial Action Coding System (FACS); these labels serve as targets for supervised learning. The proposed scheme is validated through simulation, and results are compared to the existing techniques like Back Propagation Neural Network (BPNN) and Support Vector Machine (SVM). The presented results substantiate that the classification scheme suggested in this paper performs extensively superior than other alternatives in terms of accuracy with minimal computational efforts. Hence, DT based affect classification algorithm is an excellent alternative for applications comprising emotion recognition.","",""
112,"Zheng Yan, Jun Wang","Robust Model Predictive Control of Nonlinear Systems With Unmodeled Dynamics and Bounded Uncertainties Based on Neural Networks",2014,"","","","",194,"2022-07-13 10:07:51","","10.1109/TNNLS.2013.2275948","","",,,,,112,14.00,56,2,8,"This paper presents a neural network approach to robust model predictive control (MPC) for constrained discrete-time nonlinear systems with unmodeled dynamics affected by bounded uncertainties. The exact nonlinear model of underlying process is not precisely known, but a partially known nominal model is available. This partially known nonlinear model is first decomposed to an affine term plus an unknown high-order term via Jacobian linearization. The linearization residue combined with unmodeled dynamics is then modeled using an extreme learning machine via supervised learning. The minimax methodology is exploited to deal with bounded uncertainties. The minimax optimization problem is reformulated as a convex minimization problem and is iteratively solved by a two-layer recurrent neural network. The proposed neurodynamic approach to nonlinear MPC improves the computational efficiency and sheds a light for real-time implementability of MPC technology. Simulation results are provided to substantiate the effectiveness and characteristics of the proposed approach.","",""
23,"S. Jaiswal, G. Nandi","Robust real-time emotion detection system using CNN architecture",2019,"","","","",195,"2022-07-13 10:07:51","","10.1007/s00521-019-04564-4","","",,,,,23,7.67,12,2,3,"","",""
5,"H. Fang, Yi Zeng, Feifei Zhao","Brain Inspired Sequences Production by Spiking Neural Networks With Reward-Modulated STDP",2021,"","","","",196,"2022-07-13 10:07:51","","10.3389/fncom.2021.612041","","",,,,,5,5.00,2,3,1,"Understanding and producing embedded sequences according to supra-regular grammars in language has always been considered a high-level cognitive function of human beings, named “syntax barrier” between humans and animals. However, some neurologists recently showed that macaques could be trained to produce embedded sequences involving supra-regular grammars through a well-designed experiment paradigm. Via comparing macaques and preschool children's experimental results, they claimed that human uniqueness might only lie in the speed and learning strategy resulting from the chunking mechanism. Inspired by their research, we proposed a Brain-inspired Sequence Production Spiking Neural Network (SP-SNN) to model the same production process, followed by memory and learning mechanisms of the multi-brain region cooperation. After experimental verification, we demonstrated that SP-SNN could also handle embedded sequence production tasks, striding over the “syntax barrier.” SP-SNN used Population-Coding and STDP mechanism to realize working memory, Reward-Modulated STDP mechanism for acquiring supra-regular grammars. Therefore, SP-SNN needs to simultaneously coordinate short-term plasticity (STP) and long-term plasticity (LTP) mechanisms. Besides, we found that the chunking mechanism indeed makes a difference in improving our model's robustness. As far as we know, our work is the first one toward the “syntax barrier” in the SNN field, providing the computational foundation for further study of related underlying animals' neural mechanisms in the future.","",""
4,"Derek Y. Chan, D. Morris, T. Polascik, M. Palmeri, K. Nightingale","Deep Convolutional Neural Networks for Displacement Estimation in ARFI Imaging",2021,"","","","",197,"2022-07-13 10:07:51","","10.1109/TUFFC.2021.3068377","","",,,,,4,4.00,1,5,1,"Ultrasound elasticity imaging in soft tissue with acoustic radiation force requires the estimation of displacements, typically on the order of several microns, from serially acquired raw data A-lines. In this work, we implement a fully convolutional neural network (CNN) for ultrasound displacement estimation. We present a novel method for generating ultrasound training data, in which synthetic 3-D displacement volumes with a combination of randomly seeded ellipsoids are created and used to displace scatterers, from which simulated ultrasonic imaging is performed using Field II. Network performance was tested on these virtual displacement volumes, as well as an experimental ARFI phantom data set and a human in vivo prostate ARFI data set. In the simulated data, the proposed neural network performed comparably to Loupas’s algorithm, a conventional phase-based displacement estimation algorithm; the rms error was $0.62~\mu \text{m}$ for the CNN and 0.73 $\mu \text{m}$ for Loupas. Similarly, in the phantom data, the contrast-to-noise ratio (CNR) of a stiff inclusion was 2.27 for the CNN-estimated image and 2.21 for the Loupas-estimated image. Applying the trained network to in vivo data enabled the visualization of prostate cancer and prostate anatomy. The proposed training method provided 26 000 training cases, which allowed robust network training. The CNN had a computation time that was comparable to Loupas’s algorithm; further refinements to the network architecture may provide an improvement in the computation time. We conclude that deep neural network-based displacement estimation from ultrasonic data is feasible, providing comparable performance with respect to both accuracy and speed compared to current standard time-delay estimation approaches.","",""
5,"Cory Shain, I. Blank, Evelina Fedorenko, E. Gibson, William Schuler","Robust effects of working memory demand during naturalistic language comprehension in language-selective cortex",2021,"","","","",198,"2022-07-13 10:07:51","","10.1101/2021.09.18.460917","","",,,,,5,5.00,1,5,1,"A standard view of human language processing is that comprehenders build richly structured mental representations of natural language utterances, word by word, using computationally costly memory operations supported by domain-general working memory resources. However, three core claims of this view have been questioned, with some prior work arguing that (1) rich word-by-word structure building is not a core function of the language comprehension system, (2) apparent working memory costs are underlyingly driven by word predictability (surprisal), and/or (3) language comprehension relies primarily on domain-general rather than domain-specific working memory resources. In this work, we simultaneously evaluate all three of these claims using naturalistic comprehension in fMRI. In each participant, we functionally localize (a) a language-selective network and (b) a ‘multiple-demand’ network that supports working memory across domains, and we analyze the responses in these two networks of interest during naturalistic story listening with respect to a range of theory-driven predictors of working memory demand under rigorous surprisal controls. Results show robust surprisal-independent effects of word-by-word memory demand in the language network and no effect of working memory demand in the multiple demand network. Our findings thus support the view that language comprehension (1) entails word-by-word structure building using (2) computationally intensive memory operations that are not explained by surprisal. However, these results challenge (3) the domain-generality of the resources that support these operations, instead indicating that working memory operations for language comprehension are carried out by the same neural resources that store linguistic knowledge. Significance Statement This study uses fMRI to investigate signatures of working memory (WM) demand during naturalistic story listening, using a broad range of theoretically motivated estimates of WM demand. Results support a strong effect of WM demand in language-selective brain regions but no effect of WM demand in “multiple demand” regions that have previously been associated with WM in non-linguistic domains. We further show evidence that WM effects in language regions are distinct from effects of word predictability. Our findings support a core role for WM in incremental language processing, using WM resources that are specialized for language.","",""
0,"Nathan C L Kong","Are models trained on temporally-continuous data streams more adversarially robust?",2021,"","","","",199,"2022-07-13 10:07:51","","","","",,,,,0,0.00,0,1,1,"Task-optimized convolutional neural networks are the most quantitatively accurate models of the primate visual system. Unlike humans, however, these models can easily be fooled by modifying their inputs with human-imperceptible image perturbations, resulting in poor adversarial robustness . Prior work showed that modifying a model’s training objective or its architecture can improve its adversarial robustness. Another ingredient in building computational models of sensory cortex is the training dataset and, to our knowledge, its effect on a model’s adversarial robustness has not been investigated. Motivated by observations that newborn chicks ( Gallus gallus ) develop more invariant visual representations when reared with more temporally-continuous visual experience, we here evaluate a model’s adversarial robustness when it is trained on a more naturalistic dataset—a longitudinal video dataset collected from the perspective of infants (SAYCam; Sullivan et al., 2020). By evaluating the adversarial robustness of models on 26 -way classiﬁcation of a set of annotated video frames from this dataset, we ﬁnd that, across multiple objective functions, models that have been pre-trained on SAYCam video frames are more adversarially robust than those that have been pre-trained on ImageNet. Our results suggest that to build models that are more adversarially robust, additional efforts should be made in curating datasets that are more similar to the natural image sequences and the visual experience that infants receive.","",""
0,"M. Tamilselvi, S. Karthikeyan","Hybrid Framework for a Robust Face Recognition System Using EVB_CNN",2021,"","","","",200,"2022-07-13 10:07:51","","10.4018/jcit.20210701.oa4","","",,,,,0,0.00,0,2,1,"Recognition of the human face is becoming an ingenious technology that enhancing its strategy gradually by finding its applications in a wide variety of fields including security and surveillance. The traditional methods that are in practise for face recognition are not adequate in producing good accuracy due to two main reasons. The first one is the pictures are affected by various uncontrolled situations such as illumination, blur, and pose, and the second one is struggling in an efficient recognition when dealing with a large number of samples. There is need for an effective face recognition as a part of life in the automated environment. The traditional methods are lagging with some parameters. To overcome the aforementioned issues, a new methodology is implemented. This methodology is a hybrid frame work combined with Eigen value-based convolutional neural networks (EVB_CNN). The EVB_CNN is designed in such a way that the significant features are extracted and classified by the softmax function and fully connected layer, respectively. The experimental analysis is carried out with AR data set and ORL data set that shows enhancement in accuracy with significant reduction in computation time with images taken over specific uncontrolled environments.","",""
