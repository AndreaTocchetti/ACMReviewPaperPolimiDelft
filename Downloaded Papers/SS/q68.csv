Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
2,"Patrick McClure, D. Moraczewski, K. Lam, Adam Thomas, Francisco Pereira","Evaluating Adversarial Robustness for Deep Neural Network Interpretability using fMRI Decoding",2020,"","","","",1,"2022-07-13 09:28:50","","","","",,,,,2,1.00,0,5,2,"While deep neural networks (DNNs) are being increasingly used to make predictions from high-dimensional, complex data, they are widely seen as uninterpretable ""black boxes"", since it can be difficult to discover what input information is used to make predictions. This ability is particularly important for applications in cognitive neuroscience and neuroinformatics. A saliency map is a common approach for producing interpretable visualizations of the relative importance of input features for a prediction. However, many methods for creating these maps fail due to focusing too much on the input or being extremely sensitive to small input noise. It is also challenging to quantitatively evaluate how well saliency maps correspond to the truly relevant input information. In this paper, we develop two quantitative evaluation procedures for saliency methods, using the fact that the Human Connectome Project (HCP) dataset contains functional magnetic resonance imaging (fMRI) data from multiple tasks per subject to create ground truth saliency maps. We then introduce an adversarial training method that makes DNNs robust to small input noise, and demonstrate that it measurably improves interpretability.","",""
9,"Adam Noack, Isaac Ahern, D. Dou, Boyang Li","An Empirical Study on the Relation Between Network Interpretability and Adversarial Robustness",2020,"","","","",2,"2022-07-13 09:28:50","","10.1007/s42979-020-00390-x","","",,,,,9,4.50,2,4,2,"","",""
22,"Akhilan Boopathy, Sijia Liu, Gaoyuan Zhang, Cynthia Liu, Pin-Yu Chen, Shiyu Chang, L. Daniel","Proper Network Interpretability Helps Adversarial Robustness in Classification",2020,"","","","",3,"2022-07-13 09:28:50","","","","",,,,,22,11.00,3,7,2,"Recent works have empirically shown that there exist adversarial examples that can be hidden from neural network interpretability (namely, making network interpretation maps visually similar), or interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show that with a proper measurement of interpretation, it is actually difficult to prevent prediction-evasion adversarial attacks from causing interpretation discrepancy, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop an interpretability-aware defensive scheme built only on promoting robust interpretation (without the need for resorting to adversarial loss minimization). We show that our defense achieves both robust classification and robust interpretation, outperforming state-of-the-art adversarial training methods against attacks of large perturbation in particular.","",""
0,"R. Ramos, Patr'icia Pereira, Helena Moniz, J. P. Carvalho, Bruno Martins","Retrieval Augmentation to Improve Robustness and Interpretability of Deep Neural Networks",2021,"","","","",4,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,5,1,"Deep neural network models have achieved state-ofthe-art results in various tasks related to vision and/or language. Despite the use of large training data, most models are trained by iterating over single input-output pairs, discarding the remaining examples for the current prediction. In this work, we actively exploit the training data to improve the robustness and interpretability of deep neural networks, using the information from nearest training examples to aid the prediction both during training and testing. Specifically, the proposed approach uses the target of the nearest input example to initialize the memory state of an LSTM model or to guide attention mechanisms. We apply this approach to image captioning and sentiment analysis, conducting experiments with both image and text retrieval. Results show the effectiveness of the proposed models for the two tasks, on the widely used Flickr8 and IMDB datasets, respectively. Our code is publicly available1.","",""
0,"Ben Zhang, Zhetong Dong, Junsong Zhang, Hongwei Lin","Functional Network: A Novel Framework for Interpretability of Deep Neural Networks",2022,"","","","",5,"2022-07-13 09:28:50","","10.48550/arXiv.2205.11702","","",,,,,0,0.00,0,4,1,"The layered structure of deep neural networks hinders the use of numerous analysis tools and thus the development of its interpretability. Inspired by the success of functional brain networks, we propose a novel framework for interpretability of deep neural networks, that is, the functional network. We construct the functional network of fully connected networks and explore its small-worldness. In our experiments, the mechanisms of regularization methods, namely, batch normalization and dropout, are revealed using graph theoretical analysis and topological data analysis. Our empirical analysis shows the following: (1) Batch normalization enhances model performance by increasing the global eﬃciency and the number of loops but reduces adversarial robustness by lowering the fault tolerance. (2) Dropout improves generalization and robustness of models by improving the functional specialization and fault tolerance. (3) The models with diﬀerent regularizations can be clustered correctly according to their functional topological diﬀerences, reﬂecting the great potential of the functional network and topological data analysis in interpretability.","",""
0,"Henry Eigen, Amir Sadovnik","TopKConv: Increased Adversarial Robustness Through Deeper Interpretability",2021,"","","","",6,"2022-07-13 09:28:50","","10.1109/ICMLA52953.2021.00011","","",,,,,0,0.00,0,2,1,"Vulnerability to adversarial inputs remains an issue for deep neural networks. Attackers can slightly modify inputs in order to cause adverse behavior in otherwise highly accurate networks. In addition to making these networks less secure for real world applications, this also emphasizes a misalignment between the features the network uses to make decisions and the ones humans use. In this work we propose that more interpretable networks should yield more robust ones since they are able to rely on features that are more understandable to humans. More specifically, we take inspiration from interpretability based approaches to adversarial robustness, and propose a sparsity based defense to counter the impact of overparameterization on adversarial vulnerability. Building off of the work of the Dynamic-K algorithm, which introduces dynamic routing to fully connected layers in order to encourage sparse, interpretable predictions, we propose TopKConv, a novel method of reducing the number of activation channels used to construct each convolutional feature map. The incorporation of TopKConv alongside Dynamic-k results in a significant increase in adversarial accuracy at no cost to benign accuracy. Further, this is achieved with no fine tuning of or adversarial training.","",""
1,"Haonan Sun, Luqi Liang, Chunlin Wang, Yi Wu, Fei Yang, M. Rong","Prediction of the Electrical Strength and Boiling Temperature of the Substitutes for Greenhouse Gas SF₆ Using Neural Network and Random Forest",2020,"","","","",7,"2022-07-13 09:28:50","","10.1109/ACCESS.2020.3004519","","",,,,,1,0.50,0,6,2,"Finding substitutes for sulfur hexafluoride (SF6), a gas with extremely high global warming potential, has been a persistent effort for years in the field of high voltage power equipment, which focuses on the evaluation of the electrical strength and boiling temperature for the practical purpose. Following up the previous proposed linear regression models, this work introduces machine learning algorithms including artificial neural network (ANN) and random forest (RF) as the potential approaches to predict the electrical strength and boiling temperature. Based on a series of descriptors derived from the molecular structure of 74 molecules, the performance of three different methods: multiple linear regression, artificial neural network and random forest are compared and assessed in terms of the sensitivity to the sample size, prediction accuracy and stability, and the interpretability of predictors. Considering the available data are limited, random forest shows superior performance with higher robustness and efficiency. The same approaches were applied to the boiling temperature and random forest produced better results as well. Besides, the variable importance ranked by RF improves understanding of the correlation between the molecular properties and electrical strength. It provides important insights to analyze the properties of the SF6 substitutes during the design and synthesis of the new eco-friendly gases in power equipment.","",""
1,"Federico Amato, Fabian Guignard, P. Jacquet, M. Kanevski","On Feature Selection Using Anisotropic General Regression Neural Network",2020,"","","","",8,"2022-07-13 09:28:50","","","","",,,,,1,0.50,0,4,2,"The presence of irrelevant features in the input dataset tends to reduce the interpretability and predictive quality of machine learning models. Therefore, the development of feature selection methods to recognize irrelevant features is a crucial topic in machine learning. Here we show how the General Regression Neural Network used with an anisotropic Gaussian Kernel can be used to perform feature selection. A number of numerical experiments are conducted using simulated data to study the robustness of the proposed methodology and its sensitivity to sample size. Finally, a comparison with four other feature selection methods is performed on several real world datasets.","",""
4,"Theodoros Tsiligkaridis, Jay Roberts","Second Order Optimization for Adversarial Robustness and Interpretability",2020,"","","","",9,"2022-07-13 09:28:50","","","","",,,,,4,2.00,2,2,2,"Deep neural networks are easily fooled by small perturbations known as adversarial attacks. Adversarial Training (AT) is a technique aimed at learning features robust to such attacks and is widely regarded as a very effective defense. However, the computational cost of such training can be prohibitive as the network size and input dimensions grow. Inspired by the relationship between robustness and curvature, we propose a novel regularizer which incorporates first and second order information via a quadratic approximation to the adversarial loss. The worst case quadratic loss is approximated via an iterative scheme. It is shown that using only a single iteration in our regularizer achieves stronger robustness than prior gradient and curvature regularization schemes, avoids gradient obfuscation, and, with additional iterations, achieves strong robustness with significantly lower training time than AT. Further, it retains the interesting facet of AT that networks learn features which are well-aligned with human perception. We demonstrate experimentally that our method produces higher quality human-interpretable features than other geometric regularization techniques. These robust features are then used to provide human-friendly explanations to model predictions.","",""
0,"Ben Qi, Liguo Zhang, Jin'gang Liang, J. Tong","Combinatorial Techniques for Fault Diagnosis in Nuclear Power Plants Based on Bayesian Neural Network and Simplified Bayesian Network-Artificial Neural Network",2022,"","","","",10,"2022-07-13 09:28:50","","10.3389/fenrg.2022.920194","","",,,,,0,0.00,0,4,1,"Knowledge-driven and data-driven methods are the two representative categories of intelligent technologies used in fault diagnosis in nuclear power plants. Knowledge-driven methods have advantages in interpretability and robustness, while data-driven methods have better performance in ease of modeling and inference efficiency. Given the complementarity of the two methods, a combination of them is a worthwhile investigation. In this work, we introduce two new techniques based on Bayesian theory (knowledge-driven) and artificial neural network (data-driven) for fault diagnosis in nuclear power plants. The first approach exploits an integrated technique, Bayesian Neural Network (BNN), which introduces Bayesian theory into the neural network to provide confidence in diagnosis. The second approach, denoted as Simplified Bayesian Network-Artificial Neural Network (SBN-ANN), adopts a hierarchical diagnosis idea, which firstly uses a simplified Bayesian network to diagnose fault types and then a neural network to diagnose the severity of faults. The two new techniques are implemented and verified with simulated faults data of a typical pressurized water reactor. Compared with single-algorithmic diagnostic approaches such as Bayesian network and neural network, the new combinatorial techniques show better performance in diagnostic precision. The results suggest the feasibility to develop the data and knowledge dual-drive technologies for fault diagnosis.","",""
13,"S. Saralajew, Lars Holdijk, Maike Rees, T. Villmann","Prototype-based Neural Network Layers: Incorporating Vector Quantization",2018,"","","","",11,"2022-07-13 09:28:50","","","","",,,,,13,3.25,3,4,4,"Neural networks currently dominate the machine learning community and they do so for good reasons. Their accuracy on complex tasks such as image classification is unrivaled at the moment and with recent improvements they are reasonably easy to train. Nevertheless, neural networks are lacking robustness and interpretability. Prototype-based vector quantization methods on the other hand are known for being robust and interpretable. For this reason, we propose techniques and strategies to merge both approaches. This contribution will particularly highlight the similarities between them and outline how to construct a prototype-based classification layer for multilayer networks. Additionally, we provide an alternative, prototype-based, approach to the classical convolution operation. Numerical results are not part of this report, instead the focus lays on establishing a strong theoretical framework. By publishing our framework and the respective theoretical considerations and justifications before finalizing our numerical experiments we hope to jump-start the incorporation of prototype-based learning in neural networks and vice versa.","",""
1,"Shaojie Xu, J. Vaughan, Jie Chen, Aijun Zhang, A. Sudjianto","Traversing the Local Polytopes of ReLU Neural Networks: A Unified Approach for Network Verification",2021,"","","","",12,"2022-07-13 09:28:50","","","","",,,,,1,1.00,0,5,1,"Although neural networks (NNs) with ReLU activation functions have found success in a wide range of applications, their adoption in risk-sensitive settings has been limited by the concerns on robustness and interpretability. Previous works to examine robustness and to improve interpretability partially exploited the piecewise linear function form of ReLU NNs. In this paper, we explore the unique topological structure that ReLU NNs create in the input space, identifying the adjacency among the partitioned local polytopes and developing a traversing algorithm based on this adjacency. Our polytope traversing algorithm can be adapted to verify a wide range of network properties related to robustness and interpretability, providing an unified approach to examine the network behavior. As the traversing algorithm explicitly visits all local polytopes, it returns a clear and full picture of the network behavior within the traversed region. The time and space complexity of the traversing algorithm is determined by the number of a ReLU NN’s partitioning hyperplanes passing through the traversing region.","",""
1,"F. Santos, C. Zanchettin, L. Matos, P. Novais","On the Impact of Interpretability Methods in Active Image Augmentation Method",2021,"","","","",13,"2022-07-13 09:28:50","","10.1093/jigpal/jzab006","","",,,,,1,1.00,0,4,1,"Robustness is a significant constraint in machine learning models. The performance of the algorithms must not deteriorate when training and testing with slightly different data. Deep neural network models achieve awe-inspiring results in a wide range of applications of computer vision. Still, in the presence of noise or region occlusion, some models exhibit inaccurate performance even with data handled in training. Besides, some experiments suggest deep learning models sometimes use incorrect parts of the input information to perform inference. Activate Image Augmentation (ADA) is an augmentation method that uses interpretability methods to augment the training data and improve its robustness to face the described problems. Although ADA presented interesting results, its original version only used the Vanilla Backpropagation interpretability to train the U-Net model. In this work, we propose an extensive experimental analysis of the interpretability method’s impact on ADA. We use five interpretability methods: Vanilla Backpropagation, Guided Backpropagation, GradCam, Guided GradCam, and InputXGradient. The results show that all methods achieve similar performance at the ending of training, but when combining ADA with GradCam, the U-Net model presented an impressive fast convergence.","",""
0,"Lingkun Kong, Dewang Chen, Ruijun Cheng","WRNFS: Width Residual Neuro Fuzzy System, a Fast-Learning Algorithm with High Interpretability",2022,"","","","",14,"2022-07-13 09:28:50","","10.3390/app12125810","","",,,,,0,0.00,0,3,1,"Although the deep neural network has a strong fitting ability, it is difficult to be applied to safety-critical fields because of its poor interpretability. Based on the adaptive neuro-fuzzy inference system (ANFIS) and the concept of residual network, a width residual neuro-fuzzy system (WRNFS) is proposed to improve the interpretability performance in this paper. WRNFS is used to transform a regression problem of high-dimensional data into the sum of several low-dimensional neuro-fuzzy systems. The ANFIS model in the next layer is established based on the low dimensional data and the residual of the ANFIS model in the former layer. The performance of WRNFS is compared with traditional ANFIS on three data sets. The results showed that WRNFS has high interpretability (fewer layers, fewer fuzzy rules, and fewer adjustable parameters) on the premise of satisfying the fitting accuracy. The interpretability, complexity, time efficiency, and robustness of WRNFS are greatly improved when the input number of single low-dimensional systems decreases.","",""
0,"Phong Le, W. Zuidema","DoLFIn: Distributions over Latent Features for Interpretability",2020,"","","","",15,"2022-07-13 09:28:50","","10.18653/V1/2020.COLING-MAIN.127","","",,,,,0,0.00,0,2,2,"Interpreting the inner workings of neural models is a key step in ensuring the robustness and trustworthiness of the models, but work on neural network interpretability typically faces a trade-off: either the models are too constrained to be very useful, or the solutions found by the models are too complex to interpret. We propose a novel strategy for achieving interpretability that – in our experiments – avoids this trade-off. Our approach builds on the success of using probability as the central quantity, such as for instance within the attention mechanism. In our architecture, DoLFIn (Distributions over Latent Features for Interpretability), we do no determine beforehand what each feature represents, and features go altogether into an unordered set. Each feature has an associated probability ranging from 0 to 1, weighing its importance for further processing. We show that, unlike attention and saliency map approaches, this set-up makes it straight-forward to compute the probability with which an input component supports the decision the neural model makes. To demonstrate the usefulness of the approach, we apply DoLFIn to text classification, and show that DoLFIn not only provides interpretable solutions, but even slightly outperforms the classical CNN and BiLSTM text classifiers on the SST2 and AG-news datasets.","",""
3,"Hao Shen, Sihong Chen, Ran Wang","A study on the uncertainty of convolutional layers in deep neural networks",2020,"","","","",16,"2022-07-13 09:28:50","","10.1007/S13042-021-01278-9","","",,,,,3,1.50,1,3,2,"","",""
15,"V. Couteaux, O. Nempont, G. Pizaine, I. Bloch","Towards Interpretability of Segmentation Networks by Analyzing DeepDreams",2019,"","","","",17,"2022-07-13 09:28:50","","10.1007/978-3-030-33850-3_7","","",,,,,15,5.00,4,4,3,"","",""
0,"Mohammadreza Amirian, Lukas Tuggener, R. Chavarriaga, Y. Satyawan, F. Schilling, F. Schwenker, Thilo Stadelmann","Two to trust : AutoML for safe modelling and interpretable deep learning for robustness",2020,"","","","",18,"2022-07-13 09:28:50","","10.21256/ZHAW-20217","","",,,,,0,0.00,0,7,2,"With great power comes great responsibility. The success of machine learning, especially deep learning, in research and practice has attracted a great deal of interest, which in turn necessitates increased trust. Sources of mistrust include matters of model genesis (“Is this really the appropriate model?”) and interpretability (“Why did the model come to this conclusion?”, “Is the model safe from being easily fooled by adversaries?”). In this paper, two partners for the trustworthiness tango are presented: recent advances and ideas, as well as practical applications in industry in (a) Automated machine learning (AutoML), a powerful tool to optimize deep neural network architectures and finetune hyperparameters, which promises to build models in a safer and more comprehensive way; (b) Interpretability of neural network outputs, which addresses the vital question regarding the reasoning behind model predictions and provides insights to improve robustness against adversarial attacks.","",""
0,"","VISUAL INTERPRETABILITY ALONE HELPS ADVER-",2019,"","","","",19,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,0,3,"Recent works have empirically shown that there exist adversarial examples that can be hidden from neural network interpretability, and interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.","",""
4,"Kenji Suzuki, M. Reyes, T. Syeda-Mahmood","Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support: Second International Workshop, iMIMIC 2019, and 9th International Workshop, ML-CDS 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 17, 2019, Proceedings",2019,"","","","",20,"2022-07-13 09:28:50","","10.1007/978-3-030-33850-3","","",,,,,4,1.33,1,3,3,"","",""
7,"Vasisht Duddu, N. Pillai, D. V. Rao, V. Balas","Fault Tolerance of Neural Networks in Adversarial Settings",2019,"","","","",21,"2022-07-13 09:28:50","","10.3233/JIFS-179677","","",,,,,7,2.33,2,4,3,"Artificial Intelligence systems require a through assessment of different pillars of trust, namely, fairness, interpretability, data and model privacy, reliability (safety) and robustness against against adversarial attacks. While these research problems have been extensively studied in isolation, an understanding of the trade-off between different pillars of trust is lacking. To this extent, the trade-off between fault tolerance, privacy and adversarial robustness is evaluated for the specific case of Deep Neural Networks, by considering two adversarial settings under a security and a privacy threat model. Specifically, this work studies the impact of the fault tolerance of the Neural Network on training the model by adding noise to the input (Adversarial Robustness) and noise to the gradients (Differential Privacy). While training models with noise to inputs, gradients or weights enhances fault tolerance, it is observed that adversarial robustness and fault tolerance are at odds with each other. On the other hand, ($\epsilon,\delta$)-Differentially Private models enhance the fault tolerance, measured using generalisation error, theoretically has an upper bound of $e^{\epsilon} - 1 + \delta$. This novel study of the trade-off between different elements of trust is pivotal for training a model which satisfies the requirements for different pillars of trust simultaneously.","",""
0,"Suraj Srinivas, Kyle Matoba, Himabindu Lakkaraju, F. Fleuret","Flatten the Curve: Efficiently Training Low-Curvature Neural Networks",2022,"","","","",22,"2022-07-13 09:28:50","","10.48550/arXiv.2206.07144","","",,,,,0,0.00,0,4,1,"The highly non-linear nature of deep neural networks causes them to be susceptible to adversarial examples and have unstable gradients which hinders interpretability. However, existing methods to solve these issues, such as adversarial training, are expensive and often sacrifice predictive accuracy. In this work, we consider curvature, which is a mathematical quantity which encodes the degree of non-linearity. Using this, we demonstrate low-curvature neural networks (LCNNs) that obtain drastically lower curvature than standard models while exhibiting similar predictive performance, which leads to improved robustness and stable gradients, with only a marginally increased training time. To achieve this, we minimize a data-independent upper bound on the curvature of a neural network, which decomposes overall curvature in terms of curvatures and slopes of its constituent layers. To efficiently minimize this bound, we introduce two novel architectural components: first, a non-linearity called centered-softplus that is a stable variant of the softplus non-linearity, and second, a Lipschitzconstrained batch normalization layer. Our experiments show that LCNNs have lower curvature, more stable gradients and increased off-the-shelf adversarial robustness when compared to their standard high-curvature counterparts, all without affecting predictive performance. Our approach is easy to use and can be readily incorporated into existing neural network models.","",""
11,"Ming Jin, Heng Chang, Wenwu Zhu, S. Sojoudi","Power up! Robust Graph Convolutional Network via Graph Powering",2019,"","","","",23,"2022-07-13 09:28:50","","","","",,,,,11,3.67,3,4,3,"Graph convolutional networks (GCNs) are powerful tools for graph-structured data. However, they have been recently shown to be vulnerable to topological attacks. To enhance adversarial robustness, we go beyond spectral graph theory to robust graph theory. By challenging the classical graph Laplacian, we propose a new convolution operator that is provably robust in the spectral domain and is incorporated in the GCN architecture to improve expressivity and interpretability. By extending the original graph to a sequence of graphs, we also propose a robust training paradigm that encourages transferability across graphs that span a range of spatial and spectral characteristics. The proposed approaches are demonstrated in extensive experiments to simultaneously improve performance in both benign and adversarial situations. Introduction Graph convolutional networks (GCNs) are powerful extensions of convolutional neural networks (CNN) to graphstructured data. Recently, GCNs and variants have been applied to a wide range of domains, achieving state-of-the-art performances in social networks (Kipf and Welling 2017), traffic prediction (Rahimi, Cohn, and Baldwin 2018), recommendation systems (Ying et al. 2018), applied chemistry and biology (Kearnes et al. 2016; Fout et al. 2017), and natural language processing (Atwood and Towsley 2016; Hamilton, Ying, and Leskovec 2017; Bastings et al. 2017; Marcheggiani and Titov 2017), just to name a few (Zhou et al. 2018; Wu et al. 2019). GCNs belong to a family of spectral methods that deal with spectral representations of graphs (Zhou et al. 2018; Wu et al. 2019). A fundamental ingredient of GCNs is the graph convolution operation defined by the graph Laplacian in the Fourier domain: gθ ? x := ĝθ(L)x, (1) where x ∈ R is the graph signal on the set of vertices V and ĝθ is a spectral function applied to the graph Laplacian L := D − A (where D and A are the degree matrix and *The two first authors made equal contributions. This work was conducted during Heng Chang’s visit to Professor Somayeh Sojoudi’s group at UC Berkeley. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. the adjacency matrix, respectively). Because this operation is computational intensive for large graphs and non-spatially localized (Bruna et al. 2014), early attempts relied on a parameterization with smooth coefficients (Henaff, Bruna, and LeCun 2015) or a truncated expansion in terms of of Chebyshev polynomials (Hammond, Vandergheynst, and Gribonval 2011). By further restricting the Chebyshev polynomial order by 2, the approach in (Kipf and Welling 2017) referred henceforth as the vanilla GCN pushed the state-of-the-art performance of semi-supervised learning. The network has the following layer-wise update rule: H := ψ ( AHW (l) ) , (2) where H is the l-th layer hidden state (with H := X as nodal features),W (l) is the l-th layer weight matrix, ψ is the usual point-wise activation function, and A is the convolution operator chosen to be the degree weighted Laplacian with some slight modifications (Kipf and Welling 2017). Subsequent GCN variants have different architectures, but they all share the use of the Laplacian matrix as the convolution operator (Zhou et al. 2018; Wu et al. 2019). Why Not Graph Laplacian? Undoubtedly, the Laplacian operator (and its variants, e.g., normalized/powered Laplacian) plays a central role in spectral theory, and is a natural choice for a variety of spectral algorithms such as principal component analysis, clustering and linear embeddings (Chung and Graham 1997; Belkin and Niyogi 2002). So what can be problematic? From a spatial perspective, GCNs with d layers cannot acquire nodal information beyond its d-distance neighbors; hence, it severely limits its scope of data fusion. Recent works (Lee et al. 2018; Abu-El-Haija et al. 2018, 2019; Wu et al. 2019) alleviated this issue by directly powering the graph Laplacian. From a spectral perspective, one could demand better spectral properties, given that GCN is fundamentally a particular (yet effective) approximation of the spectral convolution (1). A key desirable property for generic spectral methods is known as “spectral separation,” namely the spectrum should comprise a few dominant eigenvalues whose associated eigenvectors reveal the sought structure in the graph. A well-known prototype is the Ramanujan property, for which the second The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)","",""
1,"A. Alhazmi, W. Zhang, Quan Z. Sheng, Abdulwahab Aljubairy","Analyzing the Sensitivity of Deep Neural Networks for Sentiment Analysis: A Scoring Approach",2020,"","","","",24,"2022-07-13 09:28:50","","10.1109/IJCNN48605.2020.9207000","","",,,,,1,0.50,0,4,2,"Deep Neural Networks (DNNs) have gained significant popularity in various Natural Language Processing tasks. However, the lack of interpretability of DNNs induces challenges to evaluate the robustness of DNNs. In this paper, we particularly focus on DNNs on sentiment analysis and conduct an empirical investigation on the sensitivity of DNNs. Specifically, we apply a scoring function to rank words importance without depending on the parameters or structure of the deep neural model. Then, we scan characteristics of these words to identify the model’s weakness and perturb words to craft targeted attacks that exploit this weakness. We conduct extensive experiments on different neural network models across several real-world datasets. We report four intriguing findings: i) modern deep learning models for sentiment analysis ignore important sentiment terms such as opinion adjectives (i.e., amazing or terrible), ii) adjective words contribute to fooling sentiment analysis models more than other Parts-of-Speech (POS) categories, iii) changing or removing up to 10 adjectives words in a review text only decreases the accuracy up to 2%, and iv) modern models are unable to recognize the difference between an objective and a subjective review text1.","",""
1,"S. Booth, Ankit J. Shah, Yilun Zhou, J. Shah","Sampling Prediction-Matching Examples in Neural Networks: A Probabilistic Programming Approach",2020,"","","","",25,"2022-07-13 09:28:50","","","","",,,,,1,0.50,0,4,2,"Though neural network models demonstrate impressive performance, we do not understand exactly how these black-box models make individual predictions. This drawback has led to substantial research devoted to understand these models in areas such as robustness, interpretability, and generalization ability. In this paper, we consider the problem of exploring the prediction level sets of a classifier using probabilistic programming. We define a prediction level set to be the set of examples for which the predictor has the same specified prediction confidence with respect to some arbitrary data distribution. Notably, our sampling-based method does not require the classifier to be differentiable, making it compatible with arbitrary classifiers. As a specific instantiation, if we take the classifier to be a neural network and the data distribution to be that of the training data, we can obtain examples that will result in specified predictions by the neural network. We demonstrate this technique with experiments on a synthetic dataset and MNIST. Such level sets in classification may facilitate human understanding of classification behaviors.","",""
40,"Patrick Esser, Robin Rombach, B. Ommer","A Disentangling Invertible Interpretation Network for Explaining Latent Representations",2020,"","","","",26,"2022-07-13 09:28:50","","10.1109/cvpr42600.2020.00924","","",,,,,40,20.00,13,3,2,"Neural networks have greatly boosted performance in computer vision by learning powerful representations of input data. The drawback of end-to-end training for maximal overall performance are black-box models whose hidden representations are lacking interpretability: Since distributed coding is optimal for latent layers to improve their robustness, attributing meaning to parts of a hidden feature vector or to individual neurons is hindered. We formulate interpretation as a translation of hidden representations onto semantic concepts that are comprehensible to the user. The mapping between both domains has to be bijective so that semantic modifications in the target domain correctly alter the original representation. The proposed invertible interpretation network can be transparently applied on top of existing architectures with no need to modify or retrain them. Consequently, we translate an original representation to an equivalent yet interpretable one and backwards without affecting the expressiveness and performance of the original. The invertible interpretation network disentangles the hidden representation into separate, semantically meaningful concepts. Moreover, we present an efficient approach to define semantic concepts by only sketching two images and also an unsupervised strategy. Experimental evaluation demonstrates the wide applicability to interpretation of existing classification and image generation networks as well as to semantically guided image manipulation.","",""
0,"William Knauth","The Self-Simplifying Machine: Exploiting the Structure of Piecewise Linear Neural Networks to Create Interpretable Models",2020,"","","","",27,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,1,2,"Today, it is more important than ever before for users to have trust in the models they use. As Machine Learning models fall under increased regulatory scrutiny and begin to see more applications in high-stakes situations, it becomes critical to explain our models. Piecewise Linear Neural Networks (PLNN) with the ReLU activation function have quickly become extremely popular models due to many appealing properties; however, they still present many challenges in the areas of robustness and interpretation. To this end, we introduce novel methodology toward simplification and increased interpretability of Piecewise Linear Neural Networks for classification tasks. Our methods include the use of a trained, deep network to produce a well-performing, single-hidden-layer network without further stochastic training, in addition to an algorithm to reduce flat networks to a smaller, more interpretable size with minimal loss in performance. On these methods, we conduct preliminary studies of model performance, as well as a case study on Wells Fargo's Home Lending dataset, together with visual model interpretation.","",""
13,"Wei Zhu, Qiang Qiu, A. Calderbank, G. Sapiro, Xiuyuan Cheng","Scale-Equivariant Neural Networks with Decomposed Convolutional Filters",2019,"","","","",28,"2022-07-13 09:28:50","","","","",,,,,13,4.33,3,5,3,"Encoding the input scale information explicitly into the representation learned by a convolutional neural network (CNN) is beneficial for many vision tasks especially when dealing with multiscale input signals. We study, in this paper, a scale-equivariant CNN architecture with joint convolutions across the space and the scaling group, which is shown to be both sufficient and necessary to achieve scale-equivariant representations. To reduce the model complexity and computational burden, we decompose the convolutional filters under two pre-fixed separable bases and truncate the expansion to low-frequency components. A further benefit of the truncated filter expansion is the improved deformation robustness of the equivariant representation. Numerical experiments demonstrate that the proposed scale-equivariant neural network with decomposed convolutional filters (ScDCFNet) achieves significantly improved performance in multiscale image classification and better interpretability than regular CNNs at a reduced model size.","",""
0,"Zhongfan Jia, Chenglong Bao, Kaisheng Ma","Exploring Frequency Domain Interpretation of Convolutional Neural Networks",2019,"","","","",29,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,3,3,"Many existing interpretation methods of convolutional neural networks (CNNs) mainly analyze in spatial domain, yet model interpretability in frequency domain has been rarely studied. To the best of our knowledge, there is no study on the interpretation of modern CNNs from the perspective of the frequency proportion of filters. In this work, we analyze the frequency properties of filters in the first layer as it is the entrance of information and relatively more convenient for analysis. By controlling the proportion of different frequency filters in the training stage, the network classification accuracy and model robustness is evaluated and our results reveal that it has a great impact on the robustness to common corruptions. Moreover, a learnable modulation of frequency proportion with perturbation in power spectrum is proposed from the perspective of frequency domain. Experiments on CIFAR-10-C show 10.97% average robustness gains for ResNet-18 with negligible natural accuracy degradation.","",""
60,"L. Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari","Implicit Deep Learning",2019,"","","","",30,"2022-07-13 09:28:50","","10.1137/20M1358517","","",,,,,60,20.00,15,4,3,"Implicit deep learning prediction rules generalize the recursive rules of feedforward neural networks. Such rules are based on the solution of a fixed-point equation involving a single vector of hidden features, which is thus only implicitly defined. The implicit framework greatly simplifies the notation of deep learning, and opens up many new possibilities, in terms of novel architectures and algorithms, robustness analysis and design, interpretability, sparsity, and network architecture optimization.","",""
3,"O. Intrator, N. Intrator","Robust Interpretation of Neural-Network Models",1997,"","","","",31,"2022-07-13 09:28:50","","","","",,,,,3,0.12,2,2,25,"Na than Intrator Tel-Aviv University and Brown University nin©cns.brown.edu Artificial Neural Network seem very promising for regression and classification, especially for large covariate spaces. These methods represent a non-linear function as a composition of low dimensional ridge functions and therefore appear to be less sensitive to the dimensionality of the covariate space. However, due to non uniqueness of a global minimum and the existence of (possibly) many local minima, the model revealed by the network is non stable. We introduce a method to interpret neural network results which uses novel robustification techniques. This results in a robust interpretation of the model employed by the network. Simulated data from known models is used to demonstrate the interpretability results and to demonstrate the effects of different regularization methods on the robustness of the model. Graphical methods are introduced to present the interpretation results. We further demonstrate how interaction between covariates can be revealed. From this study we conclude that the interpretation method works well, but that NN models may sometimes be misinterpreted, especially if the approximations to the true model are less robust.","",""
408,"Drew A. Hudson, Christopher D. Manning","Compositional Attention Networks for Machine Reasoning",2018,"","","","",32,"2022-07-13 09:28:50","","","","",,,,,408,102.00,204,2,4,"We present the MAC network, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. MAC moves away from monolithic black-box neural architectures towards a design that encourages both transparency and versatility. The model approaches problems by decomposing them into a series of attention-based reasoning steps, each performed by a novel recurrent Memory, Attention, and Composition (MAC) cell that maintains a separation between control and memory. By stringing the cells together and imposing structural constraints that regulate their interaction, MAC effectively learns to perform iterative reasoning processes that are directly inferred from the data in an end-to-end approach. We demonstrate the model's strength, robustness and interpretability on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the model is computationally-efficient and data-efficient, in particular requiring 5x less data than existing models to achieve strong results.","",""
3,"Javier Viaña, Kelly Cohen","Extension to Multidimensional Problems of a Fuzzy- based Explainable & Noise-Resilient Algorithm",2021,"","","","",33,"2022-07-13 09:28:50","","","","",,,,,3,3.00,2,2,1,"While Deep Neural Networks (DNNs) have shown incredible performance in a variety of data, they are brittle and opaque: easily fooled by the presence of noise, and difficult to understand the underlying reasoning for their predictions or choices. This focus on accuracy at the expense of interpretability and robustness caused little concern since, until recently, DNNs were employed primarily for scientific and limited commercial work. An increasing, widespread use of artificial intelligence and growing emphasis on user data protections, however, motivates the need for robust solutions with explainable methods and results. In this work, we extend a novel fuzzy based algorithm for regression to multidimensional problems. Previous research demonstrated that this approach outperforms neural network benchmarks while using only 5% of the number of the parameters.","",""
4,"Chengping Rao, Hao Sun, Yang Liu","Embedding Physics to Learn Spatiotemporal Dynamics from Sparse Data",2021,"","","","",34,"2022-07-13 09:28:50","","","","",,,,,4,4.00,1,3,1,"Modeling nonlinear spatiotemporal dynamical systems has primarily relied on partial differential equations (PDEs) that are typically derived from first principles. However, the explicit formulation of PDEs for many underexplored processes, such as climate systems, biochemical reaction and epidemiology, remains uncertain or partially unknown, where very sparse measurement data is yet available. To tackle this challenge, we propose a novel deep learning architecture that forcibly embedded known physics knowledge in a residual-recurrent Π-block network, to facilitate the learning of the spatiotemporal dynamics in a data-driven manner. The coercive embedding mechanism of physics, fundamentally different from physics-informed neural networks based on loss penalty, ensures the network to rigorously obey given physics. Numerical experiments demonstrate that the resulting learning paradigm that embeds physics possesses remarkable accuracy, robustness, interpretability and generalizability for learning spatiotemporal dynamics.","",""
3,"Marco Casadio, M. Daggitt, E. Komendantskaya, Wen Kokke, Daniel Kienitz, Rob Stewart","Property-driven Training: All You (N)Ever Wanted to Know About",2021,"","","","",35,"2022-07-13 09:28:50","","","","",,,,,3,3.00,1,6,1,"Neural networks are known for their ability to detect general patterns in noisy data. This makes them a popular tool for perception components in complex AI systems. Paradoxically, they are also known for being vulnerable to adversarial attacks. In response, various methods such as adversarial training, data-augmentation and Lipschitz robustness training have been proposed as means of improving their robustness. However, as this paper explores, these training methods each optimise for a different definition of robustness. We perform an in-depth comparison of these different definitions, including their relationship, assumptions, interpretability and verifiability after training. We also look at constraint-driven training, a general approach designed to encode arbitrary constraints, and show that not all of these definitions are directly encodable. Finally we perform experiments to compare the applicability and efficacy of the training methods at ensuring the network obeys these different definitions. These results highlight that even the encoding of such a simple piece of knowledge such as robustness in neural network training is fraught with difficult choices and pitfalls.","",""
1,"Shayan Aziznejad, Thomas Debarre, M. Unser","Sparsest Univariate Learning Models Under Lipschitz Constraint",2021,"","","","",36,"2022-07-13 09:28:50","","10.1109/ojsp.2022.3157082","","",,,,,1,1.00,0,3,1,"Beside the minimizationof the prediction error, two of the most desirable properties of a regression scheme are stability and interpretability. Driven by these principles, we propose continuous-domain formulations for one-dimensional regression problems. In our first approach, we use the Lipschitz constant as a regularizer, which results in an implicit tuning of the overall robustness of the learned mapping. In our second approach, we control the Lipschitz constant explicitly using a user-defined upper-bound and make use of a sparsity-promoting regularizer to favor simpler (and, hence, more interpretable) solutions. The theoretical study of the latter formulation is motivated in part by its equivalence, which we prove, with the training of a Lipschitz-constrained two-layer univariate neural network with rectified linear unit (ReLU) activations and weight decay. By proving representer theorems, we show that both problems admit global minimizers that are continuous and piecewise-linear (CPWL) functions. Moreover, we propose efficient algorithms that find the sparsest solution of each problem: the CPWL mapping with the least number of linear regions. Finally, we illustrate numerically the outcome of our formulations.","",""
15,"Zihao Wang, Yang Su, Saimeng Jin, W. Shen, Jingzheng Ren, Xiang-ping Zhang, J. Clark","A novel unambiguous strategy of molecular feature extraction in machine learning assisted predictive models for environmental properties",2020,"","","","",37,"2022-07-13 09:28:50","","10.1039/d0gc01122c","","",,,,,15,7.50,2,7,2,"Environmental properties of compounds provide significant information in treating organic pollutants, which drives the chemical process and environmental science toward eco-friendly technology. Traditional group contribution methods play an important role in property estimations, whereas various disadvantages emerge in their applications, such as scattered predicted values for certain groups of compounds. In order to address such issues, an extraction strategy for molecular features is proposed in this research, which is characterized by interpretability and discriminating power with regard to isomers. Based on the Henry's law constant data of organic compounds in water, we developed a hybrid predictive model that integrates the proposed strategy in conjunction with a neural network framework. The structure of the predictive model is optimized using cross-validation and grid search to improve its robustness. Moreover, the predictive model is improved by introducing the plane of best fit descriptor as input and adopting k-means clustering in sampling. In contrast with reported models in the literature, the developed predictive model demonstrates improved generality, higher accuracy, and fewer molecular features used in its development.","",""
0,"Heng Yin, Hengwei Zhang, Zheming Li, Zhilin Liu","Improving the Transferability of Adversarial Examples with Image Affine Transformation",2021,"","","","",38,"2022-07-13 09:28:50","","10.1088/1742-6596/1955/1/012052","","",,,,,0,0.00,0,4,1,"Deep learning is widely regarded as a black-box technology. We all know its performance is very good, but we have limited understanding of why it is so good. At present, there are many researches on the interpretability of deep neural network. By studying adversarial example, we can understand the internal semantics of neural network and find the decision boundary with problems, which in turn helps to improve the robustness and performance of neural network and its interpretability. With the development of adversarial example research, more and more adversarial example generation methods are proposed. Although the attack from adversarial example poses a great security threat to the deep learning system, it can also be used as an effective tool to measure the robustness and reliability of the model, and the attack and defense are two mutually promoting processes. Therefore, how to generate adversarial example with stronger attack ability is worth further study. And this study proposes a method named Affine-Invariant Method, aimed to improve the transferability of adversarial examples in black-box environment.","",""
0,"Keke Du, Shane Chang, Huixiang Wen, Hao Zhang","Fighting Adversarial Images With Interpretable Gradients",2021,"","","","",39,"2022-07-13 09:28:50","","10.1145/3472634.3472644","","",,,,,0,0.00,0,4,1,"Adversarial images are specifically designed to fool neural networks into making a wrong decision about what they are looking at, which severely degrade neural network accuracy. Recently, empirical and theoretical evidence suggests that robust neural network models tend to have better interpretable gradients. Therefore, we speculate that improving the interpretability of the gradients of the neural network models may also help to improve the robustness of the models. Two methods are used to add gradient-dependent constraint terms to the loss function of neural network models and both improve the robustness of the models. The first method adds the fussed lasso penalty term of the saliency maps to the loss function of the neural network models, which makes the saliency maps arrange in a natural way to improve the interpretability of the saliency maps, and uses the gradient enhancement for relu instead of relu to strengthen the constraint of regularization term on saliency maps. In the second method, the cosine similarity penalty term between the input gradients and the image contour is added to the loss function of the model to constrain the approximation between the input gradients and the image contour. This method has a certain biological significance, because the contour information of the image is used in the human visual system to recognize the image. Both methods improve the interpretability of model‘s gradients and the first method exceeds most regularization methods except adversarial training on MNIST and the second method even exceeds the adversarial training under white-box attacks on CIFAR-10 and CIFAR-100.","",""
4,"D. Budden, Adam H. Marblestone, Eren Sezener, Tor Lattimore, Greg Wayne, J. Veness","Gaussian Gated Linear Networks",2020,"","","","",40,"2022-07-13 09:28:50","","","","",,,,,4,2.00,1,6,2,"We propose the Gaussian Gated Linear Network (G-GLN), an extension to the recently proposed GLN family of deep neural networks. Instead of using backpropagation to learn features, GLNs have a distributed and local credit assignment mechanism based on optimizing a convex objective. This gives rise to many desirable properties including universality, data-efficient online learning, trivial interpretability and robustness to catastrophic forgetting. We extend the GLN framework from classification to multiple regression and density modelling by generalizing geometric mixing to a product of Gaussian densities. The G-GLN achieves competitive or state-of-the-art performance on several univariate and multivariate regression benchmarks, and we demonstrate its applicability to practical tasks including online contextual bandits and density estimation via denoising.","",""
5,"Tsung-Yen Yang, Karthik Narasimham","Robust and Interpretable Grounding of Spatial References with Relation Networks",2020,"","","","",41,"2022-07-13 09:28:50","","10.18653/v1/2020.findings-emnlp.172","","",,,,,5,2.50,3,2,2,"Learning representations of spatial references in natural language is a key challenge in tasks like autonomous navigation and robotic manipulation. Recent work has investigated various neural architectures for learning multi-modal representations for spatial concepts. However, the lack of explicit reasoning over entities makes such approaches vulnerable to noise in input text or state observations. In this paper, we develop effective models for understanding spatial references in text that are robust and interpretable, without sacrificing performance. We design a text-conditioned relation network whose parameters are dynamically computed with a cross-modal attention module to capture fine-grained spatial relations between entities. This design choice provides interpretability of learned intermediate outputs. Experiments across three tasks demonstrate that our model achieves superior performance, with a 17% improvement in predicting goal locations and a 15% improvement in robustness compared to state-of-the-art systems.","",""
15,"Chen Liu, Ryota Tomioka, V. Cevher","On Certifying Non-uniform Bound against Adversarial Attacks",2019,"","","","",42,"2022-07-13 09:28:50","","","","",,,,,15,5.00,5,3,3,"This work studies the robustness certification problem of neural network models, which aims to find certified adversary-free regions as large as possible around data points. In contrast to the existing approaches that seek regions bounded uniformly along all input features, we consider non-uniform bounds and use it to study the decision boundary of neural network models. We formulate our target as an optimization problem with nonlinear constraints. Then, a framework applicable for general feedforward neural networks is proposed to bound the output logits so that the relaxed problem can be solved by the augmented Lagrangian method. Our experiments show the non-uniform bounds have larger volumes than uniform ones. Compared with normal models, the robust models have even larger non-uniform bounds and better interpretability. Further, the geometric similarity of the non-uniform bounds gives a quantitative, data-agnostic metric of input features’ robustness.","",""
2,"Hsu-Kun Wu, Yih-Lon Lin, J. Hsieh, J. Jeng","Study on semiparametric Wilcoxon fuzzy neural networks",2012,"","","","",43,"2022-07-13 09:28:50","","10.1007/s00500-011-0730-3","","",,,,,2,0.20,1,4,10,"","",""
7,"Ashkan Khakzar, Shadi Albarqouni, Nassir Navab","Learning Interpretable Features via Adversarially Robust Optimization",2019,"","","","",44,"2022-07-13 09:28:50","","10.1007/978-3-030-32226-7_88","","",,,,,7,2.33,2,3,3,"","",""
20,"Xi Peng, Joey Tianyi Zhou, Hongyuan Zhu","k-meansNet: When k-means Meets Differentiable Programming",2018,"","","","",45,"2022-07-13 09:28:50","","","","",,,,,20,5.00,7,3,4,"In this paper, we study two challenging problems. The first one is how to implement \textit{k}-means in the neural network, which enjoys efficient training based on the stochastic algorithm. The second one is how to enhance the interpretability of network design for clustering. To solve the problems, we propose a neural network which is a novel formulation of the vanilla $k$-means objective. Our contribution is in twofold. From the view of neural networks, the proposed \textit{k}-meansNet is with explicit interpretability in neural processing. We could understand not only why the network structure is presented like itself but also why it could perform data clustering. Such an interpretable neural network remarkably differs from the existing works that usually employ visualization technique to explain the result of the neural network. From the view of \textit{k}-means, three highly desired properties are achieved, i.e. robustness to initialization, the capability of handling new coming data, and provable convergence. Extensive experimental studies show that our method achieves promising performance comparing with 12 clustering methods on some challenging datasets.","",""
4,"Z. Su, M. Pietikäinen, Li Liu","BIRD: Learning Binary and Illumination Robust Descriptor for Face Recognition",2019,"","","","",46,"2022-07-13 09:28:50","","","","",,,,,4,1.33,1,3,3,"Recently face recognition has made significantly progress due to the advancement of large scale Deep Convolutional Neural Network (DeepCNNs). Despite the great success, the known deficiencies of DeepCNNs have not been addressed, such as the need for too much labeled training data, energy hungry, lack of theoretical interpretability, lack of robustness to image transformations and degradations, and vulnerable to attacks, which limit DeepCNNs to be used in many real world applications. Therefore, these factors make previous predominating Local Binary Patterns (LBP) based face recognition methods still irreplaceable. In this paper we propose a novel approach called BIRD (learning Binary and Illumination Robust Descriptor) for face representation, which nicely balances the three criteria: distinctiveness, robustness, and computationally inexpensive cost. We propose to learn discriminative and compact binary codes directly from six types of Pixel Difference Vectors (PDVs). For each type of binary codes, we cluster and pool these compact binary codes to obtain a histogram representation of each face image. Six global histograms derived from six types of learned compact binary codes are fused for the final face recognition. Experimental results on the CAS_PERL_R1 and LFW databases indicate the performance of our BIRD surpasses all previous binary based face recognition methods on the two evaluated datasets. More impressively, the proposed BIRD is shown to be highly robust to illumination changes, and produces 89.5% on the CAS_PEAL_R1 illumination subset, which, we believe, is so far the best reported results on this dataset. Our code is made available 1.","",""
3,"Huifeng Guo, Ruiming Tang, Yunming Ye, Feng Liu, Yuzhou Zhang","A Novel KNN Approach for Session-Based Recommendation",2019,"","","","",47,"2022-07-13 09:28:50","","10.1007/978-3-030-16145-3_30","","",,,,,3,1.00,1,5,3,"","",""
1,"H. Cooper, G. Iyengar, Ching-Yung Lin","Deep Influence Diagrams: An Interpretable and Robust Decision Support System",2019,"","","","",48,"2022-07-13 09:28:50","","10.1007/978-3-030-20485-3_35","","",,,,,1,0.33,0,3,3,"","",""
1,"Meet H. Soni, I. Sheikh, Sunil Kumar Kopparapu","Label-Driven Time-Frequency Masking for Robust Speech Command Recognition",2019,"","","","",49,"2022-07-13 09:28:50","","10.1007/978-3-030-27947-9_29","","",,,,,1,0.33,0,3,3,"","",""
3,"Huifeng Guo, Ruiming Tang, Yunming Ye, Feng Liu, Yuzhou Zhang","An Adjustable Heat Conduction based KNN Approach for Session-based Recommendation",2018,"","","","",50,"2022-07-13 09:28:50","","","","",,,,,3,0.75,1,5,4,"The KNN approach, which is widely used in recommender systems because of its efficiency, robustness and interpretability, is proposed for session-based recommendation recently and outperforms recurrent neural network models. It captures the most recent co-occurrence information of items by considering the interaction time. However, it neglects the co-occurrence information of items in the historical behavior which is interacted earlier and cannot discriminate the impact of items and sessions with different popularity. Due to these observations, this paper presents a new contextual KNN approach to address these issues for session-based recommendation. Specifically, a diffusion-based similarity method is proposed for considering the popularity of vertices in session-item bipartite network, and a candidate selection method is proposed to capture the items that are co-occurred with different historical clicked items in the same session efficiently. Comprehensive experiments are conducted to demonstrate the effectiveness of our KNN approach over the state-of-the-art KNN approach for session-based recommendation on three benchmark datasets.","",""
19,"Mohsen Hajiloo, H. Rabiee, Mahdi Anooshahpour","Fuzzy support vector machine: an efficient rule-based classification technique for microarrays",2013,"","","","",51,"2022-07-13 09:28:50","","10.1186/1471-2105-14-S13-S4","","",,,,,19,2.11,6,3,9,"","",""
1,"Zack Hodari, Simon King","A learned emotion space for emotion recognition and emotive speech synthesis",2017,"","","","",52,"2022-07-13 09:28:50","","","","",,,,,1,0.20,1,2,5,"Emotion is a complex phenomenon that contributes heavily to human communication. Typically, human-computer interaction and text-to-speech systems do not account for emotion information, possibly due to lack of accurate emotion recognition and emotive speech synthesis methods. It seems likely that emotion recognition and synthesis has the ability to greatly improve how humans interface with machines. Existing methods in speech recognition make use of a wide range of features and models. However, these methods make use of either categorical, or appraisalbased emotion descriptions. We believe these have flaws that limit their ability to describe emotion. We present several neural network models for emotion recognition. In addition, we propose an abstract emotion space that avoids the flaws of existing emotion descriptions. We use stimulation to improve the interpretability of our emotion space, along with multi-task learning to improve its robustness. Finally, we investigate auxiliary features for style adaptation in statistical parametric speech synthesis, evaluating both our emotion space and other descriptions of emotion. The results indicate that our recognition models are state-of-the-art. However, evaluation using speech synthesis shows that our emotion space is no more informative than existing emotion descriptions. Additionally, we investigate a convolutional recognition model using the spectrogram, which outperforms other spectrogram based methods.","",""
2,"Tobias Rodemann, Lars Gräning, K. Nishikawa","Automatic energy management controller design for hybrid electric vehicles",2016,"","","","",53,"2022-07-13 09:28:50","","10.1109/SSCI.2016.7850089","","",,,,,2,0.33,1,3,6,"Due to strict CO2 emission limits, the optimal design of controllers for hybrid cars is an increasingly important topic for the automotive industry. Most current approaches to controller design rely solely on engineering knowledge. Utilizing technologies from computational intelligence is not yet common practice. In this work we evaluate how simple controllers can automatically be extracted from optimal control strategies computed by Dynamic Programming. We compare artificial neural network and decision tree based controllers in terms of performance (fuel consumption), stability, robustness, and interpretability, and we investigate the dependency on specific drive cycles used for generating optimal control. Our findings indicate that automatically derived controllers can result in a performance 1–2% below optimal fuel economy, but we observe a large variety in performance and in the controller structure for different drive cycles, thus, underlining the relevance of the correct choice of the drive cycles used for controller development. We also outline the impact of typical learning related issues like overfitting on the practical development process.","",""
20,"A. Várkonyi-Kóczy","Model Based Anytime Soft Computing Approaches in Engineering Applications",2009,"","","","",54,"2022-07-13 09:28:50","","10.1007/978-3-642-00448-3_4","","",,,,,20,1.54,20,1,13,"","",""
77,"Yu Zhang, P. Tiňo, A. Leonardis, K. Tang","A Survey on Neural Network Interpretability",2020,"","","","",55,"2022-07-13 09:28:50","","10.1109/TETCI.2021.3100641","","",,,,,77,38.50,19,4,2,"Along with the great success of deep neural networks, there is also growing concern about their black-box nature. The interpretability issue affects people's trust on deep learning systems. It is also related to many ethical problems, e.g., algorithmic discrimination. Moreover, interpretability is a desired property for deep networks to become powerful tools in other research fields, e.g., drug discovery and genomics. In this survey, we conduct a comprehensive review of the neural network interpretability research. We first clarify the definition of interpretability as it has been used in many different contexts. Then we elaborate on the importance of interpretability and propose a novel taxonomy organized along three dimensions: type of engagement (passive vs. active interpretation approaches), the type of explanation, and the focus (from local to global interpretability). This taxonomy provides a meaningful 3D view of distribution of papers from the relevant literature as two of the dimensions are not simply categorical but allow ordinal subcategories. Finally, we summarize the existing interpretability evaluation methods and suggest possible research directions inspired by our new taxonomy.","",""
1,"Y. Lu, Ilgiz Murzakhanov, Spyros Chatzivasileiadis","Neural network interpretability for forecasting of aggregated renewable generation",2021,"","","","",56,"2022-07-13 09:28:50","","10.1109/SmartGridComm51999.2021.9631993","","",,,,,1,1.00,0,3,1,"With the rapid growth of renewable energy, lots of small photovoltaic (PV) prosumers emerge. Due to the uncertainty of solar power generation, there is a need for aggregated prosumers to predict solar power generation and whether solar power generation will be larger than load. This paper presents two interpretable neural networks to solve the problem: one binary classification neural network and one regression neural network. The neural networks are built using TensorFlow. The global feature importance and local feature contributions are examined by three gradient-based methods: Integrated Gradients, Expected Gradients, and DeepLIFT. Moreover, we detect abnormal cases when predictions might fail by estimating the prediction uncertainty using Bayesian neural networks. Neural networks, which are interpreted by the gradient-based methods and complemented with uncertainty estimation, provide robust and explainable forecasting for decision-makers.","",""
0,"Lech Szymanski, B. McCane, C. Atkinson","Switched linear projections and inactive state sensitivity for deep neural network interpretability",2019,"","","","",57,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,3,3,"We introduce switched linear projections for expressing the activity of a neuron in a ReLU-based deep neural network in terms of a single linear projection in the input space. The method works by isolating the active subnetwork, a series of linear transformations, that completely determine the entire computation of the deep network for a given input instance. We also propose that for interpretability it is more instructive and meaningful to focus on the patterns that deactive the neurons in the network, which are ignored by the exisiting methods that implicitly track only the active aspect of the network's computation. We introduce a novel interpretability method for the inactive state sensitivity (Insens). Comparison against existing methods shows that Insens is more robust (in the presence of noise), more complete (in terms of patterns that affect the computation) and a very effective interpretability method for deep neural networks.","",""
11,"Chaithanya Kumar Mummadi, Ranjitha Subramaniam, Robin Hutmacher, J. Vitay, Volker Fischer, J. H. Metzen","Does enhanced shape bias improve neural network robustness to common corruptions?",2021,"","","","",58,"2022-07-13 09:28:50","","","","",,,,,11,11.00,2,6,1,"Convolutional neural networks (CNNs) learn to extract representations of complex features, such as object shapes and textures to solve image recognition tasks. Recent work indicates that CNNs trained on ImageNet are biased towards features that encode textures and that these alone are sufficient to generalize to unseen test data from the same distribution as the training data but often fail to generalize to out-of-distribution data. It has been shown that augmenting the training data with different image styles decreases this texture bias in favor of increased shape bias while at the same time improving robustness to common corruptions, such as noise and blur. Commonly, this is interpreted as shape bias increasing corruption robustness. However, this relationship is only hypothesized. We perform a systematic study of different ways of composing inputs based on natural images, explicit edge information, and stylization. While stylization is essential for achieving high corruption robustness, we do not find a clear correlation between shape bias and robustness. We conclude that the data augmentation caused by style-variation accounts for the improved corruption robustness and increased shape bias is only a byproduct.","",""
393,"A. Ross, Finale Doshi-Velez","Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients",2017,"","","","",59,"2022-07-13 09:28:50","","10.1609/aaai.v32i1.11504","","",,,,,393,78.60,197,2,5,"    Deep neural networks have proven remarkably effective at solving many classification problems, but have been criticized recently for two major weaknesses: the reasons behind their predictions are uninterpretable, and the predictions themselves can often be fooled by small adversarial perturbations. These problems pose major obstacles for the adoption of neural networks in domains that require security or transparency. In this work, we evaluate the effectiveness of defenses that differentiably penalize the degree to which small changes in inputs can alter model predictions. Across multiple attacks, architectures, defenses, and datasets, we find that neural networks trained with this input gradient regularization exhibit robustness to transferred adversarial examples generated to fool all of the other models. We also find that adversarial examples generated to fool gradient-regularized models fool all other models equally well, and actually lead to more ""legitimate,"" interpretable misclassifications as rated by people (which we confirm in a human subject experiment). Finally, we demonstrate that regularizing input gradients makes them more naturally interpretable as rationales for model predictions. We conclude by discussing this relationship between interpretability and robustness in deep neural networks.   ","",""
0,"Patrick McClure, D. Moraczewski, K. Lam, Adam Thomas, Francisco Pereira","Improving the Interpretability of fMRI Decoding using Deep Neural Networks and Adversarial Robustness.",2020,"","","","",60,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,5,2,"Deep neural networks (DNNs) are being increasingly used to make predictions from functional magnetic resonance imaging (fMRI) data. However, they are widely seen as uninterpretable ""black boxes"", as it can be difficult to discover what input information is used by the DNN in the process, something important in both cognitive neuroscience and clinical applications. A saliency map is a common approach for producing interpretable visualizations of the relative importance of input features for a prediction. However, methods for creating maps often fail due to DNNs being sensitive to input noise, or by focusing too much on the input and too little on the model. It is also challenging to evaluate how well saliency maps correspond to the truly relevant input information, as ground truth is not always available. In this paper, we review a variety of methods for producing gradient-based saliency maps, and present a new adversarial training method we developed to make DNNs robust to input noise, with the goal of improving interpretability. We introduce two quantitative evaluation procedures for saliency map methods in fMRI, applicable whenever a DNN or linear model is being trained to decode some information from imaging data. We evaluate the procedures using a synthetic dataset where the complex activation structure is known, and on saliency maps produced for DNN and linear models for task decoding in the Human Connectome Project (HCP) dataset. Our key finding is that saliency maps produced with different methods vary widely in interpretability, in both in synthetic and HCP fMRI data. Strikingly, even when DNN and linear models decode at comparable levels of performance, DNN saliency maps score higher on interpretability than linear model saliency maps (derived via weights or gradient). Finally, saliency maps produced with our adversarial training method outperform those from other methods.","",""
9,"Adam Noack, Isaac Ahern, D. Dou, Boyang Li","Does Interpretability of Neural Networks Imply Adversarial Robustness?",2019,"","","","",61,"2022-07-13 09:28:50","","","","",,,,,9,3.00,2,4,3,"The success of deep neural networks is clouded by two issues: (1) a vulnerability to adversarial examples and (2) a tendency to be uninterpretable. Interestingly, recent empirical evidence in the literature as well as theoretical analysis on simple models suggest these two seemingly disparate issues are actually connected. In particular, robust models tend to be more interpretable than non-robust models.  In this paper, we provide evidence for the claim that this relationship is bidirectional. Viz., models that are optimized to have interpretable gradients are more robust to adversarial examples than models trained in a standard manner. With further analysis and experiments on standard image classification datasets, we identify two factors behind this phenomenon---namely the suppression of the gradient's magnitude and the selective use of features guided by high-quality interpretations---which explain model behaviors under various regularization and target interpretation settings.","",""
27,"Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang, Xiang Chen","Interpreting and Evaluating Neural Network Robustness",2019,"","","","",62,"2022-07-13 09:28:50","","10.24963/ijcai.2019/583","","",,,,,27,9.00,5,6,3,"Recently, adversarial deception becomes one of the most considerable threats to deep neural networks. However, compared to extensive research in new designs of various adversarial attacks and defenses, the neural networks' intrinsic robustness property is still lack of thorough investigation. This work aims to qualitatively interpret the adversarial attack and defense mechanisms through loss visualization, and establish a quantitative metric to evaluate the model's intrinsic robustness. The proposed robustness metric identifies the upper bound of a model's prediction divergence in the given domain and thus indicates whether the model can maintain a stable prediction. With extensive experiments, our metric demonstrates several advantages over conventional testing accuracy based robustness estimation: (1) it provides a uniformed evaluation to models with different structures and parameter scales; (2) it over-performs conventional accuracy based robustness evaluation and provides a more reliable evaluation that is invariant to different test settings; (3) it can be fast generated without considerable testing cost.","",""
19,"Chongzhi Zhang, Aishan Liu, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing Ma, Tianlin Li","Interpreting and Improving Adversarial Robustness of Deep Neural Networks With Neuron Sensitivity",2019,"","","","",63,"2022-07-13 09:28:50","","10.1109/TIP.2020.3042083","","",,,,,19,6.33,3,7,3,"Deep neural networks (DNNs) are vulnerable to adversarial examples where inputs with imperceptible perturbations mislead DNNs to incorrect results. Despite the potential risk they bring, adversarial examples are also valuable for providing insights into the weakness and blind-spots of DNNs. Thus, the interpretability of a DNN in the adversarial setting aims to explain the rationale behind its decision-making process and makes deeper understanding which results in better practical applications. To address this issue, we try to explain adversarial robustness for deep models from a new perspective of neuron sensitivity which is measured by neuron behavior variation intensity against benign and adversarial examples. In this paper, we first draw the close connection between adversarial robustness and neuron sensitivities, as sensitive neurons make the most non-trivial contributions to model predictions in the adversarial setting. Based on that, we further propose to improve adversarial robustness by stabilizing the behaviors of sensitive neurons. Moreover, we demonstrate that state-of-the-art adversarial training methods improve model robustness by reducing neuron sensitivities, which in turn confirms the strong connections between adversarial robustness and neuron sensitivity. Extensive experiments on various datasets demonstrate that our algorithm effectively achieves excellent results. To the best of our knowledge, we are the first to study adversarial robustness using neuron sensitivities.","",""
17,"Yi-Shan Lin, Wen-Chuan Lee, Z. B. Celik","What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors",2020,"","","","",64,"2022-07-13 09:28:50","","10.1145/3447548.3467213","","",,,,,17,8.50,6,3,2,"EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the input parts deemed important to arrive at a decision for a specific target. However, it remains challenging to quantify the correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for the systematic evaluation of explanations that an XAI method generates. We evaluate seven state-of-the-art model-free and model-specific post-hoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We found six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region. We made our code available at https://github.com/yslin013/evalxai.","",""
0,"A. Chistyakova, Maria Cherepnina, K. Arkhipenko, Sergey D. Kuznetsov, Chang-Seok Oh, Sebeom Park","Evaluation of interpretability methods for adversarial robustness on real-world datasets",2021,"","","","",65,"2022-07-13 09:28:50","","10.1109/ivmem53963.2021.00007","","",,,,,0,0.00,0,6,1,"Adversarial training is considered the most powerful approach for robustness against attacks on deep neural networks involving adversarial examples. However, recent works have shown that the similar robustness level can be achieved by other means, namely interpretability-based regularization. We evaluate these interpretability-based approaches on real-world ResNet models trained on CIFAR-10 and ImageNet datasets. Our results show that interpretability can marginally improve robustness when combined with adversarial training, however, they bring additional computational complexity making these approaches questionable for such models and datasets.","",""
367,"V. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, S. Gordon, C. Hung, Brent Lance","EEGNet: A Compact Convolutional Neural Network for EEG-based Brain-Computer Interfaces",2021,"","","","",66,"2022-07-13 09:28:50","","","","",,,,,367,367.00,61,6,1,"Objective: Brain computer interfaces (BCI) enable direct communication with a computer, using neural activity as the control signal. This neural signal is generally chosen from a variety of well-studied electroencephalogram (EEG) signals. For a given BCI paradigm, feature extractors and classifiers are tailored to the distinct characteristics of its expected EEG control signal, limiting its application to that specific signal. Convolutional Neural Networks (CNNs), which have been used in computer vision and speech recognition to perform automatic feature extraction and classification, have successfully been applied to EEG-based BCIs; however, they have mainly been applied to single BCI paradigms and thus it remains unclear how these architectures generalize to other paradigms. Here, we ask if we can design a single CNN architecture to accurately classify EEG signals from different BCI paradigms, while simultaneously being as compact as possible (defined as the number of parameters in the model). Approach: In this work we introduce EEGNet, a compact convolutional neural network for EEG-based BCIs. We introduce the use of depthwise and separable convolutions to construct an EEG-specific model which encapsulates well-known EEG feature extraction concepts for BCI. We compare EEGNet, both for within-subject and cross-subject classification, to current state-of-the-art approaches across four BCI paradigms: P300 visual-evoked potentials, error-related negativity responses (ERN), movement-related cortical potentials (MRCP), and sensory motor rhythms (SMR). Results: We show that EEGNet generalizes across paradigms better than, and achieves comparably high performance to, the reference algorithms when only limited training data is available. We also show that EEGNet effectively generalizes to both ERP and oscillatory-based BCIs. In addition, we demonstrate three different approaches to visualize the contents of a trained EEGNet model to enable interpretation of the learned features. Significance: Our results suggest that EEGNet is robust enough to learn a wide variety of interpretable features over a range of BCI tasks, suggesting that the observed performances were not due to artifact or noise sources in the data. Our models can be found at: https://github.com/vlawhern/arl-eegmodels. 1 ar X iv :1 61 1. 08 02 4v 4 [ cs .L G ] 1 6 M ay 2 01 8","",""
5,"Hui Guo, Shu Hu, Xin Wang, Ming-Ching Chang, Siwei Lyu","Robust Attentive Deep Neural Network for Exposing GAN-generated Faces",2021,"","","","",67,"2022-07-13 09:28:50","","","","",,,,,5,5.00,1,5,1,"GAN-based techniques that generate and synthesize realistic faces have caused severe social concerns and security problems. Existing methods for detecting GAN-generated faces can perform well on limited public datasets. However, images from existing public datasets do not represent real-world scenarios well enough in terms of view variations and data distributions (where real faces largely outnumber synthetic faces). The stateof-the-art methods do not generalize well in real-world problems and lack the interpretability of detection results. Performance of existing GAN-face detection models degrades significantly when facing imbalanced data distributions. To address these shortcomings, we propose a robust, attentive, end-to-end network that can spot GAN-generated faces by analyzing their eye inconsistencies. Specifically, our model learns to identify inconsistent eye components by localizing and comparing the iris artifacts between the two eyes automatically. Our deep network addresses the imbalance learning issues by considering the AUC loss and the traditional cross-entropy loss jointly. Comprehensive evaluations of the FFHQ dataset in terms of both balanced and imbalanced scenarios demonstrate the superiority of the proposed method.","",""
4,"Ying Du, Yadong Liu, Xuhong Wang, Jian Fang, G. Sheng, Xiuchen Jiang","Predicting Weather-Related Failure Risk in Distribution Systems Using Bayesian Neural Network",2021,"","","","",68,"2022-07-13 09:28:50","","10.1109/TSG.2020.3019263","","",,,,,4,4.00,1,6,1,"The reliability of distribution systems is often challenged under unfavorable weather conditions, where weather-related failures occur with high probability. Predicting the number of weather-related failures in distribution systems can provide guiding information for operation and maintenance decisions, improving the risk management capability of utility companies. This article proposes a novel Bayesian Neural Network (BNN) based model to predict weather-related failures caused by wind, rain and lightning. Superior prediction performance of the BNN based model is verified by contrast experiments with other advanced prediction models under four different evaluation metrics. BNN based prediction model presents remarkable robustness, especially in the prediction of high failure levels. In addition, compared to most previous used prediction models without any prediction confidence feedback, BNN based prediction model has the capability of uncertainty estimation. The confidence interval of prediction results can be obtained, which provides sufficient information for guiding risk management of utility companies. An effective operation and maintenance guiding scheme based on the analysis of prediction uncertainty is proposed, which fully excavates the interpretability of the proposed model and enrich the application value of the model.","",""
3,"A. Piccardo, R. Cappuccio, G. Bottoni, D. Cecchin, L. Mazzella, A. Cirone, S. Righi, Martina Ugolini, P. Bianchi, P. Bertolaccini, E. Lorenzini, M. Massollo, A. Castaldi, F. Fiz, L. Strada, A. Cistaro, M. Del Sette","The role of the deep convolutional neural network as an aid to interpreting brain [18F]DOPA PET/CT in the diagnosis of Parkinson’s disease",2021,"","","","",69,"2022-07-13 09:28:50","","10.1007/s00330-021-07779-z","","",,,,,3,3.00,0,17,1,"","",""
2,"S. Barland, Franccois Gustave","Convolutional neural network for self-mixing interferometric displacement sensing.",2021,"","","","",70,"2022-07-13 09:28:50","","10.1364/OE.419844","","",,,,,2,2.00,1,2,1,"Self-mixing interferometry is a well established interferometric measurement technique. In spite of the robustness and simplicity of the concept, interpreting the self-mixing signal is often complicated in practice, which is detrimental to measurement availability. Here we discuss the use of a convolutional neural network to reconstruct the displacement of a target from the self-mixing signal in a semiconductor laser. The network, once trained on periodic displacement patterns, can reconstruct arbitrarily complex displacement in different alignment conditions and setups. The approach validated here is amenable to generalization to modulated schemes or even to totally different self-mixing sensing tasks.","",""
2,"Zhifang Liao, Haihui Pan, Xiaoping Fan, Yan Zhang, Li Kuang","Multiple Wavelet Convolutional Neural Network for Short-Term Load Forecasting",2020,"","","","",71,"2022-07-13 09:28:50","","10.1109/JIOT.2020.3026733","","",,,,,2,1.00,0,5,2,"Although the accuracy of load forecasting has been studied by many works, the actual deployability of a model is rarely considered. In this work, we consider the actual deployability of a model from four aspects: 1) the prediction performance of the model; 2) the robustness of the model; 3) the dependence of the model on external data; and 4) the storage size of the model. From these four aspects, we propose a multiple wavelet convolutional neural network (MWCNN) for load forecasting. On two public data sets, we verified the performance and robustness of the MWCNN. The MWCNN only uses load data, and the storage size of the model is only 497 kB, which shows that MWCNN has good deployability. In addition, our MWCNN prediction results are interpretable. The experimental results show that the MWCNN can effectively capture the periodic characteristics of load data.","",""
1,"Yanghuan Xu, Dong-cheng Wang, Bo Duan, Hua-xin Yu, Hongmin Liu","Copper Strip Surface Defect Detection Model Based on Deep Convolutional Neural Network",2021,"","","","",72,"2022-07-13 09:28:50","","10.3390/app11198945","","",,,,,1,1.00,0,5,1,"Surface defect automatic detection has great significance for copper strip production. The traditional machine vision for surface defect automatic detection of copper strip needs artificial feature design, which has a long cycle, and poor ability of versatility and robustness. However, deep learning can effectively solve these problems. Therefore, based on the deep convolution neural network and the transfer learning strategy, an intelligent recognition model of surface defects of copper strip is established in this paper. Firstly, the defects were classified in accordance with the mechanism and morphology, and the surface defect dataset of copper strip was established by comprehensively adopting image acquisition and image augmentation. Then, a two-class discrimination model was established to achieve the accurate discrimination of perfect and defect images. On this basis, four CNN models were adopted for the recognition of defect images. Among these models, the EfficientNet model through transfer learning strategy had the best comprehensive performance with a recognition accuracy rate of 93.05%. Finally, the interpretability and deficiency of the model were analysed by the class activation map and confusion matrix, which point toward the direction of further optimization for future research.","",""
1,"Michael Yeung, L. Rundo, Yang Nan, E. Sala, C. Schönlieb, Guang Yang","Calibrating the Dice loss to handle neural network overconfidence for biomedical image segmentation",2021,"","","","",73,"2022-07-13 09:28:50","","","","",,,,,1,1.00,0,6,1,"The Dice similarity coefficient (DSC) is both a widely used metric and loss function for biomedical image segmentation due to its robustness to class imbalance. However, it is well known that the DSC loss is poorly calibrated, resulting in overconfident predictions that cannot be usefully interpreted in biomedical and clinical practice. Performance is often the only metric used to evaluate segmentations produced by deep neural networks, and calibration is often neglected. However, calibration is important for translation into biomedical and clinical practice, providing crucial contextual information to model predictions for interpretation by scientists and clinicians. In this study, we identify poor calibration as an emerging challenge of deep learning based biomedical image segmentation. We provide a simple yet effective extension of the DSC loss, named the DSC++ loss, that selectively modulates the penalty associated with overconfident, incorrect predictions. As a standalone loss function, the DSC++ loss achieves significantly improved calibration over the conventional DSC loss across five well-validated open-source biomedical imaging datasets. Similarly, we observe significantly improved when integrating the DSC++ loss into four DSC-based loss functions. Finally, we use softmax thresholding to illustrate that well calibrated outputs enable tailoring of precision-recall bias, an important post-processing technique to adapt the model predictions to suit the biomedical or clinical task. The DSC++ loss overcomes the major limitation of the DSC, providing a suitable loss function for training deep learning segmentation models for use in biomedical and clinical practice.","",""
3,"Artur Petrosyan, M. Sinkin, M. Lebedev, A. Ossadtchi","Decoding and interpreting cortical signals with a compact convolutional neural network",2021,"","","","",74,"2022-07-13 09:28:50","","10.1088/1741-2552/abe20e","","",,,,,3,3.00,1,4,1,"Objective. Brain–computer interfaces (BCIs) decode information from neural activity and send it to external devices. The use of Deep Learning approaches for decoding allows for automatic feature engineering within the specific decoding task. Physiologically plausible interpretation of the network parameters ensures the robustness of the learned decision rules and opens the exciting opportunity for automatic knowledge discovery. Approach. We describe a compact convolutional network-based architecture for adaptive decoding of electrocorticographic (ECoG) data into finger kinematics. We also propose a novel theoretically justified approach to interpreting the spatial and temporal weights in the architectures that combine adaptation in both space and time. The obtained spatial and frequency patterns characterizing the neuronal populations pivotal to the specific decoding task can then be interpreted by fitting appropriate spatial and dynamical models. Main results. We first tested our solution using realistic Monte-Carlo simulations. Then, when applied to the ECoG data from Berlin BCI competition IV dataset, our architecture performed comparably to the competition winners without requiring explicit feature engineering. Using the proposed approach to the network weights interpretation we could unravel the spatial and the spectral patterns of the neuronal processes underlying the successful decoding of finger kinematics from an ECoG dataset. Finally we have also applied the entire pipeline to the analysis of a 32-channel EEG motor-imagery dataset and observed physiologically plausible patterns specific to the task. Significance. We described a compact and interpretable CNN architecture derived from the basic principles and encompassing the knowledge in the field of neural electrophysiology. For the first time in the context of such multibranch architectures with factorized spatial and temporal processing we presented theoretically justified weights interpretation rules. We verified our recipes using simulations and real data and demonstrated that the proposed solution offers a good decoder and a tool for investigating motor control neural mechanisms.","",""
4,"Theodoros Tsiligkaridis, Jay Roberts","On Frank-Wolfe Optimization for Adversarial Robustness and Interpretability",2020,"","","","",75,"2022-07-13 09:28:50","","","","",,,,,4,2.00,2,2,2,"Deep neural networks are easily fooled by small perturbations known as adversarial attacks. Adversarial Training (AT) is a technique that approximately solves a robust optimization problem to minimize the worst-case loss and is widely regarded as the most effective defense against such attacks. While projected gradient descent (PGD) has received most attention for approximately solving the inner maximization of AT, Frank-Wolfe (FW) optimization is projection-free and can be adapted to any L norm. A Frank-Wolfe adversarial training approach is presented and is shown to provide as competitive level of robustness as PGD-AT without much tuning for a variety of architectures. We empirically show that robustness is strongly connected to the L magnitude of the adversarial perturbation and that more locally linear loss landscapes tend to have larger L distortions despite having the same L∞ distortion. We provide theoretical guarantees on the magnitude of the distortion for FW that depend on local geometry which FW-AT exploits. It is empirically shown that FW-AT achieves strong robustness to white-box attacks and black-box attacks and offers improved resistance to gradient masking. Further, FW-AT allows networks to learn highquality human-interpretable features which are then used to generate counterfactual explanations to model predictions by using dense and sparse adversarial perturbations.","",""
2,"Gurpreet Singh, Soumyajit Gupta, Matt Lease, Clint N. Dawson","TIME: A Transparent, Interpretable, Model-Adaptive and Explainable Neural Network for Dynamic Physical Processes",2020,"","","","",76,"2022-07-13 09:28:50","","","","",,,,,2,1.00,1,4,2,"Partial Differential Equations are infinite dimensional encoded representations of physical processes. However, imbibing multiple observation data towards a coupled representation presents significant challenges. We present a fully convolutional architecture that captures the invariant structure of the domain to reconstruct the observable system. The proposed architecture is significantly low-weight compared to other networks for such problems. Our intent is to learn coupled dynamic processes interpreted as deviations from true kernels representing isolated processes for model-adaptivity. Experimental analysis shows that our architecture is robust and transparent in capturing process kernels and system anomalies. We also show that high weights representation is not only redundant but also impacts network interpretability. Our design is guided by domain knowledge, with isolated process representations serving as ground truths for verification. These allow us to identify redundant kernels and their manifestations in activation maps to guide better designs that are both interpretable and explainable unlike traditional deep-nets.","",""
8,"S. Candemir, Richard D. White, M. Demirer, Vikash Gupta, M. Bigelow, L. Prevedello, B. Erdal","Automated coronary artery atherosclerosis detection and weakly supervised localization on coronary CT angiography with a deep 3-dimensional convolutional neural network",2019,"","","","",77,"2022-07-13 09:28:50","","","","",,,,,8,2.67,1,7,3,"We propose a fully automated algorithm based on a deep learning framework enabling screening of a Coronary Computed Tomography Angiography (CCTA) examination for confident detection of the presence or absence of coronary artery atherosclerosis. The system starts with extracting the coronary arteries and their branches from CCTA datasets and representing them with multi-planar reformatted volumes; pre-processing and augmentation techniques are then applied to increase the robustness and generalization ability of the system. A 3-Dimensional Convolutional Neural Network (3D CNN) is utilized to model pathological changes (e.g., atherosclerotic plaques) in coronary vessels. The system learns the discriminatory features between vessels with and without atherosclerosis. The discriminative features at the final convolutional layer are visualized with a saliency map approach to provide visual clues related to atherosclerosis likelihood and location. We have evaluated the system on a reference dataset representing 247 patients with atherosclerosis and 246 patients free of atherosclerosis. With 5-fold cross-validation, an Accuracy = 90:9%, Positive Predictive Value = 58:8%, Sensitivity = 68:9%, Specificity of 93:6%, and Negative Predictive Value (NPV) = 96:1% are achieved at the artery/branch level with threshold 0.5. The average area under the receiver operating characteristic curve is 0.91. The system indicates a high NPV, which may be potentially useful for assisting interpreting physicians in excluding coronary atherosclerosis in patients with acute chest pain.","",""
82,"Christian Etmann, Sebastian Lunz, P. Maass, C. Schönlieb","On the Connection Between Adversarial Robustness and Saliency Map Interpretability",2019,"","","","",78,"2022-07-13 09:28:50","","","","",,,,,82,27.33,21,4,3,"Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non-robust counterparts. We aim to quantify this behavior by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows,so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.","",""
4,"Sajjad Amini, S. Ghaemmaghami","Towards Improving Robustness of Deep Neural Networks to Adversarial Perturbations",2020,"","","","",79,"2022-07-13 09:28:50","","10.1109/TMM.2020.2969784","","",,,,,4,2.00,2,2,2,"Deep neural networks have presented superlative performance in many machine learning based perception and recognition tasks, where they have even outperformed human precision in some applications. However, it has been found that human perception system is much more robust to adversarial perturbation, as compared to these artificial networks. It has been shown that a deep architecture with a lower Lipschitz constant can generalize better and tolerate higher level of adversarial perturbation. Smooth regularization has been proposed to control the Lipschitz constant of a deep architecture and in this work, we show how a deep convolutional neural network (CNN), based on non-smooth regularization of convolution and fully connected layers, can present enhanced generalization and robustness to adversarial perturbation, simultaneously. We propose two non-smooth regularizers that present specific features for adversarial samples with different levels of signal-to-noise ratios. The regularizers build direct interconnections for the weight matrices in each layer, through which they control the Lipschitz constant of architecture and improve the consistency of input-output mapping of the network. This leads to more reliable and interpretable network mapping and reduces abrupt changes in the networks output. We develop an efficient algorithm to solve the non-smooth learning problems, which presents a gradual complexity addition property. Our simulation results over three benchmark datasets signify the superiority of the proposed formulations over previously reported methods for improving the robustness of deep architecture, towards human robustness to adversarial samples.","",""
21,"Weiqi Ji, Sili Deng","Autonomous Discovery of Unknown Reaction Pathways from Data by Chemical Reaction Neural Network",2020,"","","","",80,"2022-07-13 09:28:50","","10.1021/acs.jpca.0c09316","","",,,,,21,10.50,11,2,2,"Chemical reactions occur in energy, environmental, biological, and many other natural systems, and the inference of the reaction networks is essential to understand and design the chemical processes in engineering and life sciences. Yet, revealing the reaction pathways for complex systems and processes is still challenging because of the lack of knowledge of the involved species and reactions. Here, we present a neural network approach that autonomously discovers reaction pathways from the time-resolved species concentration data. The proposed chemical reaction neural network (CRNN), by design, satisfies the fundamental physics laws, including the law of mass action and the Arrhenius law. Consequently, the CRNN is physically interpretable such that the reaction pathways can be interpreted, and the kinetic parameters can be quantified simultaneously from the weights of the neural network. The inference of the chemical pathways is accomplished by training the CRNN with species concentration data via stochastic gradient descent. We demonstrate the successful implementations and the robustness of the approach in elucidating the chemical reaction pathways of several chemical engineering and biochemical systems. The autonomous inference by the CRNN approach precludes the need for expert knowledge in proposing candidate networks and addresses the curse of dimensionality in complex systems. The physical interpretability also makes the CRNN capable of not only fitting the data for a given system but also developing knowledge of unknown pathways that could be generalized to similar chemical systems.","",""
6,"Fengchao Xiong, Jun Zhou, Shuyin Tao, Jianfeng Lu, Y. Qian","SNMF-Net: Learning a Deep Alternating Neural Network for Hyperspectral Unmixing",2021,"","","","",81,"2022-07-13 09:28:50","","10.1109/TGRS.2021.3081177","","",,,,,6,6.00,1,5,1,"Hyperspectral unmixing is recognized as an important tool to learn the constituent materials and corresponding distribution in a scene. The physical spectral mixture model is always important to tackle this problem because of its highly ill-posed nature. In this article, we introduce a linear spectral mixture model (LMM)-based end-to-end deep neural network named SNMF-Net for hyperspectral unmixing. SNMF-Net shares an alternating architecture and benefits from both model-based methods and learning-based methods. On the one hand, SNMF-Net is of high physical interpretability as it is built by unrolling <inline-formula> <tex-math notation=""LaTeX"">$L_{p}$ </tex-math></inline-formula> sparsity constrained nonnegative matrix factorization (<inline-formula> <tex-math notation=""LaTeX"">$L_{p}$ </tex-math></inline-formula>-NMF) model belonging to LMM families. On the other hand, all the parameters and submodules of SNMF-Net can be seamlessly linked with the alternating optimization algorithm of <inline-formula> <tex-math notation=""LaTeX"">$L_{p}$ </tex-math></inline-formula>-NMF and unmixing problem. This enables us to reasonably integrate the prior knowledge on unmixing, the optimization algorithm, and the sparse representation theory into the network for robust learning, so as to improve unmixing. Experimental results on the synthetic and real-world data show the advantages of the proposed SNMF-Net over many state-of-the-art methods.","",""
0,"Wu Fei, CentraleSupélec","Leveraging Model Interpretability and Stability to increase Model Robustness",2019,"","","","",82,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,2,3,"State of the art Deep Neural Networks (DNN) can now achieve above human level accuracy on image classification tasks. However their outstanding performances come along with a complex inference mechanism making them arduously interpretable models. In order to understand the underlying prediction rules of DNNs, Dhamdhere et al. [3] propose an interpretability method to break down a DNN prediction score as sum of its hidden unit contributions, in the form of a metric called conductance. Analyzing conductances of DNN hidden units, we find out there is a difference in how wrong and correct predictions are inferred. We identify distinguishable patterns of hidden unit activations for wrong and correct predictions. We then use an error detector in the form of a binary classifier on top of the DNN to automatically discriminate wrong and correct predictions of the DNN based on their hidden unit activations. Detected wrong predictions are discarded, increasing the model robustness. A different approach to distinguish wrong and correct predictions of DNNs is proposed by Wang et al. [20] whose method is based on the premise that input samples leading a DNN into making wrong predictions are less stable to the DNN weight changes than correctly classified input samples. In our study, we compare both methods and find out by combining them that better detection of wrong predictions can be achieved.","",""
0,"Fei Wu, T. Michel, Alexandre Briot","Leveraging Model Interpretability and Stability to increase Model Robustness",2019,"","","","",83,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,3,3,"State of the art Deep Neural Networks (DNN) can now achieve above human level accuracy on image classification tasks. However their outstanding performances come along with a complex inference mechanism making them arduously interpretable models. In order to understand the underlying prediction rules of DNNs, Dhamdhere et al. propose an interpretability method to break down a DNN prediction score as sum of its hidden unit contributions, in the form of a metric called conductance. Analyzing conductances of DNN hidden units, we find out there is a difference in how wrong and correct predictions are inferred. We identify distinguishable patterns of hidden unit activations for wrong and correct predictions. We then use an error detector in the form of a binary classifier on top of the DNN to automatically discriminate wrong and correct predictions of the DNN based on their hidden unit activations. Detected wrong predictions are discarded, increasing the model robustness. A different approach to distinguish wrong and correct predictions of DNNs is proposed by Wang et al. whose method is based on the premise that input samples leading a DNN into making wrong predictions are less stable to the DNN weight changes than correctly classified input samples. In our study, we compare both methods and find out by combining them that better detection of wrong predictions can be achieved.","",""
1,"Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, Jun Xu, Zhimeng Guo, Hui Liu, Jiliang Tang, Suhang Wang","A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability",2022,"","","","",84,"2022-07-13 09:28:50","","10.48550/arXiv.2204.08570","","",,,,,1,1.00,0,8,1,"Graph Neural Networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trustworthy GNNs in various aspects are emerging to prevent the harm from GNN models and increase the users’ trust in GNNs. In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the related methods and formulate the general frameworks for the multiple categories of trustworthy GNNs. We also discuss the future research directions of each aspect and connections between these aspects to help achieve trustworthiness. Neural Networks:","",""
0,"Yipeng Du, Jian Liu","IENet: a robust convolutional neural network for EEG based brain-computer interfaces",2022,"","","","",85,"2022-07-13 09:28:50","","10.1088/1741-2552/ac7257","","",,,,,0,0.00,0,2,1,"Objective. Brain-computer interfaces (BCIs) based on electroencephalogram (EEG) develop into novel application areas with more complex scenarios, which put forward higher requirements for the robustness of EEG signal processing algorithms. Deep learning can automatically extract discriminative features and potential dependencies via deep structures, demonstrating strong analytical capabilities in numerous domains such as computer vision and natural language processing. Making full use of deep learning technology to design a robust algorithm that is capable of analyzing EEG across BCI paradigms is our main work in this paper. Approach. Inspired by InceptionV4 and InceptionTime architecture, we introduce a neural network ensemble named InceptionEEG-Net (IENet), where multi-scale convolutional layer and convolution of length 1 enable model to extract rich high-dimensional features with limited parameters. In addition, we propose the average receptive field (RF) gain for convolutional neural networks (CNNs), which optimizes IENet to detect long patterns at a smaller cost. We compare with the current state-of-the-art methods across five EEG-BCI paradigms: steady-state visual evoked potentials (VEPs), epilepsy EEG, overt attention P300 VEPs, covert attention P300 visual-EPs and movement-related cortical potentials. Main results. The classification results show that the generalizability of IENet is on par with the state-of-the-art paradigm-agnostic models on test datasets. Furthermore, the feature explainability analysis of IENet illustrates its capability to extract neurophysiologically interpretable features for different BCI paradigms, ensuring the reliability of algorithm. Significance. It can be seen from our results that IENet can generalize to different BCI paradigms. And it is essential for deep CNNs to increase the RF size using average RF gain.","",""
0,"Bingxin Zhou, Yuanhong Jiang, Yu Guang Wang, Jingwei Liang, Junbin Gao, Shirui Pan, Xiaoqun Zhang","Graph Neural Network for Local Corruption Recovery",2022,"","","","",86,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,7,1,"Graph neural networks (GNNs) have seen a surge of development for exploiting the relational information of input graphs. Nevertheless, messages propagating through a graph contain both interpretable patterns and small perturbations. Despite global noise could be distributed over the entire graph data, it is not uncommon that corruptions appear well-concealed and merely pollute local regions while still having a vital influence on the GNN learning and prediction performance. This work tackles the graph recovery problem from local poisons by a robustness representation learning. Our developed strategy identifies regional graph perturbations and formulates a robust hidden feature representation for GNNs. A mask function pinpointed the anomalies without prior knowledge, and an `p,q regularizer defends local poisonings through pursuing sparsity in the framelet domain while maintaining a conditional closeness between the observation and new representation. The proposed robust computational unit alleviates the inertial alternating direction method of multipliers to achieve an efficient solution. Extensive experiments show that our new model recovers graph representations from local pollution and achieves excellent performance.","",""
28,"Zeon Trevor Fernando, Jaspreet Singh, Avishek Anand","A study on the Interpretability of Neural Retrieval Models using DeepSHAP",2019,"","","","",87,"2022-07-13 09:28:50","","10.1145/3331184.3331312","","",,,,,28,9.33,9,3,3,"A recent trend in IR has been the usage of neural networks to learn retrieval models for text based adhoc search. While various approaches and architectures have yielded significantly better performance than traditional retrieval models such as BM25, it is still difficult to understand exactly why a document is relevant to a query. In the ML community several approaches for explaining decisions made by deep neural networks have been proposed -- including DeepSHAP which modifies the DeepLift algorithm to estimate the relative importance (shapley values) of input features for a given decision by comparing the activations in the network for a given image against the activations caused by a reference input. In image classification, the reference input tends to be a plain black image. While DeepSHAP has been well studied for image classification tasks, it remains to be seen how we can adapt it to explain the output of Neural Retrieval Models (NRMs). In particular, what is a good ""black"" image in the context of IR? In this paper we explored various reference input document construction techniques. Additionally, we compared the explanations generated by DeepSHAP to LIME (a model agnostic approach) and found that the explanations differ considerably. Our study raises concerns regarding the robustness and accuracy of explanations produced for NRMs. With this paper we aim to shed light on interesting problems surrounding interpretability in NRMs and highlight areas of future work.","",""
0,"Zhangheng Li, Tianlong Chen, Linyi Li, Bo Li, Zhangyang Wang","Can pruning improve certified robustness of neural networks?",2022,"","","","",88,"2022-07-13 09:28:50","","10.48550/arXiv.2206.07311","","",,,,,0,0.00,0,5,1,"—With the rapid development of deep learning, the sizes of neural networks become larger and larger so that the training and inference often overwhelm the hardware resources. Given the fact that neural networks are often over-parameterized, one effective way to reduce such computational overhead is neural network pruning, by removing redundant parameters from trained neural networks. It has been recently observed that pruning can not only reduce computational overhead but also can improve empirical robustness of deep neural networks (NNs), potentially owing to removing spurious correlations while preserving the predictive accuracies. This paper for the ﬁrst time demonstrates that pruning can generally improve certiﬁed robustness for ReLU-based NNs under the complete veriﬁcation setting. Using the popular Branch-and-Bound (BaB) framework, we ﬁnd that pruning can enhance the estimated bound tightness of certiﬁed robustness veriﬁcation, by alleviating linear relaxation and sub-domain split problems. We empirically verify our ﬁndings with off-the-shelf pruning methods and further present a new stability-based pruning method tailored for reducing neuron instability, that outperforms existing pruning methods in enhancing certiﬁed robustness. Our experiments show that by appropriately pruning an NN, its certiﬁed accuracy can be boosted up to 8.2% under standard training, and up to 24.5% under adversarial training on the CIFAR10 dataset. We additionally observe the existence of certiﬁed lottery tickets that can match both standard and certiﬁed robust accuracies of the original dense models across different datasets. Our ﬁndings offer a new angle to study the intriguing interaction between sparsity and robustness, i.e. interpreting the interaction of sparsity and certiﬁed robustness via neuron stability. Codes are available at:","",""
2,"F. Hu, Jiaxin Jiang, P. Yin","Interpretable Prediction of Protein-Ligand Interaction by Convolutional Neural Network",2019,"","","","",89,"2022-07-13 09:28:50","","10.1109/BIBM47256.2019.8982989","","",,,,,2,0.67,1,3,3,"Evaluation of protein-ligand interaction is a crucial step in the process of drug discovery. Recently, several methods based on deep learning have gained impressive binary classification performance on protein-ligand binding prediction. However, lack of three-dimensional complex data still limits the accuracy and robustness of evaluation of protein-ligand binding affinity, as well as the prediction of their binding sites. In this paper, we propose a novel convolutional neural network based method for estimating the binding affinity between protein and ligand using only 1D sequence data. Even with the same amount of sample size, this model outperforms other structure-dependent traditional and machine learning based methods in terms of both binary classification and regression task. Furthermore, we use this model to identify the key amino acid residues of protein that are vital for binding interaction, which provides biological interpretation.","",""
2,"Mingxing Xu, Wenrui Dai, Yangmei Shen, H. Xiong","MSGCNN: Multi-scale Graph Convolutional Neural Network for Point Cloud Segmentation",2019,"","","","",90,"2022-07-13 09:28:50","","10.1109/BigMM.2019.00-35","","",,,,,2,0.67,1,4,3,"Point cloud has emerged as a scalable and flexible geometric representation for 3D data. Graph convolutional neural networks (GCNNs) have shown superior performance and robustness in point cloud processing with structure-awareness and permutation invariance. However, naive graph convolution networks are limited in point cloud segmentation tasks especially in the border areas of multiple segmentation instances due to the lack of multi-scale feature extraction ability. In this paper, we propose a novel multi-scale graph convolutional neural network (MSGCNN) to allow multi-scale feature learning for fine-grained point cloud segmentation. The proposed geometrical interpretable multi-scale point cloud processing framework is able to considerately enlarge the graph filters receptive fields and exploit discriminative multi-scale structure-aware point features for the superior segmentation performance against naive graph convolution networks especially in border area. Experimental results for part segmentation task on ShapeNet datasets show that MSGCNN achieves competitive performance with state-of-the-arts. In comparison to naive graph convolution networks, MSGCNN is shown to obtain better visual quality in the border area. We further validate that our model is robust to data point missing and noise perturbation with the learned multi-scale structure-aware point features.","",""
0,"Pau","Neural network signal understanding for instrumentation",2019,"","","","",91,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,1,3,"This paper reports on the use of neural signal interpretation theory and techniques for the purpose of classifying the shapes of a set of instrumentation signals, in order to calibrate devices, diagnose anomalies, generate tuning/settings, and interpret the measurement results. Neural signal understanding research is surveyed, and the selected implementation is described with its performance in terms of correct classification rates and robustness to noise. Formal results on neural net training time and sensitivity to weights are given. A theory for neural control is given using functional link nets and an explanation technique is designed to help neural signal understanding. The results of this are compared to those of a knowledge-based signal interpretation system within the context of the same specific instrument and data. Keywords-Neural understanding, calibration, signal understanding, control theory, neural control, training time, sensitivity to noise, explanation facilities, knowledge-based signal interpretation, instrumentation, analytical instrumentation.","",""
33,"A. Szab'o, C. Castelnovo","Neural network wave functions and the sign problem",2020,"","","","",92,"2022-07-13 09:28:50","","10.1103/PHYSREVRESEARCH.2.033075","","",,,,,33,16.50,17,2,2,"Neural quantum states (NQS) are a promising approach to study many-body quantum physics. However, they face a major challenge when applied to lattice models: Convolutional networks struggle to converge to ground states with a nontrivial sign structure. We tackle this problem by proposing a neural network architecture with a simple, explicit, and interpretable phase ansatz, which can robustly represent such states and achieve state-of-the-art variational energies for both conventional and frustrated antiferromagnets. In the latter case, our approach uncovers low-energy states that exhibit the Marshall sign rule and are therefore inconsistent with the expected ground state. Such states are the likely cause of the obstruction for NQS-based variational Monte Carlo to access the true ground states of these systems. We discuss the implications of this observation and suggest potential strategies to overcome the problem.","",""
0,"R. Sathish, Debdoot Sheet","Unit Impulse Response as an Explainer of Redundancy in a Deep Convolutional Neural Network",2019,"","","","",93,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,2,3,"Convolutional neural networks (CNN) are generally designed with a heuristic initialization of network architecture and trained for a certain task. This often leads to overparametrization after learning and induces redundancy in the information flow paths within the network. This robustness and reliability is at the increased cost of redundant computations. Several methods have been proposed which leverage metrics that quantify the redundancy in each layer. However, layer-wise evaluation in these methods disregards the long-range redundancy which exists across depth on account of the distributed nature of the features learned by the model. In this paper, we propose (i) a mechanism to empirically demonstrate the robustness in performance of a CNN on account of redundancy across its depth, (ii) a method to identify the systemic redundancy in response of a CNN across depth using the understanding of unit impulse response, we subsequently demonstrate use of these methods to interpret redundancy in few networks as example. These techniques provide better insights into the internal dynamics of a CNN","",""
6,"Yashas B L Samaga, Shampa Raghunathan, U. D. Priyakumar","SCONES: Self-Consistent Neural Network for Protein Stability Prediction Upon Mutation.",2021,"","","","",94,"2022-07-13 09:28:50","","10.26434/CHEMRXIV.14729445.V1","","",,,,,6,6.00,2,3,1,"Engineering proteins to have desired properties by mutating amino acids at specific sites is commonplace. Such engineered proteins must be stable to function. Experimental methods used to determine stability at throughputs required to scan the protein sequence space thoroughly are laborious. To this end, many machine learning based methods have been developed to predict thermodynamic stability changes upon mutation. These methods have been evaluated for symmetric consistency by testing with hypothetical reverse mutations. In this work, we propose transitive data augmentation, evaluating transitive consistency with our new Stransitive data set, and a new machine learning based method, the first of its kind, that incorporates both symmetric and transitive properties into the architecture. Our method, called SCONES, is an interpretable neural network that predicts small relative protein stability changes for missense mutations that do not significantly alter the structure. It estimates a residue's contributions toward protein stability (ΔG) in its local structural environment, and the difference between independently predicted contributions of the reference and mutant residues is reported as ΔΔG. We show that this self-consistent machine learning architecture is immune to many common biases in data sets, relies less on data than existing methods, is robust to overfitting, and can explain a substantial portion of the variance in experimental data.","",""
7,"Yuzhe Li, Shiyi Cheng, Yujia Xue, L. Tian","Displacement-agnostic coherent imaging through scatter with an interpretable deep neural network.",2020,"","","","",95,"2022-07-13 09:28:50","","10.1364/oe.411291","","",,,,,7,3.50,2,4,2,"Coherent imaging through scatter is a challenging task. Both model-based and data-driven approaches have been explored to solve the inverse scattering problem. In our previous work, we have shown that a deep learning approach can make high-quality and highly generalizable predictions through unseen diffusers. Here, we propose a new deep neural network model that is agnostic to a broader class of perturbations including scatterer change, displacements, and system defocus up to 10× depth of field. In addition, we develop a new analysis framework for interpreting the mechanism of our deep learning model and visualizing its generalizability based on an unsupervised dimension reduction technique. We show that our model can unmix the scattering-specific information and extract the object-specific information and achieve generalization under different scattering conditions. Our work paves the way to a robust and interpretable deep learning approach to imaging through scattering media.","",""
3,"T. Trimborn, Stephan Gerster, G. Visconti","Spectral methods to study the robustness of residual neural networks with infinite layers",2019,"","","","",96,"2022-07-13 09:28:50","","10.3934/fods.2020012","","",,,,,3,1.00,1,3,3,"Recently, neural networks (NN) with an infinite number of layers have been introduced. Especially for these very large NN the training procedure is very expensive. Hence, there is interest to study their robustness with respect to input data to avoid unnecessarily retraining the network. Typically, model-based statistical inference methods, e.g. Bayesian neural networks, are used to quantify uncertainties. Here, we consider a special class of residual neural networks and we study the case, when the number of layers can be arbitrarily large. Then, kinetic theory allows to interpret the network as a dynamical system, described by a partial differential equation. We study the robustness of the mean-field neural network with respect to perturbations in initial data by applying UQ approaches on the loss functions.","",""
4,"Jacob M. Springer, M. Mitchell, Garrett T. Kenyon","Adversarial Perturbations Are Not So Weird: Entanglement of Robust and Non-Robust Features in Neural Network Classifiers",2021,"","","","",97,"2022-07-13 09:28:50","","10.2172/1823733","","",,,,,4,4.00,1,3,1,"Neural networks trained on visual data are wellknown to be vulnerable to often imperceptible adversarial perturbations. The reasons for this vulnerability are still being debated in the literature. Recently Ilyas et al. (2019) showed that this vulnerability arises, in part, because neural network classifiers rely on highly predictive but brittle “non-robust” features. In this paper we extend the work of Ilyas et al. by investigating the nature of the input patterns that give rise to these features. In particular, we hypothesize that in a neural network trained in a standard way, non-robust features respond to small, “non-semantic” patterns that are typically entangled with larger, robust patterns, known to be more human-interpretable, as opposed to solely responding to statistical artifacts in a dataset. Thus, adversarial examples can be formed via minimal perturbations to these small, entangled patterns. In addition, we demonstrate a corollary of our hypothesis: robust classifiers are more effective than standard (non-robust) ones as a source for generating transferable adversarial examples in both the untargeted and targeted settings. The results we present in this paper provide new insight into the nature of the non-robust features responsible for adversarial vulnerability of neural network classifiers.","",""
1,"Peng Lu, Yang Gao, Hao Xi, Yabin Zhang, Chao Gao, Bing Zhou, Hongpo Zhang, Liwei Chen, Xiaobo Mao","KecNet: A Light Neural Network for Arrhythmia Classification Based on Knowledge Reinforcement",2021,"","","","",98,"2022-07-13 09:28:50","","10.1155/2021/6684954","","",,,,,1,1.00,0,9,1,"Acquiring electrocardiographic (ECG) signals and performing arrhythmia classification in mobile device scenarios have the advantages of short response time, almost no network bandwidth consumption, and human resource savings. In recent years, deep neural networks have become a popular method to efficiently and accurately simulate nonlinear patterns of ECG data in a data-driven manner but require more resources. Therefore, it is crucial to design deep learning (DL) algorithms that are more suitable for resource-constrained mobile devices. In this paper, KecNet, a lightweight neural network construction scheme based on domain knowledge, is proposed to model ECG data by effectively leveraging signal analysis and medical knowledge. To evaluate the performance of KecNet, we use the Association for the Advancement of Medical Instrumentation (AAMI) protocol and the MIT-BIH arrhythmia database to classify five arrhythmia categories. The result shows that the ACC, SEN, and PRE achieve 99.31%, 99.45%, and 98.78%, respectively. In addition, it also possesses high robustness to noisy environments, low memory usage, and physical interpretability advantages. Benefiting from these advantages, KecNet can be applied in practice, especially wearable and lightweight mobile devices for arrhythmia classification.","",""
40,"J. Hao, Youngsoon Kim, Tae-Kyung Kim, Mingon Kang","PASNet: pathway-associated sparse deep neural network for prognosis prediction from high-throughput data",2018,"","","","",99,"2022-07-13 09:28:50","","10.1186/s12859-018-2500-z","","",,,,,40,10.00,10,4,4,"","",""
0,"Ethan Harris, Daniela Mihai, Jonathon S. Hare","How Convolutional Neural Network Architecture Biases Learned Opponency and Color Tuning",2021,"","","","",100,"2022-07-13 09:28:50","","10.1162/neco_a_01356","","",,,,,0,0.00,0,3,1,"Recent work suggests that changing convolutional neural network (CNN) architecture by introducing a bottleneck in the second layer can yield changes in learned function. To understand this relationship fully requires a way of quantitatively comparing trained networks. The fields of electrophysiology and psychophysics have developed a wealth of methods for characterizing visual systems that permit such comparisons. Inspired by these methods, we propose an approach to obtaining spatial and color tuning curves for convolutional neurons that can be used to classify cells in terms of their spatial and color opponency. We perform these classifications for a range of CNNs with different depths and bottleneck widths. Our key finding is that networks with a bottleneck show a strong functional organization: almost all cells in the bottleneck layer become both spatially and color opponent, and cells in the layer following the bottleneck become nonopponent. The color tuning data can further be used to form a rich understanding of how color a network encodes color. As a concrete demonstration, we show that shallower networks without a bottleneck learn a complex nonlinear color system, whereas deeper networks with tight bottlenecks learn a simple channel opponent code in the bottleneck layer. We develop a method of obtaining a hue sensitivity curve for a trained CNN that enables high-level insights that complement the low-level findings from the color tuning data. We go on to train a series of networks under different conditions to ascertain the robustness of the discussed results. Ultimately our methods and findings coalesce with prior art, strengthening our ability to interpret trained CNNs and furthering our understanding of the connection between architecture and learned representation. Trained models and code for all experiments are available at https://github.com/ecs-vlc/opponency.","",""
0,"Ethan Harris, Daniela Mihai, Jonathon S. Hare","How Convolutional Neural Network Architecture Biases Learned Opponency and Color Tuning",2021,"","","","",101,"2022-07-13 09:28:50","","10.1162/neco_a_01356","","",,,,,0,0.00,0,3,1,"Recent work suggests that changing convolutional neural network (CNN) architecture by introducing a bottleneck in the second layer can yield changes in learned function. To understand this relationship fully requires a way of quantitatively comparing trained networks. The fields of electrophysiology and psychophysics have developed a wealth of methods for characterizing visual systems that permit such comparisons. Inspired by these methods, we propose an approach to obtaining spatial and color tuning curves for convolutional neurons that can be used to classify cells in terms of their spatial and color opponency. We perform these classifications for a range of CNNs with different depths and bottleneck widths. Our key finding is that networks with a bottleneck show a strong functional organization: almost all cells in the bottleneck layer become both spatially and color opponent, and cells in the layer following the bottleneck become nonopponent. The color tuning data can further be used to form a rich understanding of how color a network encodes color. As a concrete demonstration, we show that shallower networks without a bottleneck learn a complex nonlinear color system, whereas deeper networks with tight bottlenecks learn a simple channel opponent code in the bottleneck layer. We develop a method of obtaining a hue sensitivity curve for a trained CNN that enables high-level insights that complement the low-level findings from the color tuning data. We go on to train a series of networks under different conditions to ascertain the robustness of the discussed results. Ultimately our methods and findings coalesce with prior art, strengthening our ability to interpret trained CNNs and furthering our understanding of the connection between architecture and learned representation. Trained models and code for all experiments are available at https://github.com/ecs-vlc/opponency.","",""
2,"Tianyu Du, S. Ji, Lujia Shen, Yao Zhang, Jinfeng Li, Jie Shi, Chengfang Fang, Jianwei Yin, R. Beyah, Ting Wang","Cert-RNN: Towards Certifying the Robustness of Recurrent Neural Networks",2021,"","","","",102,"2022-07-13 09:28:50","","10.1145/3460120.3484538","","",,,,,2,2.00,0,10,1,"Certifiable robustness, the functionality of verifying whether the given region surrounding a data point admits any adversarial example, provides guaranteed security for neural networks deployed in adversarial environments. A plethora of work has been proposed to certify the robustness of feed-forward networks, e.g., FCNs and CNNs. Yet, most existing methods cannot be directly applied to recurrent neural networks (RNNs), due to their sequential inputs and unique operations. In this paper, we present Cert-RNN, a general framework for certifying the robustness of RNNs. Specifically, through detailed analysis for the intrinsic property of the unique function in different ranges, we exhaustively discuss different cases for the exact formula of bounding planes, based on which we design several precise and efficient abstract transformers for the unique calculations in RNNs. Cert-RNN significantly outperforms the state-of-the-art methods (e.g., POPQORN) in terms of (i) effectiveness -- it provides much tighter robustness bounds, and (ii) efficiency -- it scales to much more complex models. Through extensive evaluation, we validate Cert-RNN's superior performance across various network architectures (e.g., vanilla RNN and LSTM) and applications (e.g., image classification, sentiment analysis, toxic comment detection, and malicious URL detection). For instance, for the RNN-2-32 model on the MNIST sequence dataset, the robustness bound certified by Cert-RNN is on average 1.86 times larger than that by POPQORN. Besides certifying the robustness of given RNNs, Cert-RNN also enables a range of practical applications including evaluating the provable effectiveness for various defenses (i.e., the defense with a larger robustness region is considered to be more robust), improving the robustness of RNNs (i.e., incorporating Cert-RNN with verified robust training) and identifying sensitive words (i.e., the word with the smallest certified robustness bound is considered to be the most sensitive word in a sentence), which helps build more robust and interpretable deep learning systems. We will open-source Cert-RNN for facilitating the DNN security research.","",""
32,"Jiaming Shen, Zhihong Shen, Chenyan Xiong, Chi Wang, Kuansan Wang, Jiawei Han","TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network",2020,"","","","",103,"2022-07-13 09:28:50","","10.1145/3366423.3380132","","",,,,,32,16.00,5,6,2,"Taxonomies consist of machine-interpretable semantics and provide valuable knowledge for many web applications. For example, online retailers (e.g., Amazon and eBay) use taxonomies for product recommendation, and web search engines (e.g., Google and Bing) leverage taxonomies to enhance query understanding. Enormous efforts have been made on constructing taxonomies either manually or semi-automatically. However, with the fast-growing volume of web content, existing taxonomies will become outdated and fail to capture emerging knowledge. Therefore, in many applications, dynamic expansions of an existing taxonomy are in great demand. In this paper, we study how to expand an existing taxonomy by adding a set of new concepts. We propose a novel self-supervised framework, named TaxoExpan, which automatically generates a set of ⟨query concept, anchor concept⟩ pairs from the existing taxonomy as training data. Using such self-supervision data, TaxoExpan learns a model to predict whether a query concept is the direct hyponym of an anchor concept. We develop two innovative techniques in TaxoExpan: (1) a position-enhanced graph neural network that encodes the local structure of an anchor concept in the existing taxonomy, and (2) a noise-robust training objective that enables the learned model to be insensitive to the label noise in the self-supervision data. Extensive experiments on three large-scale datasets from different domains demonstrate both the effectiveness and the efficiency of TaxoExpan for taxonomy expansion.","",""
3,"Tom Dupré la Tour, Mi Lu, Michael Eickenberg, J. Gallant","A finer mapping of convolutional neural network layers to the visual cortex",2021,"","","","",104,"2022-07-13 09:28:50","","","","",,,,,3,3.00,1,4,1,"There is increasing interest in understanding similarities and differences between convolutional neural networks (CNNs) and the visual cortex. A common approach is to use some specific layer of a pre-trained CNN as a source of features to predict brain activity recorded during a visual task. Associating each brain region to the best predicting CNN layer reveals a gradual change over the visual cortex. However, this winner-take-all mapping is non-robust, because consecutive CNN layers are strongly correlated and have similar prediction accuracies. Moreover, this mapping is usually performed on static stimuli, which ignores the temporal component of human vision. When the mapping is performed on video stimuli, the features are extracted frame-by-frame and downsampled using an anti-aliasing low-pass filter, which removes high temporal frequencies that could be informative. To address the first issue and improve the non-robust winner-take-all approach, we propose to fit a joint model on all layers simultaneously. The model is fit with banded ridge regression, where a separate regularization hyperparameter is learned for each layer. By performing a selection over layers, this model effectively removes non-predictive or redundant layers and disentangles the contributions of each layer. We show that using a joint model increases prediction accuracy and leads to finer mappings from CNN layers to the visual cortex. To address the second issue and preserve more high frequency information, we propose to filter the features with a set of band-pass filters. We show that using the envelopes of the filtered signals as additional features further increases prediction accuracy. Introduction Convolutional neural networks (CNNs) were inspired originally by the anatomy of the brain, and they have been remarkably successful in computer vision [1, 2]. However, these networks still fail in many tasks that humans can perform easily. Therefore, there is increasing interest in understanding the similarities and differences between CNNs and the brain. To investigate this issue, a common method is to use some specific layer of a pre-trained CNN as a source of features to fit a brain encoding model [3]. With this approach, many studies have shown that early CNN layers best predict brain activity in low-level visual areas, while late layers best predict brain activity in intermediate and higher-level visual areas, with gradual changes of layer mapping over the cortical surface [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]. A similar approach has also been applied to speech [14, 15] and language tasks [16, 17]. One problem with this approach is that there are strong correlations between CNN activations from one layer to the next. This confound causes different layers to have similar predictive power in encoding models [8, 16, 17, 18]. It is thus hard to separate which part of the predictive power is specific to a layer and which part is shared with other layers. Most studies ignore this issue and select the best-predicting layer for each voxel [5, 6, 8, 9, 14, 19, 11], but this winner-take-all approach is not robust and it ignores potential complementarities between layers. Some studies use variance partitioning [20] or canonical component analysis [21] to disentangle the different layers, but these approaches cannot disentangle more than two or three layers. 3rd Workshop on Shared Visual Representations in Human and Machine Intelligence (SVRHM 2021) of the Neural Information Processing Systems (NeurIPS) conference, Virtual. To address this issue, we use banded ridge regression, which has been shown to disentangle contributions from correlated feature spaces in encoding models [22]. Specifically, we fit a predictive model using features from all layers at once, grouping the features by layers, and learning optimal regularization for each layer through cross-validation. We show empirically that this joint model performs a selection over layers, effectively removing non-predictive or redundant layers, and disentangling the contributions of each layer on each voxel. Using this joint model increases prediction accuracy, and leads to smoother cortical maps of layer mapping. A second problem of the conventional approach is that it oversimplifies the temporal aspect of visual processing. Indeed, most studies only use static image stimuli [5, 6, 7, 8, 11, 13], which entirely ignores the temporal component of human vision. Some studies use video stimuli [8, 9] and extract features frame by frame from an image-based CNN. Then, the features are downsampled to the brain imaging sampling frequency (typically 0.5 Hz), using an anti-aliasing low-pass filter [8, 9]. However, this low-pass filter is suboptimal, because it removes valuable high-frequency information contained in the CNN activations. Indeed, a video stimulus induces brain activity linked to movement, and this brain activity has been shown to be poorly predicted by low-pass filtered static features [23]. To address this issue, we first use video stimuli and extract features frame by frame from an imagebased CNN. Then, to preserve more high temporal frequency information, we filter the features with a set of band-pass filters, and extract the envelope of the filtered signals. The envelopes are then used as additional features which increase prediction accuracy of the model. Note that we specifically do not use a video-based CNN to be able to compare both approaches on the same CNN architecture. We expect further gain in prediction accuracy by using the set of band-pass filters on features extracted from a video-based CNN. These two methodological improvements pave the way for high-precision mappings between CNNs and human brains, which may help both designing and interpreting CNNs, and defining highresolution information pathways over the cortical surface. 1 Conventional approach The conventional approach for mapping CNN layers to brain regions [4, 5, 6, 7, 8, 9, 19, 11, 12, 13] follows the voxelwise encoding model framework [24, 25]. First, brain activity is recorded while subjects perceive a visual stimulus. Then, the same stimulus is presented to a pretrained CNN, activations are extracted from intermediate CNN layers and preprocessed into features (see below). Finally, a regression model is trained on each voxel to predict brain activity from the features. 1.1 Feature extraction To extract features, each image of the stimulus is first presented to a pretrained image-based CNN. Then, the activations of one convolutional or fully-connected layer are extracted (typically after ReLU and max-pooling layers). With a video stimulus, features are extracted frame by frame from an imagebased CNN, before being down-sampled to the brain imaging sampling frequency (typically 0.5 Hz). Next, a compressive nonlinearity x 7→ log(1 + x) is applied, and features are centered individually along the train set. Then, to account for the delay between the stimulus and the hemodynamic response, features are either convolved with a hemodynamic response function, or duplicated with multiple temporal delays. This process is repeated for each CNN layer. Limitations. With a video stimulus, the feature extraction process contains a down-sampling step to match the brain imaging sampling frequency (typically 0.5 Hz). This down-sampling is typically done with an anti-aliasing low-pass filter [8, 9]. However, this low-pass filter likely removes valuable information from the CNN activations. Indeed, a video stimulus induces brain activity linked to movement, and this activity has been shown to be poorly predicted by low-pass filtered features [23]. 1.2 Winner-take-all model In the conventional approach, a separate ridge regression [26] is fit to the features extracted from each layer of the CNN independently. Then, the best layer is selected for each voxel, based on cross-validated prediction accuracy. Finally, differences in terms of selected layers are analyzed across the brain. The approach thus produces a mapping from CNN layers to brain regions.","",""
5,"Dimitrios Sakkos, Kevin D. McCay, Claire Marcroft, N. Embleton, Samiran Chattopadhyay, Edmond S. L. Ho","Identification of Abnormal Movements in Infants: A Deep Neural Network for Body Part-Based Prediction of Cerebral Palsy",2021,"","","","",105,"2022-07-13 09:28:50","","10.1109/ACCESS.2021.3093469","","",,,,,5,5.00,1,6,1,"The early diagnosis of cerebral palsy is an area which has recently seen significant multi-disciplinary research. Diagnostic tools such as the General Movements Assessment (GMA), have produced some very promising results, however these manual methods can be laborious. The prospect of automating these processes is seen as key in advancing this field of study. In our previous works, we examined the viability of using pose-based features extracted from RGB video sequences to undertake classification of infant body movements based upon the GMA. In this paper, we propose a new deep learning framework for this classification task. We also propose a visualization framework which identifies body-parts with the greatest contribution towards a classification decision. The inclusion of a visualization framework is an important step towards automation as it helps make the decisions made by the machine learning framework interpretable. We directly compare the proposed framework’s classification with several other methods from the literature using two independent datasets. Our experimental results show that the proposed method performs more consistently and more robustly than our previous pose-based techniques as well as other features from related works in this setting. We also find that our visualization framework helps provide greater interpretability, enhancing the likelihood of the adoption of these technologies within the medical domain.","",""
21,"G. Portwood, B. Nadiga, J. Saenz, D. Livescu","Interpreting neural network models of residual scalar flux",2020,"","","","",106,"2022-07-13 09:28:50","","10.1017/jfm.2020.861","","",,,,,21,10.50,5,4,2,"Abstract We show that, in addition to providing effective and competitive closures, when analysed in terms of the dynamics and physically relevant diagnostics, artificial neural networks (ANNs) can be both interpretable and provide useful insights into the on-going task of developing and improving turbulence closures. In the context of large-eddy simulations (LES) of a passive scalar in homogeneous isotropic turbulence, exact subfilter fluxes obtained by filtering direct numerical simulations are used both to train deep ANN models as a function of filtered variables, and to optimise the coefficients of a turbulent Prandtl number LES closure. A priori analysis of the subfilter scalar variance transfer rate demonstrates that learnt ANN models outperform optimised turbulent Prandtl number closures and Clark-type gradient models. Next, a posteriori solutions are obtained with each model over several integral time scales. These experiments reveal, with single- and multi-point diagnostics, that ANN models temporally track exact resolved scalar variance with greater accuracy compared to other subfilter flux models for a given filter length scale. Finally, we interpret the artificial neural networks statistically with differential sensitivity analysis to show that the ANN models feature a dynamics reminiscent of so-called ‘mixed models’, where mixed models are understood as comprising both a structural and functional component. Besides enabling enhanced-accuracy LES of passive scalars henceforth, we anticipate this work to contribute to utilising neural network models as a tool in interpretability, robustness and model discovery.","",""
10,"Julian Viereck, Jules Kozolinsky, Alexander Herzog, L. Righetti","Learning a Structured Neural Network Policy for a Hopping Task",2017,"","","","",107,"2022-07-13 09:28:50","","10.1109/LRA.2018.2861466","","",,,,,10,2.00,3,4,5,"In this letter, we present a method for learning a reactive policy for a simple dynamic locomotion task involving hard impact and switching contacts where we assume the contact location and contact timing to be unknown. To learn such a policy, we use optimal control to optimize a local controller for a fixed environment and contacts. We learn the contact-rich dynamics for our underactuated systems along these trajectories in a sample efficient manner. We use the optimized policies to learn the reactive policy in form of a neural network. Using a new neural network architecture, we are able to preserve more information from the local policy and make its output interpretable in the sense that its output in terms of desired trajectories, feedforward commands and gains can be interpreted. Extensive simulations demonstrate the robustness of the approach to changing environments, outperforming a model-free gradient policy based methods on the same tasks in simulation. Finally, we show that the learned policy can be robustly transferred on a real robot.","",""
8,"D. Valeriani, K. Simonyan","A microstructural neural network biomarker for dystonia diagnosis identified by a DystoniaNet deep learning platform",2020,"","","","",108,"2022-07-13 09:28:50","","10.1073/pnas.2009165117","","",,,,,8,4.00,4,2,2,"Significance This research identified a microstructural neural network biomarker for objective and accurate diagnosis of isolated dystonia based on the disorder pathophysiology using an advanced deep learning algorithm, DystoniaNet, and raw structural brain images of large cohorts of patients with isolated focal dystonia and healthy controls. DystoniaNet significantly outperformed shallow machine-learning pipelines and substantially exceeded the current agreement rates between clinicians, reaching an overall accuracy of 98.8% in diagnosing different forms of isolated focal dystonia. These results suggest that DystoniaNet could serve as an objective, robust, and generalizable algorithmic platform of dystonia diagnosis for enhanced clinical decision-making. Implementation of the identified biomarker for objective and accurate diagnosis of dystonia may be transformative for clinical management of this disorder. Isolated dystonia is a neurological disorder of heterogeneous pathophysiology, which causes involuntary muscle contractions leading to abnormal movements and postures. Its diagnosis is remarkably challenging due to the absence of a biomarker or gold standard diagnostic test. This leads to a low agreement between clinicians, with up to 50% of cases being misdiagnosed and diagnostic delays extending up to 10.1 y. We developed a deep learning algorithmic platform, DystoniaNet, to automatically identify and validate a microstructural neural network biomarker for dystonia diagnosis from raw structural brain MRIs of 612 subjects, including 392 patients with three different forms of isolated focal dystonia and 220 healthy controls. DystoniaNet identified clusters in corpus callosum, anterior and posterior thalamic radiations, inferior fronto-occipital fasciculus, and inferior temporal and superior orbital gyri as the biomarker components. These regions are known to contribute to abnormal interhemispheric information transfer, heteromodal sensorimotor processing, and executive control of motor commands in dystonia pathophysiology. The DystoniaNet-based biomarker showed an overall accuracy of 98.8% in diagnosing dystonia, with a referral of 3.5% of cases due to diagnostic uncertainty. The diagnostic decision by DystoniaNet was computed in 0.36 s per subject. DystoniaNet significantly outperformed shallow machine-learning algorithms in benchmark comparisons, showing nearly a 20% increase in its diagnostic performance. Importantly, the microstructural neural network biomarker and its DystoniaNet platform showed substantial improvement over the current 34% agreement on dystonia diagnosis between clinicians. The translational potential of this biomarker is in its highly accurate, interpretable, and generalizable performance for enhanced clinical decision-making.","",""
2,"Sara Aqab, Muhammad Usman","Handwriting Recognition using Artificial Intelligence Neural Network and Image Processing",2020,"","","","",109,"2022-07-13 09:28:50","","10.14569/ijacsa.2020.0110719","","",,,,,2,1.00,1,2,2,"Due to increased usage of digital technologies in all sectors and in almost all day to day activities to store and pass information, Handwriting character recognition has become a popular subject of research. Handwriting remains relevant, but people still want to have Handwriting copies converted into electronic copies that can be communicated and stored electronically. Handwriting character recognition refers to the computer's ability to detect and interpret intelligible Handwriting input from Handwriting sources such as touch screens, photographs, paper documents, and other sources. Handwriting characters remain complex since different individuals have different handwriting styles. This paper aims to report the development of a Handwriting character recognition system that will be used to read students and lectures Handwriting notes. The development is based on an artificial neural network, which is a field of study in artificial intelligence. Different techniques and methods are used to develop a Handwriting character recognition system. However, few of them focus on neural networks. The use of neural networks for recognizing Handwriting characters is more efficient and robust compared with other computing techniques. The paper also outlines the methodology, design, and architecture of the Handwriting character recognition system and testing and results of the system development. The aim is to demonstrate the effectiveness of neural networks for Handwriting character recognition.","",""
2,"G. Baudat, John B. Hayes","A star-test wavefront sensor using neural network analysis",2020,"","","","",110,"2022-07-13 09:28:50","","10.1117/12.2568018","","",,,,,2,1.00,1,2,2,"We describe a new, simple wavefront sensing method that uses a single measurement of a defocused star and a neural network to determine low-order wavefront components. The neural net is trained on computed diffracted star image data at 640 nm to output annular Zernike terms for an obscured circular aperture over a discrete range of all values. In the context of an actual star, the neural-net also provides the Fried’s parameter as an estimation of atmospheric turbulence. It is shown that the neural-net can produce a robust, high accuracy solution of the wavefront based on a single measurement. The method can also be used to simultaneously determine both on-axis and fielddependent wavefront performance from a single measurement of stars throughout the field. The prototype system can run at a rate of about 1 Hz with Python interpreted code, but higher speeds, up to video rates, are possible with compilation, proper hardware and optimization. This technique is particularly useful for low-order active-optics control and for optical alignment. A key advantage of this new method is that it only requires a single camera making it a simple cost-effective solution that can take advantage of an existing camera that may already be in an optical system. Results for this method are compared to high-precision interferometric data taken with a 4D Technology, PhaseCam interferometer and with an Innovations Foresight StarWave Shack Hartmann sensor from ALCOR SYSTEM under well-controlled conditions to validate performance. We also look at how the system has been implemented to use starlight for aligning multiple mirror telescopes in the presence of atmospheric seeing.","",""
4,"Praveenram Balachandar, K. Michmizos","A Spiking Neural Network Emulating the Structure of the Oculomotor System Requires No Learning to Control a Biomimetic Robotic Head",2020,"","","","",111,"2022-07-13 09:28:50","","10.1109/BioRob49111.2020.9224303","","",,,,,4,2.00,2,2,2,"Robotic vision introduces requirements for real-time processing of fast-varying, noisy information in a continuously changing environment. In a real-world environment, convenient assumptions, such as static camera systems and deep learning algorithms devouring high volumes of ideally slightlyvarying data are hard to survive. Leveraging on recent studies on the neural connectome associated with eye movements, we designed a neuromorphic oculomotor controller and placed it at the heart of our in-house biomimetic robotic head prototype. The controller is unique in the sense that (1) all data are encoded and processed by a spiking neural network (SNN), and (2) by mimicking the associated brain areas’ topology, the SNN is biologically interpretable and requires no training to operate. Here, we report the robot’s target tracking ability, demonstrate that its eye kinematics are similar to those reported in human eye studies and show that a biologically-constrained learning, although not required for the SNN’s function, can be used to further refine its performance. This work aligns with our ongoing effort to develop energy-efficient neuromorphic SNNs and harness their emerging intelligence to control biomimetic robots with versatility and robustness.","",""
1,"Ethan Harris, Daniela Mihai, Jonathon S. Hare","How Convolutional Neural Network Architecture Biases Learned Opponency and Colour Tuning",2020,"","","","",112,"2022-07-13 09:28:50","","","","",,,,,1,0.50,0,3,2,"Recent work suggests that changing Convolutional Neural Network (CNN) architecture by introducing a bottleneck in the second layer can yield changes in learned function. To understand this relationship fully requires a way of quantitatively comparing trained networks. The fields of electrophysiology and psychophysics have developed a wealth of methods for characterising visual systems which permit such comparisons. Inspired by these methods, we propose an approach to obtaining spatial and colour tuning curves for convolutional neurons, which can be used to classify cells in terms of their spatial and colour opponency. We perform these classifications for a range of CNNs with different depths and bottleneck widths. Our key finding is that networks with a bottleneck show a strong functional organisation: almost all cells in the bottleneck layer become both spatially and colour opponent, cells in the layer following the bottleneck become non-opponent. The colour tuning data can further be used to form a rich understanding of how colour is encoded by a network. As a concrete demonstration, we show that shallower networks without a bottleneck learn a complex non-linear colour system, whereas deeper networks with tight bottlenecks learn a simple channel opponent code in the bottleneck layer. We further develop a method of obtaining a hue sensitivity curve for a trained CNN which enables high level insights that complement the low level findings from the colour tuning data. We go on to train a series of networks under different conditions to ascertain the robustness of the discussed results. Ultimately, our methods and findings coalesce with prior art, strengthening our ability to interpret trained CNNs and furthering our understanding of the connection between architecture and learned representation. Trained models and code for all experiments are available at https://github.com/ecs-vlc/opponency.","",""
1,"Zhucheng Zhan, Noshad Hossenei, Olivier B. Poirion, M. Westerhoff, Eun-Young Choi, T. Ching, L. Garmire","Two-stage Neural-network based Prognosis Models using Pathological Image and Transcriptomic Data: An Application in Hepatocellular Carcinoma Patient Survival Prediction",2020,"","","","",113,"2022-07-13 09:28:50","","10.1101/2020.01.25.20016832","","",,,,,1,0.50,0,7,2,"Pathological images are easily accessible data type with potential as prognostic biomarkers. Here we extend Cox-nnet, a neural network based prognosis method previously used for transcriptomics data, to predict patient survival using hepatocellular carcinoma (HCC) pathological images. Cox-nnet based imaging predictions are more robust and accurate than Cox-PH. Moreover, using a novel two-stage Cox-nnet complex model, we are able to combine pathology image and transcriptomics RNA-Seq data to make impressively accurate prognosis predictions, with C-index close to 0.90 and log-ranked p-value of 4e-21 in the testing dataset. This work provides a new, biologically relevant and relatively interpretable solution to the challenge of integrating multi-modal and multiple types of data, particularly for survival prediction.","",""
1,"Ying L. Becker, Lingfeng Guo, Odilbek Nurmamatov","Assessing Asset Tail Risk with Artificial Intelligence: The Application of Artificial Neural Network",2020,"","","","",114,"2022-07-13 09:28:50","","10.1108/s2514-465020200000008002","","",,,,,1,0.50,0,3,2,"Value at risk (VaR) and expected shortfall (ES) are popular market risk measurements. The former is not coherent but robust, whereas the latter is coherent but less interpretable, only conditionally backtestable and less robust. In this chapter, we compare an innovative artificial neural network (ANN) model with a time series model in the context of forecasting VaR and ES of the univariate time series of four asset classes: US large capitalization equity index, European large cap equity index, US bond index, and US dollar versus euro exchange rate price index for the period of January 4, 1999, to December 31, 2018. In general, the ANN model has more favorable backtesting results as compared to the autoregressive moving average, generalized autoregressive conditional heteroscedasticity (ARMA-GARCH) time series model. In terms of forecasting accuracy, the ANN model has much fewer in-sample and out-of-sample exceptions than those of the ARMA-GARCH model.","",""
0,"Amit Sahu, Noelia V'allez, Rosana Rodr'iguez-Bobada, Mohamad Alhaddad, Omar Moured, G. Neugschwandtner","Application of the Neural Network Dependability Kit in Real-World Environments",2020,"","","","",115,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,6,2,"In this paper, we provide a guideline for using the Neural Network Dependability Kit (NNDK) during the development process of NN models, and show how the algorithm is applied in two image classification use cases. The case studies demonstrate the usage of the dependability kit to obtain insights about the NN model and how they informed the development process of the neural network model. After interpreting neural networks via the different metrics available in the NNDK, the developers were able to increase the NNs' accuracy, trust the developed networks, and make them more robust. In addition, we obtained a novel application-oriented technique to provide supporting evidence for an NN's classification result to the user. In the medical image classification use case, it was used to retrieve case images from the training dataset that were similar to the current patient's image and could therefore act as a support for the NN model's decision and aid doctors in interpreting the results.","",""
0,"Pau","Neural network signal understanding for instrumentation",2018,"","","","",116,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,1,4,"This paper reports on the use of neural signal interpretation theory and techniques for the purpose of classifying the shapes of a set of instrumentation signals, in order to calibrate devices, diagnose anomalies, generate tuning/settings, and interpret the measurement results. Neural signal understanding research is surveyed, and the selected implementation is described with its performance in terms of correct classification rates and robustness to noise. Formal results on neural net training time and sensitivity to weights are given. A theory for neural control is given using functional link nets and an explanation technique is designed to help neural signal understanding. The results of this are compared to those of a knowledge-based signal interpretation system within the context of the same specific instrument and data. Keywords-Neural understanding, calibration, signal understanding, control theory, neural control, training time, sensitivity to noise, explanation facilities, knowledge-based signal interpretation, instrumentation, analytical instrumentation.","",""
0,"Pau","Neural network signal understanding for instrumentation",2018,"","","","",117,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,1,4,"This paper reports on the use of neural signal interpretation theory and techniques for the purpose of classifying the shapes of a set of instrumentation signals, in order to calibrate devices, diagnose anomalies, generate tuning/settings, and interpret the measurement results. Neural signal understanding research is surveyed, and the selected implementation is described with its performance in terms of correct classification rates and robustness to noise. Formal results on neural net training time and sensitivity to weights are given. A theory for neural control is given using functional link nets and an explanation technique is designed to help neural signal understanding. The results of this are compared to those of a knowledge-based signal interpretation system within the context of the same specific instrument and data. Keywords-Neural understanding, calibration, signal understanding, control theory, neural control, training time, sensitivity to noise, explanation facilities, knowledge-based signal interpretation, instrumentation, analytical instrumentation.","",""
0,"Pau","Neural network signal understanding for instrumentation",2018,"","","","",118,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,1,4,"This paper reports on the use of neural signal interpretation theory and techniques for the purpose of classifying the shapes of a set of instrumentation signals, in order to calibrate devices, diagnose anomalies, generate tuning/settings, and interpret the measurement results. Neural signal understanding research is surveyed, and the selected implementation is described with its performance in terms of correct classification rates and robustness to noise. Formal results on neural net training time and sensitivity to weights are given. A theory for neural control is given using functional link nets and an explanation technique is designed to help neural signal understanding. The results of this are compared to those of a knowledge-based signal interpretation system within the context of the same specific instrument and data. Keywords-Neural understanding, calibration, signal understanding, control theory, neural control, training time, sensitivity to noise, explanation facilities, knowledge-based signal interpretation, instrumentation, analytical instrumentation.","",""
405,"David Alvarez-Melis, T. Jaakkola","Towards Robust Interpretability with Self-Explaining Neural Networks",2018,"","","","",119,"2022-07-13 09:28:50","","","","",,,,,405,101.25,203,2,4,"Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general – explicitness, faithfulness, and stability – and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.","",""
101,"Fatemeh Fahimi, Zhuo Zhang, Wooi-Boon Goh, Tih-Shih Lee, K. Ang, Cuntai Guan","Inter-subject transfer learning with an end-to-end deep convolutional neural network for EEG-based BCI.",2019,"","","","",120,"2022-07-13 09:28:50","","10.1088/1741-2552/aaf3f6","","",,,,,101,33.67,17,6,3,"OBJECTIVE Despite the effective application of deep learning (DL) in brain-computer interface (BCI) systems, the successful execution of this technique, especially for inter-subject classification, in cognitive BCI has not been accomplished yet. In this paper, we propose a framework based on the deep convolutional neural network (CNN) to detect the attentive mental state from single-channel raw electroencephalography (EEG) data.   APPROACH We develop an end-to-end deep CNN to decode the attentional information from an EEG time series. We also explore the consequences of input representations on the performance of deep CNN by feeding three different EEG representations into the network. To ensure the practical application of the proposed framework and avoid time-consuming re-training, we perform inter-subject transfer learning techniques as a classification strategy. Eventually, to interpret the learned attentional patterns, we visualize and analyse the network perception of the attention and non-attention classes.   MAIN RESULTS The average classification accuracy is 79.26%, with only 15.83% of 120 subjects having an accuracy below 70% (a generally accepted threshold for BCI). This is while with the inter-subject approach, it is literally difficult to output high classification accuracy. This end-to-end classification framework surpasses conventional classification methods for attention detection. The visualization results demonstrate that the learned patterns from the raw data are meaningful.   SIGNIFICANCE This framework significantly improves attention detection accuracy with inter-subject classification. Moreover, this study sheds light on the research on end-to-end learning; the proposed network is capable of learning from raw data with the least amount of pre-processing, which in turn eliminates the extensive computational load of time-consuming data preparation and feature extraction.","",""
2,"G. Portwood, B. Nadiga, J. Saenz, D. Livescu","Analysis and interpretation of out-performing neural network residual flux models",2020,"","","","",121,"2022-07-13 09:28:50","","","","",,,,,2,1.00,1,4,2,"We present novel approaches for the development, evaluation and interpretation of artificial neural networks (ANNs) for subfilter closures and demonstrate their usage in the context of large-eddy simulations (LES) of a passive scalar in homogeneous isotropic turbulence. Exact subfilter fluxes obtained by filtering direct numerical simulations (DNS) are used both to train deep ANN models as a function of filtered variables, and to optimise the coefficients of common spatio-temporally local LES closures. \textit{A-priori} analysis with respect to important dynamical features such as backscatter and subfilter scalar variance transfer rate, reveals that learnt ANN models out-performs optimised, turbulent Prandtl number closure models and gradient models. Next, \textit{a-posteriori} solutions are obtained with each model over several integral timescales. These solutions are obtained by explicitly filtering DNS-resolved velocity in order to isolate sources of error to subfilter flux closure. These experiments reveal that ANN models temporally track resolved scalar variance with greater accuracy compared to other subfilter flux models for a given filter length scale. Similarly, moments of scalar two-point structure functions reveal that trained neural network models reproduce statistics of ground-truth DNS with greater fidelity compared to common algebraic closure models. Finally, we interpret the artificial neural networks statistically with differential sensitivity analysis to show that the ANN models learns dynamics reminiscent of so-called ""mixed models"", where mixed models are understood as comprising both a structural and functional component. Besides enabling enhanced-accuracy LES of passive scalars henceforth, we anticipate this work to contribute to utilising well-performing neural network models as a tool in interpretability, robustness and model discovery.","",""
16,"Leandro Maciel, R. Ballini","DESIGN A NEURAL NETWORK FOR TIME SERIES FINANCIAL FORECASTING: ACCURACY AND ROBUSTNESS ANALISYS",2009,"","","","",122,"2022-07-13 09:28:50","","","","",,,,,16,1.23,8,2,13,"Normal 0 21 Neural Networks are an artificial intelligence method for modeling complex target functions. For certain types of problems, such as learning to interpret complex real-world sensor data, Artificial Neural Networks (ANNs) are among the most effective learning methods currently know. During the last decade they have been widely applied to the domain of financial time series prediction and their importance in this field is growing. Also, it is the Artificial Intelligence Technique that deals best with uncertainly. The present work aims to analyze the neural networks for financial time series forecasting. Specifically the ability to predict future trends of North American, European and Brazilian Stock Markets. Accuracy is compared against a traditional forecasting method, generalized autoregressive conditional heteroscedasticity (GARCH). Furthermore, it is examined the best choice of network design for each sample of data. It was concluded that ANNs do have the capability to forecast the stock markets studied and, if properly trained, can improve the robustness according to the network structure.","",""
409,"Shuai Li, W. Li, Chris Cook, Ce Zhu, Yanbo Gao","Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN",2018,"","","","",123,"2022-07-13 09:28:50","","10.1109/CVPR.2018.00572","","",,,,,409,102.25,82,5,4,"Recurrent neural networks (RNNs) have been widely used for processing sequential data. However, RNNs are commonly difficult to train due to the well-known gradient vanishing and exploding problems and hard to learn long-term patterns. Long short-term memory (LSTM) and gated recurrent unit (GRU) were developed to address these problems, but the use of hyperbolic tangent and the sigmoid action functions results in gradient decay over layers. Consequently, construction of an efficiently trainable deep network is challenging. In addition, all the neurons in an RNN layer are entangled together and their behaviour is hard to interpret. To address these problems, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed in this paper, where neurons in the same layer are independent of each other and they are connected across layers. We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs. Experimental results have shown that the proposed IndRNN is able to process very long sequences (over 5000 time steps), can be used to construct very deep networks (21 layers used in the experiment) and still be trained robustly. Better performances have been achieved on various tasks by using IndRNNs compared with the traditional RNN and LSTM.","",""
35,"Gunjan Verma, A. Swami","Error Correcting Output Codes Improve Probability Estimation and Adversarial Robustness of Deep Neural Networks",2019,"","","","",124,"2022-07-13 09:28:50","","","","",,,,,35,11.67,18,2,3,"Modern machine learning systems are susceptible to adversarial examples; inputs which clearly preserve the characteristic semantics of a given class, but whose classification is (usually confidently) incorrect. Existing approaches to adversarial defense generally rely on modifying the input, e.g. quantization, or the learned model parameters, e.g. via adversarial training. However, recent research has shown that most such approaches succumb to adversarial examples when different norms or more sophisticated adaptive attacks are considered. In this paper, we propose a fundamentally different approach which instead changes the way the output is represented and decoded. This simple approach achieves state-of-the-art robustness to adversarial examples for L 2 and L ∞ based adversarial perturbations on MNIST and CIFAR10. In addition, even under strong white-box attacks, we find that our model often assigns adversarial examples a low probability; those with high probability are usually interpretable, i.e. perturbed towards the perceptual boundary between the original and adversarial class. Our approach has several advantages: it yields more meaningful probability estimates, is extremely fast during training and testing, requires essentially no architectural changes to existing discriminative learning pipelines, is wholly complementary to other defense approaches including adversarial training, and does not sacrifice benign test set performance","",""
1,"Haozhe Lin, Yushun Fan, Jia Zhang, Bing Bai","MSP-RNN: Multi-Step Piecewise Recurrent Neural Network for Predicting the Tendency of Services Invocation",2022,"","","","",125,"2022-07-13 09:28:50","","10.1109/tsc.2020.2966487","","",,,,,1,1.00,0,4,1,"Driven by the widespread application of Service-Oriented Architecture (SOA), an increasing number of services and mashups have been developed and published onto the Internet in the past decades. With the number keeping on burgeoning, predicting the tendency of services invocation will provide various roles in service ecosystems with promising opportunities. However, services invocation bear three unique characteristics, which give rise to difficulties in predicting them. First, enormous services show different and complicated traits, like periodicity, nonlinearity and nonstationarity. Second, services providing similar or compensatory functions make up intricate relationship. Third, the combination dependencies between mashups and their comprising component services further amplify the difficulty. Given these factors, we have developed a tailored model Multi-Step Piecewise Recurrent Neural Network (MSP-RNN) to predict the tendency of services invocation. In MSP-RNN, Long Short Term Memory (LSTM) units are used to extract universal features. Based on these features, we have developed a piecewise regressive mechanism to make prediction discriminatingly. Besides, we have developed a multi-step prediction strategy to further enhance prediction accuracy and robustness. Extensive experiments in real-world data set with interpretable analysis show that MSP-RNN predicts the tendency of services invocation more accurately, i.e., by 3.7 percent in terms of symmetric mean absolute percentage error (SMAPE), than state-of-the-art baseline methods.","",""
46,"Xiaoxiao Li, N. Dvornek, Yuan Zhou, Juntang Zhuang, P. Ventola, J. Duncan","Graph Neural Network for Interpreting Task-fMRI Biomarkers",2019,"","","","",126,"2022-07-13 09:28:50","","10.1007/978-3-030-32254-0_54","","",,,,,46,15.33,8,6,3,"","",""
12,"Diego Manzanas Lopez, Patrick Musau, Hoang-Dung Tran, Taylor T. Johnson","Verification of Closed-loop Systems with Neural Network Controllers",2019,"","","","",127,"2022-07-13 09:28:50","","10.29007/btv1","","",,,,,12,4.00,3,4,3,"This benchmark suite presents a detailed description of a series of closed-loop control systems with artificial neural network controllers. In many applications, feed-forward neural networks are heavily involved in the implementation of controllers by learning and representing control laws through several methods such as model predictive control (MPC) and reinforcement learning (RL). The type of networks that we consider in this manuscript are feed-forward neural networks consisting of multiple hidden layers with ReLU activation functions and a linear activation function in the output layer. While neural network controllers have been able to achieve desirable performance in many contexts, they also present a unique challenge in that it is difficult to provide any guarantees about the correctness of their behavior or reason about the stability a system that employs their use. Thus, from a controls perspective, it is necessary to verify them in conjunction with their corresponding plants in closed-loop. While there have been a handful of works proposed towards the verification of closed-loop systems with feed-forward neural network controllers, this area still lacks attention and a unified set of benchmark examples on which verification techniques can be evaluated and compared. Thus, to this end, we present a range of closed-loop control systems ranging from two to six state variables, and a range of controllers with sizes in the range of eleven neurons to a few hundred neurons in more complex systems. Category: Academic Difficulty: High Acknowledgement The material presented in this paper is based upon work supported by the National Science Foundation (NSF) under grant number SHF 1736323, the Air Force Office of Scientific Research (AFOSR) through contract numbers FA9550-15-1-0258, FA9550-16-10246, and FA9550-18-1-0122, and the Defense Advanced Research Projects Agency (DARPA) through contract number FA8750-18-C-0089. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of AFOSR, DARPA, or NSF G. Frehse and M. Althoff (eds.), ARCH19 (EPiC Series in Computing, vol. 61), pp. 201–210 Closed-loop Systems with Neural Network Controllers Manzanas Lopez, Musau, Tran and Johnson 1 Context and Origins. In recent years, advances in Artficial Intelligence (AI) have enabled a diverse range of technologies that are directly impacting people’s everyday lives [16]. Particularly, within this space, machine learning methods such as Deep Learning (DL) have achieved levels of accuracy and performance that are competitive or better than humans for tasks such as pattern and image recognition [12], natural language processing [7], and knowledge representation and reasoning [15,22]. Despite these achievements, there have been reservations about incorporating them into safety critical systems [11] due to their susceptibility to unexpected and errant behavior from a slight perturbation in their inputs [18]. Furthermore, neural networks are often viewed as ""black boxes"" since the underlying operation of the neuron activations is often indiscernible [22]. In light of these challenges, there has been significant work towards the creation of methods and verification tools that can formally reason about the behavior of neural networks [22]. However, the vast majority of these techniques have only been able to deal with feed-forward neural networks with piecewise-linear activation functions [4]. Additionally, the bulk of these methods have primarily considered the verification of input-output properties of neural networks in isolation [22], and there are only a handful of works that have explicitly addressed the verification of closed-loop control systems with neural network controllers [5, 8, 19–21]. One of the central challenges in verifying neural network control systems is that applying existing methodology to these systems is not straightforward [9], and a simple combination of verification tools for non-linear ordinary differential equations along with a neural network reachability tool suffers from severe overestimation errors [5]. Still, the verification of closed loop neural network systems is deeply important as they naturally arise in safety critical systems [5] such as autonomous vehicles, and complex control systems that make use of model predictive control and reinforcement learning [16]. Thus, there is a compelling need for methods and advanced software tools that can effectively deal with the complexities exhibited by these systems [5]. Inspired by a shortage of verification methods for closed-loop neural network control systems in the research literature, the central contribution of this paper is the provision of a set of executable benchmarks that have been synthesized using methods such as reinforcement learning [17], and model predictive control [14]. The problems elucidated in the paper are modeled using Simulink/Stateflow (SLSF) and are available at the following github repository1. We aim to provide a thorough problem description to which the numerous tools and approaches for non-linear systems and neural network verification present in the research community can be evaluated and compared [22]. If the research community is able to devise acceptable solutions to the aformentioned challenges they will stimulate the development of robust and intelligent systems with the potential to bring unparalleled benefits to numerous application domains. 2 Description of benchmarks. In this manuscript, we present a set of linear and non-linear closed-loop systems with continuoustime plants and feedforward neural networks controllers trained using different controls schemes such as reinforcement learning or model predictive control (MPC). A typical architecture describing the structure of these systems is displayed in Figure 2.1. All the neural networks 1https://github.com/verivital/ARCH-2019","",""
10,"Sampo Kuutti, R. Bowden, Harita Joshi, Robert de Temple, Saber Fallah","Safe Deep Neural Network-Driven Autonomous Vehicles Using Software Safety Cages",2019,"","","","",128,"2022-07-13 09:28:50","","10.1007/978-3-030-33617-2_17","","",,,,,10,3.33,2,5,3,"","",""
0,"S. Wein, A. Schüller, Ana Maria Tom'e, W. M. Malloni, M. Greenlee, E. Lang","Forecasting Brain Activity Based on Models of Spatio-Temporal Brain Dynamics: A Comparison of Graph Neural Network Architectures",2021,"","","","",129,"2022-07-13 09:28:50","","10.1162/netn_a_00252","","",,,,,0,0.00,0,6,1,"  Comprehending the interplay between spatial and temporal characteristics of neural dynamics can contribute to our understanding of information processing in the human brain. Graph neural networks (GNNs) provide a new possibility to interpret graph structured signals like those observed in complex brain networks. In our study we compare different spatio-temporal GNN architectures and study their ability to model neural activity distributions obtained in functional MRI (fMRI) studies. We evaluate the performance of the GNN models on a variety of scenarios in MRI studies and also compare it to a VAR model, which is currently often used for directed functional connectivity analysis. We show that by learning localized functional interactions on the anatomical substrate, GNN based approaches are able to robustly scale to large network studies, even when available data are scarce. By including anatomical connectivity as the physical substrate for information propagation, such GNNs also provide a multi-modal perspective on directed connectivity analysis, offering a novel possibility to investigate the spatio-temporal dynamics in brain networks.","",""
7,"Yongbing Zhang, Yangzhe Liu, Xiu Li, Shaowei Jiang, Krishna Dixit, Xinfeng Zhang, Xiangyang Ji","PgNN: Physics-guided Neural Network for Fourier Ptychographic Microscopy",2019,"","","","",130,"2022-07-13 09:28:50","","","","",,,,,7,2.33,1,7,3,"Fourier ptychography (FP) is a newly developed computational imaging approach that achieves both high resolution and wide field of view by stitching a series of low-resolution images captured under angle-varied illumination. So far, many supervised data-driven models have been applied to solve inverse imaging problems. These models need massive amounts of data to train, and are limited by the dataset characteristics. In FP problems, generic datasets are always scarce, and the optical aberration varies greatly under different acquisition conditions. To address these dilemmas, we model the forward physical imaging process as an interpretable physics-guided neural network (PgNN), where the reconstructed image in the complex domain is considered as the learnable parameters of the neural network. Since the optimal parameters of the PgNN can be derived by minimizing the difference between the model-generated images and real captured angle-varied images corresponding to the same scene, the proposed PgNN can get rid of the problem of massive training data as in traditional supervised methods. Applying the alternate updating mechanism and the total variation regularization, PgNN can flexibly reconstruct images with improved performance. In addition, the Zernike mode is incorporated to compensate for optical aberrations to enhance the robustness of FP reconstructions. As a demonstration, we show our method can reconstruct images with smooth performance and detailed information in both simulated and experimental datasets. In particular, when validated in an extension of a high-defocus, high-exposure tissue section dataset, PgNN outperforms traditional FP methods with fewer artifacts and distinguishable structures.","",""
13,"Igor Kvasić, N. Mišković, Z. Vukic","Convolutional Neural Network Architectures for Sonar-Based Diver Detection and Tracking",2019,"","","","",131,"2022-07-13 09:28:50","","10.1109/OCEANSE.2019.8867461","","",,,,,13,4.33,4,3,3,"Autonomous underwater navigation presents a whole set of challenges to be resolved in order to become adequately accurate and reliable. That is particularly critical when human divers work in close collaboration with autonomous underwater vehicles (AUVs). In absence of global positioning signals underwater, acoustic based sensors such as LBL (long-baseline), SBL (short-baseline) and USBL (ultrashort-baseline) are commonly used for navigation and localization. In addition to these low-bandwidth and high latency technologies, cameras and sonars can provide position measurements relative to the vehicle which can be used as an aid for navigation as well as for keeping a safe working distance between the diver and the AUV. While optical cameras are highly affected by water turbidity and lighting conditions, sonar images often become hard to interpret using conventional image processing methods due to image granulation and high noise levels.This paper focuses on finding a robust and reliable sonar image processing method for detection and tracking of human divers using convolutional neural networks. Machine learning algorithms are making a huge impact in computer vision applications but are not always considered when it comes to sonar image processing. After presenting commonly used image processing techniques the paper will focus on giving an overview of state-of-the-art machine learning algorithms and explore their performance in custom sonar image dataset processing. Finally, the performance of these algorithms will be compared on a set of sonar recordings to determine their reliability and applicability in a real-time operation.","",""
1,"Noor D. Al-shakarchy, I. H. Ali","Abnormal head movement classification using deep neural network DNN",2019,"","","","",132,"2022-07-13 09:28:50","","10.1063/1.5123123","","",,,,,1,0.33,1,2,3,"Abnormal head movements play a crucial role in diagnoisis of varity diseases. Moreover, different studies considered with these type of information. In addition, the gestures based mainly on head movement which can be employed in many applications such as using head-nodding or shaking to feedback content-related feedback, detect and interpret the emotion, gaze orientation, focus of attention, driver assistance system and so on. In this paper, a new method proposed to detect and classify the flopping head movements as normal or abnormal based on Convolution Neural Network CNN in the term of special sense interaction and behavioral studies with this movement. The proposed system based on deep learning employing the Convolution Neural Network CNN as most promising approach to deal with lighting condition (illumination change) and distortion and noise. Normal Abnormal Head Movement Dataset (NAHM) static images dataset gathered and used in proposed system implementation. This dataset provided the various image conditions and subjects to prevent the overfitting and under fitting problem that appears with publically datasets. The proposed frame work presents robust learning ability based on accuracy and lose functions which achieved training loss: 0.0106, training accuracy: 0.9980, validation loss: 0.0968 and validation accuracy: 0.9831.Abnormal head movements play a crucial role in diagnoisis of varity diseases. Moreover, different studies considered with these type of information. In addition, the gestures based mainly on head movement which can be employed in many applications such as using head-nodding or shaking to feedback content-related feedback, detect and interpret the emotion, gaze orientation, focus of attention, driver assistance system and so on. In this paper, a new method proposed to detect and classify the flopping head movements as normal or abnormal based on Convolution Neural Network CNN in the term of special sense interaction and behavioral studies with this movement. The proposed system based on deep learning employing the Convolution Neural Network CNN as most promising approach to deal with lighting condition (illumination change) and distortion and noise. Normal Abnormal Head Movement Dataset (NAHM) static images dataset gathered and used in proposed system implementation. This dataset provided the various image...","",""
1,"Cedrique Rovile Njieutcheu Tassi","Bayesian Convolutional Neural Network: Robustly Quantify Uncertainty for Misclassifications Detection",2019,"","","","",133,"2022-07-13 09:28:50","","10.1007/978-3-030-37548-5_10","","",,,,,1,0.33,1,1,3,"","",""
180,"Noah D. Brenowitz, C. Bretherton","Prognostic Validation of a Neural Network Unified Physics Parameterization",2018,"","","","",134,"2022-07-13 09:28:50","","10.1029/2018GL078510","","",,,,,180,45.00,90,2,4,"Weather and climate models approximate diabatic and sub‐grid‐scale processes in terms of grid‐scale variables using parameterizations. Current parameterizations are designed by humans based on physical understanding, observations, and process modeling. As a result, they are numerically efficient and interpretable, but potentially oversimplified. However, the advent of global high‐resolution simulations and observations enables a more robust approach based on machine learning. In this letter, a neural network‐based parameterization is trained using a near‐global aqua‐planet simulation with a 4‐km resolution (NG‐Aqua). The neural network predicts the apparent sources of heat and moisture averaged onto (160 km)2 grid boxes. A numerically stable scheme is obtained by minimizing the prediction error over multiple time steps rather than single one. In prognostic single‐column model tests, this scheme matches both the fluctuations and equilibrium of NG‐Aqua simulation better than the Community Atmosphere Model does.","",""
42,"G. Basalyga, E. Salinas","When Response Variability Increases Neural Network Robustness to Synaptic Noise",2005,"","","","",135,"2022-07-13 09:28:50","","10.1162/neco.2006.18.6.1349","","",,,,,42,2.47,21,2,17,"Cortical sensory neurons are known to be highly variable, in the sense that responses evoked by identical stimuli often change dramatically from trial to trial. The origin of this variability is uncertain, but it is usually interpreted as detrimental noise that reduces the computational accuracy of neural circuits. Here we investigate the possibility that such response variability might in fact be beneficial, because it may partially compensate for a decrease in accuracy due to stochastic changes in the synaptic strengths of a network. We study the interplay between two kinds of noise, response (or neuronal) noise and synaptic noise, by analyzing their joint influence on the accuracy of neural networks trained to perform various tasks. We find an interesting, generic interaction: when fluctuations in the synaptic connections are proportional to their strengths (multiplicative noise), a certain amount of response noise in the input neurons can significantly improve network performance, compared to the same network without response noise. Performance is enhanced because response noise and multiplicative synaptic noise are in some ways equivalent. So if the algorithm used to find the optimal synaptic weights can take into account the variability of the model neurons, it can also take into account the variability of the synapses. Thus, the connection patterns generated with response noise are typically more resistant to synaptic degradation than those obtained without response noise. As a consequence of this interplay, if multiplicative synaptic noise is present, it is better to have response noise in the network than not to have it. These results are demonstrated analytically for the most basic network consisting of two input neurons and one output neuron performing a simple classification task, but computer simulations show that the phenomenon persists in a wide range of architectures, including recurrent (attractor) networks and sensorimotor networks that perform coordinate transformations. The results suggest that response variability could play an important dynamic role in networks that continuously learn.","",""
0,"Mark R. P. Thomas, B. Martin, Katie A. Kowarski, B. Gaudet, S. Matwin","Interpreting the latent representations of a convolutional neural network trained on spectrograms",2019,"","","","",136,"2022-07-13 09:28:50","","10.1121/1.5137277","","",,,,,0,0.00,0,5,3,"Recent work [1,2] has shown that Convolutional Neural Networks (CNNs) trained on spectrograms of acoustic signals are capable of learning high-level latent representations for the purpose of detecting and classifying the vocalizations of endangered baleen whales. The aforementioned latent representations were used in the development of an automated system that was capable of detecting the vocalizations of blue, fin, and sei whales against non-biological and ambient noise sources to a high degree of accuracy (0.961, F-1 Score = 0.899). In this work, we conduct an exploratory analysis of the same latent representations using statistical machine learning approaches as well as by visualizing the convolutional feature maps learned by the CNN. Through this analysis we attempt to interpret what properties of a spectrogram are easily and/or most often exploited by the CNN during training in order to improve upon the state-of-the-art and develop more robust detection systems going forward. [1] M. Thomas, B. Martin, K. Kowarski, B. Gaudet, and S. Matwin, Marine Mammal Species Classification using Convolutional Neural Networks and a Novel Acoustic Representation, ECML PKDD 2019 (Springer, Cham, 2019). [2] M. Thomas, ""Towards a novel data representation for classifying acoustic signals,"" in Canadian Conference on Artificial Intelligence (Springer, Cham, 2019).","",""
22,"Matthew L. Leavitt, Ari S. Morcos","Towards falsifiable interpretability research",2020,"","","","",137,"2022-07-13 09:28:50","","","","",,,,,22,11.00,11,2,2,"Methods for understanding the decisions of and mechanisms underlying deep neural networks (DNNs) typically rely on building intuition by emphasizing sensory or semantic features of individual examples. For instance, methods aim to visualize the components of an input which are ""important"" to a network's decision, or to measure the semantic properties of single neurons. Here, we argue that interpretability research suffers from an over-reliance on intuition-based approaches that risk-and in some cases have caused-illusory progress and misleading conclusions. We identify a set of limitations that we argue impede meaningful progress in interpretability research, and examine two popular classes of interpretability methods-saliency and single-neuron-based approaches-that serve as case studies for how overreliance on intuition and lack of falsifiability can undermine interpretability research. To address these concerns, we propose a strategy to address these impediments in the form of a framework for strongly falsifiable interpretability research. We encourage researchers to use their intuitions as a starting point to develop and test clear, falsifiable hypotheses, and hope that our framework yields robust, evidence-based interpretability methods that generate meaningful advances in our understanding of DNNs.","",""
6,"Souvik Kundu, K. Sim, M. Gales","Incorporating a Generative Front-End Layer to Deep Neural Network for Noise Robust Automatic Speech Recognition",2016,"","","","",138,"2022-07-13 09:28:50","","10.21437/Interspeech.2016-760","","",,,,,6,1.00,2,3,6,"It is difficult to apply well-formulated model-based noise adaptation approaches to Deep Neural Network (DNN) due to the lack of interpretability of the model parameters. In this paper, we propose incorporating a generative front-end layer (GFL), which is parameterised by Gaussian Mixture Model (GMM), into the DNN. A GFL can be easily adapted to different noise conditions by applying the model-based Vector Taylor Series (VTS) to the underlying GMM. We show that incorporating a GFL to DNN yields 12.1% relative improvement over a baseline multi-condition DNN. We also show that the proposed system performs significantly better than the noise aware training method, where the per-utterance estimated noise parameters are appended to the acoustic features.","",""
2,"Hyungyu Lee, Ho Bae, Sungroh Yoon","Gradient Masking of Label Smoothing in Adversarial Robustness",2021,"","","","",139,"2022-07-13 09:28:50","","10.1109/ACCESS.2020.3048120","","",,,,,2,2.00,1,3,1,"Deep neural networks (DNNs) have achieved impressive results in several image classification tasks. However, these architectures are unstable for adversarial examples (AEs) such as inputs crafted by a hardly perceptible perturbation with the intent of causing neural networks to make errors. AEs must be considered to prevent accidents in areas such as unmanned car driving using visual object detection in Internet of Things (IoT) networks. Gaussian noise with label smoothing or logit squeezing can be used to increase the robustness against AEs in the training of DNNs. However, from a model interpretability aspect, Gaussian noise with label smoothing does not increase the adversarial robustness of the model. To resolve this problem, we tackle the AE instead of measuring the accuracy of the model against AEs. Considering that a robust model shows a small curvature of the loss surface, we propose a metric to measure the strength of the AEs and the robustness of the model. Furthermore, we introduce a method to verify the existence of the obfuscated gradients of the model based on the black-box attack sanity check method. The proposed method enables us to identify a gradient masking problem wherein the model does not provide useful gradients and exploits false defenses. We evaluate our technique against representative adversarially trained models using the CIFAR10, CIFAR100, SVHN, and Restricted ImageNet datasets. Our results show that the performance of some false defense models decreases by up to 32% compared to the previous evaluation metrics. Moreover, our metric reveals that traditional metrics used to measure the robustness of the model may produce false results.","",""
44,"Minh Nguyen Nhat To, Q. Vu, B. Turkbey, P. Choyke, J. T. Kwak","Deep dense multi-path neural network for prostate segmentation in magnetic resonance imaging",2018,"","","","",140,"2022-07-13 09:28:50","","10.1007/s11548-018-1841-4","","",,,,,44,11.00,9,5,4,"","",""
1,"Vignesh Srinivasan, Nils Strodthoff, Jackie Ma, Alexander Binder, K. Muller, W. Samek","On the Robustness of Pretraining and Self-Supervision for a Deep Learning-based Analysis of Diabetic Retinopathy",2021,"","","","",141,"2022-07-13 09:28:50","","","","",,,,,1,1.00,0,6,1,"Objective: There is an increasing number of medical use-cases where classification algorithms based on deep neural networks reach performance levels that are competitive with human medical experts. To alleviate the challenges of small dataset sizes, these systems often rely on pretraining. In this work, we aim to assess the broader implications of these approaches. Methods: For diabetic retinopathy grading as exemplary use case, we compare the impact of different training procedures including recently established self-supervised pretraining methods based on contrastive learning. To this end, we investigate different aspects such as quantitative performance, statistics of the learned feature representations, interpretability and robustness to image distortions. Results: Our results indicate that models initialized from ImageNet pretraining report a significant increase in performance, generalization and robustness to image distortions. In particular, selfsupervised models show further benefits to supervised models. Conclusion: Self-supervised models with ini∗VS, NS, JM and WS are with the Department of Artificial Intelligence, Fraunhofer Heinrich Hertz Institute, 10587 Berlin, Germany (email: firstname.lastname@hhi.fraunhofer.de). NS and WS are also with BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany. AB is with the Department of Informatics, Oslo University, 0373 Oslo, Norway. KRM is with the Machine Learning Group, Technische Universität Berlin, 10587 Berlin, Germany, and also with the Department of Artificial Intelligence, Korea University, Seoul 136-713, South Korea, the Max Planck Institute for Informatics, 66123 Saarbrücken, Germany, and BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany. (e-mail: klaus-robert.mueller@tuberlin.de). tialization from ImageNet pretraining not only report higher performance, they also reduce overfitting to large lesions along with improvements in taking into account minute lesions indicative of the progression of the disease. Significance: Understanding the effects of pretraining in a broader sense that goes beyond simple performance comparisons is of crucial importance for the broader medical imaging community beyond the use-case considered in this work.","",""
692,"W. Samek, Alexander Binder, G. Montavon, S. Lapuschkin, K. Müller","Evaluating the Visualization of What a Deep Neural Network Has Learned",2015,"","","","",142,"2022-07-13 09:28:50","","10.1109/TNNLS.2016.2599820","","",,,,,692,98.86,138,5,7,"Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the “importance” of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.","",""
14,"Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, D. Dou","Interpretable Deep Learning: Interpretations, Interpretability, Trustworthiness, and Beyond",2021,"","","","",143,"2022-07-13 09:28:50","","","","",,,,,14,14.00,2,8,1,"Deep neural networks have been well-known for their superb performance in handling various machine learning and artificial intelligence tasks. However, due to their over-parameterized black-box nature, it is often difficult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal the ways that deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Specifically, we introduce and clarify two basic concepts— interpretations and interpretability—that people usually get confused. First of all, to address the research efforts in interpretations, we elaborate the design of several recent interpretation algorithms, from different perspectives, through proposing a new taxonomy. Then, to understand the results of interpretation, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the existing work in evaluating models’ interpretability using “trustworthy” interpretation algorithms. Finally, we review and discuss the connections between deep models’ interpretations and other factors, such as adversarial robustness and data augmentations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches.","",""
0,"P. Lagrave, M. Riou","Toward Geometrical Robustness with Hybrid Deep Learning and Differential Invariants Theory",2021,"","","","",144,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,2,1,"Symmetries are ubiquitous in physics problems and these should be taken into account when neural networks are used to approximate their solutions. Embedding symmetries within the neural networks by using equivariant layers has been shown to be efficient from an accuracy standpoint. Building equivariant structures also appears appealing from the robustness standpoint since the use of correct-by-design algorithms alleviates the verification step, which is a prerequisite to any critical applications such as safety and military related tasks. However, generically enforcing equivariance in neural networks requires the use of cumbersome operators such has group-based convolution kernels, for which the outputs may be hard to interpret. In this paper, we introduce EqPdeNet, an alternative method in which equivariant partial differential equations are embedded within the first layer of a neural network. This approach provides approximate equivariance with respect to any Lie group action and allows combining several types of equivariance within the same network. Moreover, the structure of the associated partial differential equations can be directly related to the physical nature of the input data, making this approach particularly appealing from an interpretability standpoint when compared to the use of group-based convolution kernels.","",""
3,"Sahil Singla, Surbhi Singla, S. Feizi","Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100",2021,"","","","",145,"2022-07-13 09:28:50","","","","",,,,,3,3.00,1,3,1,"Training convolutional neural networks (CNNs) with a strict Lipschitz constraint under the l2 norm is useful for provable adversarial robustness, interpretable gradients and stable training. While 1-Lipschitz CNNs can be designed by enforcing a 1-Lipschitz constraint on each layer, training such networks requires each layer to have an orthogonal Jacobian matrix (for all inputs) to prevent the gradients from vanishing during backpropagation. A layer with this property is said to be Gradient Norm Preserving (GNP). In this work, we introduce a procedure to certify the robustness of 1-Lipschitz CNNs by relaxing the orthogonalization of the last linear layer of the network that significantly advances the state of the art for both standard and provable robust accuracies on CIFAR-100 (gains of 4.80% and 4.71%, respectively). We further boost their robustness by introducing (i) a novel Gradient Norm preserving activation function called the Householder activation function (that includes every GroupSort activation) and (ii) a certificate regularization. On CIFAR-10, we achieve significant improvements over prior works in provable robust accuracy (5.81%) with only a minor drop in standard accuracy (−0.29%). Code for reproducing all experiments in the paper is available at https://github.com/singlasahil14/SOC.","",""
9,"Jingyuan Wang, Yufan Wu, Mingxuan Li, Xin Lin, Junjie Wu, Chao Li","Interpretability is a Kind of Safety: An Interpreter-based Ensemble for Adversary Defense",2020,"","","","",146,"2022-07-13 09:28:50","","10.1145/3394486.3403044","","",,,,,9,4.50,2,6,2,"While having achieved great success in rich real-life applications, deep neural network (DNN) models have long been criticized for their vulnerability to adversarial attacks. Tremendous research efforts have been dedicated to mitigating the threats of adversarial attacks, but the essential trait of adversarial examples is not yet clear, and most existing methods are yet vulnerable to hybrid attacks and suffer from counterattacks. In light of this, in this paper, we first reveal a gradient-based correlation between sensitivity analysis-based DNN interpreters and the generation process of adversarial examples, which indicates the Achilles's heel of adversarial attacks and sheds light on linking together the two long-standing challenges of DNN: fragility and unexplainability. We then propose an interpreter-based ensemble framework called X-Ensemble for robust adversary defense. X-Ensemble adopts a novel detection-rectification process and features in building multiple sub-detectors and a rectifier upon various types of interpretation information toward target classifiers. Moreover, X-Ensemble employs the Random Forests (RF) model to combine sub-detectors into an ensemble detector for adversarial hybrid attacks defense. The non-differentiable property of RF further makes it a precious choice against the counterattack of adversaries. Extensive experiments under various types of state-of-the-art attacks and diverse attack scenarios demonstrate the advantages of X-Ensemble to competitive baseline methods.","",""
9,"Andrei Margeloiu, N. Simidjievski, M. Jamnik, Adrian Weller","Improving Interpretability in Medical Imaging Diagnosis using Adversarial Training",2020,"","","","",147,"2022-07-13 09:28:50","","","","",,,,,9,4.50,2,4,2,"We investigate the influence of adversarial training on the interpretability of convolutional neural networks (CNNs), specifically applied to diagnosing skin cancer. We show that gradient-based saliency maps of adversarially trained CNNs are significantly sharper and more visually coherent than those of standardly trained CNNs. Furthermore, we show that adversarially trained networks highlight regions with significant color variation within the lesion, a common characteristic of melanoma. We find that fine-tuning a robust network with a small learning rate further improves saliency maps' sharpness. Lastly, we provide preliminary work suggesting that robustifying the first layers to extract robust low-level features leads to visually coherent explanations.","",""
12,"Javier Martínez García, Dominik Zoeke, M. Vossiek","MIMO-FMCW Radar-Based Parking Monitoring Application With a Modified Convolutional Neural Network With Spatial Priors",2018,"","","","",148,"2022-07-13 09:28:50","","10.1109/ACCESS.2018.2857007","","",,,,,12,3.00,4,3,4,"Radar imaging is a competitive option for smart city applications over optical approaches, as it raises no privacy concerns. The inherent difficulty of interpreting radar signals can be overcome using deep learning techniques to leverage the capabilities of monitoring sensors with a minimum of human intervention. In this paper, we use a modified convolutional neural network (CNN) for classifying radar images in order to detect vacant parking spaces with a 77-GHz imaging radar. Although training CNNs for radar-image classification is challenging due to poor generalization performance caused by the lack of labeled training data, the modified architecture takes into account the properties of the radar image in order to introduce prior information into the model and improve performance. A MIMO-FMCW radar is utilized to render a slant-range image of a parking scenario, and the image patches corresponding to each parking location are classified independently in the CNN. Since the radiation pattern of a MIMO array varies as a function of the scanning angle, the corresponding spatial coordinate of each patch is included as an additional feature in the upper layers of the network. This allows the model to combine local features from each patch with global scenario information in order to learn robust features that generalize properly to new scenarios. Several models are trained end to end with data from four different parking scenarios and evaluated in a 4-fold cross-validation scheme, and performance is improved when spatial prior information is included.","",""
2,"Hui Qv, Jihao Yin, C. DiMarzio","Design of augmented dictionary for sparse representation based on neural network",2015,"","","","",149,"2022-07-13 09:28:50","","10.1109/IGARSS.2015.7325782","","",,,,,2,0.29,1,3,7,"An efficient and flexible dictionary designing algorithm is proposed for sparse and redundant signal representation. The proposed Augmented Dictionary (AD) is based on a new dictionary model with an augmented form compared to the conventional model. With this model, we can bridge the gap between the classic dictionary learning approaches, which have general structure yet lack computational efficiency, and the artificial neural network theory, which has potential high parallel computational efficiency but poor universality of structure. In this paper, we discuss the advantages of augmented dictionary, and interpret how the augmented dictionary can be trained with labeled samples. The proposed neural network based augmented dictionary designing method enjoys some important features, such as high accuracy, strong robustness and desired computational efficiency. As a demonstration of these benefits, we present high-quality hyperspectral image classification results based on the new algorithm.","",""
7,"Yedi Zhang, Zhe Zhao, Guangke Chen, Fu Song, Taolue Chen","BDD4BNN: A BDD-based Quantitative Analysis Framework for Binarized Neural Networks",2021,"","","","",150,"2022-07-13 09:28:50","","10.1007/978-3-030-81685-8_8","","",,,,,7,7.00,1,5,1,"","",""
6,"Changhao Chen, Chris Xiaoxuan Lu, Bing Wang, A. Trigoni, A. Markham","DynaNet: Neural Kalman Dynamical Model for Motion Estimation and Prediction",2019,"","","","",151,"2022-07-13 09:28:50","","10.1109/TNNLS.2021.3112460","","",,,,,6,2.00,1,5,3,"Dynamical models estimate and predict the temporal evolution of physical systems. State-space models (SSMs) in particular represent the system dynamics with many desirable properties, such as being able to model uncertainty in both the model and measurements, and optimal (in the Bayesian sense) recursive formulations, e.g., the Kalman filter. However, they require significant domain knowledge to derive the parametric form and considerable hand tuning to correctly set all the parameters. Data-driven techniques, e.g., recurrent neural networks, have emerged as compelling alternatives to SSMs with wide success across a number of challenging tasks, in part due to their impressive capability to extract relevant features from rich inputs. They, however, lack interpretability and robustness to unseen conditions. Thus, data-driven models are hard to be applied in safety-critical applications, such as self-driving vehicles. In this work, we present DynaNet, a hybrid deep learning and time-varying SSM, which can be trained end-to-end. Our neural Kalman dynamical model allows us to exploit the relative merits of both SSM and deep neural networks. We demonstrate its effectiveness in the estimation and prediction on a number of physically challenging tasks, including visual odometry, sensor fusion for visual-inertial navigation, and motion prediction. In addition, we show how DynaNet can indicate failures through investigation of properties, such as the rate of innovation (Kalman gain).","",""
52,"Mathias Lechner, Ramin M. Hasani, Alexander Amini, T. Henzinger, D. Rus, R. Grosu","Neural circuit policies enabling auditable autonomy",2020,"","","","",152,"2022-07-13 09:28:50","","10.1038/s42256-020-00237-3","","",,,,,52,26.00,9,6,2,"","",""
430,"Amirata Ghorbani, Abubakar Abid, James Y. Zou","Interpretation of Neural Networks is Fragile",2017,"","","","",153,"2022-07-13 09:28:50","","10.1609/aaai.v33i01.33013681","","",,,,,430,86.00,143,3,5,"In order for machine learning to be trusted in many applications, it is critical to be able to reliably explain why the machine learning algorithm makes certain predictions. For this reason, a variety of methods have been developed recently to interpret neural network predictions by providing, for example, feature importance maps. For both scientific robustness and security reasons, it is important to know to what extent can the interpretations be altered by small systematic perturbations to the input data, which might be generated by adversaries or by measurement biases. In this paper, we demonstrate how to generate adversarial perturbations that produce perceptively indistinguishable inputs that are assigned the same predicted label, yet have very different interpretations. We systematically characterize the robustness of interpretations generated by several widely-used feature importance interpretation methods (feature importance maps, integrated gradients, and DeepLIFT) on ImageNet and CIFAR-10. In all cases, our experiments show that systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly susceptible to adversarial attack. Our analysis of the geometry of the Hessian matrix gives insight on why robustness is a general challenge to current interpretation approaches.","",""
50,"Kenneth P. Smith, A. D. Kang, J. Kirby","Automated Interpretation of Blood Culture Gram Stains by Use of a Deep Convolutional Neural Network",2017,"","","","",154,"2022-07-13 09:28:50","","10.1128/JCM.01521-17","","",,,,,50,10.00,17,3,5,"ABSTRACT Microscopic interpretation of stained smears is one of the most operator-dependent and time-intensive activities in the clinical microbiology laboratory. Here, we investigated application of an automated image acquisition and convolutional neural network (CNN)-based approach for automated Gram stain classification. Using an automated microscopy platform, uncoverslipped slides were scanned with a 40× dry objective, generating images of sufficient resolution for interpretation. We collected 25,488 images from positive blood culture Gram stains prepared during routine clinical workup. These images were used to generate 100,213 crops containing Gram-positive cocci in clusters, Gram-positive cocci in chains/pairs, Gram-negative rods, or background (no cells). These categories were targeted for proof-of-concept development as they are associated with the majority of bloodstream infections. Our CNN model achieved a classification accuracy of 94.9% on a test set of image crops. Receiver operating characteristic (ROC) curve analysis indicated a robust ability to differentiate between categories with an area under the curve of >0.98 for each. After training and validation, we applied the classification algorithm to new images collected from 189 whole slides without human intervention. Sensitivity and specificity were 98.4% and 75.0% for Gram-positive cocci in chains and pairs, 93.2% and 97.2% for Gram-positive cocci in clusters, and 96.3% and 98.1% for Gram-negative rods. Taken together, our data support a proof of concept for a fully automated classification methodology for blood-culture Gram stains. Importantly, the algorithm was highly adept at identifying image crops with organisms and could be used to present prescreened, classified crops to technologists to accelerate smear review. This concept could potentially be extended to all Gram stain interpretive activities in the clinical laboratory.","",""
252,"Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, D. Ranasinghe, S. Nepal","STRIP: a defence against trojan attacks on deep neural networks",2019,"","","","",155,"2022-07-13 09:28:50","","10.1145/3359789.3359790","","",,,,,252,84.00,42,6,3,"A recent trojan attack on deep neural network (DNN) models is one insidious variant of data poisoning attacks. Trojan attacks exploit an effective backdoor created in a DNN model by leveraging the difficulty in interpretability of the learned model to misclassify any inputs signed with the attacker's chosen trojan trigger. Since the trojan trigger is a secret guarded and exploited by the attacker, detecting such trojan inputs is a challenge, especially at run-time when models are in active operation. This work builds STRong Intentional Perturbation (STRIP) based run-time trojan attack detection system and focuses on vision system. We intentionally perturb the incoming input, for instance by superimposing various image patterns, and observe the randomness of predicted classes for perturbed inputs from a given deployed model---malicious or benign. A low entropy in predicted classes violates the input-dependence property of a benign model and implies the presence of a malicious input---a characteristic of a trojaned input. The high efficacy of our method is validated through case studies on three popular and contrasting datasets: MNIST, CIFAR10 and GTSRB. We achieve an overall false acceptance rate (FAR) of less than 1%, given a preset false rejection rate (FRR) of 1%, for different types of triggers. Using CIFAR10 and GTSRB, we have empirically achieved result of 0% for both FRR and FAR. We have also evaluated STRIP robustness against a number of trojan attack variants and adaptive attacks.","",""
41,"K. Islam, R. G. Raj","Real-Time (Vision-Based) Road Sign Recognition Using an Artificial Neural Network",2017,"","","","",156,"2022-07-13 09:28:50","","10.3390/s17040853","","",,,,,41,8.20,21,2,5,"Road sign recognition is a driver support function that can be used to notify and warn the driver by showing the restrictions that may be effective on the current stretch of road. Examples for such regulations are ‘traffic light ahead’ or ‘pedestrian crossing’ indications. The present investigation targets the recognition of Malaysian road and traffic signs in real-time. Real-time video is taken by a digital camera from a moving vehicle and real world road signs are then extracted using vision-only information. The system is based on two stages, one performs the detection and another one is for recognition. In the first stage, a hybrid color segmentation algorithm has been developed and tested. In the second stage, an introduced robust custom feature extraction method is used for the first time in a road sign recognition approach. Finally, a multilayer artificial neural network (ANN) has been created to recognize and interpret various road signs. It is robust because it has been tested on both standard and non-standard road signs with significant recognition accuracy. This proposed system achieved an average of 99.90% accuracy with 99.90% of sensitivity, 99.90% of specificity, 99.90% of f-measure, and 0.001 of false positive rate (FPR) with 0.3 s computational time. This low FPR can increase the system stability and dependability in real-time applications.","",""
2,"Shushan He, H. Zha, X. Ye","Network Diffusions via Neural Mean-Field Dynamics",2020,"","","","",157,"2022-07-13 09:28:50","","","","",,,,,2,1.00,1,3,2,"We propose a novel learning framework based on neural mean-field dynamics for inference and estimation problems of diffusion on networks. Our new framework is derived from the Mori-Zwanzig formalism to obtain an exact evolution of the node infection probabilities, which renders a delay differential equation with memory integral approximated by learnable time convolution operators, resulting in a highly structured and interpretable RNN. Directly using cascade data, our framework can jointly learn the structure of the diffusion network and the evolution of infection probabilities, which are cornerstone to important downstream applications such as influence maximization. Connections between parameter learning and optimal control are also established. Empirical study shows that our approach is versatile and robust to variations of the underlying diffusion network models, and significantly outperform existing approaches in accuracy and efficiency on both synthetic and real-world data.","",""
1,"Benjamin Filtjens, P. Ginis, A. Nieuwboer, M. R. Afzal, J. Spildooren, B. Vanrumste, P. Slaets","Modelling and identification of characteristic kinematic features preceding freezing of gait with convolutional neural networks and layer-wise relevance propagation",2021,"","","","",158,"2022-07-13 09:28:50","","10.1186/s12911-021-01699-0","","",,,,,1,1.00,0,7,1,"","",""
7,"D. Barić, P. Fumić, D. Horvatic, T. Lipić","Benchmarking Attention-Based Interpretability of Deep Learning in Multivariate Time Series Predictions",2021,"","","","",159,"2022-07-13 09:28:50","","10.3390/e23020143","","",,,,,7,7.00,2,4,1,"The adaptation of deep learning models within safety-critical systems cannot rely only on good prediction performance but needs to provide interpretable and robust explanations for their decisions. When modeling complex sequences, attention mechanisms are regarded as the established approach to support deep neural networks with intrinsic interpretability. This paper focuses on the emerging trend of specifically designing diagnostic datasets for understanding the inner workings of attention mechanism based deep learning models for multivariate forecasting tasks. We design a novel benchmark of synthetically designed datasets with the transparent underlying generating process of multiple time series interactions with increasing complexity. The benchmark enables empirical evaluation of the performance of attention based deep neural networks in three different aspects: (i) prediction performance score, (ii) interpretability correctness, (iii) sensitivity analysis. Our analysis shows that although most models have satisfying and stable prediction performance results, they often fail to give correct interpretability. The only model with both a satisfying performance score and correct interpretability is IMV-LSTM, capturing both autocorrelations and crosscorrelations between multiple time series. Interestingly, while evaluating IMV-LSTM on simulated data from statistical and mechanistic models, the correctness of interpretability increases with more complex datasets.","",""
24,"Tianyu Kang, W. Ding, Luoyan Zhang, D. Ziemek, Kourosh Zarringhalam","A biological network-based regularized artificial neural network model for robust phenotype prediction from gene expression data",2017,"","","","",160,"2022-07-13 09:28:50","","10.1186/s12859-017-1984-2","","",,,,,24,4.80,5,5,5,"","",""
25,"K. Islam, R. G. Raj, G. Mujtaba","Recognition of Traffic Sign Based on Bag-of-Words and Artificial Neural Network",2017,"","","","",161,"2022-07-13 09:28:50","","10.3390/SYM9080138","","",,,,,25,5.00,8,3,5,"The traffic sign recognition system is a support system that can be useful to give notification and warning to drivers. It may be effective for traffic conditions on the current road traffic system. A robust artificial intelligence based traffic sign recognition system can support the driver and significantly reduce driving risk and injury. It performs by recognizing and interpreting various traffic sign using vision-based information. This study aims to recognize the well-maintained, un-maintained, standard, and non-standard traffic signs using the Bag-of-Words and the Artificial Neural Network techniques. This research work employs a Bag-of-Words model on the Speeded Up Robust Features descriptors of the road traffic signs. A robust classifier Artificial Neural Network has been employed to recognize the traffic sign in its respective class. The proposed system has been trained and tested to determine the suitable neural network architecture. The experimental results showed high accuracy of classification of traffic signs including complex background images. The proposed traffic sign detection and recognition system obtained 99.00% classification accuracy with a 1.00% false positive rate. For real-time implementation and deployment, this marginal false positive rate may increase reliability and stability of the proposed system.","",""
6,"Emanuele La Malfa, Min Wu, L. Laurenti, Benjie Wang, A. Hartshorn, M. Kwiatkowska","Assessing Robustness of Text Classification through Maximal Safe Radius Computation",2020,"","","","",162,"2022-07-13 09:28:50","","10.18653/v1/2020.findings-emnlp.266","","",,,,,6,3.00,1,6,2,"Neural network NLP models are vulnerable to small modifications of the input that maintain the original meaning but result in a different prediction. In this paper, we focus on robustness of text classification against word substitutions, aiming to provide guarantees that the model prediction does not change if a word is replaced with a plausible alternative, such as a synonym. As a measure of robustness, we adopt the notion of the maximal safe radius for a given input text, which is the minimum distance in the embedding space to the decision boundary. Since computing the exact maximal safe radius is not feasible in practice, we instead approximate it by computing a lower and upper bound. For the upper bound computation, we employ Monte Carlo Tree Search in conjunction with syntactic filtering to analyse the effect of single and multiple word substitutions. The lower bound computation is achieved through an adaptation of the linear bounding techniques implemented in tools CNN-Cert and POPQORN, respectively for convolutional and recurrent network models. We evaluate the methods on sentiment analysis and news classification models for four datasets (IMDB, SST, AG News and NEWS) and a range of embeddings, and provide an analysis of robustness trends. We also apply our framework to interpretability analysis and compare it with LIME.","",""
3,"Ana F. Sequeira, Tiago Gonçalves, W. Silva, João Ribeiro Pinto, Jaime S. Cardoso","An exploratory study of interpretability for face presentation attack detection",2021,"","","","",163,"2022-07-13 09:28:50","","10.1049/BME2.12045","","",,,,,3,3.00,1,5,1,"Fundação para a Ciência e a Tecnologia within project UIDB/50014/2020 and the PhD grants, Grant/Award Number: SFRH/BD/137720/2018; SFRH/BD/139468/2018; and 2020.06434.BD Abstract Biometric recognition and presentation attack detection (PAD) methods strongly rely on deep learning algorithms. Though often more accurate, these models operate as complex black boxes. Interpretability tools are now being used to delve deeper into the operation of these methods, which is why this work advocates their integration in the PAD scenario. Building upon previous work, a face PAD model based on convolutional neural networks was implemented and evaluated both through traditional PAD metrics and with interpretability tools. An evaluation on the stability of the explanations obtained from testing models with attacks known and unknown in the learning step is made. To overcome the limitations of direct comparison, a suitable representation of the explanations is constructed to quantify how much two explanations differ from each other. From the point of view of interpretability, the results obtained in intra and inter class comparisons led to the conclusion that the presence of more attacks during training has a positive effect in the generalisation and robustness of the models. This is an exploratory study that confirms the urge to establish new approaches in biometrics that incorporate interpretability tools. Moreover, there is a need for methodologies to assess and compare the quality of explanations.","",""
0,"Xi Liu, Shuhang Chen, Xiang Shen, Xiang Zhang, Yiwen Wang","A Nonlinear Maximum Correntropy Information Filter for High-Dimensional Neural Decoding",2021,"","","","",164,"2022-07-13 09:28:50","","10.3390/e23060743","","",,,,,0,0.00,0,5,1,"Neural signal decoding is a critical technology in brain machine interface (BMI) to interpret movement intention from multi-neural activity collected from paralyzed patients. As a commonly-used decoding algorithm, the Kalman filter is often applied to derive the movement states from high-dimensional neural firing observation. However, its performance is limited and less effective for noisy nonlinear neural systems with high-dimensional measurements. In this paper, we propose a nonlinear maximum correntropy information filter, aiming at better state estimation in the filtering process for a noisy high-dimensional measurement system. We reconstruct the measurement model between the high-dimensional measurements and low-dimensional states using the neural network, and derive the state estimation using the correntropy criterion to cope with the non-Gaussian noise and eliminate large initial uncertainty. Moreover, analyses of convergence and robustness are given. The effectiveness of the proposed algorithm is evaluated by applying it on multiple segments of neural spiking data from two rats to interpret the movement states when the subjects perform a two-lever discrimination task. Our results demonstrate better and more robust state estimation performance when compared with other filters.","",""
28,"Nicholas Carlini","Is AmI (Attacks Meet Interpretability) Robust to Adversarial Examples?",2019,"","","","",165,"2022-07-13 09:28:50","","","","",,,,,28,9.33,28,1,3,"No. I. ATTACKING “ATTACKS MEET INTERPRETABILITY” AmI (Attacks meet Interpretability) is an “attribute-steered” defense [3] to detect [1] adversarial examples [2] on facerecognition models. By applying interpretability techniques to a pre-trained neural network, AmI identifies “important” neurons. It then creates a second augmented neural network with the same parameters but increases the weight activations of important neurons. AmI rejects inputs where the original and augmented neural network disagree. We find that this defense (presented at at NeurIPS 2018 as a spotlight paper—the top 3% of submissions) is completely ineffective, and even defense-oblivious1 attacks reduce the detection rate to 0% on untargeted attacks. That is, AmI is no more robust to untargeted attacks than the undefended original network. Figure 1 shows selected adversarial examples that fool the AmI defense. We are incredibly grateful to the authors for releasing their source code2 which we build on3. We hope that future work will continue to release source code by publication time to accelerate progress in this field.","",""
22,"Adam Kortylewski, Qing Liu, Angtian Wang, Yihong Sun, A. Yuille","Compositional Convolutional Neural Networks: A Robust and Interpretable Model for Object Recognition under Occlusion",2020,"","","","",166,"2022-07-13 09:28:50","","10.1007/s11263-020-01401-3","","",,,,,22,11.00,4,5,2,"","",""
16,"Wei Huang, Youcheng Sun, Xing-E. Zhao, James Sharp, Wenjie Ruan, Jie Meng, Xiaowei Huang","Coverage-Guided Testing for Recurrent Neural Networks",2019,"","","","",167,"2022-07-13 09:28:50","","10.1109/TR.2021.3080664","","",,,,,16,5.33,2,7,3,"Recurrent neural networks (RNNs) have been applied to a broad range of applications such as natural language processing, drug discovery, and video recognition. This paper develops a coverage-guided testing approach for a major class of RNNs -- long short-term memory networks (LSTMs). We start from defining a family of three test metrics that are designed to quantify not only the values but also the temporal relations (including both step-wise and bounded-length) learned through LSTM's internal structures. While testing, random mutation enhanced with the coverage knowledge, i.e., targeted mutation, is designed to generate test cases. Based on these, we develop the coverage-guided testing tool testRNN. To our knowledge, this is the first time structural coverage metrics are used to test LSTMs. We extensively evaluate testRNN with a variety of LSTM benchmarks. Experiments confirm that there is a positive correlation between adversary rate and coverage rate, evidence showing that the test metrics are valid indicators of robustness evaluation. Also, we show that testRNN effectively captures erroneous behaviours in RNNs. Furthermore, meaningful information can be collected from testRNN for users to understand what the testing results represent. This is in contrast to most neural network testing works, and we believe testRNN is an important step towards interpretable neural network testing.","",""
1,"A. Rădulescu","Neural network function, density or geometry?",2013,"","","","",168,"2022-07-13 09:28:50","","","","",,,,,1,0.11,1,1,9,"We consider an oriented network in which two subgraphs (modules X and Y), with a given intra-modular edge density, are inter-connected by a fixed number of edges, in both directions. We study the adjacency spectrum of this network, focusing in particular on two aspects: the changes in the spectrum in response to varying the intra and inter-modular edge density, and the effects on the spectrum of perturbing the edge configuration, while keeping the densities fixed.  Since the general case is quite complex analytically, we adopted a combination of analytical approaches to particular cases, and numerical simulations for more results. After investigating the behavior of the mean and standard deviation of the eigenvalues, we conjectured the robustness of the adjacency spectrum to variations in edge geometry, when operating under a fixed density profile. We remark that, while this robustness increases with the size N of the network, it emerges at small sizes, and should not be thought of as a property that holds only in the large N limit. We argue that this may be helpful when studying applications on small networks of nodes.  We interpret our results in the context of existing literature, which has been placing increasing attention to random graph models, with edges connecting two given nodes with certain probabilities. We discuss whether properties such as Wigner's semicircle law, or effects of community structure, still hold in our context.  Finally, we suggest possible applications of the model to understanding synaptic restructuring during learning algorithms, and to classifying emotional responses based on the geometry of the emotion-regulatory neural circuit. In this light, we argue that future directions should be directed towards relating hardwiring to the temporal behavior of the network as a dynamical system .","",""
9,"Yiwen Guo, Long Chen, Yurong Chen, Changshui Zhang","On Connections Between Regularizations for Improving DNN Robustness",2020,"","","","",169,"2022-07-13 09:28:50","","10.1109/tpami.2020.3006917","","",,,,,9,4.50,2,4,2,"This paper analyzes regularization terms proposed recently for improving the adversarial robustness of deep neural networks (DNNs), from a theoretical point of view. Specifically, we study possible connections between several effective methods, including input-gradient regularization, Jacobian regularization, curvature regularization, and a cross-Lipschitz functional. We investigate them on DNNs with general rectified linear activations, which constitute one of the most prevalent families of models for image classification and a host of other machine learning applications. We shed light on essential ingredients of these regularizations and re-interpret their functionality. Through the lens of our study, more principled and efficient regularizations can possibly be invented in the near future.","",""
50,"Zhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, Cho-Jui Hsieh","Robustness Verification for Transformers",2020,"","","","",170,"2022-07-13 09:28:50","","","","",,,,,50,25.00,10,5,2,"Robustness verification that aims to formally certify the prediction behavior of neural networks has become an important tool for understanding the behavior of a given model and for obtaining safety guarantees. However, previous methods are usually limited to relatively simple neural networks. In this paper, we consider the robustness verification problem for Transformers. Transformers have complex self-attention layers that pose many challenges for verification, including cross-nonlinearity and cross-position dependency, which have not been discussed in previous work. We resolve these challenges and develop the first verification algorithm for Transformers. The certified robustness bounds computed by our method are significantly tighter than those by naive Interval Bound Propagation. These bounds also shed light on interpreting Transformers as they consistently reflect the importance of words in sentiment analysis.","",""
40,"Alvin Chan, Yi Tay, Y. Ong, Jie Fu","Jacobian Adversarially Regularized Networks for Robustness",2019,"","","","",171,"2022-07-13 09:28:50","","","","",,,,,40,13.33,10,4,3,"Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training examples.","",""
16,"Fan Zhang, Fábio Duarte, Ruixian Ma, Dimitrios Milioris, Hui-Ching Lin, C. Ratti","Indoor Space Recognition using Deep Convolutional Neural Network: A Case Study at MIT Campus",2016,"","","","",172,"2022-07-13 09:28:50","","","","",,,,,16,2.67,3,6,6,"In this paper, we propose a robust and parsimonious approach using Deep Convolutional Neural Network (DCNN) to recognize and interpret interior space. DCNN has achieved incredible success in object and scene recognition. In this study we design and train a DCNN to classify a pre-zoning indoor space, and from a single phone photo to recognize the learned space features, with no need of additional assistive technology. We collect more than 600,000 images inside MIT campus buildings to train our DCNN model, and achieved 97.9% accuracy in validation dataset and 81.7% accuracy in test dataset based on spatial-scale fixed model. Furthermore, the recognition accuracy and spatial resolution can be potentially improved through multiscale classification model. We identify the discriminative image regions through Class Activating Mapping (CAM) technique, to observe the model's behavior in how to recognize space and interpret it in an abstract way. By evaluating the results with misclassification matrix, we investigate the visual spatial feature of interior space by looking into its visual similarity and visual distinctiveness, giving insights into interior design and human indoor perception and wayfinding research. The contribution of this paper is threefold. First, we propose a robust and parsimonious approach for indoor navigation using DCNN. Second, we demonstrate that DCNN also has a potential capability in space feature learning and recognition, even under severe appearance changes. Third, we introduce a DCNN based approach to look into the visual similarity and visual distinctiveness of interior space.","",""
3,"Yulong Wang, Hang Su, Bo Zhang, Xiaolin Hu","Interpret Neural Networks by Extracting Critical Subnetworks",2020,"","","","",173,"2022-07-13 09:28:50","","10.1109/TIP.2020.2993098","","",,,,,3,1.50,1,4,2,"In recent years, deep neural networks have achieved excellent performance in many fields of artificial intelligence. The requirements for the interpretability and robustness of neural networks are also increasing. In this paper, we propose to understand the functional mechanism of neural networks by extracting critical subnetworks. Specifically, we denote the critical subnetworks as a group of important channels across layers such that if they were suppressed to zeros, the final test performance would deteriorate severely. This novel perspective can not only reveal the layerwise semantic behavior within the model but also present more accurate visual explanations appearing in the data through attribution methods. Moreover, we propose two adversarial example detection methods based on the properties of sample-specific and class-specific subnetworks, which provides the possibility for increasing the model robustness.","",""
5,"J. Günther, Elias Reichensdorfer, P. Pilarski, K. Diepold","Interpretable PID parameter tuning for control engineering using general dynamic neural networks: An extensive comparison",2019,"","","","",174,"2022-07-13 09:28:50","","10.1371/journal.pone.0243320","","",,,,,5,1.67,1,4,3,"Modern automation systems largely rely on closed loop control, wherein a controller interacts with a controlled process via actions, based on observations. These systems are increasingly complex, yet most deployed controllers are linear Proportional-Integral-Derivative (PID) controllers. PID controllers perform well on linear and near-linear systems but their simplicity is at odds with the robustness required to reliably control complex processes. Modern machine learning techniques offer a way to extend PID controllers beyond their linear control capabilities by using neural networks. However, such an extension comes at the cost of losing stability guarantees and controller interpretability. In this paper, we examine the utility of extending PID controllers with recurrent neural networks—–namely, General Dynamic Neural Networks (GDNN); we show that GDNN (neural) PID controllers perform well on a range of complex control systems and highlight how they can be a scalable and interpretable option for modern control systems. To do so, we provide an extensive study using four benchmark systems that represent the most common control engineering benchmarks. All control environments are evaluated with and without noise as well as with and without disturbances. The neural PID controller performs better than standard PID control in 15 of 16 tasks and better than model-based control in 13 of 16 tasks. As a second contribution, we address the lack of interpretability that prevents neural networks from being used in real-world control processes. We use bounded-input bounded-output stability analysis to evaluate the parameters suggested by the neural network, making them understandable for engineers. This combination of rigorous evaluation paired with better interpretability is an important step towards the acceptance of neural-network-based control approaches for real-world systems. It is furthermore an important step towards interpretable and safely applied artificial intelligence.","",""
14,"Francesco Donnarumma, R. Prevete, Andrea de Giorgio, Guglielmo Montone, G. Pezzulo","Learning programs is better than learning dynamics: A programmable neural network hierarchical architecture in a multi-task scenario",2016,"","","","",175,"2022-07-13 09:28:50","","10.1177/1059712315609412","","",,,,,14,2.33,3,5,6,"Distributed and hierarchical models of control are nowadays popular in computational modeling and robotics. In the artificial neural network literature, complex behaviors can be produced by composing elementary building blocks or motor primitives, possibly organized in a layered structure. However, it is still unknown how the brain learns and encodes multiple motor primitives, and how it rapidly reassembles, sequences and switches them by exerting cognitive control. In this paper we advance a novel proposal, a hierarchical programmable neural network architecture, based on the notion of programmability and an interpreter-programmer computational scheme. In this approach, complex (and novel) behaviors can be acquired by embedding multiple modules (motor primitives) in a single, multi-purpose neural network. This is supported by recent theories of brain functioning in which skilled behaviors can be generated by combining functional different primitives embedded in “reusable” areas of “recycled” neurons. Such neuronal substrate supports flexible cognitive control, too. Modules are seen as interpreters of behaviors having controlling input parameters, or programs that encode structures of networks to be interpreted. Flexible cognitive control can be exerted by a programmer module feeding the interpreters with appropriate input parameters, without modifying connectivity. Our results in a multiple T -maze robotic scenario show how this computational framework provides a robust, scalable and flexible scheme that can be iterated at different hierarchical layers permitting to learn, encode and control multiple qualitatively different behaviors.","",""
0,"Manuel Günther, Andras Rozsa, T. Boult","On the Robustness of Deep Neural Networks",2017,"","","","",176,"2022-07-13 09:28:50","","","","",,,,,0,0.00,0,3,5,"Deep Neural Networks (DNNs) have become the quasi-standard in many machine learning tasks since they obtain state-of-the-art results and outperform more traditional machine learning models for problems in vision, image and speech processing, and many other tasks. One reason that DNNs have proven very useful is that they are one of the few algorithms that can make principled use of huge databases for training. However, despite their brilliant recognition capabilities, little is known about how DNNs achieve their accuracies, and even less is known about their robustness and their limitations. After Szegedy et al. [19] showed that deep networks have some “intriguing” properties, several researchers started actively exploring the limitations, and showed how easily the networks could be attacked or fooled – demonstrating essential limits of the robustness of DNNs. This paper analyzes these two categories of attacks: adversarial images [19, 3], which are imperceptible perturbations to an input to turn it into another class, and fooling images [9], which look like none of the ∗This research is based upon work funded in part by NSF IIS-1320956 and in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&D Contract No. 2014-14071600012. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.","",""
4,"Sahil Singla, Surbhi Singla, S. Feizi","Householder Activations for Provable Robustness against Adversarial Attacks",2021,"","","","",177,"2022-07-13 09:28:50","","","","",,,,,4,4.00,1,3,1,"Training convolutional neural networks (CNNs) with a strict Lipschitz constraint under the l2 norm is useful for provable adversarial robustness, interpretable gradients and stable training. While 1-Lipschitz CNNs can be designed by enforcing a 1-Lipschitz constraint on each layer, training such networks requires each layer to have an orthogonal Jacobian matrix (for all inputs) to prevent gradients from vanishing during backpropagation. A layer with this property is said to be Gradient Norm Preserving (GNP). To construct expressive GNP activation functions, we first prove that the Jacobian of any GNP piecewise linear function is only allowed to change via Householder (HH) transformations for the function to be continuous. Building on this result, we introduce a class of nonlinear GNP activations with learnable Householder transformations called Householder activations. A householder activation parameterized by the vector v outputs (I − 2vv )z for its input z if v z ≤ 0; otherwise it outputs z. Existing GNP activations such as MaxMin can be viewed as special cases of HH activations for certain settings of these transformations. Thus, networks with HH activations have higher expressive power than those with MaxMin activations. Although networks with HH activations have nontrivial provable robustness against adversarial attacks, we further boost their robustness by (i) introducing a certificate regularization and (ii) relaxing orthogonalization of the last layer of the network. Our experiments on CIFAR-10 and CIFAR-100 show that our regularized networks with HH activations lead to significant improvements in both the standard and provable robust accuracy over the prior works (gain of 3.65% and 4.46% on CIFAR-100 respectively).","",""
4,"Taeheon Lee, Jeonghwan Hwang, Honggu Lee","TRIER: Template-Guided Neural Networks for Robust and Interpretable Sleep Stage Identification from EEG Recordings",2020,"","","","",178,"2022-07-13 09:28:50","","","","",,,,,4,2.00,1,3,2,"Neural networks often obtain sub-optimal representations during training, which degrade robustness as well as classification performances. This is a severe problem in applying deep learning to bio-medical domains, since models are vulnerable to being harmed by irregularities and scarcities in data. In this study, we propose a pre-training technique that handles this challenge in sleep staging tasks. Inspired by conventional methods that experienced physicians have used to classify sleep states from the existence of characteristic waveform shapes, or template patterns, our method introduces a cosine similarity based convolutional neural network to extract representative waveforms from training data. Afterwards, these features guide a model to construct representations based on template patterns. Through extensive experiments, we demonstrated that guiding a neural network with template patterns is an effective approach for sleep staging, since (1) classification performances are significantly enhanced and (2) robustness in several aspects are improved. Last but not least, interpretations on models showed that notable features exploited by trained experts are correctly addressed during prediction in the proposed method.","",""
3,"Nathan G. Drenkow, Numair Sani, I. Shpitser, M. Unberath","Robustness in Deep Learning for Computer Vision: Mind the gap?",2021,"","","","",179,"2022-07-13 09:28:50","","","","",,,,,3,3.00,1,4,1,"Deep neural networks for computer vision tasks are deployed in increasingly safety-critical and socially-impactful applications, motivating the need to close the gap in model performance under varied, naturally occurring imaging conditions. Robustness, ambiguously used in multiple contexts including adversarial machine learning, here then refers to preserving model performance under naturally-induced image corruptions or alterations. We perform a systematic review to identify, analyze, and summarize current definitions and progress towards non-adversarial robustness in deep learning for computer vision. We find that this area of research has received disproportionately little attention relative to adversarial machine learning, yet a significant robustness gap exists that often manifests in performance degradation similar in magnitude to adversarial conditions. To provide a more transparent definition of robustness across contexts, we introduce a structural causal model of the data generating process and interpret non-adversarial robustness as pertaining to a model’s behavior on corrupted images which correspond to low-probability samples from the unaltered data distribution. We then identify key architecture-, data augmentation-, and optimization tactics for improving neural network robustness. This causal view of robustness reveals that common practices in the current literature, both in regards to robustness tactics and evaluations, correspond to causal concepts, such as soft interventions resulting in a counterfactually-altered distribution of imaging conditions. Through our findings and analysis, we offer perspectives on how future research may mind this evident and significant non-adversarial robustness gap.","",""
96,"Yuan Gao, Qi She, Jiayi Ma, Mingbo Zhao, W. Liu, A. Yuille","NDDR-CNN: Layerwise Feature Fusing in Multi-Task CNNs by Neural Discriminative Dimensionality Reduction",2018,"","","","",180,"2022-07-13 09:28:50","","10.1109/CVPR.2019.00332","","",,,,,96,24.00,16,6,4,"In this paper, we propose a novel Convolutional Neural Network (CNN) structure for general-purpose multi-task learning (MTL), which enables automatic feature fusing at every layer from different tasks. This is in contrast with the most widely used MTL CNN structures which empirically or heuristically share features on some specific layers (e.g., share all the features except the last convolutional layer). The proposed layerwise feature fusing scheme is formulated by combining existing CNN components in a novel way, with clear mathematical interpretability as discriminative dimensionality reduction, which is referred to as Neural Discriminative Dimensionality Reduction (NDDR). Specifically, we first concatenate features with the same spatial resolution from different tasks according to their channel dimension. Then, we show that the discriminative dimensionality reduction can be fulfilled by 1x1 Convolution, Batch Normalization, and Weight Decay in one CNN. The use of existing CNN components ensures the end-to-end training and the extensibility of the proposed NDDR layer to various state-of-the-art CNN architectures in a ""plug-and-play"" manner. The detailed ablation analysis shows that the proposed NDDR layer is easy to train and also robust to different hyperparameters. Experiments on different task sets with various base network architectures demonstrate the promising performance and desirable generalizability of our proposed method. The code of our paper is available at https://github.com/ethanygao/NDDR-CNN.","",""
2,"Evan Scullion, Soren Nelson, R. Menon","Optics-free imaging of complex, non-sparse QR-codes with Deep Neural Networks",2020,"","","","",181,"2022-07-13 09:28:50","","","","",,,,,2,1.00,1,3,2,"We demonstrate optics-free imaging of complex QR-codes using a bare image sensor and a trained artificial neural network (ANN). The ANN is trained to interpret the raw sensor data for human visualization. The image sensor is placed at a specified gap from the QR code. We studied the robustness of our approach by experimentally testing the output of the ANNs with system perturbations of this gap, and the translational and rotational alignments of the QR code to the image sensor. Our demonstration opens us the possibility of using completely optics-free cameras for application-specific imaging of complex, non-sparse objects.","",""
2,"Rahul Soni, Naresh Shah, J. D. Moore","Fine-grained Uncertainty Modeling in Neural Networks",2020,"","","","",182,"2022-07-13 09:28:50","","","","",,,,,2,1.00,1,3,2,"Existing uncertainty modeling approaches try to detect an out-of-distribution point from the in-distribution dataset. We extend this argument to detect finer-grained uncertainty that distinguishes between (a). certain points, (b). uncertain points but within the data distribution, and (c). out-of-distribution points. Our method corrects overconfident NN decisions, detects outlier points and learns to say ``I don't know'' when uncertain about a critical point between the top two predictions. In addition, we provide a mechanism to quantify class distributions overlap in the decision manifold and investigate its implications in model interpretability.  Our method is two-step: in the first step, the proposed method builds a class distribution using Kernel Activation Vectors (kav) extracted from the Network. In the second step, the algorithm determines the confidence of a test point by a hierarchical decision rule based on the chi-squared distribution of squared Mahalanobis distances.  Our method sits on top of a given Neural Network, requires a single scan of training data to estimate class distribution statistics, and is highly scalable to deep networks and wider pre-softmax layer. As a positive side effect, our method helps to prevent adversarial attacks without requiring any additional training. It is directly achieved when the Softmax layer is substituted by our robust uncertainty layer at the evaluation phase.","",""
1,"Can Udomcharoenchaikit, P. Boonkwan, P. Vateekul","Adversarial Evaluation of Robust Neural Sequential Tagging Methods for Thai Language",2020,"","","","",183,"2022-07-13 09:28:50","","10.1145/3383201","","",,,,,1,0.50,0,3,2,"Sequential tagging tasks, such as Part-Of-Speech (POS) tagging and Named-Entity Recognition, are the building blocks of many natural language processing applications. Although prior works have reported promising results in standard settings, they often underperform on non-standard text, such as microblogs and social media. In this article, we introduce an adversarial evaluation scheme for the Thai language by creating adversarial examples based on known spelling errors. Furthermore, we propose novel methods including UNK masking, condition initialization with affixation embeddings, and untied-directional self-attention mechanism to enhance robustness and interpretability of the neural networks. We conducted experiments on two Thai corpora: BEST2010 and ORCHID. Our adversarial evaluation schemes reveal that bidirectional LSTM (BiLSTM) do not perform well on adversarial examples. Our best methods match the performance of the BiLSTM baseline model and outperform it on adversarial examples.","",""
18,"Francesco Donnarumma, R. Prevete, F. Chersi, G. Pezzulo","A Programmer-Interpreter Neural Network Architecture for Prefrontal Cognitive Control",2015,"","","","",184,"2022-07-13 09:28:50","","10.1142/S0129065715500173","","",,,,,18,2.57,5,4,7,"There is wide consensus that the prefrontal cortex (PFC) is able to exert cognitive control on behavior by biasing processing toward task-relevant information and by modulating response selection. This idea is typically framed in terms of top-down influences within a cortical control hierarchy, where prefrontal-basal ganglia loops gate multiple input-output channels, which in turn can activate or sequence motor primitives expressed in (pre-)motor cortices. Here we advance a new hypothesis, based on the notion of programmability and an interpreter-programmer computational scheme, on how the PFC can flexibly bias the selection of sensorimotor patterns depending on internal goal and task contexts. In this approach, multiple elementary behaviors representing motor primitives are expressed by a single multi-purpose neural network, which is seen as a reusable area of ""recycled"" neurons (interpreter). The PFC thus acts as a ""programmer"" that, without modifying the network connectivity, feeds the interpreter networks with specific input parameters encoding the programs (corresponding to network structures) to be interpreted by the (pre-)motor areas. Our architecture is validated in a standard test for executive function: the 1-2-AX task. Our results show that this computational framework provides a robust, scalable and flexible scheme that can be iterated at different hierarchical layers, supporting the realization of multiple goals. We discuss the plausibility of the ""programmer-interpreter"" scheme to explain the functioning of prefrontal-(pre)motor cortical hierarchies.","",""
1,"André Ferreira, S. Madeira, M. Gromicho, M. Carvalho, S. Vinga, Alexandra M. Carvalho","Predictive Medicine Using Interpretable Recurrent Neural Networks",2020,"","","","",185,"2022-07-13 09:28:50","","10.1007/978-3-030-68763-2_14","","",,,,,1,0.50,0,6,2,"","",""
1,"Huijun Wu, Chen Wang, R. Nock, Wei Wang, Jie Yin, Kai Lu, Liming Zhu","SMINT: Toward Interpretable and Robust Model Sharing for Deep Neural Networks",2020,"","","","",186,"2022-07-13 09:28:50","","10.1145/3381833","","",,,,,1,0.50,0,7,2,"Sharing a pre-trained machine learning model, particularly a deep neural network via prediction APIs, is becoming a common practice on machine learning as a service (MLaaS) platforms nowadays. Although deep neural networks (DNN) have shown remarkable successes in many tasks, they are also criticized for the lack of interpretability and transparency. Interpreting a shared DNN model faces two additional challenges compared with interpreting a general model. (1) Limited training data can be disclosed to users. (2) The internal structure of the models may not be available. These two challenges impede the application of most existing interpretability approaches, such as saliency maps or influence functions, for DNN models. Case-based reasoning methods have been used for interpreting decisions; however, how to select and organize the data points under the constraints of shared DNN models is not discussed. Moreover, simply providing cases as explanations may not be sufficient for supporting instance level interpretability. Meanwhile, existing interpretation methods for DNN models generally lack the means to evaluate the reliability of the interpretation. In this article, we propose a framework named Shared Model INTerpreter (SMINT) to address the above limitations. We propose a new data structure called a boundary graph to organize training points to mimic the predictions of DNN models. We integrate local features, such as saliency maps and interpretable input masks, into the data structure to help users to infer the model decision boundaries. We show that the boundary graph is able to address the reliability issues in many local interpretation methods. We further design an algorithm named hidden-layer aware p-test to measure the reliability of the interpretations. Our experiments show that SMINT is able to achieve above 99% fidelity to corresponding DNN models on both MNIST and ImageNet by sharing only a tiny fraction of training data to make these models interpretable. The human pilot study demonstrates that SMINT provides better interpretability compared with existing methods. Moreover, we demonstrate that SMINT is able to assist model tuning for better performance on different user data.","",""
1,"Rahul Soni, Naresh Shah, Chua Tat Seng, J. D. Moore","Adversarial TCAV - Robust and Effective Interpretation of Intermediate Layers in Neural Networks",2020,"","","","",187,"2022-07-13 09:28:50","","","","",,,,,1,0.50,0,4,2,"Interpreting neural network decisions and the information learned in intermediate layers is still a challenge due to the opaque internal state and shared non-linear interactions. Although (Kim et al, 2017) proposed to interpret intermediate layers by quantifying its ability to distinguish a user-defined concept (from random examples), the questions of robustness (variation against the choice of random examples) and effectiveness (retrieval rate of concept images) remain. We investigate these two properties and propose improvements to make concept activations reliable for practical use.  Effectiveness: If the intermediate layer has effectively learned a user-defined concept, it should be able to recall --- at the testing step --- most of the images containing the proposed concept. For instance, we observed that the recall rate of Tiger shark and Great white shark from the ImageNet dataset with ""Fins"" as a user-defined concept was only 18.35% for VGG16. To increase the effectiveness of concept learning, we propose A-CAV --- the Adversarial Concept Activation Vector --- this results in larger margins between user concepts and (negative) random examples. This approach improves the aforesaid recall to 76.83% for VGG16.  For robustness, we define it as the ability of an intermediate layer to be consistent in its recall rate (the effectiveness) for different random seeds. We observed that TCAV has a large variance in recalling a concept across different random seeds. For example, the recall of cat images (from a layer learning the concept of tail) varies from 18% to 86% with 20.85% standard deviation on VGG16. We propose a simple and scalable modification that employs a Gram-Schmidt process to sample random noise from concepts and learn an average ""concept classifier"". This approach improves the aforesaid standard deviation from 20.85% to 6.4%.","",""
19,"Chao Jiang, R. Vinuesa, Ruilin Chen, Junyi Mi, Shujin Laima, Hui Li","An interpretable framework of data-driven turbulence modeling using deep neural networks",2021,"","","","",188,"2022-07-13 09:28:50","","10.1063/5.0048909","","",,,,,19,19.00,3,6,1,"Reynolds-averaged Navier–Stokes simulations represent a cost-effective option for practical engineering applications, but are facing ever-growing demands for more accurate turbulence models. Recently, emerging machine learning techniques have had a promising impact on turbulence modeling, but are still in their infancy regarding widespread industrial adoption. Toward their extensive uptake, this paper presents a universally interpretable machine learning (UIML) framework for turbulence modeling, which consists of two parallel machine learning-based modules to directly infer the structural and parametric representations of turbulence physics, respectively. At each phase of model development, data reflecting the evolution dynamics of turbulence and domain knowledge representing prior physical considerations are converted into modeling knowledge. The data- and knowledge-driven UIML is investigated with a deep residual network. The following three aspects are demonstrated in detail: (i) a compact input feature parameterizing a new turbulent timescale is introduced to prevent nonunique mappings between conventional input arguments and output Reynolds stress; (ii) a realizability limiter is developed to overcome the under-constrained state of modeled stress; and (iii) fairness and noise-insensitivity constraints are included in the training procedure. Consequently, an invariant, realizable, unbiased, and robust data-driven turbulence model is achieved. The influences of the training dataset size, activation function, and network hyperparameter on the performance are also investigated. The resulting model exhibits good generalization across two- and three-dimensional flows, and captures the effects of the Reynolds number and aspect ratio. Finally, the underlying rationale behind prediction is explored.","",""
9,"Chongzhi Zhang, Aishan Liu, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing Ma, Tianlin Li","Interpreting and Improving Adversarial Robustness with Neuron Sensitivity",2019,"","","","",189,"2022-07-13 09:28:50","","","","",,,,,9,3.00,1,7,3,"Deep neural networks (DNNs) are vulnerable to adversarial examples where inputs with imperceptible perturbations mislead DNNs to incorrect results. Despite the potential risk they bring, adversarial examples are also valuable for providing insights into the weakness and blind-spots of DNNs. Thus, the interpretability of a DNN in adversarial setting aims to explain the rationale behind its decision-making process and makes deeper understanding which results in better practical applications. To address this issue, we try to explain adversarial robustness for deep models from a new perspective of neuron sensitivity which is measured by neuron behavior variation intensity against benign and adversarial examples. In this paper, we first draw the close connection between adversarial robustness and neuron sensitivities, as sensitive neurons make the most non-trivial contributions to model predictions in adversarial setting. Based on that, we further propose to improve adversarial robustness by constraining the similarities of sensitive neurons between benign and adversarial examples which stabilizes the behaviors of sensitive neurons in adversarial setting. Moreover, we demonstrate that state-of-the-art adversarial training methods improve model robustness by reducing neuron sensitivities which in turn confirms the strong connections between adversarial robustness and neuron sensitivity as well as the effectiveness of using sensitive neurons to build robust models. Extensive experiments on various datasets demonstrate that our algorithm effectively achieve excellent results.","",""
8,"Henry Kenlay, D. Thanou, Xiaowen Dong","On The Stability of Graph Convolutional Neural Networks Under Edge Rewiring",2020,"","","","",190,"2022-07-13 09:28:50","","10.1109/ICASSP39728.2021.9413474","","",,,,,8,4.00,3,3,2,"Graph neural networks are experiencing a surge of popularity within the machine learning community due to their ability to adapt to nonEuclidean domains and instil inductive biases. Despite this, their stability, i.e., their robustness to small perturbations in the input, is not yet well understood. Although there exists some results showing the stability of graph neural networks, most take the form of an upper bound on the magnitude of change due to a perturbation in the graph topology. However, the change in the graph topology captured in existing bounds tend not to be expressed in terms of structural properties, limiting our understanding of the model robustness properties. In this work, we develop an interpretable upper bound elucidating that graph neural networks are stable to rewiring between high degree nodes. This bound and further research in bounds of similar type provide further understanding of the stability properties of graph neural networks.","",""
9,"Yashaswi Pathak, Sarvesh Mehta, U. D. Priyakumar","Learning Atomic Interactions through Solvation Free Energy Prediction Using Graph Neural Networks",2021,"","","","",191,"2022-07-13 09:28:50","","10.1021/acs.jcim.0c01413","","",,,,,9,9.00,3,3,1,"Solvation free energy is a fundamental property that influences various chemical and biological processes, such as reaction rates, protein folding, drug binding, and bioavailability of drugs. In this work, we present a deep learning method based on graph networks to accurately predict solvation free energies of small organic molecules. The proposed model, comprising three phases, namely, message passing, interaction, and prediction, is able to predict solvation free energies in any generic organic solvent with a mean absolute error of 0.16 kcal/mol. In terms of accuracy, the current model outperforms all of the proposed machine learning-based models so far. The atomic interactions predicted in an unsupervised manner are able to explain the trends of free energies consistent with chemical wisdom. Further, the robustness of the machine learning-based model has been tested thoroughly, and its capability to interpret the predictions has been verified with several examples.","",""
6,"Kaveri A. Thakoor, Sharath C. Koorathota, D. Hood, P. Sajda","Robust and Interpretable Convolutional Neural Networks to Detect Glaucoma in Optical Coherence Tomography Images",2020,"","","","",192,"2022-07-13 09:28:50","","10.1109/tbme.2020.3043215","","",,,,,6,3.00,2,4,2,"Recent studies suggest that deep learning systems can now achieve performance on par with medical experts in diagnosis of disease. A prime example is in the field of ophthalmology, where convolutional neural networks (CNNs) have been used to detect retinal and ocular diseases. However, this type of artificial intelligence (AI) has yet to be adopted clinically due to questions regarding robustness of the algorithms to datasets collected at new clinical sites and a lack of explainability of AI-based predictions, especially relative to those of human expert counterparts. In this work, we develop CNN architectures that demonstrate robust detection of glaucoma in optical coherence tomography (OCT) images and test with concept activation vectors (TCAVs) to infer what image concepts CNNs use to generate predictions. Furthermore, we compare TCAV results to eye fixations of clinicians, to identify common decision-making features used by both AI and human experts. We find that employing fine-tuned transfer learning and CNN ensemble learning create end-to-end deep learning models with superior robustness compared to previously reported hybrid deep-learning/machine-learning models, and TCAV/eye-fixation comparison suggests the importance of three OCT report sub-images that are consistent with areas of interest fixated upon by OCT experts to detect glaucoma. The pipeline described here for evaluating CNN robustness and validating interpretable image concepts used by CNNs with eye movements of experts has the potential to help standardize the acceptance of new AI tools for use in the clinic.","",""
23,"Andrew J. R. Simpson","Over-Sampling in a Deep Neural Network",2015,"","","","",193,"2022-07-13 09:28:50","","","","",,,,,23,3.29,23,1,7,"Deep neural networks (DNN) are the state of the art on many engineering problems such as computer vision and audition. A key factor in the success of the DNN is scalability - bigger networks work better. However, the reason for this scalability is not yet well understood. Here, we interpret the DNN as a discrete system, of linear filters followed by nonlinear activations, that is subject to the laws of sampling theory. In this context, we demonstrate that over-sampled networks are more selective, learn faster and learn more robustly. Our findings may ultimately generalize to the human brain.","",""
5,"F. Yang, Ninghao Liu, Mengnan Du, X. Hu","Generative Counterfactuals for Neural Networks via Attribute-Informed Perturbation",2021,"","","","",194,"2022-07-13 09:28:50","","10.1145/3468507.3468517","","",,,,,5,5.00,1,4,1,"With the wide use of deep neural networks (DNN), model interpretability has become a critical concern, since explainable decisions are preferred in high-stake scenarios. Current interpretation techniques mainly focus on the feature attribution perspective, which are limited in indicating why and how particular explanations are related to the prediction. To this end, an intriguing class of explanations, named counterfactuals, has been developed to further explore the ""what-if"" circumstances for interpretation, and enables the reasoning capability on black-box models. However, generating counterfactuals for raw data instances (i.e., text and image) is still in the early stage due to its challenges on high data dimensionality and unsemantic raw features. In this paper, we design a framework to generate counterfactuals specifically for raw data instances with the proposed Attribute-Informed Perturbation (AIP). By utilizing generative models conditioned with different attributes, counterfactuals with desired labels can be obtained effectively and efficiently. Instead of directly modifying instances in the data space, we iteratively optimize the constructed attributeinformed latent space, where features are more robust and semantic. Experimental results on real-world texts and images demonstrate the effectiveness, sample quality as well as efficiency of our designed framework, and show the superiority over other alternatives. Besides, we also introduce some practical applications based on our framework, indicating its potential beyond the model interpretability aspect.","",""
67,"Changhao Chen, Stefano Rosa, Yishu Miao, Chris Xiaoxuan Lu, Wei Wu, A. Markham, A. Trigoni","Selective Sensor Fusion for Neural Visual-Inertial Odometry",2019,"","","","",195,"2022-07-13 09:28:50","","10.1109/CVPR.2019.01079","","",,,,,67,22.33,10,7,3,"Deep learning approaches for Visual-Inertial Odometry (VIO) have proven successful, but they rarely focus on incorporating robust fusion strategies for dealing with imperfect input sensory data. We propose a novel end-to-end selective sensor fusion framework for monocular VIO, which fuses monocular images and inertial measurements in order to estimate the trajectory whilst improving robustness to real-life issues, such as missing and corrupted data or bad sensor synchronization. In particular, we propose two fusion modalities based on different masking strategies: deterministic soft fusion and stochastic hard fusion, and we compare with previously proposed direct fusion baselines. During testing, the network is able to selectively process the features of the available sensor modalities and produce a trajectory at scale. We present a thorough investigation on the performances on three public autonomous driving, Micro Aerial Vehicle (MAV) and hand-held VIO datasets. The results demonstrate the effectiveness of the fusion strategies, which offer better performances compared to direct fusion, particularly in presence of corrupted data. In addition, we study the interpretability of the fusion networks by visualising the masking layers in different scenarios and with varying data corruption, revealing interesting correlations between the fusion networks and imperfect sensory input data.","",""
6,"João P. de Almeida Martins, M. Nilsson, Björn Lampinen, M. Palombo, P. T. While, C. Westin, F. Szczepankiewicz","Neural Networks for parameter estimation in microstructural MRI: a study with a high-dimensional diffusion-relaxation model of white matter microstructure",2021,"","","","",196,"2022-07-13 09:28:50","","10.1101/2021.03.12.435163","","",,,,,6,6.00,1,7,1,"Specific features of white-matter microstructure can be investigated by using biophysical models to interpret relaxation-diffusion MRI brain data. Although more intricate models have the potential to reveal more details of the tissue, they also incur time-consuming parameter estimation that may con-verge to inaccurate solutions due to a prevalence of local minima in a degenerate fitting landscape. Machine-learning fitting algorithms have been proposed to accelerate the parameter estimation and increase the robustness of the attained estimates. So far, learning-based fitting approaches have been restricted to lower-dimensional microstructural models where dense sets of training data are easy to generate. Moreover, the degree to which machine learning can alleviate the degeneracy problem is poorly understood. For conventional least-squares solvers, it has been shown that degeneracy can be avoided by acquisition with optimized relaxation-diffusion-correlation protocols that include tensor-valued diffusion encoding; whether machine-learning techniques can offset these acquisition require-ments remains to be tested. In this work, we employ deep neural networks to vastly accelerate the fitting of a recently introduced high-dimensional relaxation-diffusion model of tissue microstructure. We also develop strategies for assessing the accuracy and sensitivity of function fitting networks and use those strategies to explore the impact of acquisition protocol design on the performance of the network. The developed learning-based fitting pipelines were tested on relaxation-diffusion data acquired with optimized and sub-sampled acquisition protocols. We found no evidence that machine-learning algorithms can by themselves replace a careful design of the acquisition protocol or correct for a degenerate fitting landscape.","",""
0,"G. Henter, S. Ronanki, O. Watts, M. Wester, Zhizheng Wu, Simon King","Robust text-to-speech duration modelling with a deep neural network",2016,"","","","",197,"2022-07-13 09:28:50","","10.1121/1.4969147","","",,,,,0,0.00,0,6,6,"Accurate modeling and prediction of speech-sound durations is important for generating more natural synthetic speech. Deep neural networks (DNNs) offer powerful models, and large, found corpora of natural speech are easily acquired for training them. Unfortunately, poor quality control (e.g., transcription errors) and phenomena such as reductions and filled pauses complicate duration modelling from found speech data. To mitigate issues caused by these idiosyncrasies, we propose to incorporate methods from robust statistics into speech synthesis. Robust methods can disregard ill-fitting training-data points—errors or other outliers—to describe the typical case better. For instance, parameter estimation can be made robust by replacing maximum likelihood with a robust estimation criterion based on the density power divergence (a.k.a. the β-divergence). Alternatively, a standard approximation for output generation with mixture density networks (MDNs) can be interpreted as a robust output generation heuristic....","",""
2,"Yichuan Zhang, Yixing Lan, Qiang Fang, Xin Xu, Junxiang Li, Yujun Zeng","Efficient Reinforcement Learning from Demonstration via Bayesian Network-Based Knowledge Extraction",2021,"","","","",198,"2022-07-13 09:28:50","","10.1155/2021/7588221","","",,,,,2,2.00,0,6,1,"Reinforcement learning from demonstration (RLfD) is considered to be a promising approach to improve reinforcement learning (RL) by leveraging expert demonstrations as the additional decision-making guidance. However, most existing RLfD methods only regard demonstrations as low-level knowledge instances under a certain task. Demonstrations are generally used to either provide additional rewards or pretrain the neural network-based RL policy in a supervised manner, usually resulting in poor generalization capability and weak robustness performance. Considering that human knowledge is not only interpretable but also suitable for generalization, we propose to exploit the potential of demonstrations by extracting knowledge from them via Bayesian networks and develop a novel RLfD method called Reinforcement Learning from demonstration via Bayesian Network-based Knowledge (RLBNK). The proposed RLBNK method takes advantage of node influence with the Wasserstein distance metric (NIW) algorithm to obtain abstract concepts from demonstrations and then a Bayesian network conducts knowledge learning and inference based on the abstract data set, which will yield the coarse policy with corresponding confidence. Once the coarse policy's confidence is low, another RL-based refine module will further optimize and fine-tune the policy to form a (near) optimal hybrid policy. Experimental results show that the proposed RLBNK method improves the learning efficiency of corresponding baseline RL algorithms under both normal and sparse reward settings. Furthermore, we demonstrate that our RLBNK method delivers better generalization capability and robustness than baseline methods.","",""
3,"A. Sun, P. Jiang, M. Mudunuru, Xingyuan Chen","Explore Spatio‐Temporal Learning of Large Sample Hydrology Using Graph Neural Networks",2021,"","","","",199,"2022-07-13 09:28:50","","10.1029/2021WR030394","","",,,,,3,3.00,1,4,1,"Streamflow forecasting over gauged and ungauged basins play a vital role in water resources planning, especially under the changing climate. Increased availability of large sample hydrology data sets, together with recent advances in deep learning techniques, has presented new opportunities to explore temporal and spatial patterns in hydrological signatures for improving streamflow forecasting. The purpose of this study is to adapt and benchmark several state‐of‐the‐art graph neural network (GNN) architectures, including ChebNet, Graph Convolutional Network (GCN), and GraphWaveNet, for end‐to‐end graph learning. We explicitly represent river basins as nodes in a graph, learn the spatiotemporal nodal dependencies, and then use the learned relations to predict streamflow simultaneously across all nodes in the graph. The efficacy of the developed GNN models is investigated using the Catchment Attributes and MEteorology for Large‐sample Studies (CAMELS) data set under two settings, fixed graph topology (transductive learning), and variable graph topology (inductive learning), with the latter applicable to prediction in ungauged basins (PUB). Results indicate that GNNs are generally robust and computationally efficient, achieving similar or better performance than a baseline model trained using the long short‐term memory (LSTM) network. Further analyses are conducted to interpret the graph learning process at the edge and node levels and to investigate the effect of different model configurations. We conclude that graph learning constitutes a viable machine learning‐based method for aggregating spatiotemporal information from a multitude of sources for streamflow forecasting","",""
3,"A. Habibnia, E. Maasoumi","Forecasting in Big Data Environments: an Adaptable and Automated Shrinkage Estimation of Neural Networks (AAShNet)",2019,"","","","",200,"2022-07-13 09:28:50","","10.1007/s40953-021-00275-7","","",,,,,3,1.00,2,2,3,"","",""
