Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
24,"P. Poudel, A. Illanes, Debdoot Sheet, M. Friebe","Evaluation of Commonly Used Algorithms for Thyroid Ultrasound Images Segmentation and Improvement Using Machine Learning Approaches",2018,"","","","",1,"2022-07-13 09:22:41","","10.1155/2018/8087624","","",,,,,24,6.00,6,4,4,"The thyroid is one of the largest endocrine glands in the human body, which is involved in several body mechanisms like controlling protein synthesis and the body's sensitivity to other hormones and use of energy sources. Hence, it is of prime importance to track the shape and size of thyroid over time in order to evaluate its state. Thyroid segmentation and volume computation are important tools that can be used for thyroid state tracking assessment. Most of the proposed approaches are not automatic and require long time to correctly segment the thyroid. In this work, we compare three different nonautomatic segmentation algorithms (i.e., active contours without edges, graph cut, and pixel-based classifier) in freehand three-dimensional ultrasound imaging in terms of accuracy, robustness, ease of use, level of human interaction required, and computation time. We figured out that these methods lack automation and machine intelligence and are not highly accurate. Hence, we implemented two machine learning approaches (i.e., random forest and convolutional neural network) to improve the accuracy of segmentation as well as provide automation. This comparative study intends to discuss and analyse the advantages and disadvantages of different algorithms. In the last step, the volume of the thyroid is computed using the segmentation results, and the performance analysis of all the algorithms is carried out by comparing the segmentation results with the ground truth.","",""
0,"P. Poudel, A. Illanes, Debdoot Sheet, M. Friebe","EvaluationofCommonlyUsedAlgorithms forThyroidUltrasound Images Segmentation and Improvement Using Machine Learning Approaches",2018,"","","","",2,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,4,4,")e thyroid is one of the largest endocrine glands in the human body, which is involved in several body mechanisms like controlling protein synthesis and the body’s sensitivity to other hormones and use of energy sources. Hence, it is of prime importance to track the shape and size of thyroid over time in order to evaluate its state. )yroid segmentation and volume computation are important tools that can be used for thyroid state tracking assessment. Most of the proposed approaches are not automatic and require long time to correctly segment the thyroid. In this work, we compare three different nonautomatic segmentation algorithms (i.e., active contours without edges, graph cut, and pixel-based classifier) in freehand three-dimensional ultrasound imaging in terms of accuracy, robustness, ease of use, level of human interaction required, and computation time. We figured out that these methods lack automation andmachine intelligence and are not highly accurate. Hence, we implemented two machine learning approaches (i.e., random forest and convolutional neural network) to improve the accuracy of segmentation as well as provide automation. )is comparative study intends to discuss and analyse the advantages and disadvantages of different algorithms. In the last step, the volume of the thyroid is computed using the segmentation results, and the performance analysis of all the algorithms is carried out by comparing the segmentation results with the ground truth.","",""
5,"T. G. Thuruthel, J. Hughes, F. Iida","Joint Entropy-Based Morphology Optimization of Soft Strain Sensor Networks for Functional Robustness",2020,"","","","",3,"2022-07-13 09:22:41","","10.1109/jsen.2020.2995237","","",,,,,5,2.50,2,3,2,"Dense and distributed tactile sensors are critical for robots to achieve human-like manipulation skills. Soft robotic sensors are a potential technological solution to obtain the required high dimensional sensory information unobtrusively. However, the design of this new class of sensors is still based on human intuition or derived from traditional flex sensors. This work is a first step towards automated design of soft sensor morphologies based on optimization of information theory metrics and machine learning. Elementary simulation models are used to develop the optimized sensor morphologies that are more accurate and robust with the same number of sensors. Same configurations are replicated experimentally to validate the feasibility of such an approach for practical applications. Furthermore, we present a novel technique for drift compensation in soft strain sensors that allows us to obtain accurate contact localization. This work is an effort towards transferring the paradigm of morphological computation from soft actuator designing to soft sensor designing for high performance, resilient tactile sensory networks.","",""
1,"Á. F. Kungl","Robust learning algorithms for spiking and rate-based neural networks",2020,"","","","",4,"2022-07-13 09:22:41","","10.11588/HEIDOK.00028385","","",,,,,1,0.50,1,1,2,"Inspired by the remarkable properties of the human brain, the fields of machine learning, computational neuroscience and neuromorphic engineering have achieved significant synergistic progress in the last decade. Powerful neural network models rooted in machine learning have been proposed as models for neuroscience and for applications in neuromorphic engineering. However, the aspect of robustness is often neglected in these models. Both biological and engineered substrates show diverse imperfections that deteriorate the performance of computation models or even prohibit their implementation. This thesis describes three projects aiming at implementing robust learning with local plasticity rules in neural networks. First, we demonstrate the advantages of neuromorphic computations in a pilot study on a prototype chip. Thereby, we quantify the speed and energy consumption of the system compared to a software simulation and show how on-chip learning contributes to the robustness of learning. Second, we present an implementation of spike-based Bayesian inference on accelerated neuromorphic hardware. The model copes, via learning, with the disruptive effects of the imperfect substrate and benefits from the acceleration. Finally, we present a robust model of deep reinforcement learning using local learning rules. It shows how backpropagation combined with neuromodulation could be implemented in a biologically plausible framework. The results contribute to the pursuit of robust and powerful learning networks for biological and neuromorphic substrates.","",""
45,"A. Jalal, M. A. K. Quaid, A. S. Hasan","Wearable Sensor-Based Human Behavior Understanding and Recognition in Daily Life for Smart Environments",2018,"","","","",5,"2022-07-13 09:22:41","","10.1109/FIT.2018.00026","","",,,,,45,11.25,15,3,4,"Behavior recognition using motion sensors is getting prominence over other systems such as e-healthcare and life-log analysis systems especially in the healthcare domain for improving life expectancy and healthcare access. Accelerometers have been used in smart environments to recognize behavior since the last decade but heavy computation involved in recognizer model made them less acceptable. This paper proposed a computationally less expensive model with better recognition results for improved human behavior understanding system. Hierarchical features are used to ensure robustness as a performance attribute in the proposed system. These hierarchical features involve statistical features like signal magnitude, abrupt changes, and temporal variation among coordinates. Moreover, the extracted features are examined through the process of learning, training, and symbolization with the help of linear support vector machine. The examination of our recognition results based on feature extraction strategy show that our model excels others in terms of accuracy and computation time. The proposed system should be considered as a recommendation for systems involving human behavior recognition i.e. kindergarten, elderly at old-age houses and patients with Parkinson diseases.","",""
14,"Steven R. Young, Junjie Lu, J. Holleman, I. Arel","On the Impact of Approximate Computation in an Analog DeSTIN Architecture",2014,"","","","",6,"2022-07-13 09:22:41","","10.1109/TNNLS.2013.2283730","","",,,,,14,1.75,4,4,8,"Deep machine learning (DML) holds the potential to revolutionize machine learning by automating rich feature extraction, which has become the primary bottleneck of human engineering in pattern recognition systems. However, the heavy computational burden renders DML systems implemented on conventional digital processors impractical for large-scale problems. The highly parallel computations required to implement large-scale deep learning systems are well suited to custom hardware. Analog computation has demonstrated power efficiency advantages of multiple orders of magnitude relative to digital systems while performing nonideal computations. In this paper, we investigate typical error sources introduced by analog computational elements and their impact on system-level performance in DeSTIN-a compositional deep learning architecture. These inaccuracies are evaluated on a pattern classification benchmark, clearly demonstrating the robustness of the underlying algorithm to the errors introduced by analog computational elements. A clear understanding of the impacts of nonideal computations is necessary to fully exploit the efficiency of analog circuits.","",""
1,"Motoki Sakurai, Yosuke Ueno, Masaaki Kondo","Path Planning and Moving Obstacle Avoidance with Neuromorphic Computing",2021,"","","","",7,"2022-07-13 09:22:41","","10.1109/ISR50024.2021.9419537","","",,,,,1,1.00,0,3,1,"Neuromorphic computing has been getting attention because of its potential for fast and low-power computation, robustness, and learning capability. Though traditional machine learning applications are main target of neuromorphic computing, its characteristic of parallel and distributed processing with simple spike-based signals is useful for other types of applications such as a shortest path finding problem (SPFP) on a graph. Prior work discussed approaches for mapping SPFP to a spiking neural network (SNN). In this paper, we propose an SNN algorithm for path planning with moving obstacles. In real world situation, there are many moving obstacles (such as other cars for an autonomous driving car and human for a moving robot) around a target agent which tries to optimize its own path to the goal. Finding an effective path in such an environment is not an easy task since behavior of obstacles is sometimes unknown and there must be a huge number of candidate paths to go. Traditional methods for SPFP with a general CPU may not be effective since it should compare candidate paths and select the most suitable one every time step. We consider two agents with SNN which tries to achieve two goals: “reaching its destination promptly” and “avoiding moving obstacles properly”. Thanks to SNN properties, the agent can learn and estimate how the obstacles move. We compare the proposal approaches with an existing method on a 2D grid graph and the result shows that the proposal agents can select proper paths depending on obstacles' movement.","",""
3,"M. Alcañiz, E. Olmos-Raya, L. Abad","[Use of virtual reality for neurodevelopmental disorders. A review of the state of the art and future agenda].",2019,"","","","",8,"2022-07-13 09:22:41","","","","",,,,,3,1.00,1,3,3,"To date, the diagnostic tools for autism spectrum disorder (ASD) have been mostly based on qualitative criteria from observational information in contexts with low ecological validity. We are witnessing a growing scientific activity that proposes the use of implicit measures for the evaluation and diagnosis of ASD. These measures are based on processes of a biological and unconscious nature, underlying the capacity of human cognition, and are obtained through the acquisition and treatment of brain, physiological and behavioral responses in order to obtain the behavioral structure of the ASD patient facing a stimulus. The complex relationship between physiological responses and the behavioral structure of the ASD patient requires the use of advanced techniques of signal processing based on cognitive computation. Artificial intelligence (AI) techniques, such as machine learning and neurocomputing applied to the analysis of psychophysiological signals, have demonstrated their robustness for the classification of complex cognitive constructs. Virtual reality (VR) is a tool that allows recreating real-life situations with high sensory fidelity, but at the same time individually controlling each of the situations and stimuli that influence human behavior. It also allows the measurement in real time of human reactions to such stimuli. This document analyzes the latest scientific and technological advances relevant to its applications in the diagnosis of ASD. We conclude that VR is a very valuable tool for ASD research, especially for the evaluation and diagnosis of complex skills and competencies.","",""
0,"Wang Yan","Human computer interaction technology of expression recognition based on SVM and AdaBoost classifier",2013,"","","","",9,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,1,9,"This paper is based on the 2DPCA method and introduces a facial expression classification on method which is based on a SVM and AdaBoost algorithm. Firstly, using the method for the face detection of the gray image and obtaining the characteristics data through the wavelet transform and 2DPCA, We reduced the amount of computation effectively. Secondly, we obtain the original classifier by the SVM method to classify learning characteristics data, then through the AdaBoost algorithm to further strengthen the SVM classification results, forming the strong classifier and improving the classification ability. This ensure finishing the work expression recognition and realizing the robustness of the man-machine interaction based on the facial expression recognition of the intelligent wheelchair. The experimental results show that this method not only effectively improves the classification ability of the sample, but also reduces the computational complexity, with an average recognition rate of 92.5% in the intelligent wheelchair human-computer interaction experiment.","",""
27,"Thuy Thi Nguyen, N. Binh, H. Bischof","An active boosting-based learning framework for real-time hand detection",2008,"","","","",10,"2022-07-13 09:22:41","","10.1109/AFGR.2008.4813315","","",,,,,27,1.93,9,3,14,"Human hand detection problem has important applications in sign language and human machine interfaces. In this work, we present a novel approach for learning a vision-based hand detection system. The main contribution is a robust on-line boosting-based framework for real-time detection of a hand in unconstrained environments. The use of efficient representative features allows fast computation while dealing with vast changing of hand appearances and background. Interactive on-line training allows efficiently train and improve the detector. Moreover, we propose a strategy to efficiently improve the performance meanwhile reduce hand labeling effort. Besides, if necessary, we use a verification process to prevent ldquodriftingrdquo of classifier over time. The proposed method is practically favorable as it meets the requirements of real-time performance, accuracy and robustness. It works well with reasonable amount of training samples and is computational efficient. Experiments for detection of hands in challenging data sets show the outperform of our approach.","",""
1,"Wang Hui-rong","Robust watermarking scheme based on support vector machine in DWT domain",2008,"","","","",11,"2022-07-13 09:22:41","","","","",,,,,1,0.07,1,1,14,"A novel watermarking scheme based on support vector machine(SVM) in discrete wavelet transform(DWT) domain is proposed.By applying the DWT on blue channel of the color image,and the computation of the JND value in the DWT domain according to the human visual system(HVS) model,the watermark is embedded into the lower frequency subband at its adaptive strength.By the training of the SVM,it can memorize the relationship between the pixel at the embedding position and its’ neighbors when the image is undergone some attacks.Since the SVM have good learning and generalization ability,the proposed method can achieve good robust-ness.Experimental results show good robustness of proposed method against different attacks.","",""
9,"Jeff Yan, Su-Yang Yu","Streamlining Attacks on CAPTCHAs with a Computer Game",2009,"","","","",12,"2022-07-13 09:22:41","","","","",,,,,9,0.69,5,2,13,"CAPTCHA has been widely deployed by commercial web sites as a security technology for purposes such as anti-spam. A common approach to evaluating the robustness of CAPTCHA is the use of machine learning techniques. Critical to this approach is the acquisition of an adequate set of labeled samples, on which the learning techniques are trained. However, such a sample labeling task is difficult for computers, since the strength of CAPTCHAs stems exactly from the difficulty computers have in recognizing either distorted texts or image contents. Therefore, until now, researchers have to manually label their samples, which is tedious and expensive. In this paper, we present Magic Bullet, a computer game that for the first time turns such sample labeling into a fun experience, and that achieves a labeling accuracy of as high as 98% for free. The game leverages human computation to address a task that cannot be easily automated, and it effectively streamlines the evaluation of CAPTCHAs. The game can also be used for other constructive purposes such as 1) developing better machine learning algorithms for handwriting recognition, and 2) training people's typing skills.","",""
1,"I. Jivet, A. Brîndusescu, I. Bogdanov","A perception oriented formal model for 3D sensor depth images",2008,"","","","",13,"2022-07-13 09:22:41","","","","",,,,,1,0.07,0,3,14,"The paper presents a perception oriented formal linguistic model for 3D sensors depth images. The objective of the proposed model is a compact abstract representation of the application environment naturally translatable to a target application. The applications targeted are tightly coupled perception action subsystems involved in independent environment exploration and learning be it human or machine. Field of view depth images obtained with recently developed CMOS 3D sensors are analyzed in their capacity to provide action oriented consistent data. For the 3D scene images a selective segmented is proposed in terms of salient objects from the depth image. The segmentation robustness and computation cost where the central constraints considered. The model as proposed uses a representation of the scene depth image in terms of object area and mean center location. An original formal language representation is proposed. A context free grammar is shown to be natural to 3D sensor data. The extension of the context free grammar with attributes adds structure to the model. It is also shown that the generated language translates directly depth labeling into action planning on the environment. The attribute added semantics is shown to facilitate an easy learning of the representation for human perception. The formal representation method proposed is analyzed in its performance in terms of estimated computation time and semantic relevance for sample applications. Further development of the model for multi layered representations for more complex applications areas are finally outlined.","",""
13,"Xinlei Mi, Baiming Zou, F. Zou, J. Hu","Permutation-based identification of important biomarkers for complex diseases via machine learning models",2021,"","","","",14,"2022-07-13 09:22:41","","10.1038/s41467-021-22756-2","","",,,,,13,13.00,3,4,1,"","",""
3,"M. Halgamuge","Supervised Machine Learning Algorithms for Bioelectromagnetics: Prediction Models and Feature Selection Techniques Using Data from Weak Radiofrequency Radiation Effect on Human and Animals Cells",2020,"","","","",15,"2022-07-13 09:22:41","","10.3390/ijerph17124595","","",,,,,3,1.50,3,1,2,"The emergence of new technologies to incorporate and analyze data with high-performance computing has expanded our capability to accurately predict any incident. Supervised Machine learning (ML) can be utilized for a fast and consistent prediction, and to obtain the underlying pattern of the data better. We develop a prediction strategy, for the first time, using supervised ML to observe the possible impact of weak radiofrequency electromagnetic field (RF-EMF) on human and animal cells without performing in-vitro laboratory experiments. We extracted laboratory experimental data from 300 peer-reviewed scientific publications (1990–2015) describing 1127 experimental case studies of human and animal cells response to RF-EMF. We used domain knowledge, Principal Component Analysis (PCA), and the Chi-squared feature selection techniques to select six optimal features for computation and cost-efficiency. We then develop grouping or clustering strategies to allocate these selected features into five different laboratory experiment scenarios. The dataset has been tested with ten different classifiers, and the outputs are estimated using the k-fold cross-validation method. The assessment of a classifier’s prediction performance is critical for assessing its suitability. Hence, a detailed comparison of the percentage of the model accuracy (PCC), Root Mean Squared Error (RMSE), precision, sensitivity (recall), 1 − specificity, Area under the ROC Curve (AUC), and precision-recall (PRC Area) for each classification method were observed. Our findings suggest that the Random Forest algorithm exceeds in all groups in terms of all performance measures and shows AUC = 0.903 where k-fold = 60. A robust correlation was observed in the specific absorption rate (SAR) with frequency and cumulative effect or exposure time with SAR×time (impact of accumulated SAR within the exposure time) of RF-EMF. In contrast, the relationship between frequency and exposure time was not significant. In future, with more experimental data, the sample size can be increased, leading to more accurate work.","",""
13,"M. Grassia, M. Domenico, G. Mangioni","Machine learning dismantling and early-warning signals of disintegration in complex systems",2021,"","","","",16,"2022-07-13 09:22:41","","10.1038/s41467-021-25485-8","","",,,,,13,13.00,4,3,1,"","",""
1,"A. Tellaeche, Ignacio Fidalgo Astorquia, Juan Ignacio Vázquez Gómez, S. Saikia","Gesture-Based Human Machine Interaction Using RCNNs in Limited Computation Power Devices",2021,"","","","",17,"2022-07-13 09:22:41","","10.3390/s21248202","","",,,,,1,1.00,0,4,1,"The use of gestures is one of the main forms of human machine interaction (HMI) in many fields, from advanced robotics industrial setups, to multimedia devices at home. Almost every gesture detection system uses computer vision as the fundamental technology, with the already well-known problems of image processing: changes in lighting conditions, partial occlusions, variations in color, among others. To solve all these potential issues, deep learning techniques have been proven to be very effective. This research proposes a hand gesture recognition system based on convolutional neural networks and color images that is robust against environmental variations, has a real time performance in embedded systems, and solves the principal problems presented in the previous paragraph. A new CNN network has been specifically designed with a small architecture in terms of number of layers and total number of neurons to be used in computationally limited devices. The obtained results achieve a percentage of success of 96.92% on average, a better score than those obtained by previous algorithms discussed in the state of the art.","",""
9,"S. Basith, M. Hasan, Gwang Lee, Leyi Wei, Balachandran Manavalan","Integrative machine learning framework for the identification of cell-specific enhancers from the human genome.",2021,"","","","",18,"2022-07-13 09:22:41","","10.1093/bib/bbab252","","",,,,,9,9.00,2,5,1,"Enhancers are deoxyribonucleic acid (DNA) fragments which when bound by transcription factors enhance the transcription of related genes. Due to its sporadic distribution and similar fractions, identification of enhancers from the human genome seems a daunting task. Compared to the traditional experimental approaches, computational methods with easy-to-use platforms could be efficiently applied to annotate enhancers' functions and physiological roles. In this aspect, several bioinformatics tools have been developed to identify enhancers. Despite their spectacular performances, existing methods have certain drawbacks and limitations, including fixed length of sequences being utilized for model development and cell-specificity negligence. A novel predictor would be beneficial in the context of genome-wide enhancer prediction by addressing the above-mentioned issues. In this study, we constructed new datasets for eight different cell types. Utilizing these data, we proposed an integrative machine learning (ML)-based framework called Enhancer-IF for identifying cell-specific enhancers. Enhancer-IF comprehensively explores a wide range of heterogeneous features with five commonly used ML methods (random forest, extremely randomized tree, multilayer perceptron, support vector machine and extreme gradient boosting). Specifically, these five classifiers were trained with seven encodings and obtained 35 baseline models. The output of these baseline models was integrated and again inputted to five classifiers for the construction of five meta-models. Finally, the integration of five meta-models through ensemble learning improved the model robustness. Our proposed approach showed an excellent prediction performance compared to the baseline models on both training and independent datasets in different cell types, thus highlighting the superiority of our approach in the identification of the enhancers. We assume that Enhancer-IF will be a valuable tool for screening and identifying potential enhancers from the human DNA sequences.","",""
1,"Ajay Sharma, Ankit Gupta, V. Jaiswal","Solving Image Processing Critical Problems Using Machine Learning",2021,"","","","",19,"2022-07-13 09:22:41","","10.1007/978-981-15-9492-2_11","","",,,,,1,1.00,0,3,1,"","",""
2,"Martina Bertazzo, D. Gobbo, S. Decherchi, A. Cavalli","Machine Learning and Enhanced Sampling Simulations for Computing the Potential of Mean Force and Standard Binding Free Energy",2021,"","","","",20,"2022-07-13 09:22:41","","10.1021/acs.jctc.1c00177","","",,,,,2,2.00,1,4,1,"Computational capabilities are rapidly increasing, primarily because of the availability of GPU-based architectures. This creates unprecedented simulative possibilities for the systematic and robust computation of thermodynamic observables, including the free energy of a drug binding to a target. In contrast to calculations of relative binding free energy, which are nowadays widely exploited for drug discovery, we here push the boundary of computing the binding free energy and the potential of mean force. We introduce a novel protocol that leverages enhanced sampling, machine learning, and ad hoc algorithms to limit human intervention, computing time, and free parameters in free energy calculations. We first validate the method on a host–guest system, and then we apply the protocol to glycogen synthase kinase 3 beta, a protein kinase of pharmacological interest. Overall, we obtain a good correlation with experimental values in relative and absolute terms. While we focus on protein–ligand binding, the strategy is of broad applicability to any complex event that can be described with a path collective variable. We systematically discuss key details that influence the final result. The parameters and simulation settings are available at PLUMED-NEST to allow full reproducibility.","",""
0,"R. Qu","Recent Developments of Automated Machine Learning and Search Techniques",2021,"","","","",21,"2022-07-13 09:22:41","","10.1007/978-3-030-72069-8_1","","",,,,,0,0.00,0,1,1,"","",""
24,"Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer, M. Bethge, F. Wichmann, Wieland Brendel","Partial success in closing the gap between human and machine vision",2021,"","","","",22,"2022-07-13 09:22:41","","","","",,,,,24,24.00,3,7,1,"A few years ago, the first CNN surpassed human performance on ImageNet. However, it soon became clear that machines lack robustness on more challenging test cases, a major obstacle towards deploying machines “in the wild” and towards obtaining better computational models of human visual perception. Here we ask: Are we making progress in closing the gap between human and machine vision? To answer this question, we tested human observers on a broad range of out-ofdistribution (OOD) datasets, recording 85,120 psychophysical trials across 90 participants. We then investigated a range of promising machine learning developments that crucially deviate from standard supervised CNNs along three axes: objective function (self-supervised, adversarially trained, CLIP language-image training), architecture (e.g. vision transformers), and dataset size (ranging from 1M to 1B). Our findings are threefold. (1.) The longstanding distortion robustness gap between humans and CNNs is closing, with the best models now exceeding human feedforward performance on most of the investigated OOD datasets. (2.) There is still a substantial image-level consistency gap, meaning that humans make different errors than models. In contrast, most models systematically agree in their categorisation errors, even substantially different ones like contrastive self-supervised vs. standard supervised models. (3.) In many cases, human-to-model consistency improves when training dataset size is increased by one to three orders of magnitude. Our results give reason for cautious optimism: While there is still much room for improvement, the behavioural difference between human and machine vision is narrowing. In order to measure future progress, 17 OOD datasets with image-level human behavioural data and evaluation code are provided as a toolbox and benchmark at https://github.com/bethgelab/model-vs-human/.","",""
3,"Hrushikesh Shukla, Nakshatra Jagtap, Balaji Patil","Enhanced Twitter bot detection using ensemble machine learning",2021,"","","","",23,"2022-07-13 09:22:41","","10.1109/ICICT50816.2021.9358734","","",,,,,3,3.00,1,3,1,"Social media has been an unavoidable part of our life over the years. As it is getting popular, the number of social media bots are also increasing. Social media bots are the artificial agents who imitate as a human on social media. They are intended to like, retweet the posts which eventually can tamper with the genuineness of the trend. They can also be a menace to democracy as they can falsely influence people. Social media bots can be used for cyberbullying, terrorist activities, gaining fame, spreading wrong information, restricting freedom of speech, spamming. To detect social media bots on Twitter, we utilized metadata of Twitter profiles and applied a unique feature selection method, and also explored the power of ensemble learning to make a robust classifier.","",""
0,"A. Maru, Ajay Kumar Sharma, Mayank Patel","Hybrid Machine Learning Classification Technique for Improve Accuracy of Heart Disease",2021,"","","","",24,"2022-07-13 09:22:41","","10.1109/ICICT50816.2021.9358616","","",,,,,0,0.00,0,3,1,"The area of medical science has attracted great attention from researchers. Several causes for human early mortality have been identified by a decent number of investigators. The related literature has confirmed that diseases are caused by different reasons and one such cause is heart-based sicknesses. Many researchers proposed idiosyncratic methods to preserve human life and help health care experts to recognize, prevent and manage heart disease. Some of the convenient methodologies facilitate the expert's decision but every successful scheme has its own restrictions. The proposed approach robustly analyze an act of Hidden Markov Model (HMM), Artificial Neural Network (ANN), Support Vector Machine (SVM), and Decision Tree J48 along with the two different feature selection methods such as Correlation Based Feature Selection (CFS) and Gain Ratio. The Gain Ratio accompanies the Ranker method over a different group of statistics. After analyzing the procedure the intended method smartly builds Naive Bayes processing that utilizes the operation of two most appropriate processes with suitable layered design. Initially, the intention is to select the most appropriate method and analyzing the act of available schemes executed with different features for examining the statistics.","",""
2,"Juan I. Di Filippo, Mariela Bollini, C. Cavasotto","A Machine Learning Model to Predict Drug Transfer Across the Human Placenta Barrier",2021,"","","","",25,"2022-07-13 09:22:41","","10.3389/fchem.2021.714678","","",,,,,2,2.00,1,3,1,"The development of computational models for assessing the transfer of chemicals across the placental membrane would be of the utmost importance in drug discovery campaigns, in order to develop safe therapeutic options. We have developed a low-dimensional machine learning model capable of classifying compounds according to whether they can cross or not the placental barrier. To this aim, we compiled a database of 248 compounds with experimental information about their placental transfer, characterizing each compound with a set of ∼5.4 thousand descriptors, including physicochemical properties and structural features. We evaluated different machine learning classifiers and implemented a genetic algorithm, in a five cross validation scheme, to perform feature selection. The optimization was guided towards models displaying a low number of false positives (molecules that actually cross the placental barrier, but are predicted as not crossing it). A Linear Discriminant Analysis model trained with only four structural features resulted to be robust for this task, exhibiting only one false positive case across all testing folds. This model is expected to be useful in predicting placental drug transfer during pregnancy, and thus could be used as a filter for chemical libraries in virtual screening campaigns.","",""
0,"Agnes M Resto Irizarry, S. Esfahani, Yi Zheng, R. Yan, Patrick C. Kinnunen, Jianping Fu","Machine learning-assisted imaging analysis of a human epiblast model.",2021,"","","","",26,"2022-07-13 09:22:41","","10.1093/intbio/zyab014","","",,,,,0,0.00,0,6,1,"The human embryo is a complex structure that emerges and develops as a result of cell-level decisions guided by both intrinsic genetic programs and cell-cell interactions. Given limited accessibility and associated ethical constraints of human embryonic tissue samples, researchers have turned to the use of human stem cells to generate embryo models to study specific embryogenic developmental steps. However, to study complex self-organizing developmental events using embryo models, there is a need for computational and imaging tools for detailed characterization of cell-level dynamics at the single cell level. In this work, we obtained live cell imaging data from a human pluripotent stem cell (hPSC)-based epiblast model that can recapitulate the lumenal epiblast cyst formation soon after implantation of the human blastocyst. By processing imaging data with a Python pipeline that incorporates both cell tracking and event recognition with the use of a CNN-LSTM machine learning model, we obtained detailed temporal information of changes in cell state and neighborhood during the dynamic growth and morphogenesis of lumenal hPSC cysts. The use of this tool combined with reporter lines for cell types of interest will drive future mechanistic studies of hPSC fate specification in embryo models and will advance our understanding of how cell-level decisions lead to global organization and emergent phenomena. Insight, innovation, integration: Human pluripotent stem cells (hPSCs) have been successfully used to model and understand cellular events that take place during human embryogenesis. Understanding how cell-cell and cell-environment interactions guide cell actions within a hPSC-based embryo model is a key step in elucidating the mechanisms driving system-level embryonic patterning and growth. In this work, we present a robust video analysis pipeline that incorporates the use of machine learning methods to fully characterize the process of hPSC self-organization into lumenal cysts to mimic the lumenal epiblast cyst formation soon after implantation of the human blastocyst. This pipeline will be a useful tool for understanding cellular mechanisms underlying key embryogenic events in embryo models.","",""
12,"Rhys Evans, Ladislav Hovan, G. A. Tribello, Benjamin P. Cossins, C. Estarellas, F. Gervasio","Combining Machine Learning and Enhanced Sampling Techniques for Efficient and Accurate Calculation of Absolute Binding Free Energies",2020,"","","","",27,"2022-07-13 09:22:41","","10.1021/acs.jctc.0c00075","","",,,,,12,6.00,2,6,2,"Calculating absolute binding free energies is challenging and important. In this paper, we test some recently developed metadynamics-based methods and develop a new combination with a Hamiltonian replica-exchange approach. The methods were tested on 18 chemically diverse ligands with a wide range of different binding affinities to a complex target; namely, human soluble epoxide hydrolase. The results suggest that metadynamics with a funnel-shaped restraint can be used to calculate, in a computationally affordable and relatively accurate way, the absolute binding free energy for small fragments. When used in combination with an optimal pathlike variable obtained using machine learning or with the Hamiltonian replica-exchange algorithm SWISH, this method can achieve reasonably accurate results for increasingly complex ligands, with a good balance of computational cost and speed. An additional benefit of using the combination of metadynamics and SWISH is that it also provides useful information about the role of water in the binding mechanism.","",""
1,"Seung Jun Lee, B. Kim, M. Y. Kim","Multi-Saliency Map and Machine Learning Based Human Detection for the Embedded Top-View Imaging System",2021,"","","","",28,"2022-07-13 09:22:41","","10.1109/ACCESS.2021.3078623","","",,,,,1,1.00,0,3,1,"Compared to the side view, a top-view is robust against occlusion generated by objects located indoors. It offers a better wide view angle and much visibility of a scene. However, there are still problems to be handled. The top-view image shows asymmetrical features and radially distorted scenes around the corners, such as omnidirectional view images and self-occlusion. Conventional human detection methods are suitable for finding moving objects in front view imaging systems. And there are some limitations, such as slow execution speed due to computational complexity. In this paper, we propose an efficient method. A static saliency map with low activity and a dynamic saliency map with a lot of movement are respectively detected. These two models were fused to create a multi-saliency map, and both characteristics were used simultaneously to improve detection rates. To handle problems such as asymmetry, a rotation matrix was calculated around the center, and Histogram of Oriented Gradient (HOG) features descriptor were extracted from the multi-saliency map to create an image patch (a small image region of interest containing human candidates). For the classification of image patches, we used machine learning-based supervised learning models support-vector machine (SVM) algorithm to improve performance. As a result of the proposed algorithm, it showed low resource occupancy and achieved Average Precision of 92.3% and 96.12% when Intersection over Union were 50% and 45% respectively.","",""
12,"J. Vizcarra, M. Gearing, Michael J. Keiser, J. Glass, B. Dugger, D. Gutman","Validation of machine learning models to detect amyloid pathologies across institutions",2020,"","","","",29,"2022-07-13 09:22:41","","10.1186/s40478-020-00927-4","","",,,,,12,6.00,2,6,2,"","",""
0,"Mauricio Cortes, A. Monti, Sunny Sun, Orna Lynch, Youli Xia, Sarah Lin, M. Malamas, S. Steelman, S. Krishnamoorthy, Morag H. Stewart, E. Tozzo","Identification of Small Molecules That Induce Therapeutic Levels of Fetal Hemoglobin for Treatment of Sickle Cell Disease By Pairing Machine Learning with High-Resolution Single Cell RNA Sequencing Maps of Adult and Fetal Human Erythropoiesis",2021,"","","","",30,"2022-07-13 09:22:41","","10.1182/blood-2021-152728","","",,,,,0,0.00,0,11,1,"  Induction of fetal hemoglobin (HbF) to treat sickle cell disease (SCD) and beta-thalassemia has been validated as a clinical strategy to ameliorate these diseases. However, despite advances in cell and gene therapy to treat SCD, current approaches still require myeloablation and bone marrow transplantation, which carries risk and limits treatment accessibility. Currently, hydroxy urea (HU) is the only FDA approved small molecule for SCD that acts as a moderate inducer of HbF. Thus, the ability to develop a next generation small molecule with comparable efficacy to gene therapy approaches, but without the existing limitations of HU, would have immense clinical benefit. Towards the goal of identifying novel molecules that can robustly induce HbF, we generated a high-resolution single cell RNA sequencing (scRNAseq) dataset capturing pseudo-trajectories of adult and fetal erythropoiesis. This dataset allowed us to identify cell-states, cell-features, and gene-networks associated with fetal erythropoiesis in a data driven manner. In addition, by applying proprietary algorithms to this dataset, we identified a transcriptional signature targetable with a selected number of small molecules. Evaluation of these predicted small molecules in a 14-day human in vitro erythroid differentiation assay, identified a subset that induced HbF in mobilized peripheral blood (mPB) CD34+ hematopoietic stem and progenitor cells (HSPC) from healthy donors, as measured by % F-cells (flow cytometry) and % HbF (HPLC). CLT-1081 induced HbF (42.3% +17.26, n=4) above HU (17.16% +4.78, n=5) and BCL11A CRISPR knockdown (32.58% +10.66, n=5). We then measured globin gene expression to further characterize how CLT-1081 compared to HU induced changes in fetal and adult globin expression. CLT-1081 induced robust mRNA expression of both HBG1 and HBG2 and concomitant decrease in HBB transcript relative to HU. To determine if CLT-1081 acted by inducing the targeted transcriptional signature, we performed scRNAseq on a number of known HbF inducers, including CLT-1081, HU, and BCL11A CRISPR knockdown. Analysis revealed that CLT-1081 acted as predicted by our algorithms and activated the genes identified as drivers of HbF induction in the human in vitro erythroid differentiation assay. Furthermore, comparison between BCL11A CRISPR knockdown and CLT-1081 demonstrated strong correlation in terms of transcriptional response; CLT-1081 treatment reduced BCL11A expression in addition to other transcriptional changes, which elucidated a partial mechanism of action. In sum, we have used high dimensional data and computational tools in combination with established in vitro cell based assays to identify and validate a cell behavior marked by robust HbF expression and defined by a gene signature identified by our process. These findings demonstrate the power of Cellarity's platform, which augments a systems biology approach with machine learning to identify and develop drug candidates.      Cortes: Cellarity, Inc.: Current Employment, Current holder of stock options in a privately-held company. Monti: Cellarity, Inc.: Current Employment, Current holder of stock options in a privately-held company. Sun: Cellarity, Inc.: Current Employment, Current holder of stock options in a privately-held company. Lynch: Cellarity, Inc.: Current Employment, Current holder of stock options in a privately-held company. Xia: Cellarity, Inc.: Current Employment, Current holder of stock options in a privately-held company. Lin: Cellarity, Inc.: Current Employment, Current holder of stock options in a privately-held company. Malamas: Cellarity, Inc.: Current Employment, Current holder of stock options in a privately-held company. Steelman: Cellarity, Inc.: Current Employment, Current holder of stock options in a privately-held company. Krishnamoorthy: Cellarity, Inc.: Current Employment, Current holder of stock options in a privately-held company. Stewart: Cellarity, Inc.: Current Employment, Current holder of stock options in a privately-held company. Tozzo: Cellarity, Inc.: Current Employment, Current holder of stock options in a privately-held company. ","",""
3,"Xubo Leng, Margot Wohl, Kenichi Ishii, Pavan Nayak, Kenta Asahina","Quantitative comparison of Drosophila behavior annotations by human observers and a machine learning algorithm",2020,"","","","",31,"2022-07-13 09:22:41","","10.1101/2020.06.16.153130","","",,,,,3,1.50,1,5,2,"Automated quantification of behavior is increasingly prevalent in neuroscience research. Human judgments can influence machine-learning-based behavior classification at multiple steps in the process, for both supervised and unsupervised approaches. Such steps include the design of the algorithm for machine learning, the methods used for animal tracking, the choice of training images, and the benchmarking of classification outcomes. However, how these design choices contribute to the interpretation of automated behavioral classifications has not been extensively characterized. Here, we quantify the effects of experimenter choices on the outputs of automated classifiers of Drosophila social behaviors. Drosophila behaviors contain a considerable degree of variability, which was reflected in the confidence levels associated with both human and computer classifications. We found that a diversity of sex combinations and tracking features was important for robust performance of the automated classifiers. In particular, features concerning the relative position of flies contained useful information for training a machine-learning algorithm. These observations shed light on the importance of human influence on tracking algorithms, the selection of training images, and the quality of annotated sample images used to benchmark the performance of a classifier (the ‘ground truth’). Evaluation of these factors is necessary for researchers to accurately interpret behavioral data quantified by a machine-learning algorithm and to further improve automated classifications. Significance Statement Accurate quantification of animal behaviors is fundamental to neuroscience. Here, we quantitatively assess how human choices influence the performance of automated classifiers trained by a machine-learning algorithm. We found that human decisions about the computational tracking method, the training images, and the images used for performance evaluation impact both the classifier outputs and how human observers interpret the results. These factors are sometimes overlooked but are critical, especially because animal behavior is itself inherently variable. Automated quantification of animal behavior is becoming increasingly prevalent: our results provide a model for bridging the gap between traditional human annotations and computer-based annotations. Systematic assessment of human choices is important for developing behavior classifiers that perform robustly in a variety of experimental conditions.","",""
0,"Edoardo Vecchi, L. Pospíšil, S. Albrecht, T. O'Kane, I. Horenko","eSPA+: Scalable Entropy-Optimal Machine Learning Classification for Small Data Problems",2022,"","","","",32,"2022-07-13 09:22:41","","10.1162/neco_a_01490","","",,,,,0,0.00,0,5,1,"Abstract Classification problems in the small data regime (with small data statistic T and relatively large feature space dimension D) impose challenges for the common machine learning (ML) and deep learning (DL) tools. The standard learning methods from these areas tend to show a lack of robustness when applied to data sets with significantly fewer data points than dimensions and quickly reach the overfitting bound, thus leading to poor performance beyond the training set. To tackle this issue, we propose eSPA+, a significant extension of the recently formulated entropy-optimal scalable probabilistic approximation algorithm (eSPA). Specifically, we propose to change the order of the optimization steps and replace the most computationally expensive subproblem of eSPA with its closed-form solution. We prove that with these two enhancements, eSPA+ moves from the polynomial to the linear class of complexity scaling algorithms. On several small data learning benchmarks, we show that the eSPA+ algorithm achieves a many-fold speed-up with respect to eSPA and even better performance results when compared to a wide array of ML and DL tools. In particular, we benchmark eSPA+ against the standard eSPA and the main classes of common learning algorithms in the small data regime: various forms of support vector machines, random forests, and long short-term memory algorithms. In all the considered applications, the common learning methods and eSPA are markedly outperformed by eSPA+, which achieves significantly higher prediction accuracy with an orders-of-magnitude lower computational cost.","",""
47,"Chenru Duan, J. Janet, Fang Liu, A. Nandy, H. Kulik","Learning from Failure: Predicting Electronic Structure Calculation Outcomes with Machine Learning Models.",2019,"","","","",33,"2022-07-13 09:22:41","","10.1021/acs.jctc.9b00057","","",,,,,47,15.67,9,5,3,"High-throughput computational screening for chemical discovery mandates the automated and unsupervised simulation of thousands of new molecules and materials. In challenging materials spaces, such as open shell transition metal chemistry, characterization requires time-consuming first-principles simulation that often necessitates human intervention. These calculations can frequently lead to a null result, e.g., the calculation does not converge or the molecule does not stay intact during a geometry optimization. To overcome this challenge toward realizing fully automated chemical discovery in transition metal chemistry, we have developed the first machine learning models that predict the likelihood of successful simulation outcomes. We train support vector machine and artificial neural network classifiers to predict simulation outcomes (i.e., geometry optimization result and degree of ⟨ S2⟩ deviation) for a chosen electronic structure method based on chemical composition. For these static models, we achieve an area under the curve of at least 0.95, minimizing computational time spent on nonproductive simulations and therefore enabling efficient chemical space exploration. We introduce a metric of model uncertainty based on the distribution of points in the latent space to systematically improve model prediction confidence. In a complementary approach, we train a convolutional neural network classification model on simulation output electronic and geometric structure time series data. This dynamic model generalizes more readily than the static classifier by becoming more predictive as input simulation length increases. Finally, we describe approaches for using these models to enable autonomous job control in transition metal complex discovery.","",""
3,"Patrick C. Shih","Beyond Human-in-the-Loop: Empowering End-Users with Transparent Machine Learning",2018,"","","","",34,"2022-07-13 09:22:41","","10.1007/978-3-319-90403-0_3","","",,,,,3,0.75,3,1,4,"","",""
312,"Jonas Rauber, Wieland Brendel, M. Bethge","Foolbox: A Python toolbox to benchmark the robustness of machine learning models",2017,"","","","",35,"2022-07-13 09:22:41","","","","",,,,,312,62.40,104,3,5,"Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at https://github.com/bethgelab/foolbox. The most up-to-date documentation can be found at http://foolbox.readthedocs.io. In 2013, Szegedy et al. demonstrated that minimal perturbations, often almost imperceptible to humans, can have devastating effects on machine predictions. These so-called adversarial perturbations thus demonstrate a striking difference between human and machine perception. As a result, adversarial perturbations have been subject to many Equal contribution Centre for Integrative Neuroscience, University of Tübingen, Germany Bernstein Center for Computational Neuroscience, Tübingen, Germany International Max Planck Research School for Intelligent Systems, Tübingen, Germany Max Planck Institute for Biological Cybernetics, Tübingen, Germany Institute for Theoretical Physics, University of Tübingen, Germany. Correspondence to: Jonas Rauber <jonas.rauber@bethgelab.org>. Reliable Machine Learning in the Wild Workshop, 34 th International Conference on Machine Learning, Sydney, Australia, 2017. studies concerning the generation of such perturbations and strategies to protect machine learning models such as deep neural networks against them. A practical definition of the robustness R of a model, first used by Szegedy et al. (2013), is the average size of the minimum adversarial perturbation ρ(x) across many samples x, R = 〈ρ(x)〉 x where (1) ρ(x) = min δ d(x,x+ δ) s.t. x+ δ is adversarial (2) and d(·) is some distance measure. Unfortunately, finding the global minimum adversarial perturbation is close to impossible in any practical setting, and we thus employ heuristic attacks to find a suitable approximation. Such heuristics, however, can fail, in which case we could easily be mislead to believe that a model is robust (Brendel & Bethge, 2017). Our best strategy is thus to employ as many attacks as possible, and to use the minimal perturbation found across all attacks as an approximation to the true global minimum. At the moment, however, such a strategy is severely obstructed by two problems: first, the code for most known attack methods is either not available at all, or only available for one particular deep learning framework. Second, implementations of the same attack often differ in many details and are thus not directly comparable. Foolbox improves upon the existing Python package cleverhans by Papernot et al. (2016b) in three important aspects: 1. It interfaces with most popular machine learning frameworks such as PyTorch, Keras, TensorFlow, Theano, Lasagne and MXNet and provides a straight forward way to add support for other frameworks, 2. it provides reference implementations for more than 15 adversarial attacks with a simple and consistent API, and 3. it supports many different criteria for adversarial examples, including custom ones. This technical report is structured as follows: In section 1 we provide an overview over Foolbox and demonstrate Foolbox: A Python toolbox to benchmark the robustness of machine learning models how to benchmark a model and report the result. In section 2 we describe the adversarial attack methods that are implemented in Foolbox and explain the internal hyperparameter tuning.","",""
0,"Eric Gioe, Mohammed Raihan Uddin, Jong-Hoon Kim, Xiaolin Chen","Deterministic Lateral Displacement (DLD) Analysis Tool Utilizing Machine Learning towards High-Throughput Separation",2022,"","","","",36,"2022-07-13 09:22:41","","10.3390/mi13050661","","",,,,,0,0.00,0,4,1,"Deterministic lateral displacement (DLD) is a microfluidic method for the continuous separation of particles based on their size. There is growing interest in using DLD for harvesting circulating tumor cells from blood for further assays due to its low cost and robustness. While DLD is a powerful tool and development of high-throughput DLD separation devices holds great promise in cancer diagnostics and therapeutics, much of the experimental data analysis in DLD research still relies on error-prone and time-consuming manual processes. There is a strong need to automate data analysis in microfluidic devices to reduce human errors and the manual processing time. In this work, a reliable particle detection method is developed as the basis for the DLD separation analysis. Python and its available packages are used for machine vision techniques, along with existing identification methods and machine learning models. Three machine learning techniques are implemented and compared in the determination of the DLD separation mode. The program provides a significant reduction in video analysis time in DLD separation, achieving an overall particle detection accuracy of 97.86% with an average computation time of 25.274 s.","",""
5,"Rohan Chandra, Aniket Bera, D. Manocha","Using Graph-Theoretic Machine Learning to Predict Human Driver Behavior",2021,"","","","",37,"2022-07-13 09:22:41","","10.1109/tits.2021.3130218","","",,,,,5,5.00,2,3,1,"Studies have shown that autonomous vehicles (AVs) behave conservatively in a traffic environment composed of human drivers and do not adapt to local conditions and socio-cultural norms. It is known that socially aware AVs can be designed if there exists a mechanism to understand the behaviors of human drivers. We present an approach that leverages machine learning to predict, the behaviors of human drivers. This is similar to how humans implicitly interpret the behaviors of drivers on the road, by only observing the trajectories of their vehicles. We use graph-theoretic tools to extract driver behavior features from the trajectories and machine learning to obtain a computational mapping between the extracted trajectory of a vehicle in traffic and the driver behaviors. Compared to prior approaches in this domain, we prove that our method is robust, general, and extendable to broad-ranging applications such as autonomous navigation. We evaluate our approach on real-world traffic datasets captured in the U.S., India, China, and Singapore, as well as in simulation.","",""
0,"Sasmitha Dasanayaka, S. Silva, V. Shantha, D. Meedeniya, Thanuja D. Ambegoda","Interpretable Machine Learning for Brain Tumor Analysis Using MRI",2022,"","","","",38,"2022-07-13 09:22:41","","10.1109/ICARC54489.2022.9754131","","",,,,,0,0.00,0,5,1,"A brain tumor is a potentially fatal growth of cells in the central nervous system that can be categorized as benign or malignant. Advancements in deep learning in the recent past and the availability of high computational power have been influencing the automation of diagnosing brain tumors. DenseNet and U-Net are considered state of the art deep learning models for classification and segmentation of MRIs respectively. Despite the progress of deep learning in diagnosing using medical images, generic convolutional neural networks are still not fully adopted in clinical settings as they lack robustness and reliability. Moreover, such black-box models don’t offer a human interpretable justification as to why certain classification decisions are made, which makes them less preferable for medical diagnostics. Brain tumor segmentation and classification using deep learning techniques has been a popular research area in the last few decades but still, there are only a few models that are interpretable. In this paper, we have proposed an interpretable deep learning model which is more human understandable than existing black-box models, designed based on U-Net and DenseNet to segment and classify brain tumors using MRI. In our proposed model, we generate a heat map highlighting the contribution of each region of the input to the classification output and have validated the system using the MICCAI 2020 Brain Tumor dataset.","",""
9,"N. Furian, M. O'Sullivan, C. Walker, E. Çela","A machine learning-based branch and price algorithm for a sampled vehicle routing problem",2021,"","","","",39,"2022-07-13 09:22:41","","10.1007/s00291-020-00615-8","","",,,,,9,9.00,2,4,1,"","",""
4,"David S. Watson","Interpretable machine learning for genomics",2021,"","","","",40,"2022-07-13 09:22:41","","10.1007/s00439-021-02387-9","","",,,,,4,4.00,4,1,1,"","",""
1,"Owain Evans, W. Saunders, Andreas Stuhlmüller","Machine Learning Projects for Iterated Distillation and Amplification",2019,"","","","",41,"2022-07-13 09:22:41","","","","",,,,,1,0.33,0,3,3,"Iterated Distillation and Amplification (IDA) is a framework for training ML models. IDA is related to existing frameworks like imitation learning and reinforcement learning, but it aims to solve tasks for which humans cannot construct a suitable reward function or solve directly. This document reviews IDA and proposes three projects that explore aspects of IDA. Project 1 applies IDA to problems in highschool mathematics and investigates whether learning to decompose problems can improve performance over supervised learning. Project 2 applies IDA to neural program interpretation, where neural nets are trained on the internal behavior (execution traces) of traditional computer programs. Project 3 investigates whether adaptive computation time (varying compute at inference time as a function of the input) can improve the robustness and efficiency of IDA. Our goal in outlining these projects is to generate discussion and encourage research on IDA. We are not (as of June 2019) working on these projects, but we are interested in collaboration.","",""
6,"A. Soni, Dharamvir Dharmacharya, A. Pal, V. Srivastava, R. Shaw, Ankush Ghosh","Design of a Machine Learning-Based Self-driving Car",2021,"","","","",42,"2022-07-13 09:22:41","","10.1007/978-981-16-0598-7_11","","",,,,,6,6.00,1,6,1,"","",""
11,"S. Chaudhury, E. Duncan, Tanmaya Atre, S. Dutta, Michele D Spring, W. Leitner, E. Bergmann-Leitner","Combining immunoprofiling with machine learning to assess the effects of adjuvant formulation on human vaccine-induced immunity",2019,"","","","",43,"2022-07-13 09:22:41","","10.1080/21645515.2019.1654807","","",,,,,11,3.67,2,7,3,"ABSTRACT Adjuvants produce complex, but often subtle, effects on vaccine-induced immune responses that, nonetheless, play a critical role in vaccine efficacy. In-depth profiling of vaccine-induced cytokine, cellular, and antibody responses (“immunoprofiling”) combined with machine-learning holds the promise of identifying adjuvant-specific immune response characteristics that can guide rational adjuvant selection. Here, we profiled human immune responses induced by vaccines adjuvanted with two similar, clinically relevant adjuvants, AS01B and AS02A, and identified key distinguishing characteristics, or immune signatures, they imprint on vaccine-induced immunity. Samples for this side-by-side comparison were from malaria-naïve individuals who had received a recombinant malaria subunit vaccine (AMA-1) that targets the pre-erythrocytic stage of the parasite. Both adjuvant formulations contain the same immunostimulatory components, QS21 and MPL, thus this study reveals the subtle impact that adjuvant formulation has on immunogenicity. Adjuvant-mediated immune signatures were established through a two-step approach: First, we generated a broad immunoprofile (serological, functional and cellular characterization of vaccine-induced responses). Second, we integrated the immunoprofiling data and identify what combination of immune features was most clearly able to distinguish vaccine-induced responses by adjuvant using machine learning. The computational analysis revealed statistically significant differences in cellular and antibody responses between cohorts and identified a combination of immune features that was able to distinguish subjects by adjuvant with 71% accuracy. Moreover, the in-depth characterization demonstrated an unexpected induction of CD8+ T cells by the recombinant subunit vaccine, which is rare and highly relevant for future vaccine design.","",""
11,"P. Poudel, A. Illanes, E. Ataide, N. Esmaeili, Sathish Balakrishnan, M. Friebe","Thyroid Ultrasound Texture Classification Using Autoregressive Features in Conjunction With Machine Learning Approaches",2019,"","","","",44,"2022-07-13 09:22:41","","10.1109/ACCESS.2019.2923547","","",,,,,11,3.67,2,6,3,"The thyroid is one of the largest endocrine glands in the human body, which is involved in several body mechanisms like controlling protein synthesis, use of energy sources, and controlling the body’s sensitivity to other hormones. Thyroid segmentation and volume reconstruction are hence essential to diagnose thyroid related diseases as most of these diseases involve a change in the shape and size of the thyroid over time. Classification of thyroid texture is the first step toward the segmentation of the thyroid. The classification of texture in thyroid Ultrasound (US) images is not an easy task as it suffers from low image contrast, presence of speckle noise, and non-homogeneous texture distribution inside the thyroid region. Hence, a robust algorithmic approach is required to accurately classify thyroid texture. In this paper, we propose three machine learning based approaches: Support Vector Machine; Artificial Neural Network; and Random Forest Classifier to classify thyroid texture. The computation of features for training these classifiers is based on a novel approach recently proposed by our team, where autoregressive modeling was applied on a signal version of the 2D thyroid US images to compute 30 spectral energy-based features for classifying the thyroid and non-thyroid textures. Our approach differs from the methods proposed in the literature as they use image-based features to characterize thyroid tissues. We obtained an accuracy of around 90% with all the three methods.","",""
3,"H. Castañé, G. Baiges-Gaya, A. Hernández-Aguilera, E. Rodríguez-Tomàs, S. Fernández-Arroyo, P. Herrero, A. Delpino-Rius, N. Canela, J. Menéndez, J. Camps, J. Joven","Coupling Machine Learning and Lipidomics as a Tool to Investigate Metabolic Dysfunction-Associated Fatty Liver Disease. A General Overview",2021,"","","","",45,"2022-07-13 09:22:41","","10.3390/biom11030473","","",,,,,3,3.00,0,11,1,"Hepatic biopsy is the gold standard for staging nonalcoholic fatty liver disease (NAFLD). Unfortunately, accessing the liver is invasive, requires a multidisciplinary team and is too expensive to be conducted on large segments of the population. NAFLD starts quietly and can progress until liver damage is irreversible. Given this complex situation, the search for noninvasive alternatives is clinically important. A hallmark of NAFLD progression is the dysregulation in lipid metabolism. In this context, recent advances in the area of machine learning have increased the interest in evaluating whether multi-omics data analysis performed on peripheral blood can enhance human interpretation. In the present review, we show how the use of machine learning can identify sets of lipids as predictive biomarkers of NAFLD progression. This approach could potentially help clinicians to improve the diagnosis accuracy and predict the future risk of the disease. While NAFLD has no effective treatment yet, the key to slowing the progression of the disease may lie in predictive robust biomarkers. Hence, to detect this disease as soon as possible, the use of computational science can help us to make a more accurate and reliable diagnosis. We aimed to provide a general overview for all readers interested in implementing these methods.","",""
1,"Nikolas D Schnellbächer, Haissam Ragab, H. Nickisch, T. Wissel, C. Spink, G. Lund, M. Grass","Machine-learning-based clinical plaque detection using a synthetic plaque lesion model for coronary CTA",2021,"","","","",46,"2022-07-13 09:22:41","","10.1117/12.2582195","","",,,,,1,1.00,0,7,1,"Coronary computed tomography angiography (coronary CTA) is a robust and well-established non-invasive diagnostic tool to detect and assess coronary artery disease (CAD). The accurate detection, quantification and characterization of the coronary plaque burden has become an important part of this imaging modality. The quality and performance of modern machine-learning-based data-driven learning approaches is often impacted by either insufficient or inconsistently-labeled training data and is further subject to additional bias from human annotators. To address these shortcomings for coronary plaque characterization, we have developed a synthetic lesion generating framework for CTA applications, which can produce accurate and high-quality labeled training data for data-driven learning approaches. This approach can help to ease the manual annotation burden, which is often the limiting factor in data-driven learning algorithms and instead provides reliable ground truth data for modern deep learning approaches. Furthermore, this framework can easily be used to create custom tailored training data that can be used for pre- or post-training steps of already existing machine learning approaches for CTA applications. We tested this data generation framework by inserting synthetic lesions in 11 clinical CTA scans of healthy patients resulting in a data set of ~7000 annotated 2D slices. With this data we performed several plaque detection experiments using a data-driven machine learning approach with a neural encoder architecture. In this plaque classification task we first demonstrate that the synthetic lesion generation module can consistently perform well in recognizing unseen synthetic test data with an overall classification accuracy of 93%. Next we apply the synthetic lesion framework in a transfer learning experiment, where we demonstrate the feasibility to learn to classify real clinical plaque lesions with a purely synthetic model (overall classification accuracy 84%) that never saw real clinical lesions during model training. Second, we show that using synthetically data for pre-training with a subsequent training on clinical data can enhance the overall classification accuracy (from 91% to 92%) while strongly increasing the true positive count. We conclude that the synthetic plaque lesions model faithfully covers many important image characteristics of real plaque lesions in coronary CTA imaging and can thus help reduce the annotation burden for data-driven predictive vascular systems in this domain. This allows the creation of exhaustively annotated and site-specific customizable training data with a computationally fast forward model.","",""
1,"Pumrapee Poomka, Nittaya Kerdprasop, Kittisak Kerdprasop","Machine Learning Versus Deep Learning Performances on the Sentiment Analysis of Product Reviews",2021,"","","","",47,"2022-07-13 09:22:41","","10.18178/IJMLC.2021.11.2.1021","","",,,,,1,1.00,0,3,1,"At this current digital era, business platforms have been drastically shifted toward online stores on internet. With the internet-based platform, customers can order goods easily using their smart phones and get delivery at their place without going to the shopping mall. However, the drawback of this business platform is that customers do not really know about the quality of the products they ordered. Therefore, such platform service often provides the review section to let previous customers leave a review about the received product. The reviews are a good source to analyze customer's satisfaction. Business owners can assess review trend as either positive or negative based on a feedback score that customers had given, but it takes too much time for human to analyze this data. In this research, we develop computational models using machine learning techniques to classify product reviews as positive or negative based on the sentiment analysis. In our experiments, we use the book review data from amazon.com to develop the models. For a machine learning based strategy, the data had been transformed with the bag of word technique before developing models using logistic regression, naïve bayes, support vector machine, and neural network algorithms. For a deep learning strategy, the word embedding is a technique that we used to transform data before applying the long short-term memory and gated recurrent unit techniques. On comparing performance of machine learning against deep learning models, we compare results from the two methods with both the preprocessed dataset and the non-preprocessed dataset. The result is that the bag of words with neural network outperforms other techniques on both non-preprocess and preprocess datasets.","",""
1,"Quang-Vinh Dang","Improving the performance of the intrusion detection systems by the machine learning explainability",2021,"","","","",48,"2022-07-13 09:22:41","","10.1108/ijwis-03-2021-0022","","",,,,,1,1.00,1,1,1," Purpose This study aims to explain the state-of-the-art machine learning models that are used in the intrusion detection problem for human-being understandable and study the relationship between the explainability and the performance of the models.   Design/methodology/approach The authors study a recent intrusion data set collected from real-world scenarios and use state-of-the-art machine learning algorithms to detect the intrusion. The authors apply several novel techniques to explain the models, then evaluate manually the explanation. The authors then compare the performance of model post- and prior-explainability-based feature selection.   Findings The authors confirm our hypothesis above and claim that by forcing the explainability, the model becomes more robust, requires less computational power but achieves a better predictive performance.   Originality/value The authors draw our conclusions based on their own research and experimental works. ","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",49,"2022-07-13 09:22:41","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
879,"Tyler Martin","Interpretable Machine Learning",2019,"","","","",50,"2022-07-13 09:22:41","","","","",,,,,879,293.00,879,1,3,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.","",""
0,"Prasad K. Hajare, Sadia Kamal, S. Krishnan, A. Bagavathi","A Machine Learning Pipeline to Examine Political Bias with Congressional Speeches",2021,"","","","",51,"2022-07-13 09:22:41","","10.1109/ICMLA52953.2021.00044","","",,,,,0,0.00,0,4,1,"Computational methods to model political bias in social media involve several challenges due to heterogeneity, high-dimensionality, multiple modalities, and the scale of the data. Most of the current political bias detection methods rely heavily on the manually-labeled ground-truth data for the underlying political bias prediction tasks. Such methods are human-intensive labeling, labels related to only a specific problem, and the inability to determine the near future bias state of a social media conversation. In this work, we address such problems and give machine learning approaches to study political bias in two ideologically diverse social media forums: Gab and Twitter without the availability of human-annotated data. We propose a method to exploit the features of entities on transcripts collected from political speeches in US congress to label political bias of social media posts automatically without any human intervention. With existing machine learning algorithms we achieve the highest accuracy of 70.5% and 65.1% to predict posts on Twitter and Gab data respectively. We also present a machine learning approach that combines features from cascades and text to forecast cascade’s political bias with an accuracy of about 85%.","",""
0,"Aayushi Vishnoi, Rati Sharma","A machine learning based analysis to probe the relationship between odorant structure and olfactory behaviour in C. elegans",2021,"","","","",52,"2022-07-13 09:22:41","","10.1101/2021.07.26.453815","","",,,,,0,0.00,0,2,1,"The chemical basis of smell remains an unsolved problem, with ongoing studies mapping perceptual descriptor data from human participants to the chemical structures using computational methods. These approaches are, however, limited by linguistic capabilities and inter-individual differences in participants. We use olfactory behaviour data from the nematode C. elegans, which has isogenic populations in a laboratory setting, and employ machine learning approaches for a binary classification task predicting whether or not the worm will be attracted to a given monomolecular odorant. Among others, we use architectures based on Natural Language Processing methods on the SMILES representation of chemicals for molecular descriptor generation and show that machine learning algorithms trained on the descriptors give robust prediction results. We further show, by data augmentation, that increasing the number of samples increases the accuracy of the models. From this detailed analysis, we are able to achieve accuracies comparable to that in human studies and infer that there exists a non trivial relationship between the features of chemical structures and the nematode’s behaviour.","",""
3,"Yan Xu, Yu-Hang Zhang, Jiarui Li, Xiaoyong Pan, Tao Huang, Yu-Dong Cai","New computational tool based on machine-learning algorithms for the identification of rhinovirus infection-related genes.",2019,"","","","",53,"2022-07-13 09:22:41","","10.2174/1386207322666191129114741","","",,,,,3,1.00,1,6,3,"BACKGROUND Human rhinovirus has different identified serotypes and is the most common cause of cold in humans. To date, many genes have been discovered to be related to rhinovirus infection. However, the pathogenic mechanism of rhinovirus is difficult to elucidate through experimental approaches due to the high cost and consuming time.   METHOD AND RESULTS In this study, we presented a novel approach that relies on machine-learning algorithms and identified two genes OTOF and SOCS1. The expression levels of these genes in the blood samples can be used to accurately distinguish virus-infected and non-infected individuals.   CONCLUSION Our findings suggest the crucial roles of these two genes in rhinovirus infection and the robustness of the computational tool in dissecting pathogenic mechanisms.","",""
6,"Subash Poudyal, Z. Akhtar, D. Dasgupta, Kishor Datta Gupta","Malware Analytics: Review of Data Mining, Machine Learning and Big Data Perspectives",2019,"","","","",54,"2022-07-13 09:22:41","","10.1109/SSCI44817.2019.9002996","","",,,,,6,2.00,2,4,3,"Recent advances in cyber technologies have made human life’s easier, but it may lead to a heavy cost in terms of economic, psychological or reputation damage. For instance, these damages may be caused by variants of malware propagated in a hidden and mostly untraceable way. Malware analytics deals with the approaches and techniques utilized to generate the distinguishing characteristics of the malware for robust cyber defenses. This paper aims at presenting the current status of the malware research, challenges, and methods used to overcome those challenges using data mining, machine learning and big data perspectives. We have considered these three perspectives because of its extensive computation value, mostly fused to solve a wide range of problems from security to medical, finance and industry. These domains as an independent technique and their interrelationships depend on the nature of the dataset considered. We have also proposed a framework to overcome the challenges and open issues prevalent in malware analytics. It is hoped that this paper with the simplified presentation of the most vital approaches of malware analytics will help the inspiring researcher or a newbie in the security field to explore more as well as budding engineers to choose malware analysis as their field of study. Specifically, analysis of state-of-the-art approaches with evaluation, pros and cons discussion and the current challenges and future directions will empower all the malware enthusiasts.","",""
2,"G. Carvalho, B. Cabral, Vasco Pereira, Jorge Bernardino","A Case for Machine Learning in Edge-Oriented Computing to Enhance Mobility as a Service",2019,"","","","",55,"2022-07-13 09:22:41","","10.1109/DCOSS.2019.00101","","",,,,,2,0.67,1,4,3,"The study of human mobility tries to understand human flows and synergies with the geographical environment. Mobility as a Service (MaaS) is a new mobility concept that promises to revolutionize commuting by merging public and private transport providers around a common platform that travelers will use as a service, thus providing new research opportunities for human mobility. One of MaaS main offerings is the ability to calculate both routes and commuting strategies based on the availability of transports and specific user constraints. In this work, we discuss how Edge-Oriented Computing (EOC) and Machine Learning (ML) can contribute to extending the reach of MaaS in the upcoming years. EOC enables technologies to perform computation at the edge of the network, reducing latency and communication overheads, which 5G technologies are committed to further diminish, thus benefitting the proliferation of MaaS. Also, ML techniques are one of the most robust approaches for planning routes and predicting future movements. Finally, we present open research topics that will promote the attractiveness of MaaS.","",""
4,"R. Zicari, J. Brusseau, S. Blomberg, H. Christensen, M. Coffee, M. B. Ganapini, S. Gerke, T. Gilbert, Eleanore Hickman, E. Hildt, Sune Holm, U. Kühne, V. Madai, W. Osika, Andy Spezzatti, Eberhard Schnebel, Jesmin Jahan Tithi, Dennis Vetter, Magnus Westerlund, Reneé C. Wurth, J. Amann, Vegard Antun, Valentina Beretta, Frédérick Bruneault, Erik Campano, Boris Düdder, Alessio Gallucci, Emmanuel R. Goffi, C. Haase, Thilo Hagendorff, P. Kringen, Florian Möslein, D. Ottenheimer, M. Ozols, L. Palazzani, M. Petrin, Karin Tafur, J. Tørresen, H. Volland, G. Kararigas","On Assessing Trustworthy AI in Healthcare. Machine Learning as a Supportive Tool to Recognize Cardiac Arrest in Emergency Calls",2021,"","","","",56,"2022-07-13 09:22:41","","10.3389/fhumd.2021.673104","","",,,,,4,4.00,0,40,1,"Artificial Intelligence (AI) has the potential to greatly improve the delivery of healthcare and other services that advance population health and wellbeing. However, the use of AI in healthcare also brings potential risks that may cause unintended harm. To guide future developments in AI, the High-Level Expert Group on AI set up by the European Commission (EC), recently published ethics guidelines for what it terms “trustworthy” AI. These guidelines are aimed at a variety of stakeholders, especially guiding practitioners toward more ethical and more robust applications of AI. In line with efforts of the EC, AI ethics scholarship focuses increasingly on converting abstract principles into actionable recommendations. However, the interpretation, relevance, and implementation of trustworthy AI depend on the domain and the context in which the AI system is used. The main contribution of this paper is to demonstrate how to use the general AI HLEG trustworthy AI guidelines in practice in the healthcare domain. To this end, we present a best practice of assessing the use of machine learning as a supportive tool to recognize cardiac arrest in emergency calls. The AI system under assessment is currently in use in the city of Copenhagen in Denmark. The assessment is accomplished by an independent team composed of philosophers, policy makers, social scientists, technical, legal, and medical experts. By leveraging an interdisciplinary team, we aim to expose the complex trade-offs and the necessity for such thorough human review when tackling socio-technical applications of AI in healthcare. For the assessment, we use a process to assess trustworthy AI, called 1 Z-Inspection® to identify specific challenges and potential ethical trade-offs when we consider AI in practice.","",""
13,"I. Horenko","On a Scalable Entropic Breaching of the Overfitting Barrier for Small Data Problems in Machine Learning",2020,"","","","",57,"2022-07-13 09:22:41","","10.1162/neco_a_01296","","",,,,,13,6.50,13,1,2,"Overfitting and treatment of small data are among the most challenging problems in machine learning (ML), when a relatively small data statistics size T is not enough to provide a robust ML fit for a relatively large data feature dimension D. Deploying a massively parallel ML analysis of generic classification problems for different D and T, we demonstrate the existence of statistically significant linear overfitting barriers for common ML methods. The results reveal that for a robust classification of bioinformatics-motivated generic problems with the long short-term memory deep learning classifier (LSTM), one needs in the best case a statistics T that is at least 13.8 times larger than the feature dimension D. We show that this overfitting barrier can be breached at a 10-12 fraction of the computational cost by means of the entropy-optimal scalable probabilistic approximations algorithm (eSPA), performing a joint solution of the entropy-optimal Bayesian network inference and feature space segmentation problems. Application of eSPA to experimental single cell RNA sequencing data exhibits a 30-fold classification performance boost when compared to standard bioinformatics tools and a 7-fold boost when compared to the deep learning LSTM classifier.","",""
7,"Eric Minor, Stian D. Howard, Adam A S Green, M. Glaser, C. Park, N. Clark","End-to-end machine learning for experimental physics: using simulated data to train a neural network for object detection in video microscopy.",2019,"","","","",58,"2022-07-13 09:22:41","","10.1039/c9sm01979k","","",,,,,7,2.33,1,6,3,"We demonstrate a method for training a convolutional neural network with simulated images for usage on real-world experimental data. Modern machine learning methods require large, robust training data sets to generate accurate predictions. Generating these large training sets requires a significant up-front time investment that is often impractical for small-scale applications. Here we demonstrate a 'full-stack' computational solution, where the training data set is generated on-the-fly using a noise injection process to produce simulated data characteristic of the experimental system. We demonstrate the power of this full-stack approach by applying it to the study of topological defect annihilation in systems of liquid crystal freely-suspended films. This specific experimental system requires accurate observations of both the spatial distribution of the defects and the total number of defects, making it an ideal system for testing the robustness of the trained network. The fully trained network was found to be comparable in accuracy to human hand-annotation, with four-orders of magnitude improvement in time efficiency.","",""
0,"Michael Mishkin, M. Lipasti","Session 1 : Data Efficient and Computationally Efficient Machine Learning",2019,"","","","",59,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,2,3,"Adversarial attacks attempt to confound machine learning systems. By changing the input to a classifier slightly – for instance, tweaking image pixels – the output classification can be manipulated. Throughout the literature on attacking image classifiers, the visibility of attacks to a human observer (who might become suspicious, which we wish to avoid) is typically gauged (without adequate justification) by a p-norm of the difference between the original and modified image. Via behavioral experiments, we show that human perception of image modifications is not welldescribed by any p-norm, nor several alternative measures. This has significant impact on adversarial ML research; the robustness of attacks to human inspection relies on an accurate understanding of what humans will and will not see as ""tampering"". Title: Adversarial Learning PoC: Jerry Zhu Affiliation: UW-Madison Title: Critic After Convergence PoC: Walter Bennette Affiliation: AFRL/RIEA Email: walter.bennette.1@us.af.mil Abstract: A Generative Adversarial Network (GAN) is a neural network training regimen composed of a generative model and a critic model. Through a back and forth adversarial competition, the critic model guides the generative model to produce samples that mimic a chosen distribution. At convergence (or when the critic can no longer differentiate between real and generated samples), the desired output of a GAN is typically a generative model that is capable of producing realistic samples from that distribution. In this work we investigate the performance of the critic model after the GAN has converged. Specifically, we perform experiments on the MNIST dataset to see if a critic model can be updated outside of the adversarial regimen to again discriminate between real and generated digits. Title: Using Machine Learning for Cyber Agility and Network Intrusion Detection PoC: Nandi O. Leslie Affiliation: Army Research Laboratory Email: Nandi.O.Leslie.Ctr@mail.mil Abstract: Mission-critical cyber-physical systems (CPS), such as Internet of Things (IoT), are increasingly necessary at the tactical edge and face mounting risks to cyberattacks adversaries ranging from nonstate actors to peers. Using semi-supervised learning, we propose an anomaly-based network intrusion detection system (NIDS) to detect and classify anomalous and/or malicious traffic. With this proposed machine learning approach, we detect botnet traffic and distinguish it from the normal and background traffic in the IPv4 flow datasets. We evaluate the prediction performance results for the flow-based NIDS algorithms. We show an improvement in detection accuracy and reduction in error rates, when compared with signature-based NIDS and previous studies. In the future, we hope to combine our","",""
114,"Andreas Holzinger, M. Plass, M. Kickmeier-Rust, K. Holzinger, G. Crişan, C. Pintea, V. Palade","Interactive machine learning: experimental evidence for the human in the algorithmic loop",2018,"","","","",60,"2022-07-13 09:22:41","","10.1007/s10489-018-1361-5","","",,,,,114,28.50,16,7,4,"","",""
0,"David Zabala-Blanco, M. Mora, César A. Azurdia-Meza, Ali Dehghan Firoozabadi, Palacios Játiva Palacios Játiva, Samuel Montejo-Sánchez","Multilayer Extreme Learning Machine as Equalizer in OFDM-based Radio-over-fiber Systems",2021,"","","","",61,"2022-07-13 09:22:41","","10.1109/TLA.2021.9477280","","",,,,,0,0.00,0,6,1,"Mobile/wireless networks aim to support diverse services with numerous and sophisticated requirements, such as energy efficiency, spectral efficiency, negligible latency, robustness against time and frequency selective channels, low hardware complexity, among others. From the central station to the base stations, radio-over-fiber orthogonal frequency division multiplexing (RoF-OFDM) schemes with direct-detection are then implemented. Unfortunately, laser phase noise, chromatic fiber dispersion, and carrier frequency offset impair the orthogonality of the subcarriers; hence, deteriorating the performance of the RoF-OFDM system. In order to take all the processing tasks to the cognitive level (the last goal in the telecommunication industry), various extreme learning machines (ELMs), composed by only a single hidden layer, have been recently adopted as equalizers. The reason behind this trend comes from the lower computational complexity, higher detection accuracy, and minimum human intervention of the ELM algorithms. In this article, we introduce a multilayer ELM-based receiver for RoF schemes transmitting phase-correlated OFDM signals affected by phase and frequency errors. Results report that by appropriately setting the hyper-parameters of the multilayer ELMs, the ELM with 3 hidden layers outperforms most of the ELMs reported in the literature (the ELM with 2 hidden layers, original ELM, regularized ELM, and 2 fully-independent ELMs defined in the real domain), as well as the benchmark pilot-assisted equalizer in terms of bit error rate. Nevertheless, this benefit comes with excessive computational cost. Finally, we show that the fully-complex ELM is still the best equalizer taking into account several key metrics.","",""
2,"T. Thung, Murray E. White, Wei Dai, J. Wilksch, R. Bamert, A. Rocker, C. Stubenrauch, Daniel Williams, Cheng Huang, Ralf Schittelhelm, J. Barr, E. Jameson, S. McGowan, Yanju Zhang, Jiawei Wang, R. Dunstan, T. Lithgow","Component Parts of Bacteriophage Virions Accurately Defined by a Machine-Learning Approach Built on Evolutionary Features",2021,"","","","",62,"2022-07-13 09:22:41","","10.1128/mSystems.00242-21","","",,,,,2,2.00,0,17,1,"In response to the global problem of antimicrobial resistance, there are moves to use bacteriophages (phages) as therapeutic agents. Selecting which phages will be effective therapeutics relies on interpreting features contributing to shelf-life and applicability to diagnosed infections. ABSTRACT Antimicrobial resistance (AMR) continues to evolve as a major threat to human health, and new strategies are required for the treatment of AMR infections. Bacteriophages (phages) that kill bacterial pathogens are being identified for use in phage therapies, with the intention to apply these bactericidal viruses directly into the infection sites in bespoke phage cocktails. Despite the great unsampled phage diversity for this purpose, an issue hampering the roll out of phage therapy is the poor quality annotation of many of the phage genomes, particularly for those from infrequently sampled environmental sources. We developed a computational tool called STEP3 to use the “evolutionary features” that can be recognized in genome sequences of diverse phages. These features, when integrated into an ensemble framework, achieved a stable and robust prediction performance when benchmarked against other prediction tools using phages from diverse sources. Validation of the prediction accuracy of STEP3 was conducted with high-resolution mass spectrometry analysis of two novel phages, isolated from a watercourse in the Southern Hemisphere. STEP3 provides a robust computational approach to distinguish specific and universal features in phages to improve the quality of phage cocktails and is available for use at http://step3.erc.monash.edu/. IMPORTANCE In response to the global problem of antimicrobial resistance, there are moves to use bacteriophages (phages) as therapeutic agents. Selecting which phages will be effective therapeutics relies on interpreting features contributing to shelf-life and applicability to diagnosed infections. However, the protein components of the phage virions that dictate these properties vary so much in sequence that best estimates suggest failure to recognize up to 90% of them. We have utilized this diversity in evolutionary features as an advantage, to apply machine learning for prediction accuracy for diverse components in phage virions. We benchmark this new tool showing the accurate recognition and evaluation of phage component parts using genome sequence data of phages from undersampled environments, where the richest diversity of phage still lies.","",""
1,"T. Thung, Murray E. White, Wei Dai, J. Wilksch, R. Bamert, A. Rocker, C. Stubenrauch, Daniel Williams, Cheng Huang, Ralf Schittelhelm, J. Barr, E. Jameson, S. McGowan, Yanju Zhang, Jiawei Wang, R. Dunstan, T. Lithgow","The component parts of bacteriophage virions accurately defined by a machine-learning approach built on evolutionary features",2021,"","","","",63,"2022-07-13 09:22:41","","10.1101/2021.02.28.433281","","",,,,,1,1.00,0,17,1,"Antimicrobial resistance (AMR) continues to evolve as a major threat to human health and new strategies are required for the treatment of AMR infections. Bacteriophages (phages) that kill bacterial pathogens are being identified for use in phage therapies, with the intention to apply these bactericidal viruses directly into the infection sites in bespoke phage cocktails. Despite the great unsampled phage diversity for this purpose, an issue hampering the roll out of phage therapy is the poor quality annotation of many of the phage genomes, particularly for those from infrequently sampled environmental sources. We developed a computational tool called STEP3 to use the “evolutionary features” that can be recognized in genome sequences of diverse phages. These features, when integrated into an ensemble framework, achieved a stable and robust prediction performance when benchmarked against other prediction tools using phages from diverse sources. Validation of the prediction accuracy of STEP3 was conducted with high-resolution mass spectrometry analysis of two novel phages, isolated from a watercourse in the Southern Hemisphere. STEP3 provides a robust computational approach to distinguish specific and universal features in phages to improve the quality of phage cocktails, and is available for use at http://step3.erc.monash.edu/. IMPORTANCE In response to the global problem of antimicrobial resistance there are moves to use bacteriophages (phages) as therapeutic agents. Selecting which phages will be effective therapeutics relies on interpreting features contributing to shelf-life and applicability to diagnosed infections. However, the protein components of the phage virions that dictate these properties vary so much in sequence that best estimates suggest failure to recognize up to 90% of them. We have utilised this diversity in evolutionary features as an advantage, to apply machine learning for prediction accuracy for diverse components in phage virions. We benchmark this new tool showing the accurate recognition and evaluation of phage components parts using genome sequence data of phages from under-sampled environments, where the richest diversity of phage still lies.","",""
0,"Byoung-Tak Zhang","AFRL-AFOSR-JP-TR-2016-0014 Bio-Inspired Human-Level Machine Learning",2016,"","","","",64,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,1,6,"How can brain computation be so fast, flexible, and robust? What kinds of representational and organizational principles facilitate the biological brain to learn so efficiently and flexibly on the sub-second time scale and so reliably on the continuous lifetime scale? To understand these principles, we aimed to develop human-level machine learning technology that is fast, flexible, and reliable to adapt to a continuously changing, dynamic environment. Based on dynamic “neural” populations (neural assemblies), we constructed a “human-like” machine learning model and implement this model in “molecular” populations (molecular assemblies) using in vitro DNA computing. In the first year, we developed the dynamic hypernetwork models of neural populations in the sequential Bayesian framework for lifelong learning. In the second year, we extended it to the molecular dynamic hypernetwork model, and designed in vitro experimental protocols to implement online language learning from a stream of text corpus. In the third year, we demonstrated the use of molecular dynamic hypernetworks for multimodal visuo-linguistic concept learning from a long stream of video data and their extensions to high-level cognitive functions such as anagram solving problem. We expect that the bio-inspired human-level machine learning combined with molecular-computing implementation can offer an interesting, novel paradigm to address for flexible and reliable computing. Introduction: One of the main challenges in artificial intelligence is to develop human-like machine learning technology that is fast, flexible, and reliable to adapt to a continuously changing, dynamic environment. Converging neuroanatomical and neurophysiological evidence shows that the brain uses distributed, overlapping representations based on sparse population codes that are coordinated dynamically (Averbeck et al., 2006; Pouget et al., 2000; von der Malsburg et al., 2010). We hypothesize that brain computation exploits the huge degrees of freedom generated by a large number of memory units, ranging from neurotransmitters and neurons to DISTRIBUTION A: Distribution approved for public release. cell-assembly, and organized into multiscale complex networks in space and coordinated dynamically in time (Caroni, 2012; Freeman, 2000). The objective of this project is to build a learning-friendly computational model based on dynamic neural populations and implementing this model in self-assembling molecular populations using DNA computing. A key idea underlying this approach is that the plasticity of neural populations in the brain is based on molecular interactions at the physico-chemical level and, thus, molecular computational processes can naturally simulate human-like learning and memory. The molecular self-assembly mechanisms in DNA chemistry provide us a natural, physical medium for modeling dynamic “neural” populations (neural assemblies). Massively parallel mechanisms of in vitro DNA computing provide us a convenient tool for dealing with large populations, 10 molecules in a nano-mole, which is bigger than the numbers of 10 neurons and 10 synaptic connections in the human brain. In previous work, we experimentally demonstrated the feasibility of cognitive memory with DNA self-assembly. We showed that wet DNA computing can implement weighted-sum operations which are fundamental to perform pattern classification (Lim et al., 2010). Since pattern classification underlies many cognitive tasks, this work opened a new way of creating flexible cognitive memories in vitro with molecules. We also demonstrated the potential of the molecular self-assembly model to build associative language models automatically from language data to generate sentences (Lee et al., 2011). On the mathematical and computational modeling side we developed a probabilistic graphical model of sparse, random population codes called hypernetworks (Zhang, 2008). The model also applied to a visually-grounded language learning (Zhang 35 al., 2012), where cognitive memory consists of multimodal compound concepts which are encoded as hyperedges (molecular memory particles) and then assembled, dissembled, and reassembled to be adapted incrementally as the video sequences are observed. However, there were several challenges to achieving human-level learning and memory. First, the concept of population coding needed to be extended to deal with online, predictive learning in a changing environment. Second, representational formalisms and their translations between neural populations and molecular populations needed to be investigated. Third, the DNA computing and molecular learning technology needed be scaled up to make molecular computational simulation of the whole-brain scale, to make cognitive learning possible and to achieve human-level machine learning. In the first year of the project, we focused on constructing mathematical theories of dynamic neural populations. Building upon our previous work on the hypernetwork models of cognitive learning and memory (Zhang, 2012), we developed population-coded dynamic hypernetwork models of lifelong learning in a non-stationary, changing environment [1, 2, 6, 8, 9, 17]. In [9], we discussed our model from the perspectives of embodied cognition, multisensory integration, cognitive dynamics, perception-action cycle, and lifelong learning. We developed a sequential Bayesian framework for lifelong learning, built a taxonomy of lifelong-learning paradigms, and examined information-theoretic objective functions for each paradigm, with an emphasis on active learning. Also, in [7], we presented that DNA hybridization can be modeled as computing the inner product between DISTRIBUTION A: Distribution approved for public release. embedded vectors in a corresponding vector space, and proposed the algorithm performing learning of a binary classifier in this vector space. In the second year, we extended this to the molecular dynamic hypernetwork model, and designed in vitro experimental protocols to implement online language learning from a stream of text corpus [3, 4, 10, 14, 19, 20, 23]. To measure the difference between different information-encoded sequences, we introduced the symmetric internal loops of double stranded DNA, and which were used to recognize similar or different patterns. Through a series of training processes which is simply storing the given training data in different microtubes in each class of hypernetwork, we observed that the accuracy of sentence classification tasks increased on the corpus of TV show dialogue and our molecular learning was able to generalize the training sentences. In the third year, we demonstrated the use of molecular dynamic hypernetworks for multimodal visuo-linguistic concept learning from a long stream of video data. Motivated by the cognitive developmental process of children constructing the visually grounded concepts from multimodal stimuli (Meltzoff, 1990), we proposed a hierarchical model of automatically constructing visual-linguistic knowledge by dynamically learning concepts represented with vision and language from videos [8, 12, 15, 16, 22]. We developed a stochastic method for graph construction, i.e. a graph Monte Carlo algorithm, and our model learns the concepts by the algorithm while observing new videos, thus robustly tracing concept drift and continuously accumulating new conceptual knowledge. Using a series of approximately 200 episodes of educational cartoon videos we examined the emergence and evolution of the concept hierarchies as the video stories unfold. Through the experiment, we observed that the number of visual and linguistic nodes tends to increase, because the concepts continuously develop while observing the videos. Also, we presented a molecular computational model for human anagram solving to show the potential of application to high-level cognitive functions [5, 11, 13, 18, 21]. Our major contribution is to propose the molecular assembly model of cognitive memory and learning which can be used as a tool for simulating cognitive dynamics involved with multisensory cue integration, grounded concept learning, and interaction of vision and language. We believe that the bio-inspired human-level machine learning combined with molecular-computing implementation can offer an interesting, novel paradigm to address for flexible and reliable computing. We also expect that the cognitive memory architectures and their learning algorithms contribute to revolutionize the AI technology to be used in lifelong learning, self-organizing, sensorimotor systems. DISTRIBUTION A: Distribution approved for public release. [1 Year] The Dynamic Hypernetwork Models of Neural Populations Experiments: In the first year, we constructed a dynamic Bayesian inference framework and examined information-theoretic objective functions for lifelong learning [9]. In lifelong learning, training data are observed sequentially as learning unfolds and not kept for iterative reuse. The learning is proceeded in an online and incremental manner over an extended period in a changing environment. This requires incremental transfer of knowledge acquired from previous learning to future learning, which can be formulated as a Bayesian inference. We applied a sequential Bayesian framework for lifelong learning to build taxonomy of lifelong-learning paradigms, and examine information-theoretic objective functions for each paradigm (Figure 1). Figure 1. Lifelong learning with action-perception-learning cycle [9] Results and Discussion: We distinguished three paradigms of lifelong-learning: learning with passive and continual observations, learning with actions (but without reward feedbacks), and active learning with explicit rewards. For each of the paradigm we examined the objective functions of the lifelong learning styles: prediction errors and predictive information, empowerment which measures how much influence an agent has on its environment, and the","",""
43,"A. Abrol, Z. Fu, Mustafa S. Salman, Rogers F. Silva, Y. Du, S. Plis, V. Calhoun","Deep learning encodes robust discriminative neuroimaging representations to outperform standard machine learning",2021,"","","","",65,"2022-07-13 09:22:41","","10.1038/s41467-020-20655-6","","",,,,,43,43.00,6,7,1,"","",""
1,"Xueping Liu, Xingzuo Yue","A Set of New Hermite Kernel Functions in Kernel Extreme Learning Machine and Application in Human Action Recognition",2019,"","","","",66,"2022-07-13 09:22:41","","10.1142/S0218001419550140","","",,,,,1,0.33,1,2,3,"The kernel function has been successfully utilized in the extreme learning machine (ELM) that provides a stabilized and generalized performance and greatly reduces the computational complexity. However, the selection and optimization of the parameters constituting the most common kernel functions are tedious and time-consuming. In this study, a set of new Hermit kernel functions derived from the generalized Hermit polynomials has been proposed. The significant contributions of the proposed kernel include only one parameter selected from a small set of natural numbers; thus, the parameter optimization is greatly facilitated and excessive structural information of the sample data is retained. Consequently, the new kernel functions can be used as optimal alternatives to other common kernel functions for ELM at a rapid learning speed. The experimental results showed that the proposed kernel ELM method tends to have similar or better robustness and generalized performance at a faster learning speed than the other common kernel ELM and support vector machine methods. Consequently, when applied to human action recognition by depth video sequence, the method also achieves excellent performance, demonstrating its time-based advantage on the video image data.","",""
2,"Md Nafis Ul Alam, U. F. Chowdhury","Short k-mer abundance profiles yield robust machine learning features and accurate classifiers for RNA viruses",2020,"","","","",67,"2022-07-13 09:22:41","","10.1371/journal.pone.0239381","","",,,,,2,1.00,1,2,2,"High throughout sequencing technologies have greatly enabled the study of genomics, transcriptomics and metagenomics. Automated annotation and classification of the vast amounts of generated sequence data has become paramount for facilitating biological sciences. Genomes of viruses can be radically different from all life, both in terms of molecular structure and primary sequence. Alignment-based and profile-based searches are commonly employed for characterization of assembled viral contigs from high-throughput sequencing data. Recent attempts have highlighted the use of machine learning models for the task but these models rely entirely on DNA genomes and owing to the intrinsic genomic complexity of viruses, RNA viruses have gone completely overlooked. Here, we present a novel short k-mer based sequence scoring method that generates robust sequence information for training machine learning classifiers. We trained 18 classifiers for the task of distinguishing viral RNA from human transcripts. We challenged our models with very stringent testing protocols across different species and evaluated performance against BLASTn, BLASTx and HMMER3 searches. For clean sequence data retrieved from curated databases, our models display near perfect accuracy, outperforming all similar attempts previously reported. On de-novo assemblies of raw RNA-Seq data from cells subjected to Ebola virus, the area under the ROC curve varied from 0.6 to 0.86 depending on the software used for assembly. Our classifier was able to properly classify the majority of the false hits generated by BLAST and HMMER3 searches on the same data. The outstanding performance metrics of our model lays the groundwork for robust machine learning methods for the automated annotation of sequence data. Author Summary In this age of high-throughput sequencing, proper classification of copious amounts of sequence data remains to be a daunting challenge. Presently, sequence alignment methods are immediately assigned to the task. Owing to the selection forces of nature, there is considerable homology even between the sequences of different species which draws ambiguity to the results of alignment-based searches. Machine Learning methods are becoming more reliable for characterizing sequence data, but virus genomes are more variable than all forms of life and viruses with RNA-based genomes have gone overlooked in previous machine learning attempts. We designed a novel short k-mer based scoring criteria whereby a large number of highly robust numerical feature sets can be derived from sequence data. These features were able to accurately distinguish virus RNA from human transcripts with performance scores better than all previous reports. Our models were able to generalize well to distant species of viruses and mouse transcripts. The model correctly classifies the majority of false hits generated by current standard alignment tools. These findings strongly imply that this k-mer score based computational pipeline forges a highly informative, rich set of numerical machine learning features and similar pipelines can greatly advance the field of computational biology.","",""
5,"D. Kedziora, Katarzyna Musial, B. Gabrys","AutonoML: Towards an Integrated Framework for Autonomous Machine Learning",2020,"","","","",68,"2022-07-13 09:22:41","","","","",,,,,5,2.50,2,3,2,"Over the last decade, the long-running endeavour to automate high-level processes in machine learning (ML) has risen to mainstream prominence, stimulated by advances in optimisation techniques and their impact on selecting ML models/algorithms. Central to this drive is the appeal of engineering a computational system that both discovers and deploys high-performance solutions to arbitrary ML problems with minimal human interaction. Beyond this, an even loftier goal is the pursuit of autonomy, which describes the capability of the system to independently adjust an ML solution over a lifetime of changing contexts. However, these ambitions are unlikely to be achieved in a robust manner without the broader synthesis of various mechanisms and theoretical frameworks, which, at the present time, remain scattered across numerous research threads. Accordingly, this review seeks to motivate a more expansive perspective on what constitutes an automated/autonomous ML system, alongside consideration of how best to consolidate those elements. In doing so, we survey developments in the following research areas: hyperparameter optimisation, multi-component models, neural architecture search, automated feature engineering, meta-learning, multi-level ensembling, dynamic adaptation, multi-objective evaluation, resource constraints, flexible user involvement, and the principles of generalisation. We also develop a conceptual framework throughout the review, augmented by each topic, to illustrate one possible way of fusing high-level mechanisms into an autonomous ML system. Ultimately, we conclude that the notion of architectural integration deserves more discussion, without which the field of automated ML risks stifling both its technical advantages and general uptake.","",""
297,"Andrius Vabalas, E. Gowen, E. Poliakoff, A. Casson","Machine learning algorithm validation with a limited sample size",2019,"","","","",69,"2022-07-13 09:22:41","","10.1371/journal.pone.0224365","","",,,,,297,99.00,74,4,3,"Advances in neuroimaging, genomic, motion tracking, eye-tracking and many other technology-based data collection methods have led to a torrent of high dimensional datasets, which commonly have a small number of samples because of the intrinsic high cost of data collection involving human participants. High dimensional data with a small number of samples is of critical importance for identifying biomarkers and conducting feasibility and pilot work, however it can lead to biased machine learning (ML) performance estimates. Our review of studies which have applied ML to predict autistic from non-autistic individuals showed that small sample size is associated with higher reported classification accuracy. Thus, we have investigated whether this bias could be caused by the use of validation methods which do not sufficiently control overfitting. Our simulations show that K-fold Cross-Validation (CV) produces strongly biased performance estimates with small sample sizes, and the bias is still evident with sample size of 1000. Nested CV and train/test split approaches produce robust and unbiased performance estimates regardless of sample size. We also show that feature selection if performed on pooled training and testing data is contributing to bias considerably more than parameter tuning. In addition, the contribution to bias by data dimensionality, hyper-parameter space and number of CV folds was explored, and validation methods were compared with discriminable data. The results suggest how to design robust testing methodologies when working with small datasets and how to interpret the results of other studies based on what validation method was used.","",""
1,"K. Kalaiselvi, D. Karthika","Identifying Diseases and Diagnosis Using Machine Learning",2020,"","","","",70,"2022-07-13 09:22:41","","10.1007/978-3-030-40850-3_16","","",,,,,1,0.50,1,2,2,"","",""
1,"C. Duetz, S. Van Gassen, T. Westers, F. I. '. in 't Hout, E. Cremers, C. Alhan, C. Bachas, M. V. van Spronsen, H. Visser-Wisselaar, D. Chitu, A. D. de Graaf, J. Jansen, Y. Saeys, A. A. van de Loosdrecht","Machine Learning-Based Flow Cytometry Diagnostics in Myelodysplastic Syndromes: Validation in the HOVON89 Clinical Trial (EudraCT 2008-002195-10)",2020,"","","","",71,"2022-07-13 09:22:41","","10.1182/BLOOD-2020-136719","","",,,,,1,0.50,0,14,2,"Introduction  Flow cytometry is a recommended tool in the diagnostic work-up of cytopenic patients suspected for myelodysplastic syndromes. Currently used flow cytometry scores rely on human assessment of dysplastic features in the bone marrow. Although proven useful, these methods are labor intensive and require a high level of expertise. Therefore, we previously developed a machine learning-based workflow for flow cytometry diagnostics in MDS by combining computational cell detection and a machine learning-classifier. This workflow outperformed traditional diagnostic scores with respect to accuracy (sensitivity 85-97%, specificity 93-97%), time investment (<30 seconds) and required materials (manuscript submitted). In the present study, we validated sensitivity of the workflow in a well-characterized clinical trial cohort (HOVON89 EudraCT 2008-002195-10) of lower risk MDS patients.  Method  Patient inclusion and characteristics  Very low to intermediate risk MDS patients enrolled in the HOVON89 clinical trial (EudraCT 2008-002195-10) were included. 53 patients met the additional inclusion criteria, concerning written consent for add-on studies and availability of required flow cytometry data.  Sample preparation  Bone marrow samples were processed for flow cytometry analysis according to the European Leukemia Net guidelines. This study focused on the antibody combination optimized for assessment of myeloid progenitors and erythroid dysplasia (CD45, CD34, CD117, HLA-DR, CD71, CD36, CD105, CD33, sideward light scatter (SSC) and forward light scatter (FSC)).  Machine learning-based workflow  The machine learning-based workflow was developed in a prior study based on a reference cohort consisting of MDS patients without excess of blasts(n=67) and non-MDS cases (n=81) (Figure 1). MDS patients were diagnosed based on (cyto)morphology, cytogenetics and clinical follow-up. Non-MDS cases were patients with confirmed non-neoplastic cytopenias (n=69) and age-matched healthy individuals (n=12).  Results  In the validation cohort, the machine learning-based diagnostic workflow classified 49 out of 53 patients correctly, reaching a sensitivity of 92%. The workflow outperformed two currently used diagnostic tools for MDS flow cytometry, the Ogata score and integrated flow cytometry score (iFS). The former obtained 72% sensitivity (McNemar: p = 0.001) and the latter 83% sensitivity (McNemar: p = 0.06) in the validation cohort. Per patient, time required for automated analysis was less than 30 seconds.  All four MDS patients that classified false negatively had a normal karyotype and (very) low risk disease according to the IPSS-r. In three out of four patients, no mutations or MDS-associated immunophenotypic features were detected. One patients was diagnosed as MDS-MLD and three patients as MDS-RS-SLD according to the WHO 2016 classification.  The ten most relevant cellular features that discriminated between MDS and non-MDS patients in the reference data were confirmed in the current validation cohort. All ten features of MDS patients in the validation cohort were significantly different from non-MDS patients of the reference cohort (all features, p < 0.00001) (Figure 2). Seven out of ten features were similar in MDS patients of the validation cohort compared to those of the MDS patients of the reference cohort (p>0.05) (Figure 2).  Conclusion  In this validation study, we confirmed accuracy of machine learning-based flow cytometry diagnostics in lower risk MDS. The workflow obtained 92% sensitivity, which is in accordance with results from our previous study (85-97%), and outperformed currently used diagnostic flow cytometry scores for MDS (i.e. Ogata score and iFS). In our previous study specificity was 95% in both reference and test cohorts. Cellular features, most discriminative for diagnosis, were confirmed in the validation cohort, emphasizing robustness of the method. Additional benefits of this approach are the reduction in analysis time to less than thirty seconds per patient, reduction of required antibodies and increased reproducibility.        van de Loosdrecht: celgene: Honoraria; novartis: Honoraria. ","",""
0,"J. Wirbel, Konrad Zych, Morgan Essex, N. Karcher, Ece Kartal, G. Salazar, P. Bork, S. Sunagawa, G. Zeller","Abstract A40: The SIAMCAT R package enables statistical and machine learning analyses for case-control microbiome datasets",2020,"","","","",72,"2022-07-13 09:22:41","","10.1158/1538-7445.MVC2020-A40","","",,,,,0,0.00,0,9,2,"Alterations in microbiome composition have been linked to many human diseases, including colorectal cancer or precancerous liver diseases. Findings from microbiome-association studies are increasingly explored as promising avenues for clinical applications due to their diagnostic or therapeutic potential. However, microbiome data complexity and the lack of integrated computational tools make it difficult to arrive at robust associations and predictive disease models. Here, we present the SIAMCAT R package as a modular and user-friendly toolbox for machine learning workflows, statistical analysis, and confounder detection for case-control microbiome datasets. Previously, we conducted a machine learning meta-analysis of colorectal cancer metagenomics studies (Wirbel, Pyl et al., Nat Med 2019) using the functionalities of SIAMCAT, which is now available to the wider community via Bioconductor. We showcase how SIAMCAT is a versatile tool by applying it to a large set of metagenomic datasets that have been processed with a wide variety of taxonomic and functional profiling tools. We furthermore demonstrate how SIAMCAT can help to detect confounding factors in microbiome association studies. By making stringent machine learning workflows and analysis pipelines available through a user-friendly and flexible interface, SIAMCAT will facilitate microbiome data analyses, biomarker discovery, and thus the translation of microbiome research to clinical applications, while simultaneously improving statistical rigor and safeguarding against common machine learning pitfalls. Citation Format: Jakob Wirbel, Konrad Zych, Morgan Essex, Nicolai Karcher, Ece Kartal, Guillem Salazar, Peer Bork, Shinichi Sunagawa, Georg Zeller. The SIAMCAT R package enables statistical and machine learning analyses for case-control microbiome datasets [abstract]. In: Proceedings of the AACR Special Conference on the Microbiome, Viruses, and Cancer; 2020 Feb 21-24; Orlando, FL. Philadelphia (PA): AACR; Cancer Res 2020;80(8 Suppl):Abstract nr A40.","",""
0,"Adhideb Ghosh, A. Navarini","Biological function polarity prediction of missense variants using machine learning",2020,"","","","",73,"2022-07-13 09:22:41","","10.1101/2020.04.03.023440","","",,,,,0,0.00,0,2,2,"Functional interpretation is crucial when facing on average 20,000 missense variants per human exome, as the great majority are not associated with any underlying disease. In silico bioinformatics tools can predict the deleteriousness of variants or assess their functional impact by assigning scores, but they cannot predict whether the variant in question results in gain or loss of function at the protein level. Here, we show that machine learning can effectively predict this biological function polarity of missense variants. The new method adapts weighted gradient boosting machine approach on a set of damaging variants (1,288 loss of function and 218 gain of function variants) as annotated by the tools SIFT, PolyPhen2 and CADD. Area under the ROC curve of 0.85 illustrates high discriminative power of the classifier. Predictive performance of the classifier remains consistent against an independent set of damaging variants as highlighted by the area under the ROC curve of 0.83. This new approach may help to guide biological experiments on the clinical relevance of damaging genetic variants. Author summary Missense variant occurs when a single genetic alteration in DNA takes place and as a result a new amino acid is translated into the protein. This amino acid change can inactivate the existing protein function causing loss-of-function or produce a new function causing gain-of-function. Therefore, it is very important to interpret these functional consequences of missense variants as they often turn out to be disease causing. Each individual’s genome sequence has thousands of missense variants, out of which very few are actually associated with any underlying disease. Various computational tools have been developed to predict whether missense variants are damaging or not, but none of them can actually predict whether the damaging missense variants cause gain-of-function or loss-of-function. We have developed a new ensemble classifier to predict this biological function polarity at the protein level. The classifier combines the prediction scores of three existing bioinformatics tools and applies machine learning to make effective predictions. We have validated our classifier against an independent data set to show its high predictive power and robustness. The predictions made by our machine learning tool can be used as indicators of biological function polarity, but with further evidence on pathogenicity.","",""
0,"Byoung-Tak Zhang","Bio-Inspired Human-Level Machine Learning",2015,"","","","",74,"2022-07-13 09:22:41","","10.21236/ada636902","","",,,,,0,0.00,0,1,7,"Abstract : How can brain computation be so fast, flexible, and robust? What kinds of representational and organizational principles facilitate the biological brain to learn so efficiently and flexibly on the sub-second time scale and so reliably on the continuous lifetime scale? To understand these principles, we aimed to develop human-level machine learning technology that is fast, flexible, and reliable to adapt to a continuously changing, dynamic environment. Based on dynamic neural populations (neural assemblies), we constructed a human-like machine learning model and implement this model in molecular populations (molecular assemblies) using in vitro DNA computing. In the first year, we developed the dynamic hypernetwork models of neural populations in the sequential Bayesian framework for lifelong learning. In the second year, we extended it to the molecular dynamic hypernetwork model, and designed in vitro experimental protocols to implement online language learning from a stream of text corpus. In the third year, we demonstrated the use of molecular dynamic hypernetworks for multimodal visuo-linguistic concept learning from a long stream of video data and their extensions to high-level cognitive functions such as anagram solving problem.","",""
517,"Andreas Holzinger","Interactive machine learning for health informatics: when do we need the human-in-the-loop?",2016,"","","","",75,"2022-07-13 09:22:41","","10.1007/s40708-016-0042-6","","",,,,,517,86.17,517,1,6,"","",""
12,"Zi-Mei Zhang, Jia-Shu Wang, Hasan Zulfiqar, Hao Lv, Fu-Ying Dao, Hao Lin","Early Diagnosis of Pancreatic Ductal Adenocarcinoma by Combining Relative Expression Orderings With Machine-Learning Method",2020,"","","","",76,"2022-07-13 09:22:41","","10.3389/fcell.2020.582864","","",,,,,12,6.00,2,6,2,"Pancreatic ductal adenocarcinoma (PDAC) is an aggressive and lethal cancer deeply affecting human health. Diagnosing early-stage PDAC is the key point to PDAC patients’ survival. However, the biomarkers for diagnosing early PDAC are inexact in most cases. Therefore, it is highly desirable to identify an effective PDAC diagnostic biomarker. In the current work, we designed a novel computational approach based on within-sample relative expression orderings (REOs). A feature selection technique called minimum redundancy maximum relevance was used to pick out optimal REOs. We then compared the performances of different classification algorithms for discriminating PDAC and its adjacent normal tissues from non−PDAC tissues. The support vector machine algorithm is the best one for identifying early PDAC diagnostic biomarker. At first, a signature composed of nine gene pairs was acquired from microarray gene expression data sets. These gene pairs could produce satisfactory classification accuracy up to 97.53% in fivefold cross-validation. Subsequently, two types of data from diverse platforms, namely, microarray and RNA-Seq, were used to validate this signature. For microarray data, all (100.00%) of 115 PDAC tissues and all (100.00%) of 31 PDAC adjacent normal tissues were correctly recognized as PDAC. In addition, 88.24% of 17 non-PDAC (normal or pancreatitis) tissues were correctly classified. For the RNA-Seq data, all (100.00%) of 177 PDAC tissues and all (100.00%) of 4 PDAC adjacent normal tissues were correctly recognized as PDAC. Validation results demonstrated that the signature had a good cross-platform effect for early detection of PDAC. This work developed a new robust signature that might be a promising biomarker for early PDAC diagnosis.","",""
2,"Lorenzo Barberis Canonico, Nathan J. Mcneese, Chris Duncan","Machine Learning as Grounded Theory: Human-Centered Interfaces for Social Network Research through Artificial Intelligence",2018,"","","","",77,"2022-07-13 09:22:41","","10.1177/1541931218621287","","",,,,,2,0.50,1,3,4,"Internet technologies have created unprecedented opportunities for people to come together and through their collective effort generate large amounts of data about human behavior. With the increased popularity of grounded theory, many researchers have sought to use ever-increasingly large datasets to analyze and draw patterns about social dynamics. However, the data is simply too big to enable a single human to derive effective models for many complex social phenomena. Computational methods offer a unique opportunity to analyze a wide spectrum of sociological events by leveraging the power of artificial intelligence. Within the human factors community, machine learning has emerged as the dominant AI-approach to deal with big data. However, along with its many benefits, machine learning has introduced a unique challenge: interpretability. The models of macro-social behavior generated by AI are so complex that rarely can they translated into human understanding. We propose a new method to conduct grounded theory research by leveraging the power of machine learning to analyze complex social phenomena through social network analysis while retaining interpretability as a core feature.","",""
0,"A. Sumathi, S. Meganathan","Machine learning based pattern detection technique for diabetes mellitus prediction",2022,"","","","",78,"2022-07-13 09:22:41","","10.1002/cpe.6751","","",,,,,0,0.00,0,2,1,"In recent years, a few popular classifiers and computational tools have been introduced for the prediction of diabetes mellitus. The robustness of these methodologies is to predict and classify the diabetes mellitus. Diabetes is the most fatal disease that affects the life style of humans. More researchers have focusing on diabetes issues to design efficient classifiers for better classification and prediction. This research work is analyzed with two existing classifiers: (i) hybrid prediction model for type 2 diabetes pattern (HPMT2D) and (ii) type 2 diabetes mellitus prediction model (T2DMPM). These two classifiers are implemented in R tool. An efficient classifier must needed to diagnose and treatment well in advance. Therefore, this article proposes an efficient machine learning based pattern prediction technique called diabetes pattern detection technique using tree ensemble clustering classifier (DDTEC) to accomplish this demand. The proposed classifier is implemented in R tool with BioWeka and studied thoroughly to find classification accuracy of the patterns of type 1 diabetes, type 2 diabetes, and gestational diabetes. From the experimental results, it is noticed that the existing classifiers unable to classify and predict the patterns of various diabetes mellitus with better classification accuracy and F‐measure.","",""
1,"Chenghan Li, G. Voth","Using Machine Learning to Greatly Accelerate Path Integral Ab Initio Molecular Dynamics.",2021,"","","","",79,"2022-07-13 09:22:41","","10.33774/chemrxiv-2021-cqh6c","","",,,,,1,1.00,1,2,1,"Ab initio molecular dynamics (AIMD) has become one of the most popular and robust approaches for modeling complicated chemical, liquid, and material systems. However, the formidable computational cost often limits its widespread application in simulations of the largest-scale systems. The situation becomes even more severe in cases where the hydrogen nuclei may be better described as quantized particles using a path integral representation. Here, we present a computational approach that combines machine learning with recent advances in path integral contraction schemes, and we achieve a 2 orders of magnitude acceleration over direct path integral AIMD simulation while at the same time maintaining its accuracy.","",""
22,"A. S. Abbott, J. M. Turney, Boyi Zhang, Daniel G. A. Smith, D. Altarawy, H. Schaefer","PES-Learn: An Open-Source Software Package for the Automated Generation of Machine Learning Models of Molecular Potential Energy Surfaces.",2019,"","","","",80,"2022-07-13 09:22:41","","10.1021/acs.jctc.9b00312","","",,,,,22,7.33,4,6,3,"We introduce a free and open-source software package (PES-Learn) which largely automates the process of producing high-quality machine learning models of molecular potential energy surfaces (PESs). PES-Learn incorporates a generalized framework for producing grid points across a PES that is compatible with most electronic structure theory software. The newly generated or externally supplied PES data can then be used to train and optimize neural network or Gaussian process models in a completely automated fashion. Robust hyperparameter optimization schemes designed specifically for molecular PES applications are implemented to ensure that the best possible model for the data set is fit with high quality. The performance of PES-Learn toward fitting a few semiglobal PESs from the literature is evaluated. We also demonstrate the use of PES-Learn machine learning models in carrying out high-level vibrational configuration interaction computations on water and formaldehyde.","",""
0,"Lin Wang, Chuan Zhao, Kun Zhao, Bo Zhang, Shan Jing, Zhenxiang Chen, Kuiheng Sun","Privacy-Preserving Collaborative Computation for Human Activity Recognition",2022,"","","","",81,"2022-07-13 09:22:41","","10.1155/2022/9428610","","",,,,,0,0.00,0,7,1,"Human Activity Recognition (HAR) enables computer systems to assist users with their tasks and improve their quality of life in rehabilitation, daily life tracking, fitness, and cognitive disorder therapy. It is a hot topic in the field of machine learning, and HAR is gaining more attention among researchers due to its unique societal and economic advantages. This paper focuses on a collaborative computation scenario where a group of participants will securely and collaboratively train an accurate HAR model. The training process requires collecting a massive number of personal activity features and labels, which raises privacy problems. We decentralize the training process locally to each client in order to ensure the privacy of training data. Furthermore, we use an advanced secure aggregation algorithm to ensure that malicious participants cannot extract private information from the updated parameters even during the aggregation phase. Edge computing nodes have been introduced into our system to address the problem of data generation devices’ insufficient computing power. We replace the traditional central server with smart contract to make the system more robust and secure. We achieve the verifiability of the packaged nodes using the publicly auditability feature of blockchain. According to the experimental data, the accuracy of the HAR model trained by our proposed framework reaches 93.24%, which meets the applicability requirements. The use of secure multiparty computation techniques unavoidably increases training time, and experimental results show that a round of iterations takes 36.4 seconds to execute, which is still acceptable.","",""
166,"M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, S. Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Integrating machine learning and multiscale modeling—perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences",2019,"","","","",82,"2022-07-13 09:22:41","","10.1038/s41746-019-0193-y","","",,,,,166,55.33,17,11,3,"","",""
92,"M. Gillies, R. Fiebrink, A. Tanaka, Jérémie Garcia, Frédéric Bevilacqua, A. Héloir, Fabrizio Nunnari, W. Mackay, Saleema Amershi, Bongshin Lee, N. D'Alessandro, J. Tilmanne, T. Kulesza, Baptiste Caramiaux","Human-Centred Machine Learning",2016,"","","","",83,"2022-07-13 09:22:41","","10.1145/2851581.2856492","","",,,,,92,15.33,9,14,6,"Machine learning is one of the most important and successful techniques in contemporary computer science. It involves the statistical inference of models (such as classifiers) from data. It is often conceived in a very impersonal way, with algorithms working autonomously on passively collected data. However, this viewpoint hides considerable human work of tuning the algorithms, gathering the data, and even deciding what should be modeled in the first place. Examining machine learning from a human-centered perspective includes explicitly recognising this human work, as well as reframing machine learning workflows based on situated human working practices, and exploring the co-adaptation of humans and systems. A human-centered understanding of machine learning in human context can lead not only to more usable machine learning tools, but to new ways of framing learning computationally. This workshop will bring together researchers to discuss these issues and suggest future research questions aimed at creating a human-centered approach to machine learning.","",""
63,"Yanju Zhang, Ruopeng Xie, Jiawei Wang, A. Leier, T. Marquez-Lago, T. Akutsu, Geoffrey I. Webb, K. Chou, Jiangning Song","Computational analysis and prediction of lysine malonylation sites by exploiting informative features in an integrative machine-learning framework",2018,"","","","",84,"2022-07-13 09:22:41","","10.1093/bib/bby079","","",,,,,63,15.75,7,9,4,"As a newly discovered post-translational modification (PTM), lysine malonylation (Kmal) regulates a myriad of cellular processes from prokaryotes to eukaryotes and has important implications in human diseases. Despite its functional significance, computational methods to accurately identify malonylation sites are still lacking and urgently needed. In particular, there is currently no comprehensive analysis and assessment of different features and machine learning (ML) methods that are required for constructing the necessary prediction models. Here, we review, analyze and compare 11 different feature encoding methods, with the goal of extracting key patterns and characteristics from residue sequences of Kmal sites. We identify optimized feature sets, with which four commonly used ML methods (random forest, support vector machines, K-nearest neighbor and logistic regression) and one recently proposed [Light Gradient Boosting Machine (LightGBM)] are trained on data from three species, namely, Escherichia coli, Mus musculus and Homo sapiens, and compared using randomized 10-fold cross-validation tests. We show that integration of the single method-based models through ensemble learning further improves the prediction performance and model robustness on the independent test. When compared to the existing state-of-the-art predictor, MaloPred, the optimal ensemble models were more accurate for all three species (AUC: 0.930, 0.923 and 0.944 for E. coli, M. musculus and H. sapiens, respectively). Using the ensemble models, we developed an accessible online predictor, kmal-sp, available at http://kmalsp.erc.monash.edu/. We hope that this comprehensive survey and the proposed strategy for building more accurate models can serve as a useful guide for inspiring future developments of computational methods for PTM site prediction, expedite the discovery of new malonylation and other PTM types and facilitate hypothesis-driven experimental validation of novel malonylated substrates and malonylation sites.","",""
1,"N. Howard, Naima Chouikhi, Ahsan Adeel, Katelyn Dial, Adam Howard, A. Hussain","BrainOS: A Novel Artificial Brain-Alike Automatic Machine Learning Framework",2020,"","","","",85,"2022-07-13 09:22:41","","10.3389/fncom.2020.00016","","",,,,,1,0.50,0,6,2,"Human intelligence is constituted by a multitude of cognitive functions activated either directly or indirectly by external stimuli of various kinds. Computational approaches to the cognitive sciences and to neuroscience are partly premised on the idea that computational simulations of such cognitive functions and brain operations suspected to correspond to them can help to further uncover knowledge about those functions and operations, specifically, how they might work together. These approaches are also partly premised on the idea that empirical neuroscience research, whether following on from such a simulation (as indeed simulation and empirical research are complementary) or otherwise, could help us build better artificially intelligent systems. This is based on the assumption that principles by which the brain seemingly operate, to the extent that it can be understood as computational, should at least be tested as principles for the operation of artificial systems. This paper explores some of the principles of the brain that seem to be responsible for its autonomous, problem-adaptive nature. The brain operating system (BrainOS) explicated here is an introduction to ongoing work aiming to create a robust, integrated model, combining the connectionist paradigm underlying neural networks and the symbolic paradigm underlying much else of AI. BrainOS is an automatic approach that selects the most appropriate model based on the (a) input at hand, (b) prior experience (a history of results of prior problem solving attempts), and (c) world knowledge (represented in the symbolic way and used as a means to explain its approach). It is able to accept diverse and mixed input data types, process histories and objectives, extract knowledge and infer a situational context. BrainOS is designed to be efficient through its ability to not only choose the most suitable learning model but to effectively calibrate it based on the task at hand.","",""
0,"N. Strigul, A. Erickson","Machine-learning emulation of a forest biogeochemistry model for efficient biosphere optimization",2020,"","","","",86,"2022-07-13 09:22:41","","10.5194/egusphere-egu2020-19744","","",,,,,0,0.00,0,2,2,"  <div> <div> <div> <p>Management controls the spatial configuration of a number of landscapes globally, from forests to rangelands. The majority of landcover change and all land-use change is the result of human decision-making. As human populations and global temperatures continue to increase, an engineering approach is needed to ensure the persistence of biological diversity and natural capital critical to human well-being. Such an approach may be based on manipulating ecosystems to achieve desired future states, informed by the latest simulation models. Models of the land surface are now being used to inform policy in the form of planning and management practices. This often involves the application of models that include spatial dynamics and operate at a landscape scale. The strong correspondence between the resolution and extent of modeling and management activities at this scale, and ability to efficiently simulate the decadal-to-centennial time-scales of interest, provide managers with a credible scientific tool for anticipating future land states under different scenarios. The importance of such tools to managers has grown dramatically with the challenges posed by anthropogenic climate change. As ecosystem simulation models continually improve in precision, accuracy, and robustness, we posit that models may be mathematically optimized as a basis for optimizing the management of real-world systems. Since current ecosystem simulation models are coarse approximations of highly complex and dynamic real-world systems, such optimizations should ideally account for uncertainty and physical or biochemical constraints, thereby improving the tractability of the optimization problem. In this work, we demonstrate the emulation and optimization of a forest biogeochemistry model from the SORTIE-PPA family of models. In doing so, we provide the first demonstration of the concept of biosphere optimization (Erickson 2015), which may one day be extended to include computational genetic manipulation experiments. To perform this work, we utilize the open-source Earth-systems Research and Development Environment (ERDE) library, which contains built-in functions for performing these and other analyses with land models, with a particular focus on forests.</p> </div> </div> </div> ","",""
0,"T. Giese, Jinzhe Zeng, S. Ekesan, D. York","Combined QM/MM, Machine Learning Path Integral Approach to Compute Free Energy Profiles and Kinetic Isotope Effects in RNA Cleavage Reactions.",2022,"","","","",87,"2022-07-13 09:22:41","","10.1021/acs.jctc.2c00151","","",,,,,0,0.00,0,4,1,"We present a fast, accurate, and robust approach for determination of free energy profiles and kinetic isotope effects for RNA 2'-O-transphosphorylation reactions with inclusion of nuclear quantum effects. We apply a deep potential range correction (DPRc) for combined quantum mechanical/molecular mechanical (QM/MM) simulations of reactions in the condensed phase. The method uses the second-order density-functional tight-binding method (DFTB2) as a fast, approximate base QM model. The DPRc model modifies the DFTB2 QM interactions and applies short-range corrections to the QM/MM interactions to reproduce ab initio DFT (PBE0/6-31G*) QM/MM energies and forces. The DPRc thus enables both QM and QM/MM interactions to be tuned to high accuracy, and the QM/MM corrections are designed to smoothly vanish at a specified cutoff boundary (6 Å in the present work). The computational speed-up afforded by the QM/MM+DPRc model enables free energy profiles to be calculated that include rigorous long-range QM/MM interactions under periodic boundary conditions and nuclear quantum effects through a path integral approach using a new interface between the AMBER and i-PI software. The approach is demonstrated through the calculation of free energy profiles of a native RNA cleavage model reaction and reactions involving thio-substitutions, which are important experimental probes of the mechanism. The DFTB2+DPRc QM/MM free energy surfaces agree very closely with the PBE0/6-31G* QM/MM results, and it is vastly superior to the DFTB2 QM/MM surfaces with and without weighted thermodynamic perturbation corrections. 18O and 34S primary kinetic isotope effects are compared, and the influence of nuclear quantum effects on the free energy profiles is examined.","",""
0,"M. Peterson, Minzhen Du, Bryant Springle, Jonathan Black","SpaceDrones 2.0—Hardware-in-the-Loop Simulation and Validation for Orbital and Deep Space Computer Vision and Machine Learning Tasking Using Free-Flying Drone Platforms",2022,"","","","",88,"2022-07-13 09:22:41","","10.3390/aerospace9050254","","",,,,,0,0.00,0,4,1,"The proliferation of reusable space vehicles has fundamentally changed how assets are injected into the low earth orbit and beyond, increasing both the reliability and frequency of launches. Consequently, it has led to the rapid development and adoption of new technologies in the aerospace sector, including computer vision (CV), machine learning (ML)/artificial intelligence (AI), and distributed networking. All these technologies are necessary to enable truly autonomous “Human-out-of-the-loop” mission tasking for spaceborne applications as spacecrafts travel further into the solar system and our missions become more ambitious. This paper proposes a novel approach for space-based computer vision sensing and machine learning simulation and validation using synthetically trained models to generate the large amounts of space-based imagery needed to train computer vision models. We also introduce a method of image data augmentation known as domain randomization to enhance machine learning performance in the dynamic domain of spaceborne computer vision to tackle unique space-based challenges such as orientation and lighting variations. These synthetically trained computer vision models then apply that capability for hardware-in-the-loop testing and evaluation via free-flying robotic platforms, thus enabling sensor-based orbital vehicle control, onboard decision making, and mobile manipulation similar to air-bearing table methods. Given the current energy constraints of space vehicles using solar-based power plants, cameras provide an energy-efficient means of situational awareness when compared to active sensing instruments. When coupled with computationally efficient machine learning algorithms and methods, it can enable space systems proficient in classifying, tracking, capturing, and ultimately manipulating objects for orbital/planetary assembly and maintenance (tasks commonly referred to as In-Space Assembly and On-Orbit Servicing). Given the inherent dangers of manned spaceflight/extravehicular activities (EVAs) currently employed to perform spacecraft maintenance and the current limitation of long-duration human spaceflight outside the low earth orbit, space robotics armed with generalized sensing and control and machine learning architecture have a unique automation potential. However, the tools and methodologies required for hardware-in-the-loop simulation, testing, and validation at a large scale and at an affordable price point are in developmental stages. By leveraging a drone’s free-flight maneuvering capability, theater projection technology, synthetically generated orbital and celestial environments, and machine learning, this work strives to build a robust hardware-in-the-loop testing suite. While the focus of the specific computer vision models in this paper is narrowed down to solving visual sensing problems in orbit, this work can very well be extended to solve any problem set that requires a robust onboard computer vision, robotic manipulation, and free-flight capabilities.","",""
0,"Supatcha Lertampaiporn, A. Hongsthong, Warin Wattanapornprom, C. Thammarongtham","Ensemble-AHTPpred: A Robust Ensemble Machine Learning Model Integrated With a New Composite Feature for Identifying Antihypertensive Peptides",2022,"","","","",89,"2022-07-13 09:22:41","","10.3389/fgene.2022.883766","","",,,,,0,0.00,0,4,1,"Hypertension or elevated blood pressure is a serious medical condition that significantly increases the risks of cardiovascular disease, heart disease, diabetes, stroke, kidney disease, and other health problems, that affect people worldwide. Thus, hypertension is one of the major global causes of premature death. Regarding the prevention and treatment of hypertension with no or few side effects, antihypertensive peptides (AHTPs) obtained from natural sources might be useful as nutraceuticals. Therefore, the search for alternative/novel AHTPs in food or natural sources has received much attention, as AHTPs may be functional agents for human health. AHTPs have been observed in diverse organisms, although many of them remain underinvestigated. The identification of peptides with antihypertensive activity in the laboratory is time- and resource-consuming. Alternatively, computational methods based on robust machine learning can identify or screen potential AHTP candidates prior to experimental verification. In this paper, we propose Ensemble-AHTPpred, an ensemble machine learning algorithm composed of a random forest (RF), a support vector machine (SVM), and extreme gradient boosting (XGB), with the aim of integrating diverse heterogeneous algorithms to enhance the robustness of the final predictive model. The selected feature set includes various computed features, such as various physicochemical properties, amino acid compositions (AACs), transitions, n-grams, and secondary structure-related information; these features are able to learn more information in terms of analyzing or explaining the characteristics of the predicted peptide. In addition, the tool is integrated with a newly proposed composite feature (generated based on a logistic regression function) that combines various feature aspects to enable improved AHTP characterization. Our tool, Ensemble-AHTPpred, achieved an overall accuracy above 90% on independent test data. Additionally, the approach was applied to novel experimentally validated AHTPs, obtained from recent studies, which did not overlap with the training and test datasets, and the tool could precisely predict these AHTPs.","",""
0,"Lihong Peng, Jialiang Yang, Minxian Wang, Liqian Zhou","Editorial: Machine Learning-Based Methods for RNA Data Analysis",2022,"","","","",90,"2022-07-13 09:22:41","","10.3389/fgene.2022.828575","","",,,,,0,0.00,0,4,1,"RNA is a type of extremely important biological macromolecules, which play key roles in all aspects of life activities and biological processes through its interactions with other biological entities Wang et al. (2021); Zhang et al. (2021). Thus, it is critical to identify complex biological associations between RNA and other biological entities Mu et al. (2020); Deng et al. (2018). Although experimental methods have been applied to analyze RNA data, especially identify various associations between RNA molecules and complex diseases, they are usually timeconsuming and resource demanding. Machine learning aims to simulate human learning ways in real time and divide the existing content into knowledge structures to advance learning efficiency. It can effectively use available electronic data to boost learning performance or implement accurate prediction Mohri et al. (2018). Furthermore, it still improves more evidence-based decision-making in the area of life science Jordan and Mitchell (2015). With the advancement of next generation sequencing techniques, machine learning-based methods discovered a large number of useful information from abundant RNA data and thus provide an effective way for the analysis of RNA data. Consequently, through machine learning techniques, we can design powerful models and algorithms to discovery diverse associations between RNA molecules themselves (such as microRNAs, mRNA, circular RNAs, and long noncoding RNAs) and between RNA molecules and complex diseases. We can further infer novel molecular markers for diagnosis and prognosis of corresponding diseases based on the identified associations. Based on the assumption of “guilt-by-association” and machine learning technologies, accumulated computational methods have been developed to analyze RNA data Liu et al. (2020); Chu et al. (2021). However, the performance of most methods remains unsatisfying due to data complexity and heterogeneity. Therefore, this research topic serves as a forum to develop new machine learning algorithms to improve RNA data analyses. MicroRNAs (miRNAs) are a class of short and endogenous noncoding RNAs Wang et al. (2020); Chen et al. (2019a). miRNAs can control gene expression based on translational repression or messenger RNA (mRNA) degradation and exhibit strong associations with a variety of disease including neurodegenerative diseases and cancers Saliminejad et al. (2019). Chen et al. designed a few representative machine learning-based algorithms to identify potential microRNA-disease associations Chen et al. (2018, 2019b). To find robust biomarkers associated with prostate cancer, Ning et al. designed amulti-omics data fusion method by integrating directed random walk and Support Vector Machine (SVM). They compared their proposed pathway-based method with five other methods including the Median method, Mean method, component analysis method, pathway activity inference method based on Edited by: William C. Cho, QEH, Hong Kong SAR, China","",""
0,"Xiangyu Fu","GomokuPro: An Implementation of Enhanced Machine Learning Algorithm Utilizing Convolutional Neural Network in Gomoku Strategy and Predictions Model",2022,"","","","",91,"2022-07-13 09:22:41","","10.1109/ICSP54964.2022.9778476","","",,,,,0,0.00,0,1,1,"In this paper, the researcher proposes a machine learning Gomoku model, GomokuPro, which is based on CNN (Convolutional Neural Network) as well as Monte Carlo tree search. The purpose of this research aggregation is to solve the previous problem of artificial intelligence algorithms taking too long to compute in board games. The researchers turned a more multidimensional fractional model into an adaptive model based on a single machine learning strategy previously developed by the researchers and capable of making judgments and predictions based on the following human algorithms. Due to the limitations of the hardware used for testing, the researchers found that the results were not applicable to some specific cases. Although this algorithm significantly reduces the computational effort, it still does not provide robust performance in some cases. The computational effort of convolutional neural networks is also influenced by the type of data. However, most of the time, machine learning models are still able to make master predictions.","",""
203,"François Bry, Clemens Schefels, C. Wieser","Human computation",2018,"","","","",92,"2022-07-13 09:22:41","","10.1515/itit-2018-0007","","",,,,,203,50.75,68,3,4,"Until the middle of the 20th century, a reader would have stumbled against the phrase “Human Computation” because, at that time, computers were humans carrying out calculations, not machines. Back then, a special issue on “Machine Computation” would have aroused much interest. Nowadays, things are the other way around: A reader is likely to stumble against the phrase “Human Computation” because it is, nowadays, common knowledge that machines outperform human beings in a wide range of tasks; one might even wonder how humans could contribute to computations in a useful manner! Indeed, many human skills are far away from being fully taken over by machines. For example, reading comprehension, image recognition, or finding heuristic solutions for complex computational tasks like the traveling salesman problem still is beyond the capabilities of machines. Such tasks still are within humans’ reserved domain. It therefore seems natural that combining both the skills of humans and of machines can result in a higher problem solving competence both in quantity and quality. This insight paves the way for “Human Computation” as we know it today. This special issue introduces “Human Computation” through presentations of current research projects. The first article, “Mobile Learning in Environmental CitizenScience:An initial survey of current practice inGermany”, reports on a survey of mobile learning among environmental citizen science projects. This first article offers a good overview of why projects of very different types rely on “Citizen Science”, a form of “Human Computation”. The second article, “Design and Implementation of a Platform for the Citizen Science Project Migraine Radar”, describes a Citizen Science platform based on a software","",""
16,"Phil Legg, Jim E. Smith, A. Downing","Visual analytics for collaborative human-machine confidence in human-centric active learning tasks",2019,"","","","",93,"2022-07-13 09:22:41","","10.1186/s13673-019-0167-8","","",,,,,16,5.33,5,3,3,"","",""
3,"W. Pedrycz, S.-M., Chen","Distributed Machine Learning on Smart-Gateway Network Towards Real-time Indoor Data Analytics",2018,"","","","",94,"2022-07-13 09:22:41","","","","",,,,,3,0.75,1,3,4,"Computational intelligence techniques are intelligent computational methodologies such as neural network to solve real-world complex problems. One example is to design a smart agent to make decisions within environment in response to the presence of human beings. Smart building/home is a typical computational intelligence based system enriched with sensors to gather information and processors to analyze it. Indoor computational intelligence based agents can perform behavior or feature extraction from environmental data such as power, temperature, and lighting data, and hence further help improve comfort level for human occupants in building. The current indoor system cannot address dynamic ambient change with a real-time response under emergency because processing backend in cloud takes latency. Therefore, in this chapter we have introduced distributed machine learning algorithms (SVM and neural network) mapped on smart-gateway networks. Scalability and robustness are considered to perform real-time data analytics. Furthermore, as the success of system depends on the trust of users, network intrusion detection for smart gateway has also been developed to provide system security. Experimental results have shown that with a distributed machine learning mapped on smart-gateway networks real-time data analytics can be performed to support sensitive, responsive and adaptive intelligent systems. Hantao Huang Nanyang Technological University, 50 Nanyang Avenue, Block S3.2, Level B2, Singapore 639798. Tel.: (65) 6592 1844, Fax: (65) 6316 4416, E-mail: HHUANG013@e.ntu.edu.sg Rai Suleman Khalid Nanyang Technological University, 50 Nanyang Avenue, Block S3.2, Level B2, Singapore 639798. Tel.: (65) 6592 1844, Fax: (65) 6316 4416, E-mail: RAIS0001@e.ntu.edu.sg Hao Yu Nanyang Technological University, 50 Nanyang Avenue, Block S3.2, Level B2, Singapore 639798. Tel.: (65) 6592 1844, Fax: (65) 6316 4416, E-mail: haoyu@ntu.edu.sg 2 H. Huang, S. K. Rai and H. Yu","",""
11,"G. Tsihrintzis, Dionisios N. Sotiropoulos, L. Jain","Machine Learning Paradigms: Advances in Data Analytics",2018,"","","","",95,"2022-07-13 09:22:41","","10.1007/978-3-319-94030-4_1","","",,,,,11,2.75,4,3,4,"","",""
14,"B. Ansell, B. Pope, P. Georgeson, Samantha J. Emery-Corbin, A. Jex","Annotation of the Giardia proteome through structure-based homology and machine learning",2018,"","","","",96,"2022-07-13 09:22:41","","10.1093/gigascience/giy150","","",,,,,14,3.50,3,5,4,"Abstract Background Large-scale computational prediction of protein structures represents a cost-effective alternative to empirical structure determination with particular promise for non-model organisms and neglected pathogens. Conventional sequence-based tools are insufficient to annotate the genomes of such divergent biological systems. Conversely, protein structure tolerates substantial variation in primary amino acid sequence and is thus a robust indicator of biochemical function. Structural proteomics is poised to become a standard part of pathogen genomics research; however, informatic methods are now required to assign confidence in large volumes of predicted structures. Aims Our aim was to predict the proteome of a neglected human pathogen, Giardia duodenalis, and stratify predicted structures into high- and lower-confidence categories using a variety of metrics in isolation and combination. Methods We used the I-TASSER suite to predict structural models for ∼5,000 proteins encoded in G. duodenalis and identify their closest empirically-determined structural homologues in the Protein Data Bank. Models were assigned to high- or lower-confidence categories depending on the presence of matching protein family (Pfam) domains in query and reference peptides. Metrics output from the suite and derived metrics were assessed for their ability to predict the high-confidence category individually, and in combination through development of a random forest classifier. Results We identified 1,095 high-confidence models including 212 hypothetical proteins. Amino acid identity between query and reference peptides was the greatest individual predictor of high-confidence status; however, the random forest classifier outperformed any metric in isolation (area under the receiver operating characteristic curve = 0.976) and identified a subset of 305 high-confidence-like models, corresponding to false-positive predictions. High-confidence models exhibited greater transcriptional abundance, and the classifier generalized across species, indicating the broad utility of this approach for automatically stratifying predicted structures. Additional structure-based clustering was used to cross-check confidence predictions in an expanded family of Nek kinases. Several high-confidence-like proteins yielded substantial new insight into mechanisms of redox balance in G. duodenalis—a system central to the efficacy of limited anti-giardial drugs. Conclusion Structural proteomics combined with machine learning can aid genome annotation for genetically divergent organisms, including human pathogens, and stratify predicted structures to promote efficient allocation of limited resources for experimental investigation.","",""
4687,"Yonghui Wu, M. Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, M. Krikun, Yuan Cao, Qin Gao, Klaus Macherey, J. Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Y. Kato, Taku Kudo, H. Kazawa, K. Stevens, George Kurian, Nishant Patil, Wei Wang, C. Young, Jason R. Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, G. Corrado, Macduff Hughes, J. Dean","Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",2016,"","","","",97,"2022-07-13 09:22:41","","","","",,,,,4687,781.17,469,31,6,"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.","",""
2,"Shubhankar Dutta, K. Bose","Remodelling structure-based drug design using machine learning.",2021,"","","","",98,"2022-07-13 09:22:41","","10.1042/ETLS20200253","","",,,,,2,2.00,1,2,1,"To keep up with the pace of rapid discoveries in biomedicine, a plethora of research endeavors had been directed toward Rational Drug Development that slowly gave way to Structure-Based Drug Design (SBDD). In the past few decades, SBDD played a stupendous role in identification of novel drug-like molecules that are capable of altering the structures and/or functions of the target macromolecules involved in different disease pathways and networks. Unfortunately, post-delivery drug failures due to adverse drug interactions have constrained the use of SBDD in biomedical applications. However, recent technological advancements, along with parallel surge in clinical research have led to the concomitant establishment of other powerful computational techniques such as Artificial Intelligence (AI) and Machine Learning (ML). These leading-edge tools with the ability to successfully predict side-effects of a wide range of drugs have eventually taken over the field of drug design. ML, a subset of AI, is a robust computational tool that is capable of data analysis and analytical model building with minimal human intervention. It is based on powerful algorithms that use huge sets of 'training data' as inputs to predict new output values, which improve iteratively through experience. In this review, along with a brief discussion on the evolution of the drug discovery process, we have focused on the methodologies pertaining to the technological advancements of machine learning. This review, with specific examples, also emphasises the tremendous contributions of ML in the field of biomedicine, while exploring possibilities for future developments.","",""
7,"Adrián Carballal, C. Fernandez-Lozano, J. Heras, Juan Romero","Transfer learning features for predicting aesthetics through a novel hybrid machine learning method",2019,"","","","",99,"2022-07-13 09:22:41","","10.1007/s00521-019-04065-4","","",,,,,7,2.33,2,4,3,"","",""
0,"Yifan Zhou, Peng Zhang","Quantum Machine Learning for Power System Stability Assessment",2021,"","","","",100,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,2,1,"Transient stability assessment (TSA), a cornerstone for resilient operations of today’s interconnected power grids, is a grand challenge yet to be addressed since the genesis of electric power systems. This paper is a confluence of quantum computing, data science and machine learning to potentially resolve the aforementioned challenge caused by high dimensionality, nonlinearity and uncertainty. We devise a quantum TSA (qTSA) method, a low-depth, high expressibility quantum neural network, to enable scalable and efficient data-driven transient stability prediction for bulk power systems. qTSA renders the intractable TSA straightforward and effortless in the Hilbert space, and provides rich information that enables unprecedentedly resilient and secure power system operations. Extensive experiments on quantum simulators and real quantum computers verify the accuracy, noise-resilience, scalability and universality of qTSA. qTSA underpins a solid foundation of a quantum-enabled, ultra-resilient power grid which will benefit the people as well as various commercial and industrial sectors. Introduction Texas’ and California’s rolling outages [1, 2] in recent years signaled that our existing power infrastructures can hardly sustain the ever-expanding communities and deep integration of low-inertia renewables [3, 4]. The situations are rapidly deteriorating as our power grids are increasingly integrating massive DERs, such as intermittent rooftop solar photovoltaics (PVs), as well as solar farms and offshore wind systems, and have been subject to more frequent weather events [5, 6]. A key technology to secure today’s bulk power grids is transient stability assessment (TSA) which aims to determine the ability of the system to ride-through large disturbances (contingencies) and to reach the post-contingency steady-state [7]. Transient instability is a fast phenomenon typically taking only a few seconds for the bulk system to collapse after contingencies occur. Due to this very nature, system operators in the control center never have sufficient time to steer the power system away from instability upon the occurrence of contingencies. For this reason, we have to rely on computer-based TSA without any manual interaction from human operators to assess the transient stability of the system [8]. TSA, however, is a grand challenge yet to be addressed since the genesis of power systems in the era of Tesla, Edison and Westinghouse [9]. Interconnected power systems are the largest and most complicated man-made dynamical systems on this planet. Those bulk systems are highly nonlinear, exhibit multi-scale behaviors spatially and temporarily, and are increasingly stochastic and uncertain due to deep integration of renewable energy resources. The majority of the TSA methods being used in power industry rely on the explicit integration or implicit integration of differential equation models of the bulk power systems, which are known to be intractable to handle large power systems [10–12]. Discrete events such as frequent plug-and-plays of renewables, microgrids and loads in today’s power systems [13, 14] and unknown models due to data privacy issues [15] make existing TSA methods even more computationally formidable even if they are executed on the powerful and expensive real-time simulators. Even worse, a large number of power system TSA must be conducted to examine the stability of the system in relation to massive ‘N− k’ contingencies (k components have failed in a power system with N components), which further impedes the application of TSA in real-world power system operations. All the aforementioned challenges have made classical TSA prohibitively difficult for the online operation of large interconnected grids. Today’s power systems are undergoing an Enlightenment, where the confluence of big data, quantum computing and machine learning altogether is to drive a regime shift in the analysis and operation of our critical power infrastructures. Big data is the force behind the revolution: massive new types of intelligent electronic sensors such as synchronized phasor measurement units (PMUs), advanced metering infrastructure (AMI) meters and remote terminal units (RTUs) [16] are continuously generating gigantic volumes of data which allow for the development of data-driven power system analytics. Most recently, the successes in exploiting the potential of quantum supremacy [17, 18] shed lights on a ‘quantum leap’ of computing capabilities. The power of quantum computing is derived from its ability to prepare and maintain complex superpositions of ar X iv :2 10 4. 04 85 5v 2 [ qu an tph ] 2 3 M ay 2 02 1 quantum states across many quantum degrees of freedom. While classically the number of required physical resources N grows exponentially with the system complexity n, N grows linearly with n in a quantum computer, resulting in exponential speedups over classical computing. Furthermore, highly entangled states, very difficult to represent on classical computers, are easily represented on a quantum computer [19, 20]. Therefore, the intractable power system problems aforementioned, if formulated properly through programmable quantum circuits, can be executed efficiently on a quantum computer. Inheriting the exponential speedup of quantum computing in tensor manipulation [21], the swift growth in quantum machine learning (QML) techniques [22–24] ignites new hopes of developing unprecedentedly scalable and efficient data-driven power system analytics. QML is promisingly efficacious for data processing and model training in high-dimensional space that are intractable for classical algorithms [25, 26]. Ideally, unique quantum operators such as superposition and entanglement, which can not be represented by classical operators, enable a superior representation of complicated data relationships [27–29]. Nevertheless, the existing noisy quantum devices are still restrictive, hindering the implementing of QML if deep quantum circuits are needed. This paper is the first attempt to unlock the potential of QML for power system TSA. A low-depth, high expressibility quantum neural network (QNN)-based transient stability assessment (qTSA) method is devised to enable scalable, reliable and efficient data-driven transient stability prediction. In particular, we are focusing on designing an efficient qTSA circuit that is feasible to pursue on near-term devices, considering the noisy-intermediate-scale quantum (NISQ) era [30, 31], but general enough to be directly expandable to the noise-free quantum computer of a distant future (5-10 years). We have designed systematical studies which have demonstrated the robustness, accuracy and fidelity of qTSA on real-scale power systems. Our qTSA has shown consistently high performance on quantum computers at different noise levels. Stability assessment plays a central role in nearly every field of the life sciences, physics and engineering. Our qTSA therefore is promising to positively impact the modeling, analysis, controlling and securing various high-dimensional, nonlinear, hybrid dynamical systems. In particular, qTSA underpins a solid foundation of a quantum-enabled resilient power grid, i.e., tomorrow’s unprecedentedly autonomic power infrastructure towards self-configuration, self-healing, self-optimization and selfprotection against grid changes, renewable power injections, faults, disastrous events and cyberattacks. Such a quantum-enabled grid will benefit the people as well as various commercial and industrial sectors. Results Power system transients are generically modelled as a set of nonlinear differential algebraic equations: { Ẋ = FD(X ,Y ) 0 = FA(X ,Y ) (1a) (1b) where X and Y separately denote the differential variables and algebraic variables; (1a) formulates the nonlinear dynamics of power devices, such as generators (e.g., synchronous machines, distributed energy resources), controllers (e.g., governors, exciters, inverters), power loads, etc; (1b) formulates the instantaneous power flow of the entire power grid. TSA appraises a power system’s capability of resisting large disturbance [7]. Denote Z = (X ,Y ), and φ(t,Z) as the orbit of (1) starting from Z. An asymptotically stable equilibrium point (SEP) Zs of (1) satisfies that: (a) Zs is Lyapunov stable; (b) there exists an open neighborhood O of Zs such that ∀Z ∈O converges to Zs when t approaches infinity [32]. The stability region of Zs encloses all the states that can be attracted by Zs within an infinite time: A(Zs) = {Z ∈ Rn : lim t→∞ φ(t,Z) = Zs} (2) Stability region theory states that system stability after a large disturbance is determined by whether the post-disturbance state is within the stability region of an SEP [32]. Therefore, to formulate the data-driven TSA, the idea is to establish a direct mapping between the post-disturbance power system states and the stability results [33, 34]. The keystone of qTSA, different from classical machine learning techniques, is that the transient stability features in a Euclidean space Z ∈ E are embedded into quantum states in a Hilbert space |ψ〉 ∈H through a variational quantum circuit (VQC), which serves as a QNN to explicitly separate the stable and unstable samples. Our key innovation is a design of low-depth, high expressibility qTSA, as presented in Fig. 1 (see also Methods). Fig. 1(a) first visualizes the low-depth variational qTSA circuit which addresses the dimensionality and nonlinearity obstacles in TSA as well as the nonnegligible noise and source limitations on near-term quantum devices. Kernel ingredients in qTSA circuit include (see Methods): 1) Non-Gaussian feature encoding: |ψE〉=UE(pE ,Z) |0〉 which adopts parameterized, activation-enhanced quantum gates to enable a flexible, nonlinear and dimension-free encoding of power system stability features Z;","",""
0,"Yifan Zhou, Peng Zhang","Quantum Machine Learning for Power System Stability Assessment",2021,"","","","",101,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,2,1,"Transient stability assessment (TSA), a cornerstone for resilient operations of today’s interconnected power grids, is a grand challenge yet to be addressed since the genesis of electric power systems. This paper is a confluence of quantum computing, data science and machine learning to potentially resolve the aforementioned challenge caused by high dimensionality, nonlinearity and uncertainty. We devise a quantum TSA (qTSA) method, a low-depth, high expressibility quantum neural network, to enable scalable and efficient data-driven transient stability prediction for bulk power systems. qTSA renders the intractable TSA straightforward and effortless in the Hilbert space, and provides rich information that enables unprecedentedly resilient and secure power system operations. Extensive experiments on quantum simulators and real quantum computers verify the accuracy, noise-resilience, scalability and universality of qTSA. qTSA underpins a solid foundation of a quantum-enabled, ultra-resilient power grid which will benefit the people as well as various commercial and industrial sectors. Introduction Texas’ and California’s rolling outages [1, 2] in recent years signaled that our existing power infrastructures can hardly sustain the ever-expanding communities and deep integration of low-inertia renewables [3, 4]. The situations are rapidly deteriorating as our power grids are increasingly integrating massive DERs, such as intermittent rooftop solar photovoltaics (PVs), as well as solar farms and offshore wind systems, and have been subject to more frequent weather events [5, 6]. A key technology to secure today’s bulk power grids is transient stability assessment (TSA) which aims to determine the ability of the system to ride-through large disturbances (contingencies) and to reach the post-contingency steady-state [7]. Transient instability is a fast phenomenon typically taking only a few seconds for the bulk system to collapse after contingencies occur. Due to this very nature, system operators in the control center never have sufficient time to steer the power system away from instability upon the occurrence of contingencies. For this reason, we have to rely on computer-based TSA without any manual interaction from human operators to assess the transient stability of the system [8]. TSA, however, is a grand challenge yet to be addressed since the genesis of power systems in the era of Tesla, Edison and Westinghouse [9]. Interconnected power systems are the largest and most complicated man-made dynamical systems on this planet. Those bulk systems are highly nonlinear, exhibit multi-scale behaviors spatially and temporarily, and are increasingly stochastic and uncertain due to deep integration of renewable energy resources. The majority of the TSA methods being used in power industry rely on the explicit integration or implicit integration of differential equation models of the bulk power systems, which are known to be intractable to handle large power systems [10–12]. Discrete events such as frequent plug-and-plays of renewables, microgrids and loads in today’s power systems [13, 14] and unknown models due to data privacy issues [15] make existing TSA methods even more computationally formidable even if they are executed on the powerful and expensive real-time simulators. Even worse, a large number of power system TSA must be conducted to examine the stability of the system in relation to massive ‘N− k’ contingencies (k components have failed in a power system with N components), which further impedes the application of TSA in real-world power system operations. All the aforementioned challenges have made classical TSA prohibitively difficult for the online operation of large interconnected grids. Today’s power systems are undergoing an Enlightenment, where the confluence of big data, quantum computing and machine learning altogether is to drive a regime shift in the analysis and operation of our critical power infrastructures. Big data is the force behind the revolution: massive new types of intelligent electronic sensors such as synchronized phasor measurement units (PMUs), advanced metering infrastructure (AMI) meters and remote terminal units (RTUs) [16] are continuously generating gigantic volumes of data which allow for the development of data-driven power system analytics. Most recently, the successes in exploiting the potential of quantum supremacy [17, 18] shed lights on a ‘quantum leap’ of computing capabilities. The power of quantum computing is derived from its ability to prepare and maintain complex superpositions of ar X iv :2 10 4. 04 85 5v 2 [ qu an tph ] 2 3 M ay 2 02 1 quantum states across many quantum degrees of freedom. While classically the number of required physical resources N grows exponentially with the system complexity n, N grows linearly with n in a quantum computer, resulting in exponential speedups over classical computing. Furthermore, highly entangled states, very difficult to represent on classical computers, are easily represented on a quantum computer [19, 20]. Therefore, the intractable power system problems aforementioned, if formulated properly through programmable quantum circuits, can be executed efficiently on a quantum computer. Inheriting the exponential speedup of quantum computing in tensor manipulation [21], the swift growth in quantum machine learning (QML) techniques [22–24] ignites new hopes of developing unprecedentedly scalable and efficient data-driven power system analytics. QML is promisingly efficacious for data processing and model training in high-dimensional space that are intractable for classical algorithms [25, 26]. Ideally, unique quantum operators such as superposition and entanglement, which can not be represented by classical operators, enable a superior representation of complicated data relationships [27–29]. Nevertheless, the existing noisy quantum devices are still restrictive, hindering the implementing of QML if deep quantum circuits are needed. This paper is the first attempt to unlock the potential of QML for power system TSA. A low-depth, high expressibility quantum neural network (QNN)-based transient stability assessment (qTSA) method is devised to enable scalable, reliable and efficient data-driven transient stability prediction. In particular, we are focusing on designing an efficient qTSA circuit that is feasible to pursue on near-term devices, considering the noisy-intermediate-scale quantum (NISQ) era [30, 31], but general enough to be directly expandable to the noise-free quantum computer of a distant future (5-10 years). We have designed systematical studies which have demonstrated the robustness, accuracy and fidelity of qTSA on real-scale power systems. Our qTSA has shown consistently high performance on quantum computers at different noise levels. Stability assessment plays a central role in nearly every field of the life sciences, physics and engineering. Our qTSA therefore is promising to positively impact the modeling, analysis, controlling and securing various high-dimensional, nonlinear, hybrid dynamical systems. In particular, qTSA underpins a solid foundation of a quantum-enabled resilient power grid, i.e., tomorrow’s unprecedentedly autonomic power infrastructure towards self-configuration, self-healing, self-optimization and selfprotection against grid changes, renewable power injections, faults, disastrous events and cyberattacks. Such a quantum-enabled grid will benefit the people as well as various commercial and industrial sectors. Results Power system transients are generically modelled as a set of nonlinear differential algebraic equations: { Ẋ = FD(X ,Y ) 0 = FA(X ,Y ) (1a) (1b) where X and Y separately denote the differential variables and algebraic variables; (1a) formulates the nonlinear dynamics of power devices, such as generators (e.g., synchronous machines, distributed energy resources), controllers (e.g., governors, exciters, inverters), power loads, etc; (1b) formulates the instantaneous power flow of the entire power grid. TSA appraises a power system’s capability of resisting large disturbance [7]. Denote Z = (X ,Y ), and φ(t,Z) as the orbit of (1) starting from Z. An asymptotically stable equilibrium point (SEP) Zs of (1) satisfies that: (a) Zs is Lyapunov stable; (b) there exists an open neighborhood O of Zs such that ∀Z ∈O converges to Zs when t approaches infinity [32]. The stability region of Zs encloses all the states that can be attracted by Zs within an infinite time: A(Zs) = {Z ∈ Rn : lim t→∞ φ(t,Z) = Zs} (2) Stability region theory states that system stability after a large disturbance is determined by whether the post-disturbance state is within the stability region of an SEP [32]. Therefore, to formulate the data-driven TSA, the idea is to establish a direct mapping between the post-disturbance power system states and the stability results [33, 34]. The keystone of qTSA, different from classical machine learning techniques, is that the transient stability features in a Euclidean space Z ∈ E are embedded into quantum states in a Hilbert space |ψ〉 ∈H through a variational quantum circuit (VQC), which serves as a QNN to explicitly separate the stable and unstable samples. Our key innovation is a design of low-depth, high expressibility qTSA, as presented in Fig. 1 (see also Methods). Fig. 1(a) first visualizes the low-depth variational qTSA circuit which addresses the dimensionality and nonlinearity obstacles in TSA as well as the nonnegligible noise and source limitations on near-term quantum devices. Kernel ingredients in qTSA circuit include (see Methods): 1) Non-Gaussian feature encoding: |ψE〉=UE(pE ,Z) |0〉 which adopts parameterized, activation-enhanced quantum gates to enable a flexible, nonlinear and dimension-free encoding of power system stability features Z;","",""
3,"Shreya Gupta, Tapan K. Gandhi","Identification of Neural Correlates of Face Recognition Using Machine Learning Approach",2019,"","","","",102,"2022-07-13 09:22:41","","10.1007/978-981-13-8798-2_2","","",,,,,3,1.00,2,2,3,"","",""
3,"Finn Kuusisto, V. S. Costa, Zhonggang Hou, James A. Thomson, David Page, R. Stewart","Machine Learning to Predict Developmental Neurotoxicity with High-Throughput Data from 2D Bio-Engineered Tissues",2019,"","","","",103,"2022-07-13 09:22:41","","10.1109/ICMLA.2019.00055","","",,,,,3,1.00,1,6,3,"There is a growing need for fast and accurate methods for testing developmental neurotoxicity across several chemical exposure sources. Current approaches, such as in vivo animal studies, and assays of animal and human primary cell cultures, suffer from challenges related to time, cost, and applicability to human physiology. Prior work has demonstrated success employing machine learning to predict developmental neurotoxicity using gene expression data collected from human 3D tissue models exposed to various compounds. The 3D model is biologically similar to developing neural structures, but its complexity necessitates extensive expertise and effort to employ. By instead focusing solely on constructing an assay of developmental neurotoxicity, we propose that a simpler 2D tissue model may prove sufficient. We thus compare the accuracy of predictive models trained on data from a 2D tissue model with those trained on data from a 3D tissue model, and find the 2D model to be substantially more accurate. Furthermore, we find the 2D model to be more robust under stringent gene set selection, whereas the 3D model suffers substantial accuracy degradation. While both approaches have advantages and disadvantages, we propose that our described 2D approach could be a valuable tool for decision makers when prioritizing neurotoxicity screening.","",""
4,"T. Schmid","Deconstructing the Final Frontier of Artificial Intelligence: Five Theses for a Constructivist Machine Learning",2019,"","","","",104,"2022-07-13 09:22:41","","","","",,,,,4,1.33,4,1,3,"Ambiguity and diversity in human cognition can be regarded a final frontier in developing equivalent systems of artificial intelligence. Despite astonishing accomplishments, modern machine learning algorithms are still hardly more than adaptive systems. Deep neural networks, for example, represent complexity through complex connectivity but are not able to allow for abstraction and differentiation of interpretable knowledge, i.e., for key mechanisms of human cognition. Like support vector machines, random forests and other statistically motivated algorithms, they do neither reflect nor yield structures and strategies of human thinking. Therefore, we suggest to realign the use of existing machine learning tools with respect to the philosophical paradigm of constructivism, which currently is the key concept in human learning and professional teaching. Based on the idea that learning units like classifiers can be considered models with limited validity, we formulate five principles to guide a constructivist machine learning. We describe how to define such models and model limitations, how to relate them and how relationships allow to abstract and differentiate models. To this end, we propose the use of meta data for classifiers and other models. Moreover, we argue that such meta data-based machine learning results in a knowledge base that is both created by the means of automation and interpretable for humans. Over the last decade, it has become widely accepted to address computational systems intelligent. Not only journalists, but also scientists have adapted this habit in their publications. In fact, many classical engineering tasks like monitoring or regulating have profited from the employment of machine learning (Abellan-Nebot and Romero Subirón 2010; Mohanraj, Jayaraj, and Muraleedharan 2012). The same holds true for pattern recognition, most prominently in automated image and video analysis (Zafeiriou, Zhang, and Zhang 2015; Yang et al. 2011). And even though ultimate challenges like the infamous Turing test are left unsatisfied (You 2015), some exceptional results in specialized tasks like playing the game of go (Silver et al. 2016) make current learning machines look intelligent on a human level. Copyright held by the author(s). In A. Martin, K. Hinkelmann, A. Gerber, D. Lenat, F. van Harmelen, P. Clark (Eds.), Proceedings of the AAAI 2019 Spring Symposium on Combining Machine Learning with Knowledge Engineering (AAAI-MAKE 2019). Stanford University, Palo Alto, California, USA, March 25-27, 2019. A final frontier for learning systems, however, is the variety of alternative cognitive functions observable in a diverse set of individuals or from ambiguous stimuli (Kornmeier and Bach 2012). While philosophy has acknowledged and embraced the subjectivity and limitations of human cognition during the last decades (Prawat and Floden 1994), current learning systems regard cognition a complex, yet technical task to be solved. In particular, established algorithms do neither provide convincing answers to the challenges provided by an ambiguous environment; nor do they offer concepts that explicitly allow for contradictory judgements comparable to differences in social perception. The main reason for this shortcoming is that so far both algorithms and researchers have failed to incorporate a constructivist point of view. Constructivism implies not only cognition to be a highly individual phenomenon, but also humans to take an active role in their perception of the world – and that there is no such thing as a human-independent reality (Reich 2009). Yet algorithms and applications aiming to predict things other than laws of nature are implicitly founded on exactly this outdated asumption. In the following, we introduce axioms that allow machine learning to follow constructivist principles. Key features of this approach are the use of modern tools from empirical sciences, model-oriented learning, the ability to handle ambiguity, the ability to integrate supervised and unsupervised learning into a unified framework, the ability to create an individual knowledge base and the ability to abstract, differentiate or discard learned knowledge automatically. 1. The key component of cognitive functionality is a model. Since the introduction of artificial neural networks as a theoretical concept (McCulloch and Pitts 1943), many mathematicians and computer scientists have considered neurons the key component of learning systems. In education and psychology, however, cognitive functions are often seen as certain skills or abilities acquired and exposed by an individual human and described in terms like the concept of competence, which, e.g., is widely used in the modern European education system (Méhaut and Winch 2012). Functionalistic psychology explains cognitive functions of humans by the concept of mental models (Rouse and Morris 1986). Initially, mental models have been used to understand motor control, e.g., of hand movements (Veldhuyzen and Stassen 1977). In a more general sense, however, mental models are described as “hypothetical constructs” (Wickens 2000) that can be ordered hierarchically (Rasmussen 1979) and allow a human to make predictions about his physical and social environment (Oatley 1985). It has also been postulated that such models cannot be of static nature but rather underlay continuous modifications (Oatley 1985). Philosophers, too, consider models an important tool in human knowledge acquisition (Klaus 1967, p. 412) or even the only tool, respectively (Stachowiak 1973, p. 56). While varying and concurring theoretical definitions exist, most model concepts assume an image, an origin of the image and a relationship between them. This definition is, e.g., matched by the idea of mathematical modeling as proposed by Heinrich Hertz and others (Hertz 1894; Hamilton 1982). With the rise of robotics and artificial intelligence, engineers have adapted and extended this idea by postulating the concept of a cybernetic model, which involves a generalized subject and an object of the model (Rose 2009). Cybernetics, however, did neither reflect time-related aspects nor issues involved with individual model subjects. This matter was adressed by Herbert Stachowiak, who was influenced by cybernetics when developing his General Model Theory (Hof 2018). He postulated any model to be limited to specific subjects, specific temporal ranges and specific purposes (Stachowiak 1973, p. 133). Limitations, to this end, are considered a matter of fact rather than a matter of definition. Thus, such models circumvent ambiguity by viewing an otherwise ambiguous model with unknown validity limits as a number of models of limited validity. 2. Learning constitutes from constructing, reconstructing or deconstructing models. Modern education is dominated by the ideas of constructivism and constructivist learning (Fox 2001). At its heart, this approach is based on the assumption that humans acquire knowledge and competences actively and individually through processes called construction, reconstruction and deconstruction (Duffy and Jonassen 1992). Construction is associated with creation, innovation and production and implies searching for variations, combinations or transfers of knowledge (Reich 2004, p. 145). Analogously, reconstruction is associated with application, repetition or imitation and implies searching for order, patterns or models (Reich 2004, p. 145). Deconstruction is in the context of constructivism associated with reconsideration, doubt and modification and implies searching for omissions, additions and defective parts of acquired knowledge (Reich 2004, p. 145). Learning algorithms have been used for half a century to transform sample data into models in a mathematical sense, that is: into generalized mathematical relationships between image and origin. The two major approaches or objectives, known as supervised and unsupervised learning, either do or do not require a given target parameter. Artificial neural networks and their relatives are among the most popular and prominent algorithms for learning with a given target parameter (Singh, Thakur, and Sharma 2016), but statistically motivated approaches like support vector machines (Cristianini and Shawe-Taylor 2000) or random forests (Breiman 2001) are also widely used for supervised learning; a specialized field of supervised learning is reinforcement learning, which is popular in robotics (Kober and Peters 2012) and adaptive control (Lewis, Vrabie, and Vamvoudakis 2012). For unsupervised learning, too, biologically inspired approaches like self-organizing maps (Kohonen 2001) as well as statistically motivated approaches like k-means (Jain 2010) are employed. To some extent, machine learning parallels modern education concepts. A construction process in the constructivist sense may be matched by an unsupervised learning, i.e., identifying clusterings or dimensionality reduction, and can, e.g., be implemented with self-organizing maps, kmeans, autoencoders or feature clustering (Schmid 2018). A reconstruction process in the constructivist sense may be matched by a supervised learning, i.e., classification or regression tasks, and can, e.g., be implemented with artificial neural networks or random forests (Schmid 2018). Few researchers, however, have discussed a constructivist approach to machine learning (Drescher 1989; Quartz 1993), and even less how to design a deconstruction process. While domainspecific applications with manual re-engineering options exist (Herbst and Karagiannis 2000), to the best of our knowledge, there is currently only one working implementation of an algorithmic deconstruction process (Schmid 2018). 3. Deconstructing models computationally requires model-based meta data. In order to automate and implement a deconstruction process, successfully learned models must be held available for comparison or re-training. More over, possible matchings with n","",""
4,"S. Nomm, Alejandro Guerra-Manzanares, Hayretdin Bahsi","Towards the Integration of a Post-Hoc Interpretation Step into the Machine Learning Workflow for IoT Botnet Detection",2019,"","","","",105,"2022-07-13 09:22:41","","10.1109/ICMLA.2019.00193","","",,,,,4,1.33,1,3,3,"The analysis of the interplay between the feature selection and the post-hoc local interpretation steps in a machine learning workflow followed for IoT botnet detection constitutes the research scope of the present paper. While the application of machine learning-based techniques has become a trend in cyber security, the main focus has been almost on detection accuracy. However, providing the relevant explanation for a detection decision is a vital requirement in a tiered incident handling processes of the contemporary security operations centers. Moreover, the design of intrusion detection systems in IoT networks has to take the limitations of the computational resources into consideration. Therefore, resource limitations in addition to human element of incident handling necessitate considering feature selection and interpretability at the same time in machine learning workflows. In this paper, first, we analyzed the selection of features and its implication on the data accuracy. Second, we investigated the impact of feature selection on the explanations generated at the post-hoc interpretation phase. We utilized a filter method, Fisher's Score and Local Interpretable Model-Agnostic Explanation (LIME) at feature selection and post-hoc interpretation phases, respectively. To evaluate the quality of explanations, we proposed a metric that reflects the need of the security analysts. It is demonstrated that the application of both steps for the particular case of IoT botnet detection may result in highly accurate and interpretable learning models induced by fewer features. Our metric enables us to evaluate the detection accuracy and interpretability in an integrated way.","",""
1,"N. Riel","Integrated machine learning and mechanistic modelling of Metabolic Syndrome development and dynamics",2019,"","","","",106,"2022-07-13 09:22:41","","","","",,,,,1,0.33,1,1,3,"Metabolic derailments associated with metabolic syndrome and type 2 diabetes can be studied with mixed meal tests (MMT’s). The plasma metabolome enriched with measurements of other biomarkers, such as hormones and cytokines, provide valuable information about the physiological state of an individual. An increasingly important, but complex task is to extract biomedical parameters with diagnostic value from large and multivariate datasets. We applied model-based data processing and analysis, combining computer simulation models of the human physiological system, stochastic models of uncertainties and machine learning of time-series data obtained from repeated blood sampling during MMT’s. In a mouse model of metabolic syndrome we identified differences in lipid metabolism to be associated with variation in weight gain and development of NAFLD (fatty liver disease). The computational model predicted the progression of dyslipidemia to be linked to bile acids, which was confirmed in a validation study including a larger group of mice that were followed for a longer period of time. To investigate the role of bile acids in humans with metabolic syndrome a detailed simulation model of bile acid metabolism and physiology was developed. The dozens of different bile acid species present in blood are to a large extent produced by gut bacteria. Model-based analysis of plasma bile acids provides a metabolic ‘window’ on the gut microbiome and other digestive processes in the gastrointestinal tract. The model was applied to simulate bariatric surgery in patients with metabolic syndrome. The model predicts changes in bile acid concentrations and dynamics in the small intestine to result in a stronger and faster GLP-1 response, hence insulin secretion, explaining observations of rapid glycemic improvement after surgery. The simulation model turned out to be sufficiently robust that personalized variants for individual patients could be made, using MMT plasma bile acid metabolomics as input.","",""
3,"Qi Yang, Siheng Chen, Yiling Xu, Jun Sun, M. Salman Asif, Zhan Ma","Point Cloud Distortion Quantification based on Potential Energy for Human and Machine Perception",2021,"","","","",107,"2022-07-13 09:22:41","","","","",,,,,3,3.00,1,6,1,"Distortion quantification of point clouds plays a stealth, yet vital role in a wide range of human and machine perception tasks. For human perception tasks, a distortion quantification can substitute subjective experiments to guide 3D visualization; while for machine perception tasks, a distortion quantification can work as a loss function to guide the training of deep neural networks for unsupervised learning tasks. To handle a variety of demands in many applications, a distortion quantification needs to be distortion discriminable, differentiable, and have a low computational complexity. Currently, however, there is a lack of a general distortion quantification that can satisfy all three conditions. To fill this gap, this work proposes multiscale potential energy discrepancy (MPED), a distortion quantification to measure point cloud geometry and color difference. The proposed MPED is able to capture both geometrical and color impairments by quantifying the total distortion between reference and distorted samples. By evaluating at various neighborhood sizes, the proposed MPED achieves global-local tradeoffs, capturing distortion in a multiscale fashion. We further theoretically show that classical Chamfer distance is a special case of our MPED. Extensive experimental studies validate MPED’s superiority for both human and machine perception tasks. For human perception tasks, the proposed MPED works as subjective score predictor. In terms of Spearman rank-order correlation coefficient, MPED is 4% to 35% better than other state-of-the-art distortion quantifications on SJTU-PCQA database, and 27% to 190% on LSPCQA database. For machine perception tasks, the proposed MPED is plugged in as the loss function to enable the training of deep neural networks for three tasks, including point cloud reconstruction, shape completion and upsampling. The experimental results reveal that the proposed MPED produces better results than the point-wise Chamfer distance and Earth Mover’s distance under the same network architecture. For instance, in point cloud reconstruction, MPED is over 80% and 70% better than Chamfer distance and Earth Mover’s distance in terms of Jensen-Shannon divergence, respectively. We further study the robustness and convergence rate of MPED in ablation study, and the results show that: i) MPED is robust to the variations in color space, neighborhood scale and spatial field; ii) MPED can converge to stable results with less training time and epochs using the same network model. Our code is avaliable at https://github.com/Qi-Yangsjtu/MPED.","",""
456,"Amir Mosavi, Pınar Öztürk, K. Chau","Flood Prediction Using Machine Learning Models: Literature Review",2018,"","","","",108,"2022-07-13 09:22:41","","10.3390/w10111536","","",,,,,456,114.00,152,3,4,"Floods are among the most destructive natural disasters, which are highly complex to model. The research on the advancement of flood prediction models contributed to risk reduction, policy suggestion, minimization of the loss of human life, and reduction of the property damage associated with floods. To mimic the complex mathematical expressions of physical processes of floods, during the past two decades, machine learning (ML) methods contributed highly in the advancement of prediction systems providing better performance and cost-effective solutions. Due to the vast benefits and potential of ML, its popularity dramatically increased among hydrologists. Researchers through introducing novel ML methods and hybridizing of the existing ones aim at discovering more accurate and efficient prediction models. The main contribution of this paper is to demonstrate the state of the art of ML models in flood prediction and to give insight into the most suitable models. In this paper, the literature where ML models were benchmarked through a qualitative analysis of robustness, accuracy, effectiveness, and speed are particularly investigated to provide an extensive overview on the various ML algorithms used in the field. The performance comparison of ML models presents an in-depth understanding of the different techniques within the framework of a comprehensive evaluation and discussion. As a result, this paper introduces the most promising prediction methods for both long-term and short-term floods. Furthermore, the major trends in improving the quality of the flood prediction models are investigated. Among them, hybridization, data decomposition, algorithm ensemble, and model optimization are reported as the most effective strategies for the improvement of ML methods. This survey can be used as a guideline for hydrologists as well as climate scientists in choosing the proper ML method according to the prediction task.","",""
1,"C. He, M. Mahfouf, Luis A. Torres-Salomao","An Adaptive General Type-2 Fuzzy Logic Approach for Psychophysiological State Modeling in Real-Time Human–Machine Interfaces",2021,"","","","",109,"2022-07-13 09:22:41","","10.1109/THMS.2020.3027531","","",,,,,1,1.00,0,3,1,"In this article, a new type-2 fuzzy-based modeling approach is proposed to assess human operators’ psychophysiological states for both safety and reliability of human–machine interface systems. Such a new modeling technique combines type-2 fuzzy sets with state tracking to update the rule base through a Bayesian process. These new configurations successfully lead to an adaptive, robust, and transparent computational framework that can be utilized to identify dynamic (i.e., real time) features without prior training. The proposed framework is validated on mental arithmetic cognitive real-time experiments with ten participants. It is found that the proposed framework outperforms other paradigms (i.e., an adaptive neuro-fuzzy inference system and an adaptive general type-2 fuzzy c-means modeling approach) in terms of disturbance rejection and learning capabilities. The proposed framework achieved the best performance compared to other models that have been presented in the related literature. Therefore, the new framework can be a promising development in human–machine interface systems. It can be further utilized to develop advanced control mechanisms, investigate the origins of human compromised task performance, and identify and remedy psychophysiological breakdown in the early stages.","",""
0,"E. Karampotsis, G. Dounias, J. Jantzen","Machine Learning Approaches for Pap-Smear Diagnosis: An Overview",2019,"","","","",110,"2022-07-13 09:22:41","","10.1007/978-3-030-15628-2_4","","",,,,,0,0.00,0,3,3,"","",""
3,"Sean Mullane, Ruoyan Chen, Sridhar Vemulapalli, Eli J. Draizen, Ke Wang, C. Mura, P. Bourne","Machine Learning for Classification of Protein Helix Capping Motifs",2019,"","","","",111,"2022-07-13 09:22:41","","10.1109/SIEDS.2019.8735646","","",,,,,3,1.00,0,7,3,"The biological function of a protein stems from its 3-dimensional structure, which is thermodynamically determined by the energetics of interatomic forces between its amino acid building blocks (the order of amino acids, known as the sequence, defines a protein). Given the costs (time, money, human resources) of determining protein structures via experimental means like X-ray crystallography, can we better describe and compare protein 3D structures in a robust and efficient manner, so as to gain meaningful biological insights? We begin by considering a relatively simple problem, limiting ourselves to just protein secondary structural elements. Historically, many computational methods have been devised to classify amino acid residues in a protein chain into one of several discrete “secondary structures”, of which the most well-characterized are the geometrically regular $\mathbf{a}$-helix and $\boldsymbol{\beta}$-sheet; irregular structural patterns, such as ‘turns’ and ‘loops’, are less understood. Here, we present a study of Deep Learning techniques to classify the loop-like end cap structures which delimit a-helices. Previous work used highly empirical and heuristic methods to manually classify helix capping motifs. Instead, we use structural data directly—including (i) backbone torsion angles computed from 3D structures, (ii) macromolecular feature sets (e.g., physicochemical properties), and (iii) helix cap classification data (from CAPS-DB)—as the ground truth to train a bidirectional long short–term memory (BiLSTM) model to classify helix cap residues. We tried different network architectures and scanned hyperparameters in order to train and assess several models; we also trained a Support Vector Classifier (SVC) to use as a baseline. Ultimately, we achieved 85% class-balanced accuracy with a deep BiLSTM model.","",""
0,"Mai Xu, Lai Jiang, Zhiguo Ding","Machine-learning-based saliency detection and its video decoding application in wireless multimedia communications",2019,"","","","",112,"2022-07-13 09:22:41","","10.1049/PBTE081E_CH9","","",,,,,0,0.00,0,3,3,"Saliency detection has been widely studied to predict human fixations, with various applications in wireless multimedia communications. For saliency detection, we argue that the state-of-the-art high-efficiency video-coding (HEVC) standard can be used to generate the useful features in compressed domain. Therefore, this chapter proposes to learn the video-saliency model, with regard to HEVC features. First, we establish an eye-tracking database for video-saliency detection. Through the statistical analysis on our eye-tracking database, we find out that human fixations tend to fall into the regions with large-valued HEVC features on splitting depth, bit allocation, and motion vector (MV). In addition, three observations are obtained from the further analysis on our eyetracking database. Accordingly, several features in HEVC domain are proposed on the basis of splitting depth, bit allocation, and MV. Next, a support vector machine (SVM) is learned to integrate those HEVC features together, for video-saliency detection. Since almost all video data are stored in the compressed form, our method is able to avoid both the computational cost on decoding and the storage cost on raw data. More importantly, experimental results show that the proposed method is superior to other state-of-the-art saliency-detection methods, either in compressed or uncompressed domain.","",""
7,"Hantao Huang, Rai Suleman Khalid, Hao Yu","Distributed Machine Learning on Smart-Gateway Network Towards Real-Time Indoor Data Analytics",2017,"","","","",113,"2022-07-13 09:22:41","","10.1007/978-3-319-53474-9_11","","",,,,,7,1.40,2,3,5,"","",""
1,"Dr. Anuraj Nayarisseri, Ravina Khandelwal","A Machine learning approach for the prediction of efficient iPSC modeling",2019,"","","","",114,"2022-07-13 09:22:41","","10.3390/mol2net-05-06376","","",,,,,1,0.33,1,2,3,"Cancer stands as major cause of mortality in the world, and it's the morbidity has significantly increased in both developing and developed nations. In spite of the recent advancement in cancer therapies, the clinical follow-up still lags far behind.Recent studies show that, stem cells are bestowed with distinctive functions, like tumor cells relocation, immunosuppression and production of bioactive elements, that helps in cancer targeting that bypassobstacles.Recent understanding show that Preclinical stem cell-based strategies has proved potential for targeted anti-tumor therapy applications. Stem cell applications in modulation and remodeling of immune system happens to be frequent procedure used past ten years in successfully treating tumor.Generation of human somatic cells into induced pluripotent stem cells (iPSCs) has often been a time consuming laborious intensive and expensive process. Additionally, the major problem with iPSCs is their tendency to revert to original somatic state. Hence, a robust computational model in discovering genes/molecules necessary for iPSC generation and maintenance can be a major leap towards in stem cell research.The synergistic combination of genetic relationship data, advanced computing hardware and nonlinear algorithms and could make artificially-induced pluripotent stem cells (aiPSC) a near future reality. Genes or proteins that are known to be essential in human pluripotent stem cells (hPSC) could possibly be used for system modelling. The present investigation is aimed to develop an unsupervised deep machine learning technology for the prediction of genes relevant in aiPSC production and its maintenance for both common and rare diseases making it a cost-effective approach.","",""
10,"David Zabala-Blanco, M. Mora, César A. Azurdia-Meza, A. Firoozabadi, Pablo Palacios Játiva, I. Soto","Relaxation of the Radio-Frequency Linewidth for Coherent-Optical Orthogonal Frequency-Division Multiplexing Schemes by Employing the Improved Extreme Learning Machine",2020,"","","","",115,"2022-07-13 09:22:41","","10.3390/sym12040632","","",,,,,10,5.00,2,6,2,"A coherent optical (CO) orthogonal frequency division multiplexing (OFDM) scheme gives a scalable and flexible solution for increasing the transmission rate, being extremely robust to chromatic dispersion as well as polarization mode dispersion. Nevertheless, as any coherent-detection OFDM system, the overall system performance is limited by laser phase noises. On the other hand, extreme learning machines (ELMs) have gained a lot of attention from the machine learning community owing to good generalization performance, negligible learning speed, and minimum human intervention. In this manuscript, a phase-error mitigation method based on the single-hidden layer feedforward network prone to the improved ELM algorithm for CO-OFDM systems is introduced for the first time. In the training step, two steps are distinguished. Firstly, pilots are used, which is very common in OFDM-based systems, to diminish laser phase noises as well as to correct frequency-selective impairments and, therefore, the bandwidth efficiency can be maximized. Secondly, the regularization parameter is included in the ELM to balance the empirical and structural risks, namely to minimize the root mean square error in the test stage and, consequently, the bit error rate (BER) metric. The operational principle of the real-complex (RC) ELM is analytically explained, and then, its sub-parameters (number of hidden neurons, regularization parameter, and activation function) are numerically found in order to enhance the system performance. For binary and quadrature phase-shift keying modulations, the RC-ELM outperforms the benchmark pilot-assisted equalizer as well as the fully-real ELM, and almost matches the common phase error (CPE) compensation and the ELM defined in the complex domain (C-ELM) in terms of the BER over an additive white Gaussian noise channel and different laser oscillators. However, both techniques are characterized by the following disadvantages: the CPE compensator reduces the transmission rate since an additional preamble is mandatory for channel estimation purposes, while the C-ELM requires a bounded and differentiable activation function in the complex domain and can not follow semi-supervised training. In the same context, the novel ELM algorithm can not compete with the CPE compensator and C-ELM for the 16-ary quadrature amplitude modulation. On the other hand, the novel ELM exposes a negligible computational cost with respect to the C-ELM and PAE methods.","",""
0,"S. Kumar","Abstract PO-056: Importance of artificial intelligence, machine learning deep learning in the field of medicine on the future role of the physician",2021,"","","","",116,"2022-07-13 09:22:41","","10.1158/1557-3265.ADI21-PO-056","","",,,,,0,0.00,0,1,1,"There are many ways to define the field of Artificial Intelligence. Here is one way for Artificial Intelligence is ""The Study of the computations that make it possible to perceive, reason, act and predict the future possible outcomes”. Deep learning, which is a popular research area of artificial intelligence (AI), enables the creation of end-to-end models to achieve promised results using input data. Deep learning techniques have been successfully applied in many problems such as arrhythmia detection, skin cancer classification, breast cancer detection, brain disease classification, pneumonia detection, COVID-19 from chest X-ray images, and CT scan images. Almost all hospitals have CT imaging machines; therefore, the chest CT images can be utilized for early classification of diseases. However, the chest CT classification involves a radiology expert and considerable time, which is valuable when any infection is growing at a rapid rate. Therefore, automated analysis of chest CT images is desirable to save the medical professionals precious time that shows the importance of Artificial Intelligence neural networks which is used to classify the infected patients as infected (+ve) or not (−ve). There is a vital need to detect the disease at an early stage and save the patient from the disease. Convolutional neural networks (CNN) are a powerful tool that comes under the platform of Neural Networks – Artificial Intelligence inspired by the human brain, which is extensively utilized for image classification. The hierarchical structure and efficient feature extraction characteristics from an image make CNN a dynamic model for image classification. Initially, the layers are organized into three dimensions: width, height, and depth. The neurons in a given layer do not attach to the entire set of neurons in the later layer, but only to limited neurons of it. Finally, the output is diminished to a single vector of probability scores, coordinated alongside the depth dimension. In a Convolutional Neural Network, the linear function that is used is called a convolutional layer. Each node in the hidden layer extracts different features by using image processing feature detectors. For example, in the first layer, the first node may extract the horizontal edges of an image, the second node may extract vertical edges and etc. These features are extracted using a kernel. The bottom is the original image and the top is the output of the convolutions. It is also worth noting that the output of the convolutions reduces the dimension of the original image, The next step the pooling layer happens tends to be computed after the convolutional layer. The reason why pooling is done is to further reduce the dimensions of the convolutional layer and just extract out the features to make the model more robust. AI could help to rapidly diagnose diseases if proper attention given in collecting the data. Citation Format: Subash Kumar. Importance of artificial intelligence, machine learning deep learning in the field of medicine on the future role of the physician [abstract]. In: Proceedings of the AACR Virtual Special Conference on Artificial Intelligence, Diagnosis, and Imaging; 2021 Jan 13-14. Philadelphia (PA): AACR; Clin Cancer Res 2021;27(5_Suppl):Abstract nr PO-056.","",""
0,"T. Martin, S. Areibi, G. Grewal","Effective Machine-Learning Models for Predicting Routability During FPGA Placement",2021,"","","","",117,"2022-07-13 09:22:41","","10.1109/MLCAD52597.2021.9531243","","",,,,,0,0.00,0,3,1,"The ability to efficiently and accurately predict placement routability, while avoiding the large computational cost of performing routing, is an asset when seeking to reduce total placement and routing runtime. In this paper, we present a series of simple ML models and ensembles to predict the routability of a placement solution. Ensembles based on Bagging, Boosting and Stack of classifiers are introduced to produce more accurate and robust solutions than single/simple models. Our results show an improvement in prediction accuracy and runtime compared to the best published results in the literature.","",""
0,"Wei Yan Ng, C. Cheung, D. Milea, D. Ting","Artificial intelligence and machine learning for Alzheimer’s disease: let’s not forget about the retina",2021,"","","","",118,"2022-07-13 09:22:41","","10.1136/bjophthalmol-2020-318407","","",,,,,0,0.00,0,4,1,"As the world population ages, it is estimated that the population worldwide above the age of 65 years old will increase from 420 million in 2000 to almost 1 billion by 2030. Dementia, with Alzheimer’s disease (AD) as the leading cause, is expected to rise in tandem. AD accounts for 60%–80% of all dementia cases, with an estimated 5–7 million new cases diagnosed each year. Despite intensive research, the diagnosis of AD is currently made through a combination of clinical assessment, neuroimaging and detection of biomarkers from positron emission tomography or cerebrospinal fluid examination, with patients facing issues including high costs, invasiveness of the procedures. Hence, alternative identification of AD without the use of costly or invasive tests remains a challenge that is difficult to surmount. To date, the healthcare has experienced a significant shift towards early accurate detection as well as early prevention. This importance is highlighted by the screening and surveillance of prevalent diseases such as diabetic retinopathy, breast cancer and dementia. While some of these programmes have been very successful in significantly reducing morbidity and mortality, significant amount of manpower, time and training is required for their successful execution. 10 This has lent greater weight to the adoption of healthcare technology in order to optimise the accuracy and efficiency of such programmes. Artificial intelligence (AI), through the combination of digitised big data and computational power, has emerged at the forefront of healthcare. It appears to be wellsuited to address the needs of the healthcare system: fast and accurate predictive, diagnostic and possibly therapeutic algorithms. Machine learning is able to process large amounts of digitised datasets beyond the limits of human capability, and analyse and convert these data into useful clinical insights for the physician. It is a natural fit for conditions or medical specialties that have a large reserve of labelled digitised datasets. At present, convoluted neural networks (CNN), designed to receive two or threedimensional shaped inputs, is one of the most commonly applied deep learning models in medical imaging analysis. Through the utilisation of CNN deep learning, several landmark studies extracting data from retinal images have shown a high degree of accuracy compared with human graders. 13 In order to achieve this, AI generally still requires large, welllabelled and highquality datasets. This places significant barriers to the development of successful image analysis models. The retina is one of the few select organs where image collection is easily accessible and abundant through the use of ocular imaging technologies. As the microvascular properties and neuronal structures of the retina, such as ganglion cellinner plexiform layer (GCIPL) and retinal nerve fibre layer (RNFL), 15 resemble the intracranial neuronal structure and vasculature, it provides a direct visualisation of potential intracranial changes. This combination of unique attributes can potentially provide a low cost, noninvasive analysis of the brain without the patient having to undergo costly neuroimaging to reach a diagnosis. This has attracted increasing research interest, especially in the field of AD where accurate early detection and diagnostic models still remain elusive. The pathogenesis associating retinal changes with AD is currently still unclear. While some evidence suggest the presence of amyloid beta plaques and tau neurofibrillary tangles in the retina of patients with AD, 20 the significance remains debatable. On the other hand, recent studies examining structural changes of the retina in patients with AD have indicated extensive changes including loss of macula volume, 23 thinning of the GCIPL 25 and RNFL thickness, and subfoveal choroidal thickness. In the peripheral retina, notable changes include increased drusen formation and reduced retinal vascularity. Optical coherence tomography angiogram (OCTA), which allows for noninvasive assessment of retinal vascularity, has also shown decreased vessel density, perfusion density and increased foveal avascular zone in patients with AD. 29 It is thus apparent that retinal changes, especially retinal thinning and reduction in vascularity, could potentially indicate the development of AD. In the article by Wisely et al the authors have trained an AI model with multiple imaging modalities using CNN deeplearning with the best performing models achieving an area under curve (AUC) between 0.830 and 0.841. A total of 284 eyes from 159 subjects, of which 36 were clinically diagnosed with AD by experienced neurologists, were analysed in this study. Patient data, optical coherence tomography (OCT) and OCTA quantitative data, ultrawide field retinal photography as well as retinal autofluorescence images were used in the training, validation and testing of the models. Used in isolation, the GCIPL thickness as an indicator of AD had the highest predictive value with an AUC of 0.809. When used in combination, the model that was trained with a combination of GCIPL, OCTA quantitative data and patient data had the highest AUC of 0.841. This paper represents the first attempt as well as a proof of concept at developing a CNN to detect AD using multimodal retinal images. The relatively high predictive ability of the different combination models not only serves as an important foundation for future deep learning models in predicting AD, but also proves the validity of this approach. By testing a varied combination of different imaging modalities available in an ophthalmic clinic, Wisely et al are able to determine the datasets that will provide the greatest yield in accurate prediction of AD. This could contribute to the future development of a robust screening or predictive platform that uses parameters that are costeffective and simple in acquisition. Future research and AI models are likely to expand on the use of retinal imaging and combine with clinical neurological Cataract and Comprehensive, Singapore National Eye Centre, Singapore Ophthalmology and Visual Sciences, The Chinese University of Hong Kong, Hong Kong, Hong Kong Neuroophthalmology Department, Singapore National Eye Centre, Singapore Vitreoretinal Department, Singapore National Eye Centre, Singapore","",""
11,"A. Abrol, Z. Fu, Mustafa S. Salman, Rogers F. Silva, Y. Du, S. Plis, V. Calhoun","Hype versus hope: Deep learning encodes more predictive and robust brain imaging representations than standard machine learning",2020,"","","","",119,"2022-07-13 09:22:41","","10.1101/2020.04.14.041582","","",,,,,11,5.50,2,7,2,"Previous successes of deep learning (DL) approaches on several complex tasks have hugely inflated expectations of their power to learn subtle properties of complex brain imaging data, and scale to large datasets. Perhaps as a reaction to this inflation, recent critical commentaries unfavorably compare DL with standard machine learning (SML) approaches for the analysis of brain imaging data. Yet, their conclusions are based on pre-engineered features which deprives DL of its main advantage: representation learning. Here we evaluate this and show the importance of representation learning for DL performance on brain imaging data. We report our findings from a large-scale systematic comparison of SML approaches versus DL profiled in a ten-way age and gender-based classification task on 12,314 structural MRI images. Results show that DL methods, if implemented and trained following the prevalent DL practices, have the potential to substantially improve compared to SML approaches. We also show that DL approaches scale particularly well presenting a lower asymptotic complexity in relative computational time, despite being more complex. Our analysis reveals that the performance improvement saturates as the training sample size grows, but shows significantly higher performance throughout. We also show evidence that the superior performance of DL is primarily due to the excellent representation learning capabilities and that SML methods can perform equally well when operating on representations produced by the trained DL models. Finally, we demonstrate that DL embeddings span a comprehensible projection spectrum and that DL consistently localizes discriminative brain biomarkers, providing an example of the robustness of prediction relevance estimates. Our findings highlight the presence of non-linearities in brain imaging data that DL frameworks can exploit to generate superior predictive representations for characterizing the human brain, even with currently available data sizes.","",""
328,"K. Bergen, P. Johnson, M. D. de Hoop, G. Beroza","Machine learning for data-driven discovery in solid Earth geoscience",2019,"","","","",120,"2022-07-13 09:22:41","","10.1126/science.aau0323","","",,,,,328,109.33,82,4,3,"Automating geoscience analysis Solid Earth geoscience is a field that has very large set of observations, which are ideal for analysis with machine-learning methods. Bergen et al. review how these methods can be applied to solid Earth datasets. Adopting machine-learning techniques is important for extracting information and for understanding the increasing amount of complex data collected in the geosciences. Science, this issue p. eaau0323 BACKGROUND The solid Earth, oceans, and atmosphere together form a complex interacting geosystem. Processes relevant to understanding Earth’s geosystem behavior range in spatial scale from the atomic to the planetary, and in temporal scale from milliseconds to billions of years. Physical, chemical, and biological processes interact and have substantial influence on this complex geosystem, and humans interact with it in ways that are increasingly consequential to the future of both the natural world and civilization as the finiteness of Earth becomes increasingly apparent and limits on available energy, mineral resources, and fresh water increasingly affect the human condition. Earth is subject to a variety of geohazards that are poorly understood, yet increasingly impactful as our exposure grows through increasing urbanization, particularly in hazard-prone areas. We have a fundamental need to develop the best possible predictive understanding of how the geosystem works, and that understanding must be informed by both the present and the deep past. This understanding will come through the analysis of increasingly large geo-datasets and from computationally intensive simulations, often connected through inverse problems. Geoscientists are faced with the challenge of extracting as much useful information as possible and gaining new insights from these data, simulations, and the interplay between the two. Techniques from the rapidly evolving field of machine learning (ML) will play a key role in this effort. ADVANCES The confluence of ultrafast computers with large memory, rapid progress in ML algorithms, and the ready availability of large datasets place geoscience at the threshold of dramatic progress. We anticipate that this progress will come from the application of ML across three categories of research effort: (i) automation to perform a complex prediction task that cannot easily be described by a set of explicit commands; (ii) modeling and inverse problems to create a representation that approximates numerical simulations or captures relationships; and (iii) discovery to reveal new and often unanticipated patterns, structures, or relationships. Examples of automation include geologic mapping using remote-sensing data, characterizing the topology of fracture systems to model subsurface transport, and classifying volcanic ash particles to infer eruptive mechanism. Examples of modeling include approximating the viscoelastic response for complex rheology, determining wave speed models directly from tomographic data, and classifying diverse seismic events. Examples of discovery include predicting laboratory slip events using observations of acoustic emissions, detecting weak earthquake signals using similarity search, and determining the connectivity of subsurface reservoirs using groundwater tracer observations. OUTLOOK The use of ML in solid Earth geosciences is growing rapidly, but is still in its early stages and making uneven progress. Much remains to be done with existing datasets from long-standing data sources, which in many cases are largely unexplored. Newer, unconventional data sources such as light detection and ranging (LiDAR), fiber-optic sensing, and crowd-sourced measurements may demand new approaches through both the volume and the character of information that they present. Practical steps could accelerate and broaden the use of ML in the geosciences. Wider adoption of open-science principles such as open source code, open data, and open access will better position the solid Earth community to take advantage of rapid developments in ML and artificial intelligence. Benchmark datasets and challenge problems have played an important role in driving progress in artificial intelligence research by enabling rigorous performance comparison and could play a similar role in the geosciences. Testing on high-quality datasets produces better models, and benchmark datasets make these data widely available to the research community. They also help recruit expertise from allied disciplines. Close collaboration between geoscientists and ML researchers will aid in making quick progress in ML geoscience applications. Extracting maximum value from geoscientific data will require new approaches for combining data-driven methods, physical modeling, and algorithms capable of learning with limited, weak, or biased labels. Funding opportunities that target the intersection of these disciplines, as well as a greater component of data science and ML education in the geosciences, could help bring this effort to fruition. Digital geology. Digital representation of the geology of the conterminous United States. [Geology of the Conterminous United States at 1:2,500,000 scale; a digital representation of the 1974 P. B. King and H. M. Beikman map by P. G. Schruben, R. E. Arndt, W. J. Bawiec] The list of author affiliations is available in the full article online. Understanding the behavior of Earth through the diverse fields of the solid Earth geosciences is an increasingly important task. It is made challenging by the complex, interacting, and multiscale processes needed to understand Earth’s behavior and by the inaccessibility of nearly all of Earth’s subsurface to direct observation. Substantial increases in data availability and in the increasingly realistic character of computer simulations hold promise for accelerating progress, but developing a deeper understanding based on these capabilities is itself challenging. Machine learning will play a key role in this effort. We review the state of the field and make recommendations for how progress might be broadened and accelerated.","",""
7,"Nadine Körtel, Cornelia Rücklé, You Zhou, A. Busch, Peter Hoch-Kraft, F. R. Sutandy, Jacob Haase, Mihika Pradhan, M. Musheev, D. Ostareck, A. Ostareck-Lederer, C. Dieterich, S. Hüttelmaier, C. Niehrs, O. Rausch, D. Dominissini, J. König, Kathi Zarnack","Deep and accurate detection of m6A RNA modifications using miCLIP2 and m6Aboost machine learning",2020,"","","","",121,"2022-07-13 09:22:41","","10.1093/nar/gkab485","","",,,,,7,3.50,1,18,2,"N6-methyladenosine (m6A) is the most abundant internal RNA modification in eukaryotic mRNAs and influences many aspects of RNA processing. miCLIP (m6A individual-nucleotide resolution UV crosslinking and immunoprecipitation) is an antibody-based approach to map m6A sites with single-nucleotide resolution. However, due to broad antibody reactivity, reliable identification of m6A sites from miCLIP data remains challenging. Here, we present miCLIP2 in combination with machine learning to significantly improve m6A detection. The optimised miCLIP2 results in high-complexity libraries from less input material. Importantly, we established a robust computational pipeline to tackle the inherent issue of false positives in antibody-based m6A detection. The analyses are calibrated with Mettl3 knockout cells to learn the characteristics of m6A deposition, including m6A sites outside of DRACH motifs. To make our results universally applicable, we trained a machine learning model, m6Aboost, based on the experimental and RNA sequence features. Importantly, m6Aboost allows prediction of genuine m6A sites in miCLIP2 data without filtering for DRACH motifs or the need for Mettl3 depletion. Using m6Aboost, we identify thousands of high-confidence m6A sites in different murine and human cell lines, which provide a rich resource for future analysis. Collectively, our combined experimental and computational methodology greatly improves m6A identification. Highlights miCLIP2 produces complex libraries to map m6A RNA modifications Mettl3 KO miCLIP2 allows to identify Mettl3-dependent RNA modification sites Machine learning predicts genuine m6A sites from human and mouse miCLIP2 data without Mettl3 KO m6A modifications occur outside of DRACH motifs and associate with alternative splicing","",""
0,"J. Sanz","SS11: Soft Computing Techniques for Machine Learning",2017,"","","","",122,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,1,5,"This special session is aimed at discussing recent and novel fuzzy methods to deal with the current challenges on machine learning. This research field is very active due to the large number of real-world problems that can be faced using techniques of this field. The canonical problems of this area of research are classification, regression and clustering. However, in recent years there are a great number of hot topics like the problem of imbalanced data, low quality and/or noisy instances, semi-supervised learning or multi-label and multi-instance problems among others. When tackling the previously mentioned problems, soft computing techniques are widely applied. Specifically, fuzzy systems are a common tool as they provide an interpretable model understandable by human beings whilst the results obtained are accurate, since fuzzy logic has an inherent ability to cope with the great uncertainty present in these new challenging problems. Evolutionary computation is a robust technique for optimization, learning and adaptation tasks. They can adjust the model parameters for each specific problem for the sake of enhancing their performance. The synergy between these two techniques leads to a better capability for the design and optimization of fuzzy models. Moreover, Big Data also offers new possibilities for fuzzy methods, where new challenges appear with respect to their scalability when dealing with enormous amounts of data. The special session is composed of seven contributions dealing with different topics of the machine learning field.","",""
1,"Abhishek Sarkar, Tanmay Sen, A. Roy","GraFeHTy: Graph Neural Network using Federated Learning for Human Activity Recognition",2021,"","","","",123,"2022-07-13 09:22:41","","10.1109/ICMLA52953.2021.00184","","",,,,,1,1.00,0,3,1,"Human Activity Recognition (HAR) from sensor measurements is still challenging due to noisy or lack of la-belled examples and issues concerning data privacy. Training a traditional centralized machine learning (ML) model for HAR is constrained by infrastructure availability, network connectivity, latency issues etc. Issues regarding labels of user measurements can be tackled by using semi-supervised learning while issues regarding privacy concerns can be addressed by increasingly popular Federated Learning (FL). In this work, we propose a novel algorithm GraFeHTy, a Graph Convolution Network (GCN) trained in a federated setting to alleviate these key obstructions for HAR. We construct a similarity graph from sensor measurements for each user and apply a GCN to perform semi-supervised classification of human activities by leveraging inter-relatedness and closeness of activities. The weights of the GCN are trained using federated learning where each user performs gradient descent using their local data and share only the updated weights with a central server for aggregation. The GCN helps in accurate detection of unlabelled or noisy labels in the activity sequence by borrowing information from similar labelled nodes. The federated setting for training these models ensures that user privacy is respected by transferring only the learned representations out of the device to a central server. By avoiding transfer of raw data, the algorithm also ensures that training a HAR model is not constrained by infrastructure availability in central server or low network bandwidth from edge devices. Our proposed algorithm performs better than the baseline feed-forward federated learning model in terms of both accuracy and computational complexity.","",""
0,"A. Kahng, Brucek Khailany","2nd ACM/IEEE Workshop on Machine Learning for CAD (MLCAD)",2020,"","","","",124,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,2,2,"The scaling imperative challenges us to always do better and faster, with less resources. The semiconductor industry has looked to machine learning (ML) as a design-based lever for scaling that will reduce design costs and design schedules while improving quality of results. As a result, in recent years Machine Learning for CAD (MLCAD) has dominated the conversation at leading conferences. Numerous ML-based enhancements and their benefits have been highlighted by EDA vendors and their customers. With this as backdrop, this talk will offer some thoughts on future directions for MLCAD. First, MLCAD lies on a road to “self-driving IC design tools and flows” that make design ideation and design space exploration both accurate and accessible. Eventually, MLCAD (ML for CAD) will lead us to MLDA (MLenabled Design Automation). But for this to happen, researchers and practitioners will need to deliver (i) humanquality prediction, evaluation and decision-making with no humans; (ii) design tools and flows that never require iteration and never fail; (iii) modeling of design processes that continually improves; and more. Second, the trajectory of MLCAD will need to keep three concepts in foreground: Learning, Optimization and Scaling. “Learning” seems obvious from “ML”, but it brings open questions about data and models, ranging from statistics to standards, sharing and openness. “Optimization” is the essence of CAD, and brings open questions about both synergies and boundaries with learning. “Scaling” is how practical realization of Learning and Optimization will satisfy the needs of design within an ever-tighter box of compute, schedule and other resources. Finally, there is a meta-question of how the MLCAD community will itself learn, optimize, and scale. Speaker Bio: Andrew B. Kahng is Distinguished Professor of CSE and ECE and holder of the endowed chair in highperformance computing at UC San Diego. He was visiting scientist at Cadence (1995-97) and founder/CTO at Blaze DFM (2004–06). He is coauthor of 3 books and over 500 journal and conference papers, holds 35 issued U.S. patents, and is a fellow of ACM and IEEE. He served as general chair of DAC, ISPD and other conferences, and from 2000-2016 as international chair/co-chair of the ITRS Design and System Drivers working groups. He currently serves as PI of “OpenROAD” https://theopenroadproject.org/, a $15M U.S. DARPA project targeting open-source, autonomous (“no human in the loop”) tools for IC implementation. m mlcad.itec.kit.edu Page 2 Monday, November 16, 2020 Plenary “Accelerating Chip Design with Machine Learning” 9:15am PST, 12:15pm EST, 6:15pm CET, 10:45pm India, 01:15am (Tue) Asia, 02:15am (Tue) Japan Brucek Khailany NVIDIA Austin, TX, USA Abstract As Moore’s law has provided an exponential increase in chip transistor density, the unique features we can now include in large chips are no longer predominantly limited by area constraints. Instead, new capabilities are increasingly limited by the engineering effort associated with digital design, verification, and implementation. As applications demand more performance and energy efficiency from specialization in the post-Moore’s-law era, we expect required complexity and design effort to increase. Historically, these challenges have been met through levels of abstraction and automation. Over the last few decades, Electronic Design Automation (EDA) algorithms and methodologies were developed for all aspects of chip design design verification and simulation, logic synthesis, place-and-route, and timing and physical signoff analysis. With each increase in automation, total work per chip has increased, but more work has also been offloaded from manual effort to software. Just as machine learning (ML) has transformed software in many domains, we expect advancements in ML will also transform EDA software and as a result, chip design workflows. In this talk, we highlight work from our research group and the community applying ML to various chip design prediction tasks. We show how deep convolutional neural networks and graph-based neural networks can be used in the areas of automatic design space exploration, power analysis, VLSI physical design, and analog design. We also present a future vision of an AI-assisted chip design workflow to automate optimization tasks. In this future vision, GPU acceleration, neuralnetwork predictors, and deep reinforcement learning techniques combine to automate VLSI design and optimization.As Moore’s law has provided an exponential increase in chip transistor density, the unique features we can now include in large chips are no longer predominantly limited by area constraints. Instead, new capabilities are increasingly limited by the engineering effort associated with digital design, verification, and implementation. As applications demand more performance and energy efficiency from specialization in the post-Moore’s-law era, we expect required complexity and design effort to increase. Historically, these challenges have been met through levels of abstraction and automation. Over the last few decades, Electronic Design Automation (EDA) algorithms and methodologies were developed for all aspects of chip design design verification and simulation, logic synthesis, place-and-route, and timing and physical signoff analysis. With each increase in automation, total work per chip has increased, but more work has also been offloaded from manual effort to software. Just as machine learning (ML) has transformed software in many domains, we expect advancements in ML will also transform EDA software and as a result, chip design workflows. In this talk, we highlight work from our research group and the community applying ML to various chip design prediction tasks. We show how deep convolutional neural networks and graph-based neural networks can be used in the areas of automatic design space exploration, power analysis, VLSI physical design, and analog design. We also present a future vision of an AI-assisted chip design workflow to automate optimization tasks. In this future vision, GPU acceleration, neuralnetwork predictors, and deep reinforcement learning techniques combine to automate VLSI design and optimization. Speaker Bio: Brucek Khailany joined NVIDIA in 2009 and is the Director of the ASIC and VLSI Research group. He leads research into innovative design methodologies for IC development, ML and GPU assisted EDA, and energy efficient ML accelerators. Over 10 years at NVIDIA, he has contributed to many projects in research and product groups spanning computer architecture and VLSI design. Previously, Dr. Khailany was a Co-Founder and Principal Architect at Stream Processors, Inc where he led R&D related to parallel processor architectures. At Stanford University, he led the VLSI implementation of the Imagine processor, which introduced the concepts of stream processing and partitioned register organizations. He received his PhD in Electrical Engineering from Stanford University and BSE degrees in Electrical and Computer Engineering from the University of Michigan. He is a Senior Member of the IEEE. m mlcad.itec.kit.edu Page 3 Tuesday, November 17, 2020 Keynote “SoC Design Automation with ML – It’s Time for Research” 7am PST, 10am EST, 4pm CET, 8:30pm India, 11pm Asia, midnight Japan Wolfgang Ecker Infineon Technologies Munich, Germany Abstract The AI-hype started a few years ago, with advances in object recognition. Soon the EDA research community made proposals on applying AI in EDA and all major players announced new AI-based tools at DAC 2018. Unfortunately, few new AI-based EDA-tools made it to productive use today. This talk analyses general challenges of AI in EDA, outlines promising use cases, and motivates more AI research in EDA: More HI (=Human Intelligence) is needed to make AI successful in EDA.The AI-hype started a few years ago, with advances in object recognition. Soon the EDA research community made proposals on applying AI in EDA and all major players announced new AI-based tools at DAC 2018. Unfortunately, few new AI-based EDA-tools made it to productive use today. This talk analyses general challenges of AI in EDA, outlines promising use cases, and motivates more AI research in EDA: More HI (=Human Intelligence) is needed to make AI successful in EDA. Speaker Bio: Wolfgang Ecker is Distinguished Engineer at Infineon and Adjunct Professor at Technical University of Munich. He is (co-)author of over 200 papers on modeling and design automation, received 5 best paper awards, was granted with the German EDA achievement award. He is member of Acatech, the German Academy of Science and Engineering. Wolfgang Ecker leads the Infineon Deep Learning internal think tank. In addition, he is member of the AI commission of inquiry of the German Government. m mlcad.itec.kit.edu Page 4 Tuesday, November 17, 2020 Plenary “From Tuning to Learning: Why the FPGA physical design flow offers a compelling case for ML?” 9:15am PST, 12:15pm EST, 6:15pm CET, 10:45pm India, 01:15am (Wed) Asia, 02:15am (Wed) Japan Ismail Bustany Xilinx USA Abstract ML is particularly suited for the FPGA Physical design (PD) flow since each device family generation innately provides a rich platform for device/design feature data harvesting: (1) A vast amount of device architecture-specific interconnect/layout fabric data and (2) significant amount of large design suite data from and from broad set of application domains. These bode well for developing robust predictive ML models. Furthermore, the long lifespan of these device families affords a favorable ROI. In this talk, we will highlight some data harvesting and ML solutions we have developed in Xilinx’ Vivado PD flow and share some initial results. These include a strategy recommendation framework for design closure, design classification for computational resource allocation, device characteristics modeling, and routing congestion estimation. Furthermore, we w","",""
0,"S. Vishwakarma","A Machine Learning Approach for Prediction Analysis in Data Mining",2020,"","","","",125,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,1,2,"Machine learning is an emerging technique for building analytic models for machines to ""learn"" from data and be able to do predictive analysis. The ability of machines to ""learn"" and do predictive analysis is very important in this era of big data and it has a wide range of application areas. For instance, banks and financial institutions are sometimes faced with the challenge of what risk factors to consider when advancing credit/loans to customers. For several features/attributes of the customers are normally taken into consideration, but most of these features have little predictive effect on the credit worthiness or otherwise of the customer. Furthermore, a robust and effective automated bank credit risk score that can aid in the prediction of customer credit worthiness very accurately is still a major challenge facing many banks. In this paper, we examine a real bank credit data and conduct several machine learning algorithms on the data for comparative analysis and to choose which algorithms are the best fit for learning bank credit data. The algorithms gave over 80% accuracy in prediction.  Machine Learning is a branch of Artificial Intelligence that gives systems the ability to learn automatically and improve themselves from the experience without being explicitly programmed or without intervention of human. Its main aim is to make computers learn automatically from the experience. The goal of machine learning is to understand the structure of data and fir that data into models that can be understood and utilized by people. Although machine learning is a field of computer science, it differs from traditional computational approaches. In traditional computing, algorithms are sets of explicitly programmed instructions used by computers to calculate or solve any problem. Machine learning algorithms instead allow for computers to train on data inputs and use statistical analysis in order to output values that fall within a specific range. Because of this, machine learning facilitates computers in building models from sample data in order to automate decision making processes based on data inputs. Machine learning is a continuously developing field","",""
28,"O. Schütt, J. VandeVondele","Machine Learning Adaptive Basis Sets for Efficient Large Scale Density Functional Theory Simulation",2018,"","","","",126,"2022-07-13 09:22:41","","10.1021/acs.jctc.8b00378","","",,,,,28,7.00,14,2,4,"It is chemically intuitive that an optimal atom centered basis set must adapt to its atomic environment, for example by polarizing toward nearby atoms. Adaptive basis sets of small size can be significantly more accurate than traditional atom centered basis sets of the same size. The small size and well conditioned nature of these basis sets leads to large saving in computational cost, in particular in a linear scaling framework. Here, it is shown that machine learning can be used to predict such adaptive basis sets using local geometrical information only. As a result, various properties of standard DFT calculations can be easily obtained at much lower costs, including nuclear gradients. In our approach, a rotationally invariant parametrization of the basis is obtained by employing a potential anchored on neighboring atoms to ultimately construct a rotation matrix that turns a traditional atom centered basis set into a suitable adaptive basis set. The method is demonstrated using MD simulations of liquid water, where it is shown that minimal basis sets yield structural properties in fair agreement with basis set converged results, while reducing the computational cost in the best case by a factor of 200 and the required flops by 4 orders of magnitude. Already a very small training set yields satisfactory results as the variational nature of the method provides robustness.","",""
0,"Tobias Wegel, F. Assion, David Mickisch, Florens Greßner","A Framework for Verification of Wasserstein Adversarial Robustness",2021,"","","","",127,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,4,1,"Machine learning image classifiers are susceptible to adversarial and corruption perturbations. Adding imperceptible noise to images can lead to severe misclassifications of the machine learning model. Using Lp-norms for measuring the size of the noise fails to capture human similarity perception, which is why optimal transport based distance measures like the Wasserstein metric are increasingly being used in the field of adversarial robustness. Verifying the robustness of classifiers using the Wasserstein metric can be achieved by proving the absence of adversarial examples (certification) or proving their presence (attack). In this work we present a framework based on the work by Levine and Feizi [5], which allows us to transfer existing certification methods for convex polytopes or L1-balls to the Wasserstein threat model. The resulting certification can be complete or incomplete, depending on whether convex polytopes or L1-balls were chosen. Additionally, we present a new Wasserstein adversarial attack that is projected gradient descent based and which has a significantly reduced computational burden compared to existing attack approaches.","",""
13,"Nastacia L. Goodwin, S. Nilsson, S. Golden","Rage Against the Machine: Advancing the study of aggression ethology via machine learning.",2020,"","","","",128,"2022-07-13 09:22:41","","10.1007/s00213-020-05577-x","","",,,,,13,6.50,4,3,2,"","",""
174,"Andrew F. Zahrt, J. Henle, Brennan T Rose, Yang Wang, William T. Darrow, S. Denmark","Prediction of higher-selectivity catalysts by computer-driven workflow and machine learning",2019,"","","","",129,"2022-07-13 09:22:41","","10.1126/science.aau5631","","",,,,,174,58.00,29,6,3,"Predicting catalyst selectivity Asymmetric catalysis is widely used in chemical research and manufacturing to access just one of two possible mirror-image products. Nonetheless, the process of tuning catalyst structure to optimize selectivity is still largely empirical. Zahrt et al. present a framework for more efficient, predictive optimization. As a proof of principle, they focused on a known coupling reaction of imines and thiols catalyzed by chiral phosphoric acid compounds. By modeling multiple conformations of more than 800 prospective catalysts, and then training machine-learning algorithms on a subset of experimental results, they achieved highly accurate predictions of enantioselectivities. Science, this issue p. eaau5631 A model encompassing multiple conformations of chiral phosphoric acid catalysts accurately predicts enantioselectivities. INTRODUCTION The development of new synthetic methods in organic chemistry is traditionally accomplished through empirical optimization. Catalyst design, wherein experimentalists attempt to qualitatively identify correlations between catalyst structure and catalyst efficiency, is no exception. However, this approach is plagued by numerous deficiencies, including the lack of mechanistic understanding of a new transformation, the inherent limitations of human cognitive abilities to find patterns in large collections of data, and the lack of quantitative guidelines to aid catalyst identification. Chemoinformatics provides an attractive alternative to empiricism for several reasons: Mechanistic information is not a prerequisite, catalyst structures can be characterized by three-dimensional (3D) descriptors (numerical representations of molecular properties derived from the 3D molecular structure) that quantify the steric and electronic properties of thousands of candidate molecules, and the suitability of a given catalyst candidate can be quantified by comparing its properties with a computationally derived model trained on experimental data. The ability to accurately predict a selective catalyst by using a set of less than optimal data remains a major goal for machine learning with respect to asymmetric catalysis. We report a method to achieve this goal and propose a more efficient alternative to traditional catalyst design. RATIONALE The workflow we have created consists of the following components: (i) construction of an in silico library comprising a large collection of conceivable, synthetically accessible catalysts derived from a particular scaffold; (ii) calculation of relevant chemical descriptors for each scaffold; (iii) selection of a representative subset of the catalysts [this subset is termed the universal training set (UTS) because it is agnostic to reaction or mechanism and thus can be used to optimize any reaction catalyzed by that scaffold]; (iv) collection of the training data; and (v) application of machine learning methods to generate models that predict the enantioselectivity of each member of the in silico library. These models are evaluated with an external test set of catalysts (predicting selectivities of catalysts outside of the training data). The validated models can then be used to select the optimal catalyst for a given reaction. RESULTS To demonstrate the viability of our method, we predicted reaction outcomes with substrate combinations and catalysts different from the training data and simulated a situation in which highly selective reactions had not been achieved. In the first demonstration, a model was constructed by using support vector machines and validated with three different external test sets. The first test set evaluated the ability of the model to predict the selectivity of only reactions forming new products with catalysts from the training set. The model performed well, with a mean absolute deviation (MAD) of 0.161 kcal/mol. Next, the same model was used to predict the selectivity of an external test set of catalysts with substrate combinations from the training set. The performance of the model was still highly accurate, with a MAD of 0.211 kcal/mol. Lastly, reactions forming new products with the external test catalysts were predicted with a MAD of 0.236 kcal/mol. In the second study, no reactions with selectivity above 80% enantiomeric excess were used as training data. Deep feed-forward neural networks accurately reproduced the experimental selectivity data, successfully predicting the most selective reactions. More notably, the general trends in selectivity, on the basis of average catalyst selectivity, were correctly identified. Despite omitting about half of the experimental free energy range from the training data, we could still make accurate predictions in this region of selectivity space. CONCLUSION The capability to predict selective catalysts has the potential to change the way chemists select and optimize chiral catalysts from an empirically guided to a mathematically guided approach. Chemoinformatics-guided optimization protocol. (A) Generation of a large in silico library of catalyst candidates. (B) Calculation of robust chemical descriptors. (C) Selection of a UTS. (D) Acquisition of experimental selectivity data. (E) Application of machine learning to use moderate- to low-selectivity reactions to predict high-selectivity reactions. R, any group; X, O or S; Y, OH, SH, or NHTf; PC, principal component; ΔΔG, mean selectivity. Catalyst design in asymmetric reaction development has traditionally been driven by empiricism, wherein experimentalists attempt to qualitatively recognize structural patterns to improve selectivity. Machine learning algorithms and chemoinformatics can potentially accelerate this process by recognizing otherwise inscrutable patterns in large datasets. Herein we report a computationally guided workflow for chiral catalyst selection using chemoinformatics at every stage of development. Robust molecular descriptors that are agnostic to the catalyst scaffold allow for selection of a universal training set on the basis of steric and electronic properties. This set can be used to train machine learning methods to make highly accurate predictive models over a broad range of selectivity space. Using support vector machines and deep feed-forward neural networks, we demonstrate accurate predictive modeling in the chiral phosphoric acid–catalyzed thiol addition to N-acylimines.","",""
26,"N. Boukhelifa, A. Bezerianos, E. Lutton","Evaluation of Interactive Machine Learning Systems",2018,"","","","",130,"2022-07-13 09:22:41","","10.1007/978-3-319-90403-0_17","","",,,,,26,6.50,9,3,4,"","",""
19,"C. Lintott, J. Reed","Human Computation in Citizen Science",2013,"","","","",131,"2022-07-13 09:22:41","","10.1007/978-1-4614-8806-4_14","","",,,,,19,2.11,10,2,9,"","",""
1,"Shoeb Shaikh, R. So, T. Sibindi, C. Libedinsky, A. Basu","Towards Autonomous Intra-cortical Brain Machine Interfaces: Applying Bandit Algorithms for Online Reinforcement Learning",2020,"","","","",132,"2022-07-13 09:22:41","","10.1101/2020.01.08.899641","","",,,,,1,0.50,0,5,2,"This paper presents application of Banditron - an online reinforcement learning algorithm (RL) in a discrete state intra-cortical Brain Machine Interface (iBMI) setting. We have analyzed two datasets from non-human primates (NHPs) - NHP A and NHP B each performing a 4-option discrete control task over a total of 8 days. Results show average improvements of ≈ 15%, 6% in NHP A and 15%, 21% in NHP B over state of the art algorithms - Hebbian Reinforcement Learning (HRL) and Attention Gated Reinforcement Learning (AGREL) respectively. Apart from yielding a superior decoding performance, Banditron is also the most computationally friendly as it requires two orders of magnitude less multiply-and-accumulate operations than HRL and AGREL. Furthermore, Banditron provides average improvements of at least 40%, 15% in NHPs A, B respectively compared to popularly employed supervised methods - LDA, SVM across test days. These results pave the way towards an alternate paradigm of temporally robust hardware friendly reinforcement learning based iBMIs.","",""
14,"Hongfeng Li, Hong-Qin Zhao, Hong Li","Neural-Response-Based Extreme Learning Machine for Image Classification",2019,"","","","",133,"2022-07-13 09:22:41","","10.1109/TNNLS.2018.2845857","","",,,,,14,4.67,5,3,3,"This paper proposes a novel and simple multilayer feature learning method for image classification by employing the extreme learning machine (ELM). The proposed algorithm is composed of two stages: the multilayer ELM (ML-ELM) feature mapping stage and the ELM learning stage. The ML-ELM feature mapping stage is recursively built by alternating between feature map construction and maximum pooling operation. In particular, the input weights for constructing feature maps are randomly generated and hence need not be trained or tuned, which makes the algorithm highly efficient. Moreover, the maximum pooling operation enables the algorithm to be invariant to certain transformations. During the ELM learning stage, elastic-net regularization is proposed to learn the output weight. Elastic-net regularization helps to learn more compact and meaningful output weight. In addition, we preprocess the input data with the dense scale-invariant feature transform operation to improve both the robustness and invariance of the algorithm. To evaluate the effectiveness of the proposed method, several experiments are conducted on three challenging databases. Compared with the conventional deep learning methods and other related ones, the proposed method achieves the best classification results with high computational efficiency.","",""
0,"Pinyarash Pinyoanuntapong, Wesley Houston Huff, Minwoo Lee, Chen Chen, Pu Wang","AN END-TO-END MACHINE LEARNING PERSPECTIVE ON INDUSTRIAL IOT",2022,"","","","",134,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,5,1,"IntroductIon The Internet of Things (IoT) is a network of a variety of things or objects that are able to interact with each other and cooperate with their neighbors to reach common goals [1]. Advances in wireless communication (e.g., 5G) and artificial intelligence (AI) have created a synergistic move toward Artificial Intelligence of Things (AIoT) [2] , which consists of AI-empowered IoT devices that can analyze data used within devices and make proactive, intelligent, and accurate decisions without the involvement of humans. AIoT applications such as smart surveillance camera networks, intelligent transportation, smart and connected healthcare, smart home, and smart grids have paved the way to build smart cities. Toward this, a myriad of smart edge devices need to be installed, and a large number of AIoT devices produce enormous data. The rapid growth of the size of the data and the number of devices need AI-driven automated processing, often done in a centralized manner. The centralized machine learning model needs the training data to be collocated at a common server, and therefore needs to transfer a large amount of IoT device data from the network edge to the central server. This imposes a huge burden on the communication networks while inducing severe vulnerability of data privacy. Federated learning (FL) [3] is an emerging privacy-preserving deep learning paradigm that enables distributed neural model training on edge devices while keeping their data local to prevent privacy leakage. In FL, the workers (e.g., IoT devices) only need to send their local model updates to the server that aggregates these updates to continuously improve the shared global model. This approach significantly reduces the data privacy risk by only sending and receiving the computed local models to the server or vice versa rather than sending the data itself. Moreover, FL can greatly reduce the required number of communication rounds for model convergence by increasing computation parallelization, where more IoT devices are involved as workers, and by increasing local computation, where a worker performs multiple iterations of model updates before sending the updated model to the server. Through FL, IoT devices can still learn much more accurate models with small local datasets. Classical FL relies on frequent centralized model aggregation, which may be applied for a single-hop IoT network, where the IoT devices are connected to each other and the central server over single-hop cellular or WiFi connections. Different from single-hop IoT systems that rely on cellular/WiFi systems with high infrastructure deployment and operational costs, wireless multihop IoT networks, consisting of a mesh of interconnected wireless IoT devices, have been widely exploited to build cost-efficient large-scale IoT systems [1]. However, adopting centralized FL (CFL) over a multihop IoT network faces a significant challenge: a communication bottleneck at the central server. As the number of IoT devices increases, the full network load is forced to pass through the single server node, which leads to network congestion and thus greatly slows down model convergence. In addition, the server carries the full responsibility of aggregation and has an unavoidable single point of failure. Decentralized FL (DFL) [4, 5] is an emerging FL paradigm that can effectively address the aforementioned limitations of CFL. Under DFL, in each epoch, all workers update their local models via multiple stochastic gradient descent (SGD) iterations. Then each worker averages its local model only with its neighbors. By removing the single point of aggregation, DFL can lead to a robust learning framework with increased scalability. Moreover, distributing the traffic load from a central node can maximize the utilization of a network’s bandwidth and accelerate the model convergence speed. Despite its great advantages, existing DFL solutions do have compromises and areas for possible improvement. A common DFL paradigm is synchronous optimization, where the workers from a local group (e.g., one-hop neighbors) must receive the models from each other and finish their respective model averaging before stepping into the next epoch. For example, HADFL [5] defines a synchronization topology containing workAbstrAct As Artificial Intelligence of Things (AIoT) has become increasingly important for modern AI applications,federated learning (FL) is envisioned to be the enabling technology for AIoT, especially for large-scale, data privacy-preserving scenarios. However, most existing FL is managed in a centralized manner (CFL), which confronts the limitations of scalability given the AIoT device explosion. The key challenge faced by CFL is the communication bottleneck at the central model aggregation server, which leads to a high server-to-worker communication delay and thus severely slows down the model convergence. To address this challenge, this article introduces a generic decentralized FL (DFL) framework that can operate in either synchronous (Sync-DFL) mode or asynchronous (Async-DFL) mode to alleviate the high communication congestion around the central server. Moreover, Async-DFL is the first DFL in the literature to provide a generic FL framework that is fully asynchronous and able to completely avoid worker waiting, which leads to robust distributed model training in the inherently heterogeneous IoT environments, where stragglers (i.e., slow devices) are very common due to the largely varying computing/networking speeds of IoT devices. Our DFL framework is implemented, deployed, and experimented with in both simulation and physical testbeds. The results show that Async-DFL can accelerate the convergence speed of model training twice as fast as CFL, while maintaining convergence accuracy and effectively combating the impact of the stragglers.","",""
0,"T. Nagle-McNaughton, L. Scuderi, Nicholas C. Erickson","Squeezing Data from a Rock: Machine Learning for Martian Science",2022,"","","","",135,"2022-07-13 09:22:41","","10.3390/geosciences12060248","","",,,,,0,0.00,0,3,1,"Data analysis methods have scarcely kept pace with the rapid increase in Earth observations, spurring the development of novel algorithms, storage methods, and computational techniques. For scientists interested in Mars, the problem is always the same: there is simultaneously never enough of the right data and an overwhelming amount of data in total. Finding sufficient data needles in a haystack to test a hypothesis requires hours of manual data screening, and more needles and hay are added constantly. To date, the vast majority of Martian research has been focused on either one-off local/regional studies or on hugely time-consuming manual global studies. Machine learning in its numerous forms can be helpful for future such work. Machine learning has the potential to help map and classify a large variety of both features and properties on the surface of Mars and to aid in the planning and execution of future missions. Here, we outline the current extent of machine learning as applied to Mars, summarize why machine learning should be an important tool for planetary geomorphology in particular, and suggest numerous research avenues and funding priorities for future efforts. We conclude that: (1) moving toward methods that require less human input (i.e., self- or semi-supervised) is an important paradigm shift for Martian applications, (2) new robust methods using generative adversarial networks to generate synthetic high-resolution digital terrain models represent an exciting new avenue for Martian geomorphologists, (3) more effort and money must be directed toward developing standardized datasets and benchmark tests, and (4) the community needs a large-scale, generalized, and programmatically accessible geographic information system (GIS).","",""
0,"J. McElveen, L. Krasny, Scott Nordlund","Applying matched field array processing and machine learning to computational auditory scene analysis and source separation challenges",2022,"","","","",136,"2022-07-13 09:22:41","","10.1121/10.0011162","","",,,,,0,0.00,0,3,1,"Matched field processing (MFP) techniques employing physics-based models of acoustic propagation have been successfully and widely applied to underwater target detection and localization, while machine learning (ML) techniques have enabled detection and extraction of patterns in data. Fusing MFP and ML enables the estimation of Green’s Function solutions to the Acoustic Wave Equation for waveguides from data captured in real, reverberant acoustic environments. These Green’s Function estimates can further enable the robust separation of individual sources, even in the presence of multiple loud, interfering, interposed, and competing noise sources. We first introduce MFP and ML and then discuss their application to Computational Auditory Scene Analysis (CASA) and acoustic source separation. Results from a variety of tests using a binaural headset, as well as different wearable and free-standing microphone arrays are then presented to illustrate the effects of the number and placement of sensors on the residual noise floor after separation. Finally, speculations on the similarities between this proprietary approach and the human auditory system’s use of interaural cross-correlation in formulation of acoustic spatial models will be introduced and ideas for further research proposed.","",""
1,"P. Smolensky, R. Thomas McCoy, Roland Fernandez, M. Goldrick, Jia-Hao Gao","Neurocompositional computing in human and machine intelligence: A tutorial",2022,"","","","",137,"2022-07-13 09:22:41","","","","",,,,,1,1.00,0,5,1,"The past decade has produced a revolution in Artificial Intelligence (AI), after a half-century of AI repeatedly failing to meet expectations. What explains the dramatic change from 20th-century to 21st-century AI, and how can remaining limitations of current AI be overcome? Until now, the widely accepted narrative has attributed the recent progress in AI to technical engineering advances that have yielded massive increases in the quantity of computational resources and training data available to support statistical learning in deep artificial neural networks. Although these quantitative engineering innovations are important, here we show that the latest advances in AI are not solely due to quantitative increases in computing power but also qualitative changes in how that computing power is deployed. These qualitative changes have brought about a new type of computing that we call neurocompositional computing . In neurocompositional computing, neural networks exploit two scientific principles that contemporary theory in cognitive science maintains are simultaneously necessary to enable human-level cognition. The Compositionality Principle asserts that encodings of complex information are structures that are systematically composed from simpler structured encodings. The Continuity Principle states that the encoding and processing of information is formalized with real numbers that vary continuously. These principles have seemed irreconcilable until the recent mathematical discovery that compositionality can be realized not only through the traditional discrete methods of symbolic computing, well developed in 20th-century AI, but also through novel forms of continuous neural computing—neurocompositional computing. The unprecedented progress of 21st-century AI has resulted from the use of limited—first-generation—forms of neurocompositional computing. We show that the new techniques now being deployed in second-generation neurocompositional computing create AI systems that are not only more robust and accurate than current systems, but also more comprehensible—making it possible to diagnose errors in, and exert human control over, artificial neural networks through interpretation of their internal states and direct intervention upon those states. Note: This tutorial is intended for those new to this topic, and does not assume familiarity with cognitive science, AI, or deep learning. Appendices provide more advanced material. Each figure, and the associated box explaining it, provides an exposition, illustration, or further details of a main point of the paper; in order to make these figures relatively self-contained, it has sometimes been necessary to repeat some material from the text. For a brief introduction and additional development of some of this material see [212]. . abstract mental processes”","",""
0,"Gauravkumar K. Patel","Context- and Physiology-aware Machine Learning for Upper-Limb Myocontrol",2018,"","","","",138,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,1,4,"The world around us is shaped in such a way that our hands are necessary to accomplish most activities of daily living. It is therefore undeniable that the loss of the upper limb, partial or total, represents a severe impairment. With current advancements in robotic technology, it is now possible to replace a missing limb with a dexterous upper-limb prosthesis. However, the development of a reliable human machine interface (HMI), connecting the user and the prosthesis, is still an open challenge. Essentially, the HMI defines an invariant mapping scheme to transform electromyogram (EMG) signals generated by the user into movements on the prosthetic device, thereby allowing the user to control available functions by generating appropriate (predefined) EMG signals. An HMI control driven by EMG signals is known as myoelectric control or myocontrol. EMG signals associated with a particular motor task are distinct and repeatable and therefore, it is possible to use one of the many well-known machine learning (ML) algorithms as HMI for estimating different user motor intentions. With ML-based HMIs, users can directly activate a desired prosthesis function by producing EMG signals associated to that function during supervised learning. Although conceptually promising, ML-based control has shown a limited clinical viability, mainly due to the lack of reliability and robustness during real time use. The aim of this thesis was to improve the reliability and robustness of ML based control by developing context- and physiology- aware ML methods for upper limb myocontrol. Today, most ML methods used for myoelectric control follow the conventional pattern recognition paradigm, where training data is collected using a supervised procedure and a mathematical function is fitted over the collected data to define an invariant mapping scheme between the user’s EMG and available prosthesis movements. This conventional approach has two limitations. First, the mapping scheme (between the EMG and available movements) remains static (invariant) during use and does not consider the dynamics associated with real-life use of prosthesis. Second, the mathematical function fitted over the training data is assumed to implicitly capture the physiological principles behind generation of EMG; this assumption might not be true, as many commonly applied ML methods do not model the underlying physiology. The first limitation can be solved by developing ML methods which can consider context information describing the state of the system and/or environment during prosthesis use. This context information can be acquired either directly from the user or by placing additional sensors (e.g. inertial units) on the prosthesis. The former idea of deriving context information from the user is quite interesting, as it gives to the ML an opportunity to improve control by considering user’s requirement(s) during use. This thesis proposes one ML method (called Modular Regression, see Chapter 2) which exploits user-generated context information to improve control for different activities of daily living (ADL). Specifically, the proposed ML method organizes each prosthesis function as a module, which the users can insert/remove as required to best accomplish a given ADL. Next, if additional sensors were placed on the prosthesis to automatically derive context information, the ML controller would get an opportunity to (automatically) monitor the state of the prosthesis and react accordingly to maximize reliability and robustness. This thesis proposes one ML approach (called context driven control, see Chapter 3) which utilizes context information from additional sensors to model different prosthesis states and then, the parameters of ML control were adapted to mitigate expected disturbances in each prosthesis state. Thus, with both new ML methods, the mapping scheme (between the user’s EMG and available movements) does not remain static, but becomes reactive to the context information coming from the user or additional sensors. Experiments involving functional tasks were conducted to compare the newly developed context-aware ML methods with the conventional ML-based control. The experimental results indicate that the context-aware methods significantly outperform conventional ML control. The second limitation of conventional ML approaches, i.e. the fitted mathematical function may or may not capture the latent physiology information, can be solved by designing ML methods that are aware of the underlying muscle physiology. This thesis presents one ML algorithm (based on the cosine similarity metric, see Chapter 4) which exploits the principle of muscle coordination to classify EMG for online myoelectric control. Specifically, the principle of muscle coordination states that force production for a given movement relies on the coordination of different muscles and the EMG amplitude of involved muscles scales uniformly with the amount of force exerted. And therefore, the presented physiology-aware ML method was designed based on the assumption that amplitude-related EMG features for each movement are distributed along the line joining the origin of the feature space and the average maximum voluntary contraction of the movement. This assumption led to a simple training procedure and a computationally efficient solution. The presented physiology-aware ML method was extensively compared with the state-of-the-art ML method using four functional tasks. The results indicated that the new method performs significantly better than the standard ML method, while utilizing less training data and smaller computational effort. Overall, this thesis points to the potential advantage(s) of ML methods that exploit context and physiology information for online myocontrol over standard ML methods (with a static mapping scheme and no modelling of physiology), which largely prevail in the literature. Moreover, all ML methods presented in this thesis are simple, robust and computationally efficient, and therefore, they can be directly used for interfacing most prosthetic devices available in the market, with a minor hardware upgrade.","",""
1,"C. Coglianese","Deploying Machine Learning for a Sustainable Future",2020,"","","","",139,"2022-07-13 09:22:41","","10.2307/j.ctvqc6gcq.26","","",,,,,1,0.50,1,1,2,"To meet the environmental challenges of a warming planet and an increasingly complex, high tech economy, government must become smarter about how it makes policies and deploys its limited resources. It specifically needs to build a robust capacity to analyze large volumes of environmental and economic data by using machine-learning algorithms to improve regulatory oversight, monitoring, and decision-making. Three challenges can be expected to drive the need for algorithmic environmental governance: more problems, less funding, and growing public demands. This paper explains why algorithmic governance will prove pivotal in meeting these challenges, but it also presents four likely obstacles that environmental agencies will need to surmount if they are to take full advantage of big data and predictive analytics. First, agencies must invest in upgrading their information technology infrastructure to take advantage of computational advances. Relatively modest technology investments, if made wisely, could support the use of algorithmic tools that could yield substantial savings in other administrative costs. Second, agencies will need to confront emerging concerns about privacy, fairness, and transparency associated with its reliance on Big Data and algorithmic analyses. Third, government agencies will need to strengthen their human capital so that they have the personnel who understand how to use machine learning responsibly. Finally, to work well, algorithms will need clearly defined objectives. Environmental officials will need to continue to engage with elected officials, members of the public, environmental groups, and industry representatives to forge clarity and consistency over how various risk and regulatory objectives should be specified in machine learning tools. Overall, with thoughtful planning, adequate resources, and responsible management, governments should be able to overcome the obstacles that stand in the way of the use of artificial intelligence to improve environmental sustainability. If policy makers and the public will recognize the need for smarter governance, they can then start to tackle obstacles that stand in its way and better position society for a more sustainable future.","",""
0,"Huaming Chen, Jun Shen, Lei Wang, Yaochu Jin","Towards A More Effective Bidirectional LSTM-Based Learning Model for Human-Bacterium Protein-Protein Interactions",2020,"","","","",140,"2022-07-13 09:22:41","","10.1007/978-3-030-54568-0_10","","",,,,,0,0.00,0,4,2,"","",""
2,"Jingda Wu, Zhiyu Huang, Wenhui Huang, Chen Lv","Prioritized Experience-Based Reinforcement Learning With Human Guidance for Autonomous Driving.",2022,"","","","",141,"2022-07-13 09:22:41","","10.1109/TNNLS.2022.3177685","","",,,,,2,2.00,1,4,1,"Reinforcement learning (RL) requires skillful definition and remarkable computational efforts to solve optimization and control problems, which could impair its prospect. Introducing human guidance into RL is a promising way to improve learning performance. In this article, a comprehensive human guidance-based RL framework is established. A novel prioritized experience replay mechanism that adapts to human guidance in the RL process is proposed to boost the efficiency and performance of the RL algorithm. To relieve the heavy workload on human participants, a behavior model is established based on an incremental online learning method to mimic human actions. We design two challenging autonomous driving tasks for evaluating the proposed algorithm. Experiments are conducted to access the training and testing performance and learning mechanism of the proposed algorithm. Comparative results against the state-of-the-art methods suggest the advantages of our algorithm in terms of learning efficiency, performance, and robustness.","",""
0,"Hiram Ponce","Artificial Hydrocarbon Networks: Chemical Nature Inspiration in Machine Learning",2019,"","","","",142,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,1,3,"Inspiration in nature has been widely explored, from macro to micro-scale. When looking into chemical phenomena, stability and organization are two properties that emerge. Recently, artificial hydrocarbon networks (AHN), a supervised learning method inspired in the inner structures and mechanisms of chemical compounds, have been proposed by as a data-driven approach in artificial intelligence. AHN have been successfully applied in data-driven approaches, such as: regression and classification models, control systems, signal processing, and robotics. To do so, molecules –the basic units of information in AHN– play an important role in the stability, organization and interpretability of this method. Interpretability, saving computing resources, and predictability have been handled by AHN, as any other machine learning model. This short paper aims to highlight the challenges, issues and trends of artificial hydrocarbon networks as a data-driven method. Throughout this document, it presents a description of the main insights of AHN and the efforts to tackle interpretability and training acceleration. Potential applications and future trends on AHN are also discussed. Keywords– machine learning, supervised learning, artificial organic networks, modeling, learning task Introduction Recently, artificial hydrocarbon networks (AHN), a supervised learning method inspired in the inner structures and mechanisms of chemical compounds, have been proposed as a data-driven approach in artificial intelligence [1]. This algorithm, inspired by nature, loosely mimics stability and organization of molecules in order to build organized structures made of packages of information. AHN have proved to be efficient in predictive power when modeling a data-based problem. However, the organizational property has not been strongly analyzed. If this organization capability is conducted in AHN, the output response in data-driven models will reveal, at least in a partial view, the inner structure and functionality of the systems model. So, new ways in building and training AHN are required. Thus, this paper aims to discuss challenges and trends of AHN as a data-driven method, with emphasis on interpretability and training acceleration. This document lays the foundations on AHN for implementing new training algorithms and the way to reveal the chemical nature of data-driven problems. Key Concepts of Artificial Hydrocarbon Networks AHN method was firstly proposed by Ponce and Ponce [2] as an implementation of their more general technique namely artificial organic networks (AON) [1]. In a nutshell, the purpose of the AHN method is to package information, from a set of instances, in basic units known as molecules. These molecules – composed of hydrogen and carbon atoms– are described by nonlinear functions, and they can be related among them using chemical heuristics resulting in complex units of information so-called compounds. Moreover, a set of compounds can be combined together, linearly, producing mixtures. To this end, the mixture constitutes a model [1]. Thus, the inspiration in organic compounds to develop a machine learning method considers three facts observed from nature [3]: (i) stability as the property of compounds to maintain their geometric configurations; (ii) organization based on the ground-state principle aiming to preserve energy minimization within the compounds; and (iii) multi-functionality for promoting transfer learning. For training purposes, the method considers the simple AHN training algorithm [1] which is based on the gradient descent and the numerical solution of least squares estimates via QR-factorization. This training algorithm has reported well performance in predictive power for low-dimensional input spaces, and large training time for computing suitable parameters in the model [4]. Currently, new training methods have been proposed based on hierarchical training [3] or using stochastic-parallel metaheuristic optimization [4]. The later, accelerating training in more than 3,500 times the simple AHN training algorithm. Applications of Artificial Hydrocarbon Networks Literature reports many different applications of AHN. Those can be classified as follows: function approximation and modeling [1]; robust human activity recognition systems [5]; signal processing in denoising audio and face recognition [1,6]; online advertising [6]; intelligent control systems for robotics [1,7,8,11] and mechatronics [1,9,10]; bio/medical applications [5,6,12]; and, theoretical approaches such as hybrid fuzzy-molecular inference systems [8], interpretability of the model [12] and training algorithms [3,4]. Highlights of Artificial Hydrocarbon Networks In these years of AHN, this method has reached notable contributions, as those highlighted following: • Type-II fuzzy inference like behavior – AHN have proved to be significantly similar to type-II fuzzy inference systems, handling noise and allowing experts to tune fuzzy partitions/rules [8]. • Competitive as deep learning – In [5], AHN reported to be significantly similar in performance as deep neural networks (DNN). This suggests that AHN can be trained with less data and obtaining comparable results as DNN. • Interpretability of the model – In contrast to neural networks, AHN can be partially interpreted and converted into decision trees or rules-based models. This interpretability was implemented for medical diagnosis systems [12]. • Reinforcement learning for continuous domains – AHN have been applied for continuous reinforcement learning approaches, in both states and actions, typically found in robotics [11,13]. This approach has also revealed insights on using it as a transfer learning method. Challenges and Trends of Artificial Hydrocarbon Networks Until now, AHN have been successfully applied to different problems and different learning tasks. However, there still are challenges and issues about AHN that have to be faced. Trends in the development and application of AHN can be listed as follows: (i) new training algorithms for AHN are required for better computational performance mainly in time; (ii) the next big step in AHN is parallel computing that opens the possibility for parallel processing and big data analysis; (iii) since the predictive power of AHN is well accurate, it is important to study other functions as kernels and relationships in molecules aiming to perform other approximations; (iv) hybrid approaches with AHN might improve solutions to very complex problems, such as robotics, business intelligence, healthcare, and others; (v) few efforts in dynamic modeling using AHN are reported in literature, so it is important to focus some research in this direction; (vi) transfer learning using pre-defined molecules can be done, but more studies are necessary; (vii) interpretability of machine learning models and specifically of AHN models, are of great importance for knowledge extraction, thus automatic procedures for this task are required; and (viii) open-source coding of AHN is specially required for fast adoption, as the first efforts reported in [14]. Conclusions In this short paper, we summarized the challenges, issues and trends of AHN as a data-driven method. Despite this method was proposed recently, it has been successfully applied to many different intelligent systems. However, there is a need to explore new ways on how this method can be useful. Important issues like interpretability, training acceleration and open-source efforts are also required. Finally, we believe that this method can be added as another powerful tool for practitioners, scientists and researchers in artificial intelligence community.","",""
1,"Yang Lou, Yaodong He, Lin Wang, K. Tsang, Guanrong Chen","Predicting the Robustness of Undirected Network Controllability",2020,"","","","",143,"2022-07-13 09:22:41","","10.23919/CCC50068.2020.9189097","","",,,,,1,0.50,0,5,2,"Robustness of the network controllability reflects how well a networked system can maintain its controllability against destructive attacks. The measure of the network controllability robustness is quantified by a sequence of values that record the remaining controllability of the network after a sequence of node-removal or edge-removal attacks. Traditionally, the controllability robustness is determined by attack simulations, which is computationally time consuming. In this paper, an improved method for predicting the controllability robustness of undirected networks is developed based on machine learning using a convolutional neural network. This approach is motivated by the following observations: 1) there is no clear correlation between the topological features and the controllability robustness of a general undirected network, 2) the adjacency matrix of a network can be represented as a gray-scale image, 3) the convolutional neural network technique has proved successful in image processing without human intervention. In the new framework, preprocessing and filtering are embedded, and a sufficiently large number of training datasets generated by simulations are used to train several convolutional neural networks for classification and prediction, respectively. Extensive experimental studies were carried out, which demonstrate that the proposed framework for predicting the controllability robustness of undirected networks is more accurate and reliable than the conventional single convolutional neural network predictor.","",""
6,"B. Evans, Bing Xue, Mengjie Zhang","An Adaptive and Near Parameter-free Evolutionary Computation Approach Towards True Automation in AutoML",2020,"","","","",144,"2022-07-13 09:22:41","","10.1109/CEC48606.2020.9185770","","",,,,,6,3.00,2,3,2,"A common claim of evolutionary computation methods is that they can achieve good results without the need for human intervention. However, one criticism of this is that there are still hyperparameters which must be tuned in order to achieve good performance. In this work, we propose a near parameter-free genetic programming approach, which adapts the hyperparameter values throughout evolution without ever needing to be specified manually. We apply this to the area of automated machine learning (by extending TPOT), to produce pipelines which can effectively be claimed to be free from human input, and show that the results are competitive with existing state-of-the-art which use hand-selected hyperparameter values. Pipelines begin with a randomly chosen estimator and evolve to competitive pipelines automatically. This work moves towards a truly automated approach to AutoML.","",""
0,"Gautam Chakraborty, Mridusmita Sharma, Navajit Saikia, K. K. Sarma","Soft-computation based speech recognition system for Sylheti language",2022,"","","","",145,"2022-07-13 09:22:41","","10.1007/s10772-022-09976-7","","",,,,,0,0.00,0,4,1,"","",""
296,"Edoardo Pasolli, D. Truong, Fa Malik, L. Waldron, N. Segata","Machine Learning Meta-analysis of Large Metagenomic Datasets: Tools and Biological Insights",2016,"","","","",146,"2022-07-13 09:22:41","","10.1371/journal.pcbi.1004977","","",,,,,296,49.33,59,5,6,"Shotgun metagenomic analysis of the human associated microbiome provides a rich set of microbial features for prediction and biomarker discovery in the context of human diseases and health conditions. However, the use of such high-resolution microbial features presents new challenges, and validated computational tools for learning tasks are lacking. Moreover, classification rules have scarcely been validated in independent studies, posing questions about the generality and generalization of disease-predictive models across cohorts. In this paper, we comprehensively assess approaches to metagenomics-based prediction tasks and for quantitative assessment of the strength of potential microbiome-phenotype associations. We develop a computational framework for prediction tasks using quantitative microbiome profiles, including species-level relative abundances and presence of strain-specific markers. A comprehensive meta-analysis, with particular emphasis on generalization across cohorts, was performed in a collection of 2424 publicly available metagenomic samples from eight large-scale studies. Cross-validation revealed good disease-prediction capabilities, which were in general improved by feature selection and use of strain-specific markers instead of species-level taxonomic abundance. In cross-study analysis, models transferred between studies were in some cases less accurate than models tested by within-study cross-validation. Interestingly, the addition of healthy (control) samples from other studies to training sets improved disease prediction capabilities. Some microbial species (most notably Streptococcus anginosus) seem to characterize general dysbiotic states of the microbiome rather than connections with a specific disease. Our results in modelling features of the “healthy” microbiome can be considered a first step toward defining general microbial dysbiosis. The software framework, microbiome profiles, and metadata for thousands of samples are publicly available at http://segatalab.cibio.unitn.it/tools/metaml.","",""
2,"Yang Lou, Yaodong He, Lin Wang, K. Tsang, Guanrong Chen","Knowledge-Based Prediction of Network Controllability Robustness",2020,"","","","",147,"2022-07-13 09:22:41","","10.1109/TNNLS.2021.3071367","","",,,,,2,1.00,0,5,2,"Network controllability robustness (CR) reflects how well a networked system can maintain its controllability against destructive attacks. Its measure is quantified by a sequence of values that record the remaining controllability of the network after a sequence of node-removal or edge-removal attacks. Traditionally, the CR is determined by attack simulations, which is computationally time-consuming or even infeasible. In this article, an improved method for predicting the network CR is developed based on machine learning using a group of convolutional neural networks (CNNs). In this scheme, a number of training data generated by simulations are used to train the group of CNNs for classification and prediction, respectively. Extensive experimental studies are carried out, which demonstrate that 1) the proposed method predicts more precisely than the classical single-CNN predictor; 2) the proposed CNN-based predictor provides a better predictive measure than the traditional spectral measures and network heterogeneity.","",""
1,"Ankit Kumar, K. Jani, Abhishek Kumar Jishu, Hrishitva Patel, Aditya Kumar Sharma, M. Khare","Evaluation of Deep Learning Based Human Expression Recognition on Noisy Images",2020,"","","","",148,"2022-07-13 09:22:41","","10.1109/ISCMI51676.2020.9311549","","",,,,,1,0.50,0,6,2,"Several engineers are currently attempting to recognize the emotions of an individual based on facial expressions. Previously, the images used for human expression recognition had a high resolution and were of good quality. In practice, due to factors such as the camera sensor being sensitive to temperature and illumination and noise added to the transmission of images from a camera to a computer system, noise is added to pictures. It changes the pixel values. Past research has not studied the impact of these factors in great detail. We have proposed a Convolutional Neural Network (CNN) based human expression recognition system and evaluated its performance in the presence of different types of noise. We have also compared the CNN-based model with the Support Vector machine algorithm that has been used in the past to explain why CNN-based human expression recognition is more robust than other face recognition methods. Based on the findings from the experiments, we offer suggestions for developers using human expression recognition technologies.","",""
0,"Aires da Conceição, S. Degadwala","Convolutional Neural Network Computation for Autonomous Vehicle",2020,"","","","",149,"2022-07-13 09:22:41","","10.32628/cseit2062112","","",,,,,0,0.00,0,2,2,"Self-driving vehicle is a vehicle that can drive by itself it means without human interaction. This system shows how the computer can learn and the over the art of driving using machine learning techniques. This technique includes line lane tracker, robust feature extraction and convolutional neural network.","",""
2,"A. Nazir","Seamless Automation and Integration of Machine Learning Capabilities for Big Data Analytics",2017,"","","","",150,"2022-07-13 09:22:41","","10.5121/IJDPS.2017.8301","","",,,,,2,0.40,2,1,5,"The paper aims at proposing a solution for designing and developing a seamless automation and integration of machine learning capabilities for Big Data with the following requirements: 1) the ability to seamlessly handle and scale very large amount of unstructured and structured data from diversified and heterogeneous sources; 2) the ability to systematically determine the steps and procedures needed for analyzing Big Data datasets based on data characteristics, domain expert inputs, and data pre-processing component; 3) the ability to automatically select the most appropriate libraries and tools to compute and accelerate the machine learning computations; and 4) the ability to perform Big Data analytics with high learning performance, but with minimal human intervention and supervision. The whole focus is to provide a seamless automated and integrated solution which can be effectively used to analyze Big Data with highfrequency and high-dimensional features from different types of data characteristics and different application problem domains, with high accuracy, robustness, and scalability. This paper highlights the research methodologies and research activities that we propose to be conducted by the Big Data researchers and practitioners in order to develop and support seamless automation and integration of machine learning capabilities for Big Data analytics.","",""
51,"Wen Qi, Hang Su, A. Aliverti","A Smartphone-Based Adaptive Recognition and Real-Time Monitoring System for Human Activities",2020,"","","","",151,"2022-07-13 09:22:41","","10.1109/THMS.2020.2984181","","",,,,,51,25.50,17,3,2,"Human activity recognition (HAR) using smartphones provides significant healthcare guidance for telemedicine and long-term treatment. Machine learning and deep learning (DL) techniques are widely utilized for the scientific study of the statistical models of human behaviors. However, the performance of existing HAR platforms is limited by complex physical activity. In this article, we proposed an adaptive recognition and real-time monitoring system for human activities (Ada-HAR), which is expected to identify more human motions in dynamic situations. The Ada-HAR framework introduces an unsupervised online learning algorithm that is independent of the number of class constraints. Furthermore, the adopted hierarchical clustering and classification algorithms label and classify 12 activities (five dynamics, six statics, and a series of transitions) autonomously. Finally, practical experiments have been performed to validate the effectiveness and robustness of the proposed algorithms. Compared with the methods mentioned in the literature, the results show that the DL-based classifier obtains a higher recognition rate ($\text{95.15}\%$, waist, and $\text{92.20}\%$, pocket). The decision-tree-based classifier is the fastest method for modal evolution. Finally, the Ada-HAR system can monitor human activity in real time, regardless of the direction of the smartphone.","",""
2,"Jian Zhang, Xing-wen Liang, Feng Zhou, Bo Li, Yanling Li","TYLER, a fast method that accurately predicts cyclin-dependent proteins by using computation-based motifs and sequence-derived features.",2021,"","","","",152,"2022-07-13 09:22:41","","10.3934/mbe.2021318","","",,,,,2,2.00,0,5,1,"Cyclins and related cyclin-dependent kinases play vital roles in regulating the progression in the cell cycle. Understanding the intrinsic mechanisms of cyclins promises knowledge about cell uncontrolled proliferation and prevention of cancer cells. Therefore, accurate recognition of cyclins is important for the investigation of tumor cells and biomedical engineering. This study proposes a novel sequence-based predictor named TYLER (predicT cYcLin-dEpendent pRoteins) for addressing the long challenge problem of predicting cyclin-dependent proteins (CDPs). We use information theory to compute selectively enriched CDP-related motifs and build the motif-based model. For those proteins without sharing enriched motifs, we compute sequence-derived features and construct machine learning-based models. We optimize the weights of two different models to build a more accurate predictor. We estimate these two types of models by using 5-fold cross-validations on the TRAINING dataset. We prove that the combination of two models and optimization of the corresponding weights promises decent and robust results on both TRAINING and independent TEST dataset. The empirical test demonstrates that TYLER is robust predictor and statistically significantly better than current methods. The runtime assessment reveals TYLER is a high-throughput effective method. We use TYLER to make predictions on the human proteome, and use the results to hypothesize CDPs. The latest experimental verified CDPs and GO analysis proves that some of our novel predictions shall be potential CDPs. TYLER is implemented as a public user-friendly web server at http://www.inforstation.com/webservers/TYLER/. We share all data and source code that used in this research at https://github.com/biocomputinglab/TYLER.git.","",""
1,"Nanyang Ye, Jingxuan Tang, Huayu Deng, Xiao-Yun Zhou, Qianxiao Li, Zhenguo Li, Guang-Zhong Yang, Zhanxing Zhu","Adversarial Invariant Learning",2021,"","","","",153,"2022-07-13 09:22:41","","10.1109/CVPR46437.2021.01226","","",,,,,1,1.00,0,8,1,"Though machine learning algorithms are able to achieve pattern recognition from the correlation between data and labels, the presence of spurious features in the data decreases the robustness of these learned relationships with respect to varied testing environments. This is known as out-of-distribution (OoD) generalization problem. Recently, invariant risk minimization (IRM) attempts to tackle this issue by penalizing predictions based on the unstable spurious features in the data collected from different environments. However, similar to domain adaptation or domain generalization, a prevalent non-trivial limitation in these works is that the environment information is assigned by human specialists, i.e. a priori, or determined heuristically. However, an inappropriate group partitioning can dramatically deteriorate the OoD generalization and this process is expensive and time-consuming. To deal with this issue, we propose a novel theoretically principled min-max framework to iteratively construct a worst-case splitting, i.e. creating the most challenging environment splittings for the backbone learning paradigm (e.g. IRM) to learn the robust feature representation. We also design a differentiable training strategy to facilitate the feasible gradient- based computation. Numerical experiments show that our algorithmic framework has achieved superior and stable performance in various datasets, such as Colored MNIST and Punctuated Stanford sentiment treebank (SST). Furthermore, we also find our algorithm to be robust even to a strong data poisoning attack. To the best of our knowledge, this is one of the first to adopt differentiable environment splitting method to enable stable predictions across environments without environment index information, which achieves the state-of-the-art performance on datasets with strong spurious correlation, such as Colored MNIST.","",""
21,"Sebastian Robert, S. Büttner, C. Röcker, Andreas Holzinger","Reasoning Under Uncertainty: Towards Collaborative Interactive Machine Learning",2016,"","","","",154,"2022-07-13 09:22:41","","10.1007/978-3-319-50478-0_18","","",,,,,21,3.50,5,4,6,"","",""
680,"A. Mannini, A. Sabatini","Machine Learning Methods for Classifying Human Physical Activity from On-Body Accelerometers",2010,"","","","",155,"2022-07-13 09:22:41","","10.3390/s100201154","","",,,,,680,56.67,340,2,12,"The use of on-body wearable sensors is widespread in several academic and industrial domains. Of great interest are their applications in ambulatory monitoring and pervasive computing systems; here, some quantitative analysis of human motion and its automatic classification are the main computational tasks to be pursued. In this paper, we discuss how human physical activity can be classified using on-body accelerometers, with a major emphasis devoted to the computational algorithms employed for this purpose. In particular, we motivate our current interest for classifiers based on Hidden Markov Models (HMMs). An example is illustrated and discussed by analysing a dataset of accelerometer time series.","",""
19,"S. Palazzo, C. Spampinato, I. Kavasidis, D. Giordano, Joseph Schmidt, M. Shah","Decoding Brain Representations by Multimodal Learning of Neural Activity and Visual Features",2018,"","","","",156,"2022-07-13 09:22:41","","10.1109/tpami.2020.2995909","","",,,,,19,4.75,3,6,4,"This work presents a novel method of exploring human brain-visual representations, with a view towards replicating these processes in machines. The core idea is to learn plausible computational and biological representations by correlating human neural activity and natural images. Thus, we first propose a model, EEG-ChannelNet, to learn a brain manifold for EEG classification. After verifying that visual information can be extracted from EEG data, we introduce a multimodal approach that uses deep image and EEG encoders, trained in a siamese configuration, for learning a joint manifold that maximizes a compatibility measure between visual features and brain representations. We then carry out image classification and saliency detection on the learned manifold. Performance analyses show that our approach satisfactorily decodes visual information from neural signals. This, in turn, can be used to effectively supervise the training of deep learning models, as demonstrated by the high performance of image classification and saliency detection on out-of-training classes. The obtained results show that the learned brain-visual features lead to improved performance and simultaneously bring deep models more in line with cognitive neuroscience work related to visual perception and attention.","",""
1,"T. Tasdizen, Mojtaba Seyedhosseini, Ting Liu, C. Jones, E. Jurrus","Image Segmentation for Connectomics Using Machine Learning",2014,"","","","",157,"2022-07-13 09:22:41","","10.1007/978-1-4614-7245-2_10","","",,,,,1,0.13,0,5,8,"","",""
8,"Yang Lou, Yaodong He, Lin Wang, Guanrong Chen","Predicting Network Controllability Robustness: A Convolutional Neural Network Approach",2019,"","","","",158,"2022-07-13 09:22:41","","10.1109/TCYB.2020.3013251","","",,,,,8,2.67,2,4,3,"Network controllability measures how well a networked system can be controlled to a target state, and its robustness reflects how well the system can maintain the controllability against malicious attacks by means of node removals or edge removals. The measure of network controllability is quantified by the number of external control inputs needed to recover or to retain the controllability after the occurrence of an unexpected attack. The measure of the network controllability robustness, on the other hand, is quantified by a sequence of values that record the remaining controllability of the network after a sequence of attacks. Traditionally, the controllability robustness is determined by attack simulations, which is computationally time consuming. In this article, a method to predict the controllability robustness based on machine learning using a convolutional neural network (CNN) is proposed, motivated by the observations that: 1) there is no clear correlation between the topological features and the controllability robustness of a general network; 2) the adjacency matrix of a network can be regarded as a grayscale image; and 3) the CNN technique has proved successful in image processing without human intervention. Under the new framework, a fairly large number of training data generated by simulations are used to train a CNN for predicting the controllability robustness according to the input network-adjacency matrices, without performing conventional attack simulations. Extensive experimental studies were carried out, which demonstrate that the proposed framework for predicting controllability robustness of different network configurations is accurate and reliable with very low overheads.","",""
134,"Matthew Lease","On Quality Control and Machine Learning in Crowdsourcing",2011,"","","","",159,"2022-07-13 09:22:41","","","","",,,,,134,12.18,134,1,11,"The advent of crowdsourcing has created a variety of new opportunities for improving upon traditional methods of data collection and annotation. This in turn has created intriguing new opportunities for data-driven machine learning (ML). Convenient access to crowd workers for simple data collection has further generalized to leveraging more arbitrary crowd-based human computation (von Ahn 2005) to supplement automated ML. While new potential applications of crowdsourcing continue to emerge, a variety of practical and sometimes unexpected obstacles have already limited the degree to which its promised potential can be actually realized in practice. This paper considers two particular aspects of crowdsourcing and their interplay, data quality control (QC) and ML, reflecting on where we have been, where we are, and where we might go from here.","",""
7,"Yong-Zi Chen, Zhuo Wang, Yanan Wang, G. Ying, Zhen Chen, Jiangning Song","nhKcr: a new bioinformatics tool for predicting crotonylation sites on human nonhistone proteins based on deep learning.",2021,"","","","",160,"2022-07-13 09:22:41","","10.1093/bib/bbab146","","",,,,,7,7.00,1,6,1,"Lysine crotonylation (Kcr) is a newly discovered type of protein post-translational modification and has been reported to be involved in various pathophysiological processes. High-resolution mass spectrometry is the primary approach for identification of Kcr sites. However, experimental approaches for identifying Kcr sites are often time-consuming and expensive when compared with computational approaches. To date, several predictors for Kcr site prediction have been developed, most of which are capable of predicting crotonylation sites on either histones alone or mixed histone and nonhistone proteins together. These methods exhibit high diversity in their algorithms, encoding schemes, feature selection techniques and performance assessment strategies. However, none of them were designed for predicting Kcr sites on nonhistone proteins. Therefore, it is desirable to develop an effective predictor for identifying Kcr sites from the large amount of nonhistone sequence data. For this purpose, we first provide a comprehensive review on six methods for predicting crotonylation sites. Second, we develop a novel deep learning-based computational framework termed as CNNrgb for Kcr site prediction on nonhistone proteins by integrating different types of features. We benchmark its performance against multiple commonly used machine learning classifiers (including random forest, logitboost, naïve Bayes and logistic regression) by performing both 10-fold cross-validation and independent test. The results show that the proposed CNNrgb framework achieves the best performance with high computational efficiency on large datasets. Moreover, to facilitate users' efforts to investigate Kcr sites on human nonhistone proteins, we implement an online server called nhKcr and compare it with other existing tools to illustrate the utility and robustness of our method. The nhKcr web server and all the datasets utilized in this study are freely accessible at http://nhKcr.erc.monash.edu/.","",""
488,"Cha Zhang, Yunqian Ma","Ensemble Machine Learning: Methods and Applications",2012,"","","","",161,"2022-07-13 09:22:41","","","","",,,,,488,48.80,244,2,10,"It is common wisdom that gathering a variety of views and inputs improves the process of decision making, and, indeed, underpins a democratic society. Dubbed ensemble learning by researchers in computational intelligence and machine learning, it is known to improve a decision systems robustness and accuracy. Now, fresh developments are allowing researchers to unleash the power of ensemble learning in an increasing range of real-world applications. Ensemble learning algorithms such as boosting and random forest facilitate solutions to key computational issues such as face recognition and are now being applied in areas as diverse as object tracking and bioinformatics. Responding to a shortage of literature dedicated to the topic, this volume offers comprehensive coverage of state-of-the-art ensemble learning techniques, including the random forest skeleton tracking algorithm in the Xbox Kinect sensor, which bypasses the need for game controllers. At once a solid theoretical study and a practical guide, the volume is a windfall for researchers and practitioners alike.","",""
12,"Andras Rozsa, T. Boult","Improved Adversarial Robustness by Reducing Open Space Risk via Tent Activations",2019,"","","","",162,"2022-07-13 09:22:41","","","","",,,,,12,4.00,6,2,3,"Adversarial examples contain small perturbations that can remain imperceptible to human observers but alter the behavior of even the best performing deep learning models and yield incorrect outputs. Since their discovery, adversarial examples have drawn significant attention in machine learning: researchers try to reveal the reasons for their existence and improve the robustness of machine learning models to adversarial perturbations. The state-of-the-art defense is the computationally expensive and very time consuming adversarial training via projected gradient descent (PGD). We hypothesize that adversarial attacks exploit the open space risk of classic monotonic activation functions. This paper introduces the tent activation function with bounded open space risk and shows that tents make deep learning models more robust to adversarial attacks. We demonstrate on the MNIST dataset that a classifier with tents yields an average accuracy of 91.8% against six white-box adversarial attacks, which is more than 15 percentage points above the state of the art. On the CIFAR-10 dataset, our approach improves the average accuracy against the six white-box adversarial attacks to 73.5% from 41.8% achieved by adversarial training via PGD.","",""
23,"Antonio Loquercio, Elia Kaufmann, René Ranftl, Matthias Müller, V. Koltun, D. Scaramuzza","Learning high-speed flight in the wild",2021,"","","","",163,"2022-07-13 09:22:41","","10.1126/scirobotics.abg5810","","",,,,,23,23.00,4,6,1,"Description Deep Learning enables agile flight in challenging environments with onboard sensing and computation. Quadrotors are agile. Unlike most other machines, they can traverse extremely complex environments at high speeds. To date, only expert human pilots have been able to fully exploit their capabilities. Autonomous operation with onboard sensing and computation has been limited to low speeds. State-of-the-art methods generally separate the navigation problem into subtasks: sensing, mapping, and planning. Although this approach has proven successful at low speeds, the separation it builds upon can be problematic for high-speed navigation in cluttered environments. The subtasks are executed sequentially, leading to increased processing latency and a compounding of errors through the pipeline. Here, we propose an end-to-end approach that can autonomously fly quadrotors through complex natural and human-made environments at high speeds with purely onboard sensing and computation. The key principle is to directly map noisy sensory observations to collision-free trajectories in a receding-horizon fashion. This direct mapping drastically reduces processing latency and increases robustness to noisy and incomplete perception. The sensorimotor mapping is performed by a convolutional network that is trained exclusively in simulation via privileged learning: imitating an expert with access to privileged information. By simulating realistic sensor noise, our approach achieves zero-shot transfer from simulation to challenging real-world environments that were never experienced during training: dense forests, snow-covered terrain, derailed trains, and collapsed buildings. Our work demonstrates that end-to-end policies trained in simulation enable high-speed autonomous flight through challenging environments, outperforming traditional obstacle avoidance pipelines.","",""
2,"Benjamin M. Marlin, T. Abdelzaher, G. Ciocarlie, Adam D. Cobb, Mark S. Dennison, Brian Jalaian, Lance M. Kaplan, Tiffany R. Raber, A. Raglin, P. Sharma, M. Srivastava, T. Trout, Meet P. Vadera, Maggie B. Wigness","On Uncertainty and Robustness in Large-Scale Intelligent Data Fusion Systems",2020,"","","","",164,"2022-07-13 09:22:41","","10.1109/CogMI50398.2020.00020","","",,,,,2,1.00,0,14,2,"The resurgence of AI in the recent decade dramatically changes the design of modern sensor data fusion systems, leading to new challenges, opportunities, and research directions. One of these challenges is the management of uncertainty. This paper develops a framework to reason about sources of uncertainty, develops representations of uncertainty, and investigates uncertainty mitigation strategies in modern intelligent data processing systems. Insights are developed into workflow composition that maximizes efficacy at accomplishing mission goals despite the sources of uncertainty, while leveraging a collaboration of humans, algorithms, and machine learning components.","",""
41,"M. Ladds, Adam P. Thompson, Julianna P Kadar, D. Slip, D. Hocking, R. Harcourt","Super machine learning: improving accuracy and reducing variance of behaviour classification from accelerometry",2017,"","","","",165,"2022-07-13 09:22:41","","10.1186/s40317-017-0123-1","","",,,,,41,8.20,7,6,5,"","",""
0,"J. Dietlmeier","A machine learning approach to the unsupervised segmentation of mitochondria in subcellular electron microscopy data",2016,"","","","",166,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,1,6,"Recent advances in cellular and subcellular microscopy demonstrated its potential towards unravelling the mechanisms of various diseases at the molecular level. The biggest challenge in both human- and computer-based visual analysis of micrographs is the variety of nanostructures and mitochondrial morphologies. The state-of-the-art is, however, dominated by supervised manual data annotation and early attempts to automate the segmentation process were based on supervised machine learning techniques which require large datasets for training. Given a minimal number of training sequences or none at all, unsupervised machine learning formulations, such as spectral dimensionality reduction, are known to be superior in detecting salient image structures.  This thesis presents three major contributions developed around the spectral clustering framework which is proven to capture perceptual organization features. Firstly, we approach the problem of mitochondria localization. We propose a novel grouping method for the extracted line segments which describes the normal mitochondrial morphology. Experimental findings show that the clusters obtained successfully model the inner mitochondrial membrane folding and therefore can be used as markers for the subsequent segmentation approaches. Secondly, we developed an unsupervised mitochondria segmentation framework. This method follows the evolutional ability of human vision to extrapolate salient membrane structures in a micrograph. Furthermore, we designed robust non-parametric similarity models according to Gestaltic laws of visual segregation. Experiments demonstrate that such models automatically adapt to the statistical structure of the biological domain and return optimal performance in pixel classification tasks under the wide variety of distributional assumptions. The last major contribution addresses the computational complexity of spectral clustering. Here, we introduced a new anticorrelation-based spectral clustering formulation with the objective to improve both: speed and quality of segmentation. The experimental findings showed the applicability of our dimensionality reduction algorithm to very large scale problems as well as asymmetric, dense and non-Euclidean datasets.","",""
19,"V. Kůrková, Y. Manolopoulos, B. Hammer, L. Iliadis, Ilias Maglogiannis","Artificial Neural Networks and Machine Learning – ICANN 2018",2018,"","","","",167,"2022-07-13 09:22:41","","10.1007/978-3-030-01424-7","","",,,,,19,4.75,4,5,4,"","",""
17,"L. Iliadis, Ilias Maglogiannis","Artificial Neural Networks and Machine Learning – ICANN 2018",2018,"","","","",168,"2022-07-13 09:22:41","","10.1007/978-3-030-01421-6","","",,,,,17,4.25,9,2,4,"","",""
0,"Featurization As","The Third Army Research Office ( ARO ) Workshop on Adversarial Machine Learning Talk Abstracts and Bios Data",2018,"","","","",169,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,1,4,"The proliferation of machine learning (ML) and artificial intelligence (AI) systems for military and security applications creates substantial challenges for designing and deploying such mechanisms that would learn, adapt, reason and act with Dinky, Dirty, Dynamic, Deceptive, Distributed (D5) data. While Dinky and Dirty challenges have been extensively explored in ML theory, the Dynamic challenge has been a persistent problem in ML applications (when the statistical distribution of training data differs from that of test data). The most recent Deceptive challenge is a malicious distribution shift between training and test data that amplifies the effects of the Dynamic challenge to the complete breakdown of the ML algorithms. Using the MNIST dataset as a simple calibration example, we explore the following two questions: (1) What geometric and statistical characteristics of data distribution can be exploited by an adversary with a given magnitude of the attack? (2) What counter-measures can be used to protect the constructed decision rule (at the cost of somewhat decreased performance) against malicious distribution shift within a given magnitude of the attack? While not offering a complete solution to the problem, we collect and interpret obtained observations in a way that provides practical guidance for making more adversary-resistant choices in the design of ML algorithms. Bio: Rauf Izmailov is a Senior Research Scientist at Perspecta Labs and an established researcher in mathematical and computer models for networking and control systems, machine learning, optimization, and statistical data analysis. He has more than 20 years of industry experience (including AT&T Bell Labs and NEC Labs America) in research and technical leadership of R&D teams. With Dr. Vapnik, he co-invented the new machine learning paradigm, Learning Using Privileged Information (LUPI). He was the PI on the DARPA PPAML (“Probabilistic Programming for Advanced Machine Learning”) program and is currently the PI on the DARPA D3M (“Data-Driven Discovery of Models”) program and the analytics task leader on the DARPA LADS (“Leveraging the Analog Domain for Security”) program. He is also a co-PI on the AFOSR program “Science of Information, Computation, Learning and Fusion”. Adversarial Unsupervised Learning Abstract: Nowadays more and more data are gathered for detecting and preventing cyber attacks. In cyber security applications, data analytics techniques have to deal with active adversaries that try to deceive the data analytics models and avoid being detected. The existence of such adversarial behavior motivates the development of robust and resilient adversarial learning techniques for various tasks. Most of the existing work focused on adversarial classification techniques, which assumed the existence of a large amount of labeled data instances. However, in practice, labeling the data instances often requires costly and time-consuming human expertise and becomes a significant bottleneck. Nowadays more and more data are gathered for detecting and preventing cyber attacks. In cyber security applications, data analytics techniques have to deal with active adversaries that try to deceive the data analytics models and avoid being detected. The existence of such adversarial behavior motivates the development of robust and resilient adversarial learning techniques for various tasks. Most of the existing work focused on adversarial classification techniques, which assumed the existence of a large amount of labeled data instances. However, in practice, labeling the data instances often requires costly and time-consuming human expertise and becomes a significant bottleneck. Meanwhile, a large number of unlabeled instances can also be used to understand the adversaries' behavior. To address the above mentioned challenges, we develop a novel grid based adversarial clustering algorithm. Our adversarial clustering algorithm is able to identify the normal and abnormal regions, and to draw defensive walls around the centers of the normal objects utilizing game theoretic ideas. Our algorithm also identifies the overlapping areas within large mixed clusters, and outliers which may be potential anomalies. Bio: Bowei Xi received her Ph.D. in statistics from the Department of Statistics at the University of Michigan, Ann Arbor in 2004. She is an associate professor in the Department of Statistics at Purdue University. She was a visiting faculty in the Department of Statistics at Stanford University in summer 2007, and a visiting faculty at Statistical and Applied Mathematical Sciences Institute (SAMSI) from September 2012 to May 2013. Her research focuses on multidisciplinary work involving big datasets with complex structure from very different application areas including cyber security, Internet traffic, metabolomics, machine learning, and data mining. She has a US patent on an automatic system configuration tool and has filed another patent application for identification of blood-based metabolite biomarkers of pancreatic cancer. Limitations of the Lipschitz Constant as a Defense Against Adversarial Examples Abstract: Several recent papers have discussed utilizing Lipschitz constants to limit the susceptibility of neural networks to adversarial examples. We analyze recently proposed methods for computing the Lipschitz constant. We show that the Lipschitz constant may indeed enable adversarially robust neural networks. However, the methods currently employed for computing it suffer from theoretical and practical limitations. We argue that addressing this shortcoming is a promising direction for future research into certified adversarial defenses. Several recent papers have discussed utilizing Lipschitz constants to limit the susceptibility of neural networks to adversarial examples. We analyze recently proposed methods for computing the Lipschitz constant. We show that the Lipschitz constant may indeed enable adversarially robust neural networks. However, the methods currently employed for computing it suffer from theoretical and practical limitations. We argue that addressing this shortcoming is a promising direction for future research into certified adversarial defenses. Bio: Todd Huster is a research scientist at Perspecta Labs. He has extensive experience solving challenging problems in the fields of machine learning, remote sensing, evaluation methodologies, and symbolic reasoning. He holds an M.S. in Computer Science from Wright State University. Certified Defenses Against Adversarial Examples Abstract: While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this talk, I will present some methods based on convex relaxations (with a focus on semidefinite programming) that output a certificate that for a given network and test input, no attack can force the error to exceed a certain value. I will then discuss how these certification procedures can be incorporated into neural network training to obtain provably While neural networks have achieved high accuracy on standard image classification benchmarks, their accuracy drops to nearly zero in the presence of small adversarial perturbations to test inputs. Defenses based on regularization and adversarial training have been proposed, but often followed by new, stronger attacks that defeat these defenses. Can we somehow end this arms race? In this talk, I will present some methods based on convex relaxations (with a focus on semidefinite programming) that output a certificate that for a given network and test input, no attack can force the error to exceed a certain value. I will then discuss how these certification procedures can be incorporated into neural network training to obtain provably robust networks. Finally, I will present some empirical results on the performance of attacks and different certificates on networks trained using different objectives. This is joint work with Jacob Steinhardt and Percy Liang. Bio: Aditi Raghunathan is a third year PhD student at Stanford University working with Percy Liang. She is a recipient of the Google PhD Fellowship in Machine Learning and the Open Philanthrophy Project AI Fellowship. She is primarily interested in making machine learning systems provably robust to adversarial perturbations. She is also interested in ensuring fairness in the outcomes of ML systems. She spent the summer of 2018 at Google Brain working with Ian Goodfellow and Alex Kurakin. Previously, she was an undergraduate at IIT Madras. Is Robust ML Really Robust? Abstract: Machine learning (ML) techniques are increasingly common in security applications, such as malware and intrusion detection. However, ML models are often susceptible to evasion attacks, in which an adversary makes changes to the input (such as malware) in order to avoid being detected. A conventional approach to evaluate ML robustness to such attacks, as well as to design robust ML, is by considering simplified feature-space models of attacks, where the attacker changes ML features directly to effect evasion, while minimizing or constraining the magnitude of this change. We investigate the effectiveness of this approach to designing robust ML in the face of attacks that can be realized in actual malware (realizable attacks). We demonstrate that in the context of structure-based PDF malware detection, such techniques appear to have limited effectiveness. On the other hand, they are quite effective with contentbased detectors. In either case, we show that augmenting the feature space models with conserved features (those that cannot be unilaterally modified without compromising malicious functionality","",""
0,"G. Taherzadeh","Protein Function Prediction by Machine Learning",2018,"","","","",170,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,1,4,"Overwhelmed with genomic data, determining functions of previously unseen proteins is one of the most challenging problems. While most protein functions can often be inferred from their homologous counterparts with known functions in other species, not all proteins have homologs whose functions were determined. The functional roles are performed by interactions between proteins and other biologically active molecules. Thus, the first step to identify protein function through its interaction is to detect potential binding sites of the protein. Moreover, protein functions may alter when proteins undergo some modifications. Obviously, experimental determination of functions for millions of new proteins is not practical due to vast amount of possible functions to be tested. Thus, it is highly desirable to have computational tools to prioritize possible functions for new proteins.  In this thesis, we proposed machine learning-based methods for predicting putative binding sites of proteins interacting with small molecules, specifically peptides and carbohydrates, in addition to predicting putative sites of post-translational modifications (PTMs). The main contributions of our methods lie in three aspects. First, we proposed the first predictive model to predict protein-peptide binding sites without the knowledge of the protein structure (Taherzadeh et al. 2016). The method was further improved by using experimental structures. The performance of the method is robust even if unbound structures or quality model structures built from homologs were employed, indicating the wide applicability of the method developed (Taherzadeh et al. 2017). Second, we established the first publicly available tool for predicting carbohydrate binding sites in the absence of protein structures (Taherzadeh et al. 2016). Accurate performance of this method is confirmed by predicting more binding residues in carbohydrate-binding proteins than in non-binding proteins in human proteome and by its successful application to 1000 Genomes Project. Third, we proposed a method for predicting post-translational modification (PTM) site of lysine malonylation (Taherzadeh et al.). This predictive model built from M. musculus proteins achieved comparable performance when tested on H. sapiens proteins. All aforementioned methods are thoroughly assessed on cross-validation and the independent test sets after removing homologue sequences. Consistent performance on cross-validation and independent datasets confirmed the accuracy and robustness of predictive methods. All methods significantly outperform existing techniques.","",""
0,"","Neural Network Training Using Genetic Algorithms Series In Machine Perception And Artificial Intelligence",2021,"","","","",171,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,0,1,"Knowledge-Based Intelligent Information and Engineering Systems 2Nature-inspired Methods in Chemometrics: Genetic Algorithms and Artificial Neural NetworksParallel Implementations of Backpropagation Neural Networks on TransputersEvolutionary Algorithms and Neural NetworksTraining Neural Networks Using Hybrids with Genetic AlgorithmsNeural Network Training Using Genetic AlgorithmsGene Expression ProgrammingTraining a Neural Network with a Genetic AlgorithmMethods and Applications of Artificial IntelligenceClassification and Learning Using Genetic AlgorithmsPractical Computer Vision Applications Using Deep Learning with CNNsIntelligent Hybrid SystemsNeurogenetic LearningAdvances in Neural Networks ISNN 2007Hybrid Intelligent SystemsEncyclopedia of Computer Science and TechnologyMachine LearningUsing a Genetic Algorithm in Training an Artificial Neural Network to Implement the XOR FunctionGenetic and Evolutionary Computation — GECCO 2004Handbook of Fuzzy ComputationNeural Network Data Analysis Using SimulnetTMArtificial Neural Nets and Genetic AlgorithmsApplied Soft Computing Technologies: The Challenge of ComplexityGenetic Algorithm for Artificial Neural Network Training for the Purpose of Automated Part RecognitionNEURAL NETWORKS, FUZZY LOGIC AND GENETIC ALGORITHMAutomatic Generation of Neural Network Architecture Using Evolutionary ComputationPGANETThe Sixth International Symposium on Neural Networks (ISNN 2009)Evolutionary Machine Learning TechniquesModeling Decisions for Artificial IntelligenceEmpirical Studies on the Utility of Genetic Algorithms for Training and Designing of Neural NetworksTraining Neural Networks Using Genetic AlgorithmsTraining feedforward neural networks using genetic algorithmsMetaheuristic Procedures for Training Neural NetworksArtificial Neural Nets and Genetic AlgorithmsNature-Inspired Computing: Concepts, Methodologies, Tools, and ApplicationsApplications of Evolutionary ComputingArtificial Intelligence and CreativityArtificial Neural Nets and Genetic AlgorithmsDeep Learning Using Genetic Algorithms Creativity is one of the least understood aspects of intelligence and is often seen as `intuitive' and not susceptible to rational enquiry. Recently, however, there has been a resurgence of interest in the area, principally in artificial intelligence and cognitive science, but also in psychology, philosophy, computer science, logic, mathematics, sociology, and architecture and design. This volume brings this work together and provides an overview of this rapidly developing field. It addresses a range of issues. Can computers be creative? Can they help us to understand human creativity? How can artificial intelligence (AI) enhance human creativity? How, in particular, can it contribute to the `sciences of the artificial', such as design? Does the new wave of AI (connectionism, geneticism and artificial life) offer more promise in these areas than classical, symbol-handling AI? What would the implications be for AI and cognitive science if computers could not be creative? These issues are explored in five interrelated parts, each of which is introducted and explained by a leading figure in the field. Prologue (Margaret Boden) Part I: Foundational Issues (Terry Dartnall) Part II: Creativity and Cognition (Graeme S. Halford and Robert Levinson) Part III: Creativity and Connectionism (Chris Thornton) Part IV: Creativity and Design (John Gero) Part V: Human Creativity Enhancement (Ernest Edmonds) Epilogue (Douglas Hofstadter) For researchers in AI, cognitive science, computer science, philosophy, psychology, mathematics, logic, sociology, and architecture and design; and anyone interested in the rapidly growing field of artificial intelligence and creativity.From the contents: Neural networks – theory and applications: NNs (= neural networks) classifier on continuous data domains– quantum associative memory – a new class of neuron-like discrete filters to image processing – modular NNs for improving generalisation properties – presynaptic inhibition modelling for image processing application – NN recognition system for a curvature primal sketch – NN based nonlinear temporalspatial noise rejection system – relaxation rate for improving Hopfield network – Oja's NN and influence of the learning gain on its dynamics Genetic algorithms – theory and applications: transposition: a biological-inspired mechanism to use with GAs (= genetic algorithms) – GA for decision tree induction – optimising decision classifications using GAs – scheduling tasks with intertask communication onto multiprocessors by GAs – design of robust networks with GA – effect of degenerate coding on GAs – multiple traffic signal control using a GA – evolving musical harmonisation – niched-penalty approach for constraint handling in GAs – GA with dynamic population size – GA with dynamic niche clustering for multimodal function optimisation Soft computing and uncertainty: self-adaptation of evolutionary constructed decision trees by information spreading – evolutionary programming of near optimal NNsArtificial neural networks and genetic algorithms both are areas of research","",""
13,"Liang Wang, Li Cheng, Guoying Zhao","Machine Learning for Human Motion Analysis - Theory and Practice",2009,"","","","",172,"2022-07-13 09:22:41","","10.4018/978-1-60566-900-7","","",,,,,13,1.00,4,3,13,"""With the ubiquitous presence of video data and its increasing importance in a wide range of real-world applications, it is becoming increasingly necessary to automatically analyze and interpret object motions from large quantities of footage. Machine Learning for Human Motion Analysis: Theory and Practice highlights the development of robust and effective vision-based motion understanding systems. This advanced publication addresses a broad audience including practicing professionals working with specific vision applications such as surveillance, sport event analysis, healthcare, video conferencing, and motion video indexing and retrieval.""","",""
0,"Lee","Improved Methodology for Evaluating Adversarial Robustness in Deep Neural Networks",2020,"","","","",173,"2022-07-13 09:22:41","","","","",,,,,0,0.00,0,1,2,"Deep neural networks are known to be vulnerable to adversarial perturbations, which are often imperceptible to humans but can alter predictions of machine learning systems. Since the exact value of adversarial robustness is difficult to obtain for complex deep neural networks, accuracy of the models against perturbed examples generated by attack methods is empirically used as a proxy to adversarial robustness. However, failure of attack methods to find adversarial perturbations cannot be equated with being robust. In this work, we identify three common cases that lead to overestimation of accuracy against perturbed examples generated by bounded first-order attack methods: 1) the value of cross-entropy loss numerically becoming zero when using standard floating point representation, resulting in non-useful gradients; 2) innately non-differentiable functions in deep neural networks, such as Rectified Linear Unit (ReLU) activation and MaxPool operation, incurring “gradient masking” [2]; and 3) certain regularization methods used during training inducing the model to be less amenable to first-order approximation. We show that these phenomena exist in a wide range of deep neural networks, and that these phenomena are not limited to specific defense methods they have been previously investigated for. For each case, we propose compensation methods that either address sources of inaccurate gradient computation, such as numerical saturation for near zero values and nondifferentiability, or reduce the total number of back-propagations for iterative attacks by approximating second-order information. These compensation methods can be combined with existing attack methods for a more precise empirical evaluation metric. We illustrate the impact of these three phenomena with examples of practical interest, such as benchmarking model capacity and regularization techniques for robustness. Furthermore, we show that the gap between adversarial accuracy and the guaranteed lower bound of robustness can be partially explained by these phenomena. Overall, our work shows that overestimated adversarial accuracy that is not indicative of robustness is prevalent even for conventionally trained deep neural networks, and highlights cautions of using empirical evaluation without guaranteed bounds.","",""
8,"Yusuke Sakemi, K. Morino, T. Morie, K. Aihara","A Supervised Learning Algorithm for Multilayer Spiking Neural Networks Based on Temporal Coding Toward Energy-Efficient VLSI Processor Design",2020,"","","","",174,"2022-07-13 09:22:41","","10.1109/TNNLS.2021.3095068","","",,,,,8,4.00,2,4,2,"Spiking neural networks (SNNs) are brain-inspired mathematical models with the ability to process information in the form of spikes. SNNs are expected to provide not only new machine-learning algorithms but also energy-efficient computational models when implemented in very-large-scale integration (VLSI) circuits. In this article, we propose a novel supervised learning algorithm for SNNs based on temporal coding. A spiking neuron in this algorithm is designed to facilitate analog VLSI implementations with analog resistive memory, by which ultrahigh energy efficiency can be achieved. We also propose several techniques to improve the performance on recognition tasks and show that the classification accuracy of the proposed algorithm is as high as that of the state-of-the-art temporal coding SNN algorithms on the MNIST and Fashion-MNIST datasets. Finally, we discuss the robustness of the proposed SNNs against variations that arise from the device manufacturing process and are unavoidable in analog VLSI implementation. We also propose a technique to suppress the effects of variations in the manufacturing process on the recognition performance.","",""
18,"Yanbin Wang, Zhuhong You, Liping Li, Li Cheng, Xi Zhou, Libo Zhang, Xiao Li, Tonghai Jiang","Predicting Protein Interactions Using a Deep Learning Method-Stacked Sparse Autoencoder Combined with a Probabilistic Classification Vector Machine",2018,"","","","",175,"2022-07-13 09:22:41","","10.1155/2018/4216813","","",,,,,18,4.50,2,8,4,"Protein-protein interactions (PPIs), as an important molecular process within cells, are of pivotal importance in the biochemical function of cells. Although high-throughput experimental techniques have matured, enabling researchers to detect large amounts of PPIs, it has unavoidable disadvantages, such as having a high cost and being time consuming. Recent studies have demonstrated that PPIs can be efficiently detected by computational methods. Therefore, in this study, we propose a novel computational method to predict PPIs using only protein sequence information. This method was developed based on a deep learning algorithm-stacked sparse autoencoder (SSAE) combined with a Legendre moment (LM) feature extraction technique. Finally, a probabilistic classification vector machine (PCVM) classifier is used to implement PPI prediction. The proposed method was performed on human, unbalanced-human, H. pylori, and S. cerevisiae datasets with 5-fold cross-validation and yielded very high predictive accuracies of 98.58%, 97.71%, 93.76%, and 96.55%, respectively. To further evaluate the performance of our method, we compare it with the support vector machine- (SVM-) based method. The experimental results indicate that the PCVM-based method is obviously preferable to the SVM-based method. Our results have proven that the proposed method is practical, effective, and robust.","",""
270,"A. Vellido, J. Martín-Guerrero, P. Lisboa","Making machine learning models interpretable",2012,"","","","",176,"2022-07-13 09:22:41","","","","",,,,,270,27.00,90,3,10,"Data of different levels of complexity and of ever growing diversity of characteristics are the raw materials that machine learning practitioners try to model using their wide palette of methods and tools. The obtained models are meant to be a synthetic representation of the available, observed data that captures some of their intrinsic regularities or patterns. Therefore, the use of machine learning techniques for data analysis can be understood as a problem of pattern recognition or, more informally, of knowledge discovery and data mining. There exists a gap, though, between data modeling and knowledge extraction. Models, de- pending on the machine learning techniques employed, can be described in diverse ways but, in order to consider that some knowledge has been achieved from their description, we must take into account the human cog- nitive factor that any knowledge extraction process entails. These models as such can be rendered powerless unless they can be interpreted ,a nd the process of human interpretation follows rules that go well beyond techni- cal prowess. For this reason, interpretability is a paramount quality that machine learning methods should aim to achieve if they are to be applied in practice. This paper is a brief introduction to the special session on interpretable models in machine learning, organized as part of the 20 th European Symposium on Artificial Neural Networks, Computational In- telligence and Machine Learning. It includes a discussion on the several works accepted for the session, with an overview of the context of wider research on interpretability of machine learning models.","",""
24,"Xin Liu, Anwitaman Datta, Ee-Peng Lim","Computational Trust Models and Machine Learning",2014,"","","","",177,"2022-07-13 09:22:41","","10.1201/B17778","","",,,,,24,3.00,8,3,8,"Computational Trust Models and Machine Learning provides a detailed introduction to the concept of trust and its application in various computer science areas, including multi-agent systems, online social networks, and communication systems. Identifying trust modeling challenges that cannot be addressed by traditional approaches, this book: Explains how reputation-based systems are used to determine trust in diverse online communities Describes how machine learning techniques are employed to build robust reputation systems Explores two distinctive approaches to determining credibility of resourcesone where the human role is implicit, and one that leverages human input explicitly Shows how decision support can be facilitated by computational trust models Discusses collaborative filtering-based trust aware recommendation systems Defines a framework for translating a trust modeling problem into a learning problem Investigates the objectivity of human feedback, emphasizing the need to filter out outlying opinions Computational Trust Models and Machine Learning effectively demonstrates how novel machine learning techniques can improve the accuracy of trust assessment.","",""
3,"Jinzhe Zeng, Giese Tj, S. Ekesan, D. York","Development of Range-Corrected Deep Learning Potentials for Fast, Accurate Quantum Mechanical/Molecular Mechanical Simulations of Chemical Reactions in Solution.",2021,"","","","",178,"2022-07-13 09:22:41","","10.26434/CHEMRXIV.14120447.V1","","",,,,,3,3.00,1,4,1,"We develop a new deep potential-range correction (DPRc) machine learning potential for combined quantum mechanical/molecular mechanical (QM/MM) simulations of chemical reactions in the condensed phase. The new range correction enables short-ranged QM/MM interactions to be tuned for higher accuracy, and the correction smoothly vanishes within a specified cutoff. We further develop an active learning procedure for robust neural network training. We test the DPRc model and training procedure against a series of six nonenzymatic phosphoryl transfer reactions in solution that are important in mechanistic studies of RNA-cleaving enzymes. Specifically, we apply DPRc corrections to a base QM model and test its ability to reproduce free-energy profiles generated from a target QM model. We perform these comparisons using the MNDO/d and DFTB2 semiempirical models because they differ in the way they treat orbital orthogonalization and electrostatics and produce free-energy profiles which differ significantly from each other, thereby providing us a rigorous stress test for the DPRc model and training procedure. The comparisons show that accurate reproduction of the free-energy profiles requires correction of the QM/MM interactions out to 6 Å. We further find that the model's initial training benefits from generating data from temperature replica exchange simulations and including high-temperature configurations into the fitting procedure, so the resulting models are trained to properly avoid high-energy regions. A single DPRc model was trained to reproduce four different reactions and yielded good agreement with the free-energy profiles made from the target QM/MM simulations. The DPRc model was further demonstrated to be transferable to 2D free-energy surfaces and 1D free-energy profiles that were not explicitly considered in the training. Examination of the computational performance of the DPRc model showed that it was fairly slow when run on CPUs but was sped up almost 100-fold when using NVIDIA V100 GPUs, resulting in almost negligible overhead. The new DPRc model and training procedure provide a potentially powerful new tool for the creation of next-generation QM/MM potentials for a wide spectrum of free-energy applications ranging from drug discovery to enzyme design.","",""
19,"M. Hausknecht, Wen-Ke Li, M. Mauk, P. Stone","Machine Learning Capabilities of a Simulated Cerebellum",2017,"","","","",179,"2022-07-13 09:22:41","","10.1109/TNNLS.2015.2512838","","",,,,,19,3.80,5,4,5,"This paper describes the learning and control capabilities of a biologically constrained bottom-up model of the mammalian cerebellum. Results are presented from six tasks: 1) eyelid conditioning; 2) pendulum balancing; 3) proportional–integral–derivative control; 4) robot balancing; 5) pattern recognition; and 6) MNIST handwritten digit recognition. These tasks span several paradigms of machine learning, including supervised learning, reinforcement learning, control, and pattern recognition. Results over these six domains indicate that the cerebellar simulation is capable of robustly identifying static input patterns even when randomized across the sensory apparatus. This capability allows the simulated cerebellum to perform several different supervised learning and control tasks. On the other hand, both reinforcement learning and temporal pattern recognition prove problematic due to the delayed nature of error signals and the simulator’s inability to solve the credit assignment problem. These results are consistent with previous findings which hypothesize that in the human brain, the basal ganglia is responsible for reinforcement learning, while the cerebellum handles supervised learning.","",""
5,"L. Afshar, H. Sajedi","Age Prediction based on Brain MRI Images using Extreme Learning Machine",2019,"","","","",180,"2022-07-13 09:22:41","","10.1109/CFIS.2019.8692156","","",,,,,5,1.67,3,2,3,"Age Prediction, which means setting up a machine learning system, defined by using different sets of data for training, and then the estimation of the actual age of humans, is a subject that has been studied in recent years. To achieve this, researchers have been experimenting with various body components, such as DNA, speech signals, medical images, facial images, etc. Recent researches show that brain structure changes with age or psychiatric disorders. So a useful tool for estimating the age of humans is the brain’s MRI images. Brain Magnetic Resonance Imaging (MRI) use radio waves and a robust magnetic field to create detailed images of the organs and tissues within the body. In this paper, the age of humans is predicted based on brain MRI images. To extract T1-MRI features, two different methods are proposed, then to estimate age, Extreme Learning Machine (ELM) is employed. Given that the amount of computations needed in this method and the time required to age estimation is low, the proposed method has acceptable performance.","",""
25,"Samantha Krening, K. Feigh","Interaction Algorithm Effect on Human Experience with Reinforcement Learning",2018,"","","","",181,"2022-07-13 09:22:41","","10.1145/3277904","","",,,,,25,6.25,13,2,4,"A goal of interactive machine learning (IML) is to enable people with no specialized training to intuitively teach intelligent agents how to perform tasks. Toward achieving that goal, we are studying how the design of the interaction method for a Bayesian Q-Learning algorithm impacts aspects of the human’s experience of teaching the agent using human-centric metrics such as frustration in addition to traditional ML performance metrics. This study investigated two methods of natural language instruction: critique and action advice. We conducted a human-in-the-loop experiment in which people trained two agents with different teaching methods but, unknown to each participant, the same underlying reinforcement learning algorithm. The results show an agent that learns from action advice creates a better user experience compared to an agent that learns from binary critique in terms of frustration, perceived performance, transparency, immediacy, and perceived intelligence. We identified nine main characteristics of an IML algorithm’s design that impact the human’s experience with the agent, including using human instructions about the future, compliance with input, empowerment, transparency, immediacy, a deterministic interaction, the complexity of the instructions, accuracy of the speech recognition software, and the robust and flexible nature of the interaction algorithm.","",""
1399,"P. Turaga, R. Chellappa, V. S. Subrahmanian, O. Udrea","Machine Recognition of Human Activities: A Survey",2008,"","","","",182,"2022-07-13 09:22:41","","10.1109/TCSVT.2008.2005594","","",,,,,1399,99.93,350,4,14,"The past decade has witnessed a rapid proliferation of video cameras in all walks of life and has resulted in a tremendous explosion of video content. Several applications such as content-based video annotation and retrieval, highlight extraction and video summarization require recognition of the activities occurring in the video. The analysis of human activities in videos is an area with increasingly important consequences from security and surveillance to entertainment and personal archiving. Several challenges at various levels of processing-robustness against errors in low-level processing, view and rate-invariant representations at midlevel processing and semantic representation of human activities at higher level processing-make this problem hard to solve. In this review paper, we present a comprehensive survey of efforts in the past couple of decades to address the problems of representation, recognition, and learning of human activities from video and related applications. We discuss the problem at two major levels of complexity: 1) ""actions"" and 2) ""activities."" ""Actions"" are characterized by simple motion patterns typically executed by a single human. ""Activities"" are more complex and involve coordinated actions among a small number of humans. We will discuss several approaches and classify them according to their ability to handle varying degrees of complexity as interpreted above. We begin with a discussion of approaches to model the simplest of action classes known as atomic or primitive actions that do not require sophisticated dynamical modeling. Then, methods to model actions with more complex dynamics are discussed. The discussion then leads naturally to methods for higher level representation of complex activities.","",""
1,"","Skeleton-based Human Action Recognition",2021,"","","","",183,"2022-07-13 09:22:41","","","","",,,,,1,1.00,0,0,1,"With the continuous advance of the technology and the rather growing demands in fields such as the industrial, or the medical concerning the automatization of processes and the support of the professional workers, the use of Artificial Intelligence is vastly deployed. Professional workers have to co-exist and/or collaborate with robotic agents such as Automated Guided Vehicles and robotic arms, to enhance their effectiveness and productivity. Those robotic agents have machine learning algorithms embodied to them, for the proper support of the humans. This task is also the main goal of this thesis. A gesture recognition engine is developed, using the method of a Multilayer perceptron (MLP), for the recognition of the gestures of two different datasets with a similar structure. The results of this method are also compared to those, with the use of the methods of Hidden Markov Models (HMMs) and Long-Short TermMemory (LSTM), in order to the reach to the optimal classification architecture. Apart from the methods themselves, different ways for data handling were also tested. The aim is to examine whether in the specific case and with the used data, the resampling method was more useful in terms of training or testing an algorithm, compared to training or testing with the whole amount of data. This led to observations concerning the computational cost of some experiments and the robustness of the used algorithms. As such, the scientific contribution of this thesis relies on the implementation of a robust, accurate, as well as fast computationally gesture recognition engine.","",""
0,"V. Kůrková, Y. Manolopoulos, B. Hammer, L. Iliadis, Ilias Maglogiannis","Artificial Neural Networks and Machine Learning – ICANN 2018",2018,"","","","",184,"2022-07-13 09:22:41","","10.1007/978-3-030-01418-6","","",,,,,0,0.00,0,5,4,"","",""
19,"P. Trigueiros, A. F. Ribeiro, Luis Paulo Reis","A Comparative Study of Different Image Features for Hand Gesture Machine Learning",2013,"","","","",185,"2022-07-13 09:22:41","","10.5220/0004200100510061","","",,,,,19,2.11,6,3,9,"Vision-based hand gesture interfaces require fast and extremely robust hand detection, and gesture recognition. Hand gesture recognition for human computer interaction is an area of active research in computer vision and machine learning. The primary goal of gesture recognition research is to create a system, which can identify specific human gestures and use them to convey information or for device control. In this paper we present a comparative study of seven different algorithms for hand feature extraction, for static hand gesture classification, analysed with RapidMiner in order to find the best learner. We defined our own gesture vocabulary, with 10 gestures, and we have recorded videos from 20 persons performing the gestures for later processing. Our goal in the present study is to learn features that, isolated, respond better in various situations in human-computer interaction. Results show that the radial signature and the centroid distance are the features that when used separately obtain better results, being at the same time simple in terms of computational complexity.","",""
14,"Kevin Li, C. Gibson, D. Ho, Qi Zhou, J. Kim, O. Buhisi, D. Brown, M. Gerber","Assessment of machine learning algorithms in cloud computing frameworks",2013,"","","","",186,"2022-07-13 09:22:41","","10.1109/SIEDS.2013.6549501","","",,,,,14,1.56,2,8,9,"In the past decade, digitization of information has led to a data explosion in both volume and complexity. While traditional computing frameworks have failed to provide adequate computing power for the now common data-intensive computing tasks, cloud computing provides an effective alternative to enhance computing power. Machine learning algorithms are powerful analytical methods that allow machines to recognize patterns and facilitate human learning. However, the performance of individual machine learning algorithms within each cloud computing framework remains largely unknown. Furthermore, the lack of a robust selection methodology matching input data with effective machine learning algorithms limits the ability of practitioners to make effective use of cloud computing. This research compares various machine learning algorithms on the widely adopted Apache Mahout framework and the recently introduced GraphLab framework. Whereas previous work has examined the computational architectures of various cloud computing frameworks, this work focuses on a problem-based approach to architecture selection. The experimental results demonstrate that GraphLab generally outperforms Mahout with respect to runtime, scalability, and usability. However, Mahout outperforms GraphLab when the experiment focus shifts to error measurement.","",""
11,"Junjie Lu, Steven Young, I. Arel, J. Holleman","30.10 A 1TOPS/W analog deep machine-learning engine with floating-gate storage in 0.13μm CMOS",2014,"","","","",187,"2022-07-13 09:22:41","","10.1109/isscc.2014.6757532","","",,,,,11,1.38,3,4,8,"Direct processing of raw high-dimensional data such as images and video by machine learning systems is impractical both due to prohibitive power consumption and the “curse of dimensionality,” which makes learning tasks exponentially more difficult as dimension increases. Deep machine learning (DML) mimics the hierarchical presentation of information in the human brain to achieve robust automated feature extraction, reducing the dimension of such data. However, the computational complexity of DML systems limits large-scale implementations in standard digital computers. Custom analog or mixed-mode signal processors have been reported to yield much higher energy efficiency than DSP [1-4], presenting the means of overcoming these limitations. However, the use of volatile digital memory in [1-3] precludes their use in intermittently-powered devices, and the required interfacing and internal A/D/A conversions add power and area overhead. Nonvolatile storage is employed in [4], but the lack of learning capability requires task-specific programming before operation, and precludes online adaptation.","",""
1,"S. Dankwa, Lu Yang","Securing IoT Devices: A Robust and Efficient Deep Learning with a Mixed Batch Adversarial Generation Process for CAPTCHA Security Verification",2021,"","","","",188,"2022-07-13 09:22:41","","10.3390/electronics10151798","","",,,,,1,1.00,1,2,1,"The Internet of Things environment (e.g., smart phones, smart televisions, and smart watches) ensures that the end user experience is easy, by connecting lives on web services via the internet. Integrating Internet of Things devices poses ethical risks related to data security, privacy, reliability and management, data mining, and knowledge exchange. An adversarial machine learning attack is a good practice to adopt, to strengthen the security of text-based CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart), to withstand against malicious attacks from computer hackers, to protect Internet of Things devices and the end user’s privacy. The goal of this current study is to perform security vulnerability verification on adversarial text-based CAPTCHA, based on attacker–defender scenarios. Therefore, this study proposed computation-efficient deep learning with a mixed batch adversarial generation process model, which attempted to break the transferability attack, and mitigate the problem of catastrophic forgetting in the context of adversarial attack defense. After performing K-fold cross-validation, experimental results showed that the proposed defense model achieved mean accuracies in the range of 82–84% among three gradient-based adversarial attack datasets.","",""
794,"Thomas G. Dietterich","Adaptive computation and machine learning",1998,"","","","",189,"2022-07-13 09:22:41","","","","",,,,,794,33.08,794,1,24,"All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from this is the last candidate. next esc will revert to uncompleted text. he publisher. Overview Dataset shift is a challenging situation where the joint distribution of inputs and outputs differs between the training and test stages. Covariate shift is a simpler particular case of dataset shift where only the input distribution changes (covariate denotes input), while the conditional distribution of the outputs given the inputs p(y|x) remains unchanged. Dataset shift is present in most practical applications for reasons ranging from the bias introduced by experimental design, to the mere irreproducibility of the testing conditions at training time. For example, in an image classification task, training data might have been recorded under controlled laboratory conditions, whereas the test data may show different lighting conditions. In other applications, the process that generates data is in itself adaptive. Some of our authors consider the problem of spam email filtering: successful "" spammers "" will try to build spam in a form that differs from the spam the automatic filter has been built on. Dataset shift seems to have raised relatively little interest in the machine learning community until very recently. Indeed, many machine learning algorithms are based on the assumption that the training data is drawn from exactly the same distribution as the test data on which the model will later be evaluated. Semi-supervised learning and active learning, two problems that seem very similar to covariate shift have received much more attention. How do they differ from covariate shift? Semi-supervised learning is designed to take advantage of unlabeled data present at training time, but is not conceived to be robust against changes in the input distribution. In fact, one can easily construct examples of covariate shift for which common SSL strategies such as the "" cluster assumption "" will lead to disaster. In active learning the algorithm is asked to select from the available unlabeled inputs those for which obtaining the label will be most beneficial for learning. This is very relevant in contexts where labeling data is very costly, but active learning strategies 2 Contents are not specifically design for dealing with covariate shift. This book attempts to give an overview of the different recent efforts that are being …","",""
15,"Chenwei Deng, Shuigen Wang, Zhen Li, G. Huang, Weisi Lin","Content-Insensitive Blind Image Blurriness Assessment Using Weibull Statistics and Sparse Extreme Learning Machine",2019,"","","","",190,"2022-07-13 09:22:41","","10.1109/TSMC.2017.2718180","","",,,,,15,5.00,3,5,3,"Most of the existing image blurriness assessment algorithms are proposed based on measuring image edge width, gradient, high-frequency energy, or pixel intensity variation. However, these methods are content sensitive with little consideration of image content variations, which causes variant estimations for images with different contents but same blurriness degrees. In this paper, a content-insensitive blind image blurriness assessment metric is developed utilizing Weibull statistics. Inspired by the property that the statistics of image gradient magnitude (GM) follows Weibull distribution, we parameterize the GM using <inline-formula> <tex-math notation=""LaTeX"">$\beta$ </tex-math></inline-formula> (scale parameter) and <inline-formula> <tex-math notation=""LaTeX"">$\gamma$ </tex-math></inline-formula> (shape parameter) of Weibull distribution. We also adopt skewness (<inline-formula> <tex-math notation=""LaTeX"">$\eta$ </tex-math></inline-formula>) to measure the asymmetry of the GM distribution. In order to reduce the influence of image content and achieve more robust performance, divisive normalization is then incorporated to moderate the <inline-formula> <tex-math notation=""LaTeX"">$\beta$ </tex-math></inline-formula>, <inline-formula> <tex-math notation=""LaTeX"">$\gamma$ </tex-math></inline-formula>, and <inline-formula> <tex-math notation=""LaTeX"">$\eta$ </tex-math></inline-formula>. The final image quality is predicted using a sparse extreme learning machine. Performances evaluation on the blur image subsets in LIVE, CSIQ, TID2008, and TID2013 databases demonstrate that the proposed method is highly correlated with human perception and robust with image contents. In addition, our method has low computational complexity which is suitable for online applications.","",""
5,"Jithin Jagannath, Anu Jagannath, Sean Furman, Tyler Gwin","Deep Learning and Reinforcement Learning for Autonomous Unmanned Aerial Systems: Roadmap for Theory to Deployment",2020,"","","","",191,"2022-07-13 09:22:41","","10.1007/978-3-030-77939-9_2","","",,,,,5,2.50,1,4,2,"","",""
2,"Jagadeesh Basavaiah, C. Patil","HUMAN ACTIVITY DETECTION AND ACTION RECOGNITION IN VIDEOS USING CONVOLUTIONAL NEURAL NETWORKS",2020,"","","","",192,"2022-07-13 09:22:41","","10.32890/JICT2020.19.2.1","","",,,,,2,1.00,1,2,2,"Human activity recognition from video scenes has become a significant area of research in the field of computer vision applications. Action recognition is one of the most challenging problems in the area of video analysis and it finds applications in human-computer interaction, anomalous activity detection, crowd monitoring and patient monitoring. Several approaches have been presented for human activity recognition using machine learning techniques. The main aim of this work is to detect and track human activity, and classify actions for two publicly available video databases. In this work, a novel approach of feature extraction from video sequence by combining Scale Invariant Feature Transform and optical flow computation are used where shape, gradient and orientation features are also incorporated for robust feature formulation. Tracking of human activity in the video is implemented using the Gaussian Mixture Model. Convolutional Neural Network based classification approach is used for database training and testing purposes. The activity recognition performance is evaluated for two public datasets namely Weizmann dataset and Kungliga Tekniska Hogskolan dataset with action recognition accuracy of 98.43% and 94.96%, respectively. Experimental and comparative studies have shown that the proposed approach outperformed state-of the art techniques.","",""
3,"Sijie Yang, Fei Zhu, Xinghong Ling, QUAN LIU, Peiyao Zhao","Intelligent Health Care: Applications of Deep Learning in Computational Medicine",2021,"","","","",193,"2022-07-13 09:22:41","","10.3389/fgene.2021.607471","","",,,,,3,3.00,1,5,1,"With the progress of medical technology, biomedical field ushered in the era of big data, based on which and driven by artificial intelligence technology, computational medicine has emerged. People need to extract the effective information contained in these big biomedical data to promote the development of precision medicine. Traditionally, the machine learning methods are used to dig out biomedical data to find the features from data, which generally rely on feature engineering and domain knowledge of experts, requiring tremendous time and human resources. Different from traditional approaches, deep learning, as a cutting-edge machine learning branch, can automatically learn complex and robust feature from raw data without the need for feature engineering. The applications of deep learning in medical image, electronic health record, genomics, and drug development are studied, where the suggestion is that deep learning has obvious advantage in making full use of biomedical data and improving medical health level. Deep learning plays an increasingly important role in the field of medical health and has a broad prospect of application. However, the problems and challenges of deep learning in computational medical health still exist, including insufficient data, interpretability, data privacy, and heterogeneity. Analysis and discussion on these problems provide a reference to improve the application of deep learning in medical health.","",""
2,"Valentin Bencteux, Guinther Saibro, E. Shlomovitz, P. Mascagni, S. Perretta, A. Hostettler, J. Marescaux, T. Collins","Automatic task recognition in a flexible endoscopy benchtop trainer with semi-supervised learning",2020,"","","","",194,"2022-07-13 09:22:41","","10.1007/s11548-020-02208-w","","",,,,,2,1.00,0,8,2,"","",""
13,"D. Anand, N. Kurian, S. Dhage, Neeraj Kumar, S. Rane, P. Gann, A. Sethi","Deep Learning to Estimate Human Epidermal Growth Factor Receptor 2 Status from Hematoxylin and Eosin-Stained Breast Tissue Images",2020,"","","","",195,"2022-07-13 09:22:41","","10.4103/jpi.jpi_10_20","","",,,,,13,6.50,2,7,2,"Context: Several therapeutically important mutations in cancers are economically detected using immunohistochemistry (IHC), which highlights the overexpression of specific antigens associated with the mutation. However, IHC panels can be imprecise and relatively expensive in low-income settings. On the other hand, although hematoxylin and eosin (H&E) staining used to visualize the general tissue morphology is a routine and low cost, it does not highlight any specific antigen or mutation. Aims: Using the human epidermal growth factor receptor 2 (HER2) mutation in breast cancer as an example, we strengthen the case for cost-effective detection and screening of overexpression of HER2 protein in H&E-stained tissue. Settings and Design: We use computational methods that reliably detect subtle morphological changes associated with the over-expression of mutation-specific proteins directly from H&E images. Subjects and Methods: We trained a classification pipeline to determine HER2 overexpression status of H&E stained whole slide images. Our training dataset was derived from a single hospital containing 26 (11 HER2+ and 15 HER2–) cases. We tested the classification pipeline on 26 (8 HER2+ and 18 HER2–) held-out cases from the same hospital and 45 independent cases (23 HER2+ and 22 HER2–) from the TCGA-BRCA cohort. The pipeline was composed of a stain separation module and three deep neural network modules in tandem for robustness and interpretability. Statistical Analysis Used: We evaluate our trained model through area under the curve (AUC)-receiver operating characteristic. Results: Our pipeline achieved an AUC of 0.82 (confidence interval [CI]: 0.65–0.98) on held-out cases and an AUC of 0.76 (CI: 0.61–0.89) on the independent dataset from TCGA. We also demonstrate the region-level correspondence of HER2 overexpression between a patient's IHC and H&E serial sections. Conclusions: Our work strengthens the case for automatically quantifying the overexpression of mutation-specific proteins in H&E-stained digital pathology, and it highlights the importance of multi-stage machine learning pipelines for added robustness and interpretability.","",""
1,"Anoop Sathyan","Intelligent Machine Learning Approaches for Aerospace Applications",2017,"","","","",196,"2022-07-13 09:22:41","","","","",,,,,1,0.20,1,1,5,"Machine Learning is a type of artificial intelligence that provides machines or networks the ability to learn from data without the need to explicitly program them. There are different kinds of machine learning techniques. This thesis discusses the applications of two of these approaches: Genetic Fuzzy Logic and Convolutional Neural Networks (CNN). Fuzzy Logic System (FLS) is a powerful tool that can be used for a wide variety of applications. FLS is a universal approximator that reduces the need for complex mathematics and replaces it with expert knowledge of the system to produce an input-output mapping using If-Then rules. The expert knowledge of a system can help in obtaining the parameters for small-scale FLSs, but for larger networks we will need to use sophisticated approaches that can automatically train the network to meet the design requirements. This is where Genetic Algorithms (GA) and EVE come into the picture. Both GA and EVE can tune the FLS parameters to minimize a cost function that is designed to meet the requirements of the specific problem. EVE is an artificial intelligence developed by Psibernetix that is trained to tune large scale FLSs. The parameters of an FLS can include the membership functions and rulebase of the inherent Fuzzy Inference Systems (FISs). The main issue with using the GFS is that the number of parameters in a FIS increase exponentially with the number of inputs thus making it increasingly harder to tune them. To reduce this issue, the FLSs discussed in this thesis consist of 2-input-1-output FISs in cascade (Chapter 4) or as a layer of parallel FISs (Chapter 7). We have obtained extremely good results using GFS for different applications at a reduced computational cost compared to other algorithms that are commonly used to solve the corresponding problems. In this thesis, GFSs have been designed for controlling an inverted double pendulum, a task allocation problem of clustering targets amongst a set of UAVs, a fire detection problem and the aircraft conflict resolution problem. During the last decade, CNNs have become increasingly popular in the domain of image and speech processing. CNNs have a lot more parameters compared to GFSs that are tuned using the back-propagation algorithm. CNNs typically have hundreds of thousands or maybe millions of parameters that are tuned using common cost functions such as integral squared error, softmax loss etc. Chapter 5 discusses a classification problem to classify images as humans or not and Chapter 6 discusses a regression task using CNN for producing an approximate near-optimal route for the Traveling Salesman Problem (TSP) which is regarded as one of the most complicated decision making problem. Both the GFS and CNN are used to develop intelligent systems specific to the application providing them computational efficiency, robustness in the face of uncertainties and scalability.","",""
1,"H. Ayoobiy, M. Caoz, R. Verbruggey, B. Verheijy","Argue to Learn: Accelerated Argumentation-Based Learning",2021,"","","","",197,"2022-07-13 09:22:41","","10.1109/ICMLA52953.2021.00183","","",,,,,1,1.00,0,4,1,"Human agents can acquire knowledge and learn through argumentation. Inspired by this fact, we propose a novel argumentation-based machine learning technique that can be used for online incremental learning scenarios. Existing methods for online incremental learning problems typically do not generalize well from just a few learning instances. Our previous argumentation-based online incremental learning method outperformed state-of-the-art methods in terms of accuracy and learning speed. However, it was neither memory-efficient nor computationally efficient since the algorithm used the power set of the feature values for updating the model. In this paper, we propose an accelerated version of the algorithm, with polynomial instead of exponential complexity, while achieving higher learning accuracy. The proposed method is at least $200\times$ faster than the original argumentation-based learning method and is more memory-efficient.","",""
4,"R'emi Dromnelle, B. Girard, Erwan Renaudo, R. Chatila, M. Khamassi","Coping with the variability in humans reward during simulated human-robot interactions through the coordination of multiple learning strategies*",2020,"","","","",198,"2022-07-13 09:22:41","","10.1109/RO-MAN47096.2020.9223451","","",,,,,4,2.00,1,5,2,"An important current challenge in Human-Robot Interaction (HRI) is to enable robots to learn on-the-fly from human feedback. However, humans show a great variability in the way they reward robots. We propose to address this issue by enabling the robot to combine different learning strategies, namely model-based (MB) and model-free (MF) reinforcement learning. We simulate two HRI scenarios: a simple task where the human congratulates the robot for putting the right cubes in the right boxes, and a more complicated version of this task where cubes have to be placed in a specific order. We show that our existing MB-MF coordination algorithm previously tested in robot navigation works well here without retuning parameters. It leads to the maximal performance while producing the same minimal computational cost as MF alone. Moreover, the algorithm gives a robust performance no matter the variability of the simulated human feedback, while each strategy alone is impacted by this variability. Overall, the results suggest a promising way to promote robot learning flexibility when facing variable human feedback.","",""
1,"Alessandra Sorrentino, L. Fiorini, I. Fabbricotti, D. Sancarlo, F. Ciccone, F. Cavallo","Exploring Human attitude during Human-Robot Interaction",2020,"","","","",199,"2022-07-13 09:22:41","","10.1109/RO-MAN47096.2020.9223527","","",,,,,1,0.50,0,6,2,"The aim of this work is to provide an automatic analysis to assess the user attitude when interacts with a companion robot. In detail, our work focuses on defining which combination of social cues the robot should recognize so that to stimulate the ongoing conversation and how. The analysis is performed on video recordings of 9 elderly users. From each video, low-level descriptors of the behavior of the user are extracted by using open-source automatic tools to extract information on the voice, the body posture, and the face landmarks. The assessment of 3 types of attitude (neutral, positive and negative) is performed through 3 machine learning classification algorithms: k-nearest neighbors, random decision forest and support vector regression. Since intra- and intersubject variability could affect the results of the assessment, this work shows the robustness of the classification models in both scenarios. Further analysis is performed on the type of representation used to describe the attitude. A raw and an auto-encoded representation is applied to the descriptors. The results of the attitude assessment show high values of accuracy (>0.85) both for unimodal and multimodal data. The outcome of this work can be integrated into a robotic platform to automatically assess the quality of interaction and to modify its behavior accordingly.","",""
149,"Zhuolin Jiang, Zhe L. Lin, L. Davis","Recognizing Human Actions by Learning and Matching Shape-Motion Prototype Trees",2012,"","","","",200,"2022-07-13 09:22:41","","10.1109/TPAMI.2011.147","","",,,,,149,14.90,50,3,10,"A shape-motion prototype-based approach is introduced for action recognition. The approach represents an action as a sequence of prototypes for efficient and flexible action matching in long video sequences. During training, an action prototype tree is learned in a joint shape and motion space via hierarchical K-means clustering and each training sequence is represented as a labeled prototype sequence; then a look-up table of prototype-to-prototype distances is generated. During testing, based on a joint probability model of the actor location and action prototype, the actor is tracked while a frame-to-prototype correspondence is established by maximizing the joint probability, which is efficiently performed by searching the learned prototype tree; then actions are recognized using dynamic prototype sequence matching. Distance measures used for sequence matching are rapidly obtained by look-up table indexing, which is an order of magnitude faster than brute-force computation of frame-to-frame distances. Our approach enables robust action matching in challenging situations (such as moving cameras, dynamic backgrounds) and allows automatic alignment of action sequences. Experimental results demonstrate that our approach achieves recognition rates of 92.86 percent on a large gesture data set (with dynamic backgrounds), 100 percent on the Weizmann action data set, 95.77 percent on the KTH action data set, 88 percent on the UCF sports data set, and 87.27 percent on the CMU action data set.","",""
