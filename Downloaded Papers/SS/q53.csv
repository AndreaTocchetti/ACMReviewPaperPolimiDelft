Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
0,"Mauro J. Sanchirico, Xun Jiao, C. Nataraj","AMITE: A Novel Polynomial Expansion for Analyzing Neural Network Nonlinearities.",2020,"","","","",1,"2022-07-13 09:26:01","","10.1109/TNNLS.2021.3130904","","",,,,,0,0.00,0,3,2,"Polynomial expansions are important in the analysis of neural network nonlinearities. They have been applied thereto addressing well-known difficulties in verification, explainability, and security. Existing approaches span classical Taylor and Chebyshev methods, asymptotics, and many numerical approaches. We find that, while these have useful properties individually, such as exact error formulas, adjustable domain, and robustness to undefined derivatives, there are no approaches that provide a consistent method, yielding an expansion with all these properties. To address this, we develop an analytically modified integral transform expansion (AMITE), a novel expansion via integral transforms modified using derived criteria for convergence. We show the general expansion and then demonstrate an application for two popular activation functions: hyperbolic tangent and rectified linear units. Compared with existing expansions (i.e., Chebyshev, Taylor, and numerical) employed to this end, AMITE is the first to provide six previously mutually exclusive desired expansion properties, such as exact formulas for the coefficients and exact expansion errors. We demonstrate the effectiveness of AMITE in two case studies. First, a multivariate polynomial form is efficiently extracted from a single hidden layer black-box multilayer perceptron (MLP) to facilitate equivalence testing from noisy stimulus-response pairs. Second, a variety of feedforward neural network (FFNN) architectures having between three and seven layers are range bounded using Taylor models improved by the AMITE polynomials and error formulas. AMITE presents a new dimension of expansion methods suitable for the analysis/approximation of nonlinearities in neural networks, opening new directions and opportunities for the theoretical analysis and systematic testing of neural networks.","",""
11,"Alexander Hartl, Maximilian Bachl, J. Fabini, T. Zseby","Explainability and Adversarial Robustness for RNNs",2019,"","","","",2,"2022-07-13 09:26:01","","10.1109/BigDataService49289.2020.00030","","",,,,,11,3.67,3,4,3,"Recurrent Neural Networks (RNNs) yield attractive properties for constructing Intrusion Detection Systems (IDSs) for network data. With the rise of ubiquitous Machine Learning (ML) systems, malicious actors have been catching up quickly to find new ways to exploit ML vulnerabilities for profit. Recently developed adversarial ML techniques focus on computer vision and their applicability to network traffic is not straightforward: Network packets expose fewer features than an image, are sequential and impose several constraints on their features. We show that despite these completely different characteristics, adversarial samples can be generated reliably for RNNs. To understand a classifier's potential for misclassification, we extend existing explainability techniques and propose new ones, suitable particularly for sequential data. Applying them shows that already the first packets of a communication flow are of crucial importance and are likely to be targeted by attackers. Feature importance methods show that even relatively unimportant features can be effectively abused to generate adversarial samples. We thus introduce the concept of feature sensitivity which quantifies how much potential a feature has to cause misclassification. Since traditional evaluation metrics such as accuracy are not sufficient for quantifying the adversarial threat, we propose the Adversarial Robustness Score (ARS) for comparing IDSs and show that an adversarial training procedure can significantly and successfully reduce the attack surface.","",""
6,"P. Konar, Vishal S. Ngairangbam, M. Spannowsky","Energy-weighted message passing: an infra-red and collinear safe graph neural network algorithm",2021,"","","","",3,"2022-07-13 09:26:01","","10.1007/JHEP02(2022)060","","",,,,,6,6.00,2,3,1,"","",""
0,"Mark H. Meng, Guangdong Bai, S. Teo, Zhe Hou, Yan Xiao, Yun Lin, Jin Song Dong","Adversarial Robustness of Deep Neural Networks: A Survey from a Formal Verification Perspective",2022,"","","","",4,"2022-07-13 09:26:01","","10.1109/TDSC.2022.3179131","","",,,,,0,0.00,0,7,1,"—Neural networks have been widely applied in security applications such as spam and phishing detection, intrusion prevention, and malware detection. This black-box method, however, often has uncertainty and poor explainability in applications. Furthermore, neural networks themselves are often vulnerable to adversarial attacks. For those reasons, there is a high demand for trustworthy and rigorous methods to verify the robustness of neural network models. Adversarial robustness, which concerns the reliability of a neural network when dealing with maliciously manipulated inputs, is one of the hottest topics in security and machine learning. In this work, we survey existing literature in adversarial robustness veriﬁcation for neural networks and collect 39 diversiﬁed research works across machine learning, security, and software engineering domains. We systematically analyze their approaches, including how robustness is formulated, what veriﬁcation techniques are used, and the strengths and limitations of each technique. We provide a taxonomy from a formal veriﬁcation perspective for a comprehensive understanding of this topic. We classify the existing techniques based on property speciﬁcation, problem reduction, and reasoning strategies. We also demonstrate representative techniques that have been applied in existing studies with a sample model. Finally, we discuss open questions for future research.","",""
2,"S. Meister, Mahdieu A. M. Wermes, J. Stüve, R. Groves","Explainability of deep learning classifier decisions for optical detection of manufacturing defects in the automated fiber placement process",2021,"","","","",5,"2022-07-13 09:26:01","","10.1117/12.2592584","","",,,,,2,2.00,1,4,1,"Automated fibre layup techniques are commonly used composite manufacturing processes in the aviation sector and require a manual visual inspection. Neural Network classification of defects has the potential to automate this visual inspection, however, the machine decision-making processes are hard to verify. Thus, we present an approach for visualising Convolutional Neural Network (CNN) based classifications of manufacturing defects and quantifying its robustness suitably. Our investigations have shown that especially Smoothed Integrated Gradients and DeepSHAP are particularly well suited for the visualisation of CNN classifications. The Smoothed Integrated Gradients technique also reveals advantages in robustness when evaluating degraded input images.","",""
30,"Guillermo Ortiz-Jiménez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, P. Frossard","Hold me tight! Influence of discriminative features on deep network boundaries",2020,"","","","",6,"2022-07-13 09:26:01","","","","",,,,,30,15.00,8,4,2,"Important insights towards the explainability of neural networks reside in the characteristics of their decision boundaries. In this work, we borrow tools from the field of adversarial robustness, and propose a new perspective that relates dataset features to the distance of samples to the decision boundary. This enables us to carefully tweak the position of the training samples and measure the induced changes on the boundaries of CNNs trained on large-scale vision datasets. We use this framework to reveal some intriguing properties of CNNs. Specifically, we rigorously confirm that neural networks exhibit a high invariance to non-discriminative features, and show that the decision boundaries of a DNN can only exist as long as the classifier is trained with some features that hold them together. Finally, we show that the construction of the decision boundary is extremely sensitive to small perturbations of the training samples, and that changes in certain directions can lead to sudden invariances in the orthogonal ones. This is precisely the mechanism that adversarial training uses to achieve robustness.","",""
7,"D. Gopinath, Ankur Taly, Hayes Converse, C. Pasareanu","Finding Invariants in Deep Neural Networks",2019,"","","","",7,"2022-07-13 09:26:01","","","","",,,,,7,2.33,2,4,3,"We present techniques for automatically inferring invariant properties of feed-forward neural networks. Our insight is that feed forward networks should be able to learn a decision logic that is captured in the activation patterns of its neurons. We propose to extract such decision patterns that can be considered as invariants of the network with respect to a certain output behavior. We present techniques to extract input invariants as convex predicates on the input space, and layer invariants that represent features captured in the hidden layers. We apply the techniques on the networks for the MNIST and ACASXU applications. Our experiments highlight the use of invariants in a variety of applications, such as explainability, providing robustness guarantees, detecting adversaries, simplifying proofs and network distillation.","",""
2,"Z. Wang, D. Guo, Zhangren Tu, Yihui Huang, Yirong Zhou, Jian Wang, Liubin Feng, Donghai Lin, Yongfu You, T. Agback, V. Orekhov, X. Qu","A Sparse Model-Inspired Deep Thresholding Network for Exponential Signal Reconstruction--Application in Fast Biological Spectroscopy.",2020,"","","","",8,"2022-07-13 09:26:01","","10.1109/TNNLS.2022.3144580","","",,,,,2,1.00,0,12,2,"The nonuniform sampling (NUS) is a powerful approach to enable fast acquisition but requires sophisticated reconstruction algorithms. Faithful reconstruction from partially sampled exponentials is highly expected in general signal processing and many applications. Deep learning (DL) has shown astonishing potential in this field, but many existing problems, such as lack of robustness and explainability, greatly limit its applications. In this work, by combining the merits of the sparse model-based optimization method and data-driven DL, we propose a DL architecture for spectra reconstruction from undersampled data, called MoDern. It follows the iterative reconstruction in solving a sparse model to build the neural network, and we elaborately design a learnable soft-thresholding to adaptively eliminate the spectrum artifacts introduced by undersampling. Extensive results on both synthetic and biological data show that MoDern enables more robust, high-fidelity, and ultrafast reconstruction than the state-of-the-art methods. Remarkably, MoDern has a small number of network parameters and is trained on solely synthetic data while generalizing well to biological data in various scenarios. Furthermore, we extend it to an open-access and easy-to-use cloud computing platform (XCloud-MoDern), contributing a promising strategy for further development of biological applications.","",""
5,"Amrith Krishna, S. Riedel, Andreas Vlachos","ProoFVer: Natural Logic Theorem Proving for Fact Verification",2021,"","","","",9,"2022-07-13 09:26:01","","","","",,,,,5,5.00,2,3,1,"Fact veriﬁcation systems typically rely on neural network classiﬁers for veracity prediction which lack explainability. This paper proposes ProoFVer, which uses a seq2seq model to generate natural logic-based inferences as proofs. These proofs consist of lexical mutations between spans in the claim and the evidence retrieved, each marked with a natural logic operator. Claim veracity is determined solely based on the sequence of these operators. Hence, these proofs are faithful explanations, and this makes ProoFVer faithful by construction. Currently, ProoFVer has the highest label accuracy and the second best Score in the FEVER leaderboard. Furthermore, it improves by 13.21% points over the next best model on a dataset with counterfactual instances, demonstrating its robustness. As explanations, the proofs show better overlap with human rationales than attention-based highlights and the proofs help humans predict model decisions correctly more often than using the evidence directly. 1","",""
2,"S. Sivasankaran, E. Vincent, D. Fohr","Explaining Deep Learning Models for Speech Enhancement",2021,"","","","",10,"2022-07-13 09:26:01","","10.21437/interspeech.2021-1764","","",,,,,2,2.00,1,3,1,"We consider the problem of explaining the robustness of neural networks used to compute time-frequency masks for speech enhancement to mismatched noise conditions. We employ the Deep SHapley Additive exPlanations (DeepSHAP) feature attribution method to quantify the contribution of every time-frequency bin in the input noisy speech signal to every time-frequency bin in the output time-frequency mask. We deﬁne an objective metric — referred to as the speech relevance score — that summarizes the obtained SHAP values and show that it cor-relates with the enhancement performance, as measured by the word error rate on the CHiME-4 real evaluation dataset. We use the speech relevance score to explain the generalization ability of three speech enhancement models trained using synthetically generated speech-shaped noise, noise from a professional sound effects library, or real CHiME-4 noise. To the best of our knowledge, this is the ﬁrst study on neural network explainability in the context of speech enhancement.","",""
1,"H. Mirzaalian, Mohamed E. Hussein, Leonidas Spinoulas, Jonathan May, Wael AbdAlmageed","Explaining Face Presentation Attack Detection Using Natural Language",2021,"","","","",11,"2022-07-13 09:26:01","","10.1109/FG52635.2021.9667024","","",,,,,1,1.00,0,5,1,"A large number of deep neural network based techniques have been developed to address the challenging problem of face presentation attack detection (PAD). Whereas such techniques' focus has been on improving PAD performance in terms of classification accuracy and robustness against unseen attacks and environmental conditions, there exists little attention on the explainability of PAD predictions. In this paper, we tackle the problem of explaining PAD predictions through natural language. Our approach passes feature representations of a deep layer of the PAD model to a language model to generate text describing the reasoning behind the PAD prediction. Due to the limited amount of annotated data in our study, we apply a light-weight LSTM network as our natural language generation model. We investigate how the quality of the generated explanations is affected by different loss functions, including the commonly used word-wise cross entropy loss, a sentence discriminative loss, and a sentence semantic loss. We perform our experiments using face images from a dataset consisting of 1,105 bona-fide and 924 presentation attack samples. Our quantitative and qualitative results show the effectiveness of our model for generating proper PAD explanations through text as well as the power of the sentence-wise losses. To the best of our knowledge, this is the first introduction of a joint biometrics-NLP task. Our dataset can be obtained through our GitHub page11https://github.com/ISICV/PADISI_USC_Dataset .","",""
0,"Yao Qiang, Supriya Tumkur Suresh Kumar, Marco Brocanelli, D. Zhu","Adversarially robust and explainable model compression with on-device personalization for NLP applications",2021,"","","","",12,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,4,1,"On-device Deep Neural Networks (DNNs) have recently gained more attention due to the increasing computing power of the mobile devices and the number of applications in Computer Vision (CV), Natural Language Processing (NLP), and Internet of Things (IoTs). Unfortunately, the existing efficient convolutional neural network (CNN) architectures designed for CV tasks are not directly applicable to NLP tasks and the tiny Recurrent Neural Network (RNN) architectures have been designed primarily for IoT applications. In NLP applications, although model compression has seen initial success in on-device text classification, there are at least three major challenges yet to be addressed: adversarial robustness, explainability, and personalization. Here we attempt to tackle these challenges by designing a new training scheme for model compression and adversarial robustness, including the optimization of an explainable feature mapping objective, a knowledge distillation objective, and an adversarially robustness objective. The resulting compressed model is personalized using on-device private training data via fine-tuning. We perform extensive experiments to compare our approach with both compact RNN (e.g., FastGRNN) and compressed RNN (e.g., PRADO) architectures in both natural and adversarial NLP test settings.","",""
0,"Yao Qiang, Supriya Tumkur Suresh Kumar, Marco Brocanelli, D. Zhu","Adversarially Robust and Explainable Model Compression with On-Device Personalization for Text Classification",2021,"","","","",13,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,4,1,"On-device Deep Neural Networks (DNNs) have recently gained more attention due to the increasing computing power of the mobile devices and the number of applications in Computer Vision (CV), Natural Language Processing (NLP), and Internet of Things (IoTs). Unfortunately, the existing efficient convolutional neural network (CNN) architectures designed for CV tasks are not directly applicable to NLP tasks and the tiny Recurrent Neural Network (RNN) architectures have been designed primarily for IoT applications. In NLP applications, although model compression has seen initial success in on-device text classification, there are at least three major challenges yet to be addressed: adversarial robustness, explainability, and personalization. Here we attempt to tackle these challenges by designing a new training scheme for model compression and adversarial robustness, including the optimization of an explainable feature mapping objective, a knowledge distillation objective, and an adversarially robustness objective. The resulting compressed model is personalized using on-device private training data via fine-tuning. We perform extensive experiments to compare our approach with both compact RNN (e.g., FastGRNN) and compressed RNN (e.g., PRADO) architectures in both natural and adversarial NLP test settings.","",""
0,"Mahiout Thomas, Fillatre Lionel, Deruaz-Pepin Laurent","Explainable Deep Learning Detection of Gaussian Propeller Noise with Unknown Signal-to-Noise Ratio",2021,"","","","",14,"2022-07-13 09:26:01","","10.1109/mlsp52302.2021.9596566","","",,,,,0,0.00,0,3,1,"Due to its need for robustness and reliability, underwater target detection is a challenging task for deep learning applications. Though many attempts were made to deal with this problem using expert features, few works assessed the benefit of designing deep raw waveform architecture despite its performance in other domains. This paper is focused on explainable raw waveform based neural network for underwater propeller detection. To this purpose, we design a class of Bayes explainable deep neural networks that contains neural networks whose architecture matches the structure of the optimal Bayes detector. This class is derived from a realistic acoustic model of underwater propeller noise. It is established that the approximation error of our class is as small as desired. We also show that this class can be efficiently implemented as a convolutional neural network. Numerical simulations study the risk and explainability of our class compared to a usual convolutional neural network.","",""
4,"Radek Mackowiak, Lynton Ardizzone, Ullrich Kothe, C. Rother","Generative Classifiers as a Basis for Trustworthy Computer Vision",2020,"","","","",15,"2022-07-13 09:26:01","","","","",,,,,4,2.00,1,4,2,"With the maturing of deep learning systems, trustworthiness is becoming increasingly important for model assessment. We understand trustworthiness as the combination of explainability and robustness. Generative classifiers (GCs) are a promising class of models that are said to naturally accomplish these qualities. However, this has mostly been demonstrated on simple datasets such as MNIST, SVHN and CIFAR in the past. In this work, we firstly develop an architecture and training scheme that allows for GCs to be trained on the ImageNet classification task, a more relevant level of complexity for practical computer vision. The resulting models use an invertible neural network architecture and achieve a competetive ImageNet top-1 accuracy of up to 76.2%. Secondly, we show the large potential of GCs for trustworthiness. Explainability and some aspects of robustness are vastly improved compared to standard feed-forward models, even when the GCs are just applied naively. While not all trustworthiness problems are solved completely, we argue from our observations that GCs are an extremely promising basis for further algorithms and modifications, as have been developed in the past for feedforward models to increase their trustworthiness. We release our trained model for download in the hope that it serves as a starting point for various other generative classification tasks in much the same way as pretrained ResNet models do for discriminative classification.","",""
1,"Satyam Mohla, Anshul Nasery, Biplab Banerjee, S. Chaudhuri","CognitiveCNN: Mimicking Human Cognitive Models to resolve Texture-Shape Bias",2020,"","","","",16,"2022-07-13 09:26:01","","","","",,,,,1,0.50,0,4,2,"Recent works demonstrate the texture bias in Convolutional Neural Networks (CNNs), conflicting with early works claiming that networks identify objects using shape. It is commonly believed that the cost function forces the network to take a greedy route to increase accuracy using texture, failing to explore any global statistics. We propose a novel intuitive architecture, namely CognitiveCNN, inspired from feature integration theory in psychology to utilise human-interpretable feature like shape, texture, edges etc. to reconstruct, and classify the image. We define two metrics, namely TIC and RIC to quantify the importance of each stream using attention maps. We introduce a regulariser which ensures that the contribution of each feature is same for any task, as it is for reconstruction; and perform experiments to show the resulting boost in accuracy and robustness besides imparting explainability. Lastly, we adapt these ideas to conventional CNNs and propose Augmented Cognitive CNN to achieve superior performance in object recognition.","",""
22,"Walt Woods, Jack H Chen, C. Teuscher","Adversarial explanations for understanding image classification decisions and improved neural network robustness",2019,"","","","",17,"2022-07-13 09:26:01","","10.1038/s42256-019-0104-6","","",,,,,22,7.33,7,3,3,"","",""
1,"Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, Jun Xu, Zhimeng Guo, Hui Liu, Jiliang Tang, Suhang Wang","A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability",2022,"","","","",18,"2022-07-13 09:26:01","","10.48550/arXiv.2204.08570","","",,,,,1,1.00,0,8,1,"Graph Neural Networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trustworthy GNNs in various aspects are emerging to prevent the harm from GNN models and increase the users’ trust in GNNs. In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the related methods and formulate the general frameworks for the multiple categories of trustworthy GNNs. We also discuss the future research directions of each aspect and connections between these aspects to help achieve trustworthiness. Neural Networks:","",""
33,"Zebin Yang, Aijun Zhang, A. Sudjianto","Enhancing Explainability of Neural Networks Through Architecture Constraints",2019,"","","","",19,"2022-07-13 09:26:01","","10.1109/TNNLS.2020.3007259","","",,,,,33,11.00,11,3,3,"Prediction accuracy and model explainability are the two most important objectives when developing machine learning algorithms to solve real-world problems. Neural networks are known to possess good prediction performance but suffer from a lack of model interpretability. In this article, we propose to enhance the explainability of neural networks through the following architecture constraints: 1) sparse additive subnetworks; 2) projection pursuit with orthogonality constraint; and 3) smooth function approximation. It leads to an enhanced explainable neural network (ExNN) with a superior balance between prediction performance and model interpretability. We derive sufficient identifiability conditions for the proposed ExNN model. The multiple parameters are simultaneously estimated by a modified minibatch gradient descent method based on the backpropagation algorithm for calculating the derivatives and the Cayley transform for preserving the projection orthogonality. Through simulation study under six different scenarios, we compare the proposed method to several benchmarks, including least absolute shrinkage and selection operator, support vector machine, random forest, extreme learning machine, and multilayer perceptron. It is shown that the proposed ExNN model keeps the flexibility of pursuing high prediction accuracy while attaining improved interpretability. Finally, a real data example is employed as a showcase application.","",""
37,"Maximilian Augustin, Alexander Meinke, Matthias Hein","Adversarial Robustness on In- and Out-Distribution Improves Explainability",2020,"","","","",20,"2022-07-13 09:26:01","","10.1007/978-3-030-58574-7_14","","",,,,,37,18.50,12,3,2,"","",""
5,"Boyuan Feng, Yuke Wang, Z. Wang, Yufei Ding","Uncertainty-aware Attention Graph Neural Network for Defending Adversarial Attacks",2020,"","","","",21,"2022-07-13 09:26:01","","","","",,,,,5,2.50,1,4,2,"With the increasing popularity of graph-based learning, graph neural networks (GNNs) emerge as the essential tool for gaining insights from graphs. However, unlike the conventional CNNs that have been extensively explored and exhaustively tested, people are still worrying about the GNNs' robustness under the critical settings, such as financial services. The main reason is that existing GNNs usually serve as a black-box in predicting and do not provide the uncertainty on the predictions. On the other side, the recent advancement of Bayesian deep learning on CNNs has demonstrated its success of quantifying and explaining such uncertainties to fortify CNN models. Motivated by these observations, we propose UAG, the first systematic solution to defend adversarial attacks on GNNs through identifying and exploiting hierarchical uncertainties in GNNs. UAG develops a Bayesian Uncertainty Technique (BUT) to explicitly capture uncertainties in GNNs and further employs an Uncertainty-aware Attention Technique (UAT) to defend adversarial attacks on GNNs. Intensive experiments show that our proposed defense approach outperforms the state-of-the-art solutions by a significant margin.","",""
0,"M. Careem, A. Dutta, Ngwe Thawdar","On Equivalence of Neural Network Receivers",2021,"","","","",22,"2022-07-13 09:26:01","","10.1109/ICC42927.2021.9500703","","",,,,,0,0.00,0,3,1,"Neural Network (NN) based receivers have seen limited adoption in practical systems due to a lack of explainability and performance guarantees, despite their efficacy as a data-driven tool for physical layer signal processing. In order to bridge this gap in explainability, we present an equivalent NN-based receiver that performs the same optimizations used by classical receivers for symbol detection. Achieving equivalence is crucial to explaining how a NN-based receiver classifies symbols in high-dimensional channels and determining its structure that is robust to the underlying channel with minimum training. We realize this by deriving the risk function that guarantees equivalence, which also provides a measure of the disparity between NN-based and classical receivers. Consequently, this information allows us to derive mathematically tight data-dependent bounds on the bit error rate of NN-based receivers, and empirically determine its structure that achieves minimum error rate. Extensive simulation results show the efficacy of the derived bounds and structure of NN-based receivers for single and multi-antenna systems over a variety of channels.","",""
45,"A. Rădulescu","Neural Network Spectral Robustness under Perturbations of the Underlying Graph",2016,"","","","",23,"2022-07-13 09:26:01","","10.1162/NECO_a_00798","","",,,,,45,7.50,45,1,6,"Recent studies have been using graph-theoretical approaches to model complex networks (such as social, infrastructural, or biological networks) and how their hardwired circuitry relates to their dynamic evolution in time. Understanding how configuration reflects on the coupled behavior in a system of dynamic nodes can be of great importance, for example, in the context of how the brain connectome is affecting brain function. However, the effect of connectivity patterns on network dynamics is far from being fully understood. We study the connections between edge configuration and dynamics in a simple oriented network composed of two interconnected cliques (representative of brain feedback regulatory circuitry). In this article our main goal is to study the spectra of the graph adjacency and Laplacian matrices, with a focus on three aspects in particular: (1) the sensitivity and robustness of the spectrum in response to varying the intra- and intermodular edge density, (2) the effects on the spectrum of perturbing the edge configuration while keeping the densities fixed, and (3) the effects of increasing the network size. We study some tractable aspects analytically, then simulate more general results numerically, thus aiming to motivate and explain our further work on the effect of these patterns on the network temporal dynamics and phase transitions. We discuss the implications of such results to modeling brain connectomics. We suggest potential applications to understanding synaptic restructuring in learning networks and the effects of network configuration on function of regulatory neural circuits.","",""
2,"Vedant Nanda, Till Speicher, John P. Dickerson, K. Gummadi, M. B. Zafar","Unifying Model Explainability and Robustness via Machine-Checkable Concepts",2020,"","","","",24,"2022-07-13 09:26:01","","","","",,,,,2,1.00,0,5,2,"As deep neural networks (DNNs) get adopted in an ever-increasing number of applications, explainability has emerged as a crucial desideratum for these models. In many real-world tasks, one of the principal reasons for requiring explainability is to in turn assess prediction robustness, where predictions (i.e., class labels) that do not conform to their respective explanations (e.g., presence or absence of a concept in the input) are deemed to be unreliable. However, most, if not all, prior methods for checking explanation-conformity (e.g., LIME, TCAV, saliency maps) require significant manual intervention, which hinders their large-scale deployability. In this paper, we propose a robustness-assessment framework, at the core of which is the idea of using machine-checkable concepts. Our framework defines a large number of concepts that the DNN explanations could be based on and performs the explanation-conformity check at test time to assess prediction robustness. Both steps are executed in an automated manner without requiring any human intervention and are easily scaled to datasets with a very large number of classes. Experiments on real-world datasets and human surveys show that our framework is able to enhance prediction robustness significantly: the predictions marked to be robust by our framework have significantly higher accuracy and are more robust to adversarial perturbations.","",""
7,"Lorenz Kuhn, Clare Lyle, Aidan N. Gomez, Jonas Rothfuss, Y. Gal","Robustness to Pruning Predicts Generalization in Deep Neural Networks",2021,"","","","",25,"2022-07-13 09:26:01","","","","",,,,,7,7.00,1,5,1,"Existing generalization measures that aim to capture a model’s simplicity based on parameter counts or norms fail to explain generalization in overparameterized deep neural networks. In this paper, we introduce a new, theoretically motivated measure of a network’s simplicity which we call prunability: the smallest fraction of the network’s parameters that can be kept while pruning without adversely affecting its training loss. We show that this measure is highly predictive of a model’s generalization performance across a large set of convolutional networks trained on CIFAR10, does not grow with network size unlike existing pruning-based measures, and exhibits high correlation with test set loss even in a particularly challenging double descent setting. Lastly, we show that the success of prunability cannot be explained by its relation to known complexity measures based on models’ margin, flatness of minima and optimization speed, finding that our new measure is similar to – but more predictive than – existing flatness-based measures, and that its predictions exhibit low mutual information with those of other baselines.","",""
0,"Kelei Cao, Mengchen Liu, Hang Su, Jing Wu, Jun Zhu, Shixia Liu","Analyzing the Noise Robustness of Deep Neural Networks",2020,"","","","",26,"2022-07-13 09:26:01","","10.1109/TVCG.2020.2969185","","",,,,,0,0.00,0,6,2,"Adversarial examples, generated by adding small but intentionally imperceptible perturbations to normal examples, can mislead deep neural networks (DNNs) to make incorrect predictions. Although much work has been done on both adversarial attack and defense, a fine-grained understanding of adversarial examples is still lacking. To address this issue, we present a visual analysis method to explain why adversarial examples are misclassified. The key is to compare and analyze the datapaths of both the adversarial and normal examples. A datapath is a group of critical neurons along with their connections. We formulate the datapath extraction as a subset selection problem and solve it by constructing and training a neural network. A multi-level visualization consisting of a network-level visualization of data flows, a layer-level visualization of feature maps, and a neuron-level visualization of learned features, has been designed to help investigate how datapaths of adversarial and normal examples diverge and merge in the prediction process. A quantitative evaluation and a case study were conducted to demonstrate the promise of our method to explain the misclassification of adversarial examples.","",""
17,"Kyle D. Julian, Ritchie Lee, Mykel J. Kochenderfer","Validation of Image-Based Neural Network Controllers through Adaptive Stress Testing",2020,"","","","",27,"2022-07-13 09:26:01","","10.1109/ITSC45102.2020.9294549","","",,,,,17,8.50,6,3,2,"Neural networks have become state-of-the-art for computer vision problems because of their ability to efficiently model complex functions from large amounts of data. While neural networks can be shown to perform well empirically for a variety of tasks, their performance is difficult to guarantee. Neural network verification tools have been developed that can certify robustness with respect to a given input image; however, for neural network systems used in closed-loop controllers, robustness with respect to individual images does not address multi-step properties of the neural network controller and its environment. Furthermore, neural network systems interacting in the physical world and using natural images are operating in a black-box environment, making formal verification intractable. This work combines the adaptive stress testing (AST) framework with neural network verification tools to search for the most likely sequence of image disturbances that cause the neural network controlled system to reach a failure. An autonomous aircraft taxi application is presented, and results show that the AST method finds failures with more likely image disturbances than baseline methods. Further analysis of AST results revealed an explainable cause of the failure, giving insight into the problematic scenarios that should be addressed.","",""
1,"","RECURRENT NEURAL NETWORK ARCHITECTURE",2020,"","","","",28,"2022-07-13 09:26:01","","","","",,,,,1,0.50,0,0,2,"While dynamic systems can be modeled as sequence-to-sequence tasks by deep learning using different network architectures like DNN, CNN, RNNs or neural ODEs, the resulting models often provide poor understanding of the underlying system properties. We propose a new recurrent network architecture, the Dynamic Recurrent Network (DYRNN), where the computation function is based on the discrete difference equations of basic linear system transfer functions known from dynamic system identification. This results in a more explainable model, since the learnt weights can provide insight on a system’s time dependent behaviour. It also introduces the sequences’ sampling rate as an additional model parameter, which can be leveraged, for example, for time series data augmentation and model robustness checks. The network is trained using traditional gradient descent optimization and can be used in combination with other state of the art neural network layers. We show that our new layer type yields results comparable to or better than other recurrent layer types on several system identification tasks.","",""
16,"Xuanqing Liu, Tesi Xiao, Uc Davis, Qin Cao","How Does Noise Help Robustness? Explanation and Exploration under the Neural SDE Framework",2020,"","","","",29,"2022-07-13 09:26:01","","10.1109/cvpr42600.2020.00036","","",,,,,16,8.00,4,4,2,"Neural Ordinary Differential Equation (Neural ODE) has been proposed as a continuous approximation to the ResNet architecture. Some commonly used regularization mechanisms in discrete neural networks (e.g., dropout, Gaussian noise) are missing in current Neural ODE networks. In this paper, we propose a new continuous neural network framework called Neural Stochastic Differential Equation (Neural SDE), which naturally incorporates various commonly used regularization mechanisms based on random noise injection. For regularization purposes, our framework includes multiple types of noise patterns, such as dropout, additive, and multiplicative noise, which are common in plain neural networks. We provide some theoretical analyses explaining the improved robustness of our models against input perturbations. Furthermore, we demonstrate that the Neural SDE network can achieve better generalization than the Neural ODE and is more resistant to adversarial and non-adversarial input perturbations.","",""
3,"Ian E. Nielsen, Dimah Dera, G. Rasool, N. Bouaynaya, R. Ramachandran","Robust Explainability: A tutorial on gradient-based attribution methods for deep neural networks",2021,"","","","",30,"2022-07-13 09:26:01","","10.1109/MSP.2022.3142719","","",,,,,3,3.00,1,5,1,"The rise in deep neural networks (DNNs) has led to increased interest in explaining their predictions. While many methods for this exist, there is currently no consensus on how to evaluate them. On the other hand, robustness is a popular topic for deep learning (DL) research; however, it has been hardly talked about in explainability until very recently.","",""
8,"E. Thibeau-Sutre, O. Colliot, D. Dormont, Ninon Burgos","Visualization approach to assess the robustness of neural networks for medical image classification",2019,"","","","",31,"2022-07-13 09:26:01","","10.1117/12.2548952","","",,,,,8,2.67,2,4,3,"The use of neural networks for diagnosis classification is becoming more and more prevalent in the medical imaging community. However, deep learning method outputs remain hard to explain. Another difficulty is to choose among the large number of techniques developed to analyze how networks learn, as all present different limitations. In this paper, we extended the framework of Fong and Vedaldi [IEEE International Conference on Computer Vision (ICCV), 2017] to visualize the training of convolutional neural networks (CNNs) on 3D quantitative neuroimaging data. Our application focuses on the detection of Alzheimer's disease with gray matter probability maps extracted from structural MRI. We first assessed the robustness of the visualization method by studying the coherence of the longitudinal patterns and regions identified by the network. We then studied the stability of the CNN training by computing visualization-based similarity indexes between different re-runs of the CNN. We demonstrated that the areas identified by the CNN were consistent with what is known of Alzheimer's disease and that the visualization approach extract coherent longitudinal patterns. We also showed that the CNN training is not stable and that the areas identified mainly depend on the initialization and the training process. This issue may exist in many other medical studies using deep learning methods on datasets in which the number of samples is too small and the data dimension is high. This means that it may not be possible to rely on deep learning to detect stable regions of interest in this field yet.","",""
19,"Chongzhi Zhang, Aishan Liu, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing Ma, Tianlin Li","Interpreting and Improving Adversarial Robustness of Deep Neural Networks With Neuron Sensitivity",2019,"","","","",32,"2022-07-13 09:26:01","","10.1109/TIP.2020.3042083","","",,,,,19,6.33,3,7,3,"Deep neural networks (DNNs) are vulnerable to adversarial examples where inputs with imperceptible perturbations mislead DNNs to incorrect results. Despite the potential risk they bring, adversarial examples are also valuable for providing insights into the weakness and blind-spots of DNNs. Thus, the interpretability of a DNN in the adversarial setting aims to explain the rationale behind its decision-making process and makes deeper understanding which results in better practical applications. To address this issue, we try to explain adversarial robustness for deep models from a new perspective of neuron sensitivity which is measured by neuron behavior variation intensity against benign and adversarial examples. In this paper, we first draw the close connection between adversarial robustness and neuron sensitivities, as sensitive neurons make the most non-trivial contributions to model predictions in the adversarial setting. Based on that, we further propose to improve adversarial robustness by stabilizing the behaviors of sensitive neurons. Moreover, we demonstrate that state-of-the-art adversarial training methods improve model robustness by reducing neuron sensitivities, which in turn confirms the strong connections between adversarial robustness and neuron sensitivity. Extensive experiments on various datasets demonstrate that our algorithm effectively achieves excellent results. To the best of our knowledge, we are the first to study adversarial robustness using neuron sensitivities.","",""
3,"Syed Suleman Abbas Zaidi, X. Boix, Neeraj Prasad, Sharon Gilad-Gutnick, Shlomit Ben-Ami, P. Sinha","Is Robustness To Transformations Driven by Invariant Neural Representations?",2020,"","","","",33,"2022-07-13 09:26:01","","","","",,,,,3,1.50,1,6,2,"Deep Convolutional Neural Networks (DCNNs) have demonstrated impressive robustness to recognize objects under transformations (e.g. blur or noise) when these transformations are included in the training set. A hypothesis to explain such robustness is that DCNNs develop invariant neural representations that remain unaltered when the image is transformed. Yet, to what extent this hypothesis holds true is an outstanding question, as including transformations in the training set could lead to properties different from invariance, e.g. parts of the network could be specialized to recognize either transformed or non-transformed images. In this paper, we analyze the conditions under which invariance emerges. To do so, we leverage that invariant representations facilitate robustness to transformations for object categories that are not seen transformed during training. Our results with state-of-the-art DCNNs indicate that invariant representations strengthen as the number of transformed categories in the training set is increased. This is much more prominent with local transformations such as blurring and high-pass filtering, compared to geometric transformations such as rotation and thinning, that entail changes in the spatial arrangement of the object. Our results contribute to a better understanding of invariant representations in deep learning, and the conditions under which invariance spontaneously emerges.","",""
30,"Yangming Li, Shuai Li, B. Hannaford","A Model-Based Recurrent Neural Network With Randomness for Efficient Control With Applications",2019,"","","","",34,"2022-07-13 09:26:01","","10.1109/TII.2018.2869588","","",,,,,30,10.00,10,3,3,"Recently, recurrent neural network (RNN) control schemes for redundant manipulators have been extensively studied. These control schemes demonstrate superior computational efficiency, control precision, and control robustness. However, they lack planning completeness. This paper explains why RNN control schemes suffer from the problem. Based on the analysis, this work presents a new random RNN control scheme, which 1) introduces randomness into RNN to address the planning completeness problem, 2) improves control precision with a new optimization target, and 3) improves planning efficiency through learning from exploration. Theoretical analyses are used to prove the global stability, the planning completeness, and the computational complexity of the proposed method. Software simulation is provided to demonstrate the improved robustness against noise, the planning completeness and the improved planning efficiency of the proposed method over benchmark RNN control schemes. Real-world experiments are presented to demonstrate the application of the proposed method.","",""
0,"Vedant Nanda, Junaid Ali, K. Gummadi","Unifying model explainability and robustness via reasoning labels",2019,"","","","",35,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,3,3,"Explainability in deep learning has emerged as an important topic in recent years, with several works exploring various notions and mechanisms of explainability for deep neural networks (DNNs). In this paper, we draw upon the insight that in many situations model explainability is a means to assess another related yet distinct criterion model robustness. In order to render the link between explainability and robustness more explicit, we propose to use human-understandable reasoning labels during the training process of DNNs. The reasoning labels are jointly learned with the traditional classification labels. This joint training enables the model to predict a set of reasoning labels with every predicted class label. Then, we tie model explainability and robustness by introducing a notion of prediction consistency, whereby the model predictions are accepted—or considered robust—only when the predicted class and the predicted reasoning labels follow a certain pre-specified mapping. We show that by adopting such a framework, one can improve the classification accuracy of the state-of-the-art models (on consistent samples). We further show that using this notion of consistency makes the model more robust to adversarial perturbations.","",""
0,"Yipeng Du, Jian Liu","IENet: a robust convolutional neural network for EEG based brain-computer interfaces",2022,"","","","",36,"2022-07-13 09:26:01","","10.1088/1741-2552/ac7257","","",,,,,0,0.00,0,2,1,"Objective. Brain-computer interfaces (BCIs) based on electroencephalogram (EEG) develop into novel application areas with more complex scenarios, which put forward higher requirements for the robustness of EEG signal processing algorithms. Deep learning can automatically extract discriminative features and potential dependencies via deep structures, demonstrating strong analytical capabilities in numerous domains such as computer vision and natural language processing. Making full use of deep learning technology to design a robust algorithm that is capable of analyzing EEG across BCI paradigms is our main work in this paper. Approach. Inspired by InceptionV4 and InceptionTime architecture, we introduce a neural network ensemble named InceptionEEG-Net (IENet), where multi-scale convolutional layer and convolution of length 1 enable model to extract rich high-dimensional features with limited parameters. In addition, we propose the average receptive field (RF) gain for convolutional neural networks (CNNs), which optimizes IENet to detect long patterns at a smaller cost. We compare with the current state-of-the-art methods across five EEG-BCI paradigms: steady-state visual evoked potentials (VEPs), epilepsy EEG, overt attention P300 VEPs, covert attention P300 visual-EPs and movement-related cortical potentials. Main results. The classification results show that the generalizability of IENet is on par with the state-of-the-art paradigm-agnostic models on test datasets. Furthermore, the feature explainability analysis of IENet illustrates its capability to extract neurophysiologically interpretable features for different BCI paradigms, ensuring the reliability of algorithm. Significance. It can be seen from our results that IENet can generalize to different BCI paradigms. And it is essential for deep CNNs to increase the RF size using average RF gain.","",""
1,"Shuncheng Jia, Ruichen Zuo, Tielin Zhang, Hongxing Liu, Bo Xu","Motif-topology and Reward-learning improved Spiking Neural Network for Efficient Multi-sensory Integration",2022,"","","","",37,"2022-07-13 09:26:01","","10.1109/icassp43922.2022.9746157","","",,,,,1,1.00,0,5,1,"Network architectures and learning principles are key in forming complex functions in artificial neural networks (ANNs) and spiking neural networks (SNNs). SNNs are considered the new-generation artificial networks by incorporating more biological features than ANNs, including dynamic spiking neurons, functionally specified architectures, and efficient learning paradigms. In this paper, we propose a Motiftopology and Reward-learning improved SNN (MR-SNN) for efficient multi-sensory integration. MR-SNN contains 13 types of 3-node Motif topologies which are first extracted from independent single-sensory learning paradigms and then integrated for multi-sensory classification. The experimental results showed higher accuracy and stronger robustness of the proposed MR-SNN than other conventional SNNs without using Motifs. Furthermore, the proposed reward learning paradigm was biologically plausible and can better explain the cognitive McGurk effect caused by incongruent visual and auditory sensory signals.","",""
7,"Laura Rieger, L. K. Hansen","Aggregating explainability methods for neural networks stabilizes explanations",2019,"","","","",38,"2022-07-13 09:26:01","","","","",,,,,7,2.33,4,2,3,"Despite a growing literature on explaining neural networks, no consensus has been reached on how to explain a neural network decision or how to evaluate an explanation. In fact, most works rely on manually assessing the explanation to evaluate the quality of a method. This injects uncertainty in the explanation process along several dimensions: Which explanation method to apply? Who should we ask to evaluate it and which criteria should be used for the evaluation? Our contributions in this paper are twofold. First, we investigate schemes to combine explanation methods and reduce model uncertainty to obtain a single aggregated explanation. Our findings show that the aggregation is more robust, well-aligned with human explanations and can attribute relevance to a broader set of features (completeness). Second, we propose a novel way of evaluating explanation methods that circumvents the need for manual evaluation and is not reliant on the alignment of neural networks and humans decision processes.","",""
17,"Saima Sharmin, P. Panda, Syed Shakib Sarwar, Chankyu Lee, Wachirawit Ponghiran, K. Roy","A Comprehensive Analysis on Adversarial Robustness of Spiking Neural Networks",2019,"","","","",39,"2022-07-13 09:26:01","","10.1109/IJCNN.2019.8851732","","",,,,,17,5.67,3,6,3,"In this era of machine learning models, their functionality is being threatened by adversarial attacks. In the face of this struggle for making artificial neural networks robust, finding a model, resilient to these attacks, is very important. In this work, we present, for the first time, a comprehensive analysis of the behavior of more bio-plausible networks, namely Spiking Neural Network (SNN) under state-of-the-art adversarial tests. We perform a comparative study of the accuracy degradation between conventional VGG-9 Artificial Neural Network (ANN) and equivalent spiking network with CIFAR-10 dataset in both whitebox and blackbox setting for different types of single-step and multi-step FGSM (Fast Gradient Sign Method) attacks. We demonstrate that SNNs tend to show more resiliency compared to ANN under blackbox attack scenario. Additionally, we find that SNN robustness is largely dependent on the corresponding training mechanism. We observe that SNNs trained by spike-based backpropagation are more adversarially robust than the ones obtained by ANN-to-SNN conversion rules in several whitebox and blackbox scenarios. Finally, we also propose a simple, yet, effective framework for crafting adversarial attacks from SNNs. Our results suggest that attacks crafted from SNNs following our proposed method are much stronger than those crafted from ANNs.","",""
0,"Hanxiao Tan, Helena Kotthaus","Explainability-Aware One Point Attack for Point Cloud Neural Networks",2021,"","","","",40,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,2,1,"With the proposition of neural networks for point clouds, deep learning has started to shine in the field of 3D object recognition while researchers have shown an increased interest to investigate the reliability of point cloud networks by fooling them with perturbed instances. However, most studies focus on the imperceptibility or surface consistency, with humans perceiving no perturbations on the adversarial examples. This work proposes two new attack methods: one-point attack (OPA) and critical traverse attack (CTA), which go in the opposite direction: we restrict the perturbation dimensions to a human cognizable range with the help of explainability methods, which enables the working principle or decision boundary of the models to be comprehensible through the observable perturbation magnitude. Our results show that the popular point cloud networks can be deceived with almost 100% success rate by shifting only one point from the input instance. In addition, we attempt to provide a more persuasive viewpoint of comparing the robustness of point cloud models against adversarial attacks. We also show the interesting impact of different point attribution distributions on the adversarial robustness of point cloud networks. Finally, we discuss how our approaches facilitate the explainability study for point cloud networks. To the best of our knowledge, this is the first point-cloud-based adversarial approach concerning explainability. Our code is available at https://github.com/Explain3D/ Exp-One-Point-Atk-PC. Figure 1: One point attack for point cloud networks. With the saliency map provided by the explainability method, only one point needs to be perturbed in the point set of the original instance to fool the most popular point cloud networks.","",""
0,"R. Sathish, Debdoot Sheet","Unit Impulse Response as an Explainer of Redundancy in a Deep Convolutional Neural Network",2019,"","","","",41,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,2,3,"Convolutional neural networks (CNN) are generally designed with a heuristic initialization of network architecture and trained for a certain task. This often leads to overparametrization after learning and induces redundancy in the information flow paths within the network. This robustness and reliability is at the increased cost of redundant computations. Several methods have been proposed which leverage metrics that quantify the redundancy in each layer. However, layer-wise evaluation in these methods disregards the long-range redundancy which exists across depth on account of the distributed nature of the features learned by the model. In this paper, we propose (i) a mechanism to empirically demonstrate the robustness in performance of a CNN on account of redundancy across its depth, (ii) a method to identify the systemic redundancy in response of a CNN across depth using the understanding of unit impulse response, we subsequently demonstrate use of these methods to interpret redundancy in few networks as example. These techniques provide better insights into the internal dynamics of a CNN","",""
5,"Rémi Bernhard, Pierre-Alain Moëllic, J. Dutertre","Impact of Low-Bitwidth Quantization on the Adversarial Robustness for Embedded Neural Networks",2019,"","","","",42,"2022-07-13 09:26:01","","10.1109/CW.2019.00057","","",,,,,5,1.67,2,3,3,"As the will to deploy neural network models on embedded systems grows, and considering the related memory footprint and energy consumption requirements, finding lighter solutions to store neural networks such as parameter quantization and more efficient inference methods becomes major research topics. Parallel to that, adversarial machine learning has risen recently, unveiling some critical flaws of machine learning models, especially neural networks. In particular, perturbed inputs called adversarial examples have been shown to fool a model into making incorrect predictions. In this paper, we investigate the adversarial robustness of quantized neural networks under different attacks. We show that quantization is not a robust protection when considering advanced threats and may result in severe form of gradient masking which leads to a false impression of security. However, and interestingly, we experimentally observe poor transferability capacities between full-precision and quantized models and between models with different quantization levels which we explain by the quantization value shift phenomenon and gradient misalignment.","",""
1,"Charles B. Delahunt, Pedro D. Maia, J. Kutz","Built to Last: Functional and Structural Mechanisms in the Moth Olfactory Network Mitigate Effects of Neural Injury",2018,"","","","",43,"2022-07-13 09:26:01","","10.3390/brainsci11040462","","",,,,,1,0.25,0,3,4,"Most organisms suffer neuronal damage throughout their lives, which can impair performance of core behaviors. Their neural circuits need to maintain function despite injury, which in particular requires preserving key system outputs. In this work, we explore whether and how certain structural and functional neuronal network motifs act as injury mitigation mechanisms. Specifically, we examine how (i) Hebbian learning, (ii) high levels of noise, and (iii) parallel inhibitory and excitatory connections contribute to the robustness of the olfactory system in the Manduca sexta moth. We simulate injuries on a detailed computational model of the moth olfactory network calibrated to data. The injuries are modeled on focal axonal swellings, a ubiquitous form of axonal pathology observed in traumatic brain injuries and other brain disorders. Axonal swellings effectively compromise spike train propagation along the axon, reducing the effective neural firing rate delivered to downstream neurons. All three of the network motifs examined significantly mitigate the effects of injury on readout neurons, either by reducing injury’s impact on readout neuron responses or by restoring these responses to pre-injury levels. These motifs may thus be partially explained by their value as adaptive mechanisms to minimize the functional effects of neural injury. More generally, robustness to injury is a vital design principle to consider when analyzing neural systems.","",""
62,"M. Kohlbrenner, Alexander Bauer, Shinichi Nakajima, Alexander Binder, W. Samek, S. Lapuschkin","Towards Best Practice in Explaining Neural Network Decisions with LRP",2019,"","","","",44,"2022-07-13 09:26:01","","10.1109/IJCNN48605.2020.9206975","","",,,,,62,20.67,10,6,3,"Within the last decade, neural network based predictors have demonstrated impressive — and at times superhuman — capabilities. This performance is often paid for with an intransparent prediction process and thus has sparked numerous contributions in the novel field of explainable artificial intelligence (XAI). In this paper, we focus on a popular and widely used method of XAI, the Layer-wise Relevance Propagation (LRP). Since its initial proposition LRP has evolved as a method, and a best practice for applying the method has tacitly emerged, based however on humanly observed evidence alone. In this paper we investigate — and for the first time quantify — the effect of this current best practice on feedforward neural networks in a visual object detection setting. The results verify that the layer-dependent approach to LRP applied in recent literature better represents the model’s reasoning, and at the same time increases the object localization and class discriminativity of LRP.","",""
144,"Tong Yang, Ning Sun, He Chen, Yongchun Fang","Neural Network-Based Adaptive Antiswing Control of an Underactuated Ship-Mounted Crane With Roll Motions and Input Dead Zones",2020,"","","","",45,"2022-07-13 09:26:01","","10.1109/TNNLS.2019.2910580","","",,,,,144,72.00,36,4,2,"As a type of indispensable oceanic transportation tools, ship-mounted crane systems are widely employed to transport cargoes and containers on vessels due to their extraordinary flexibility. However, various working requirements and the oceanic environment may cause some uncertain and unfavorable factors for ship-mounted crane control. In particular, to accomplish different control tasks, some plant parameters (e.g., boom lengths, payload masses, and so on) frequently change; hence, most existing model-based controllers cannot ensure satisfactory control performance any longer. For example, inaccurate gravity compensation may result in positioning errors. Additionally, due to ship roll motions caused by sea waves, residual payload swing generally exists, which may result in safety risks in practice. To solve the above-mentioned issues, this paper designs a neural network-based adaptive control method that can provide effective control for both actuated and unactuated state variables based on the original nonlinear ship-mounted crane dynamics without any linearizing operations. In particular, the proposed update law availably compensates parameter/structure uncertainties for ship-mounted crane systems. Based on a 2-D sliding surface, the boom and rope can arrive at their preset positions in finite time, and the payload swing can be completely suppressed. Furthermore, the problem of nonlinear input dead zones is also taken into account. The stability of the equilibrium point of all state variables in ship-mounted crane systems is theoretically proven by a rigorous Lyapunov-based analysis. The hardware experimental results verify the practicability and robustness of the presented control approach.","",""
19,"Yangming Li, Shuai Li, B. Hannaford","A Novel Recurrent Neural Network for Improving Redundant Manipulator Motion Planning Completeness",2018,"","","","",46,"2022-07-13 09:26:01","","10.1109/ICRA.2018.8461204","","",,,,,19,4.75,6,3,4,"Recurrent Neural Networks (RNNs) demonstrated advantages on control precision, system robustness and computational efficiency, and have been widely applied to redundant manipulator control optimization. Existing RNN control schemes locally optimize trajectories and are efficient and reliable on obstacle avoidance. However, for motion planning, they suffer from local minimum and do not have planning completeness. This work explained the cause of the planning incompleteness and addressed the problem with a novel RNN control scheme. The paper presented the proposed method in detail and analyzed the global stability and the planning completeness in theory. The proposed method was compared with other three control schemes on the precision, the robustness and the planning completeness in software simulation and the results shows the proposed method has improved precision and robustness, and planning completeness.","",""
6,"Yashas B L Samaga, Shampa Raghunathan, U. D. Priyakumar","SCONES: Self-Consistent Neural Network for Protein Stability Prediction Upon Mutation.",2021,"","","","",47,"2022-07-13 09:26:01","","10.26434/CHEMRXIV.14729445.V1","","",,,,,6,6.00,2,3,1,"Engineering proteins to have desired properties by mutating amino acids at specific sites is commonplace. Such engineered proteins must be stable to function. Experimental methods used to determine stability at throughputs required to scan the protein sequence space thoroughly are laborious. To this end, many machine learning based methods have been developed to predict thermodynamic stability changes upon mutation. These methods have been evaluated for symmetric consistency by testing with hypothetical reverse mutations. In this work, we propose transitive data augmentation, evaluating transitive consistency with our new Stransitive data set, and a new machine learning based method, the first of its kind, that incorporates both symmetric and transitive properties into the architecture. Our method, called SCONES, is an interpretable neural network that predicts small relative protein stability changes for missense mutations that do not significantly alter the structure. It estimates a residue's contributions toward protein stability (ΔG) in its local structural environment, and the difference between independently predicted contributions of the reference and mutant residues is reported as ΔΔG. We show that this self-consistent machine learning architecture is immune to many common biases in data sets, relies less on data than existing methods, is robust to overfitting, and can explain a substantial portion of the variance in experimental data.","",""
7,"Chen-Yi Lin, Xuefei Song, Lunhao Li, Yinwei Li, Mengda Jiang, Rou Sun, Huifang Zhou, Xianqun Fan","Detection of active and inactive phases of thyroid-associated ophthalmopathy using deep convolutional neural network",2021,"","","","",48,"2022-07-13 09:26:01","","10.1186/s12886-020-01783-5","","",,,,,7,7.00,1,8,1,"","",""
1,"Pragnyaban Mishra, P. Srinivas","Facial emotion recognition using deep convolutional neural network and smoothing, mixture filters applied during preprocessing stage",2021,"","","","",49,"2022-07-13 09:26:01","","10.11591/ijai.v10.i4.pp889-900","","",,,,,1,1.00,1,2,1,"The facial emotion recognition by the machine is a challenging task. From decades, researchers applied different methods to classify facial emotion into the different classes. The expansion of artificial intelligence in a form of deep convolutional neural network (CNN) changed the direction of the research. The facial emotion recognition using deep CNN is powerful in terms of taking bulk input images for processing and classify with high accuracy. It has been noticed in a few cases the classification model does not judge the facial images into appropriate classes due to the influence of noises. So, it is highly recommended to apply a noiseless image to the facial emotion recognition model for classification. We adopted a mechanism and proposed a model for classifying facial image into one of the seven classes with high accuracy. The images are smoothed before applying to the model by different smoothing process as part of image preprocessing. We claim facial emotion recognition with image smoothing by different filters or a mixture of filter are more robust than without preprocessing. The detail is explained in the subsequent sections.","",""
2,"B. Hamdaoui, Abdurrahman Elmaghbub, Siefeddine Mejri","Deep Neural Network Feature Designs for RF Data-Driven Wireless Device Classification",2021,"","","","",50,"2022-07-13 09:26:01","","10.1109/MNET.011.2000492","","",,,,,2,2.00,1,3,1,"Most prior works on deep learning-based wireless device classification using radio frequency (RF) data apply off-the-shelf deep neural network (DNN) models, which were matured mainly for domains like vision and language. However, wireless RF data possesses unique characteristics that differentiate it from these other domains. For instance, RF data encompasses intermingled time and frequency features that are dictated by the underlying hardware and protocol configurations. In addition, wireless RF communication signals exhibit cyclostationarity due to repeated patterns (PHY pilots, frame prefixes, and so on) that these signals inherently contain. In this article, we begin by explaining and showing the unsuitability as well as limitations of existing DNN feature design approaches currently proposed to be used for wireless device classification. We then present novel feature design approaches that exploit the distinct structures of RF communication signals and the spectrum emissions caused by transmitter hardware impairments to custom-make DNN models suitable for classifying wireless devices using RF signal data. Our proposed DNN feature designs substantially improve classification robustness in terms of scalability, accuracy, signature anti-cloning, and insensitivity to environment perturbations. We end the article by presenting other feature design strategies that have great potential for providing further performance improvements of the DNN-based wireless device classification, and discuss the open research challenges related to these proposed strategies.","",""
1,"Y. Lu, Ilgiz Murzakhanov, Spyros Chatzivasileiadis","Neural network interpretability for forecasting of aggregated renewable generation",2021,"","","","",51,"2022-07-13 09:26:01","","10.1109/SmartGridComm51999.2021.9631993","","",,,,,1,1.00,0,3,1,"With the rapid growth of renewable energy, lots of small photovoltaic (PV) prosumers emerge. Due to the uncertainty of solar power generation, there is a need for aggregated prosumers to predict solar power generation and whether solar power generation will be larger than load. This paper presents two interpretable neural networks to solve the problem: one binary classification neural network and one regression neural network. The neural networks are built using TensorFlow. The global feature importance and local feature contributions are examined by three gradient-based methods: Integrated Gradients, Expected Gradients, and DeepLIFT. Moreover, we detect abnormal cases when predictions might fail by estimating the prediction uncertainty using Bayesian neural networks. Neural networks, which are interpreted by the gradient-based methods and complemented with uncertainty estimation, provide robust and explainable forecasting for decision-makers.","",""
0,"Qiwei Shen, Zonghua Liu","Remote firing propagation in the neural network of C. elegans.",2021,"","","","",52,"2022-07-13 09:26:01","","10.1103/PhysRevE.103.052414","","",,,,,0,0.00,0,2,1,"Understanding the mechanisms of firing propagation in brain networks has been a long-standing problem in the fields of nonlinear dynamics and network science. In general, it is believed that a specific firing in a brain network may be gradually propagated from a source node to its neighbors and then to the neighbors' neighbors and so on. Here, we explore firing propagation in the neural network of Caenorhabditis elegans and surprisingly find an abnormal phenomenon, i.e., remote firing propagation between two distant and indirectly connected nodes with the intermediate nodes being inactivated. This finding is robust to source nodes but depends on the topology of network such as the unidirectional couplings and heterogeneity of network. Further, a brief theoretical analysis is provided to explain its mechanism and a principle for remote firing propagation is figured out. This finding provides insights for us to understand how those cognitive subnetworks emerge in a brain network.","",""
0,"Lee","Improved Methodology for Evaluating Adversarial Robustness in Deep Neural Networks",2020,"","","","",53,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,1,2,"Deep neural networks are known to be vulnerable to adversarial perturbations, which are often imperceptible to humans but can alter predictions of machine learning systems. Since the exact value of adversarial robustness is difficult to obtain for complex deep neural networks, accuracy of the models against perturbed examples generated by attack methods is empirically used as a proxy to adversarial robustness. However, failure of attack methods to find adversarial perturbations cannot be equated with being robust. In this work, we identify three common cases that lead to overestimation of accuracy against perturbed examples generated by bounded first-order attack methods: 1) the value of cross-entropy loss numerically becoming zero when using standard floating point representation, resulting in non-useful gradients; 2) innately non-differentiable functions in deep neural networks, such as Rectified Linear Unit (ReLU) activation and MaxPool operation, incurring “gradient masking” [2]; and 3) certain regularization methods used during training inducing the model to be less amenable to first-order approximation. We show that these phenomena exist in a wide range of deep neural networks, and that these phenomena are not limited to specific defense methods they have been previously investigated for. For each case, we propose compensation methods that either address sources of inaccurate gradient computation, such as numerical saturation for near zero values and nondifferentiability, or reduce the total number of back-propagations for iterative attacks by approximating second-order information. These compensation methods can be combined with existing attack methods for a more precise empirical evaluation metric. We illustrate the impact of these three phenomena with examples of practical interest, such as benchmarking model capacity and regularization techniques for robustness. Furthermore, we show that the gap between adversarial accuracy and the guaranteed lower bound of robustness can be partially explained by these phenomena. Overall, our work shows that overestimated adversarial accuracy that is not indicative of robustness is prevalent even for conventionally trained deep neural networks, and highlights cautions of using empirical evaluation without guaranteed bounds.","",""
23,"Yangming Li, Shuai Li, David E. Caballero, Muneaki Miyasaka, Andrew Lewis, B. Hannaford","Improving control precision and motion adaptiveness for surgical robot with recurrent neural network",2017,"","","","",54,"2022-07-13 09:26:01","","10.1109/IROS.2017.8206197","","",,,,,23,4.60,4,6,5,"Surgical robot research is driven by the desire of improving surgical outcomes. This paper proposed a Recurrent Neural Network based controller to address two problems: 1) improving control precision, 2) increasing adaptiveness for robot motion (explained in Section I). RNN was adopted in this work mainly because 1) the problem formulation naturally matches RNN structure, 2) RNN has advantages as an biologically inspired method. The proposed method was explained in detail and analysis shows that the proposed method is able to dynamically regulate outputs to increase the adaptiveness and the control precision. This paper uses Raven II surgical robot as an example to show the application of the proposed method, and the numeral simulation results from the proposed method and three other controllers show that the proposed method has improved precision, improved high robustness against noise and increased movement smoothness, and it keeps the manipulator links as far away as possible from physical boundaries, which potentially increases surgical safety and leads to improved surgical outcomes.","",""
15,"S. Mahdavifar, A. Ghorbani","DeNNeS: deep embedded neural network expert system for detecting cyber attacks",2020,"","","","",55,"2022-07-13 09:26:01","","10.1007/s00521-020-04830-w","","",,,,,15,7.50,8,2,2,"","",""
0,"Ingrid Fadelli","A memory-augmented, artificial neural network-based architecture",2021,"","","","",56,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,1,1,"ions that can be applied to future tasks. ""This abstraction mechanism and evolutionary training enable the learning of robust and scalable algorithmic solutions,"" the researchers explained in their paper. The team at Technische Universität Darmstadt evaluated the NHC by using it to train and run 11 different algorithms. They then tested the performance of these algorithms, along with their generalization and abstraction capabilities. The researchers found that the NHC could reliably run all 11 algorithms, while also allowing them to perform well on tasks that were more complex than those they were originally trained to complete. ""On a diverse set of 11 algorithms with varying complexities, we show that the NHC reliably learns","",""
6,"Gang Liu, Jing Wang","A Polynomial Neural Network with Controllable Precision and Human-Readable Topology for Prediction and System Identification",2020,"","","","",57,"2022-07-13 09:26:01","","","","",,,,,6,3.00,3,2,2,"Although artificial neural networks (ANNs) are successful, there is still a concern among many over their ""black box"" nature. Why do they work? Could we design a ""transparent"" network? This paper presents a controllable and readable polynomial neural network (CR-PNN) for approximation, prediction, and system identification. CR-PNN is simple enough to be described as one ""small"" formula, so that we can control the approximation precision and explain the internal structure of the network. CR-PNN, in fact, essentially is the fascinating Taylor expansion in the form of network. The number of layers represents precision. Derivatives in Taylor expansion are exactly imitated by error back-propagation algorithm. Firstly, we demonstrated that CR-PNN shows excellent analysis performance to the ""black box"" system through ten synthetic data with noise. Also, the results were compared with synthetic data to substantiate its search easily towards the global optimum. Secondly, it was verified, by ten real-world applications, that CR-PNN brought better generalization capability relative to the typical ANNs that approximate depended on the nonlinear activation function. Finally, 200,000 repeated experiments, with 4898 samples, demonstrated that CR-PNN is five times more efficient than typical ANN for one epoch and ten times more efficient than typical ANN for one forward-propagation. In short, compared with the traditional neural networks, the novelties and advantages of CR-PNN include readability of the internal structure, easy to find global optimal solution, lower computational complexity, and likely better robustness to real-world approximation. (We're strong believers in Open Source, and provide CR-PNN code for others. GitHub: this https URL)","",""
64,"K. Patan, M. Witczak, J. Korbicz","Towards Robustness in Neural Network Based Fault Diagnosis",2008,"","","","",58,"2022-07-13 09:26:01","","10.2478/v10006-008-0039-2","","",,,,,64,4.57,21,3,14,"Towards Robustness in Neural Network Based Fault Diagnosis Challenging design problems arise regularly in modern fault diagnosis systems. Unfortunately, classical analytical techniques often cannot provide acceptable solutions to such difficult tasks. This explains why soft computing techniques such as neural networks become more and more popular in industrial applications of fault diagnosis. Taking into account the two crucial aspects, i.e., the nonlinear behaviour of the system being diagnosed as well as the robustness of a fault diagnosis scheme with respect to modelling uncertainty, two different neural network based schemes are described and carefully discussed. The final part of the paper presents an illustrative example regarding the modelling and fault diagnosis of a DC motor, which shows the performance of the proposed strategy.","",""
45,"Gil Fidel, Ron Bitton, A. Shabtai","When Explainability Meets Adversarial Learning: Detecting Adversarial Examples using SHAP Signatures",2019,"","","","",59,"2022-07-13 09:26:01","","10.1109/IJCNN48605.2020.9207637","","",,,,,45,15.00,15,3,3,"State-of-the-art deep neural networks (DNNs) are highly effective in solving many complex real-world problems. However, these models are vulnerable to adversarial perturbation attacks, and despite the plethora of research in this domain, to this day, adversaries still have the upper hand in the cat and mouse game of adversarial example generation methods vs. detection and prevention methods. In this research, we present a novel detection method that uses Shapley Additive Explanations (SHAP) values computed for the internal layers of a DNN classifier to discriminate between normal and adversarial inputs. We evaluate our method by building an extensive dataset of adversarial examples over the popular CIFAR-10 and MNIST datasets, and training a neural network-based detector to distinguish between normal and adversarial inputs. We evaluate our detector against adversarial examples generated by diverse state-of-the-art attacks and demonstrate its high detection accuracy and strong generalization ability to adversarial inputs generated with different attack methods.","",""
0,"Sina Däubener, Asja Fischer","How Sampling Impacts the Robustness of Stochastic Neural Networks",2022,"","","","",60,"2022-07-13 09:26:01","","10.48550/arXiv.2204.10839","","",,,,,0,0.00,0,2,1,"Stochastic neural networks (SNNs) are random functions and predictions are gained by averaging over multiple realizations of this random function. Consequently, an adversarial attack is calculated based on one set of samples and applied to the prediction deﬁned by another set of samples. In this paper we analyze robustness in this setting by deriving a suﬃcient condition for the given prediction process to be robust against the calculated attack. This allows us to identify the factors that lead to an increased robustness of SNNs and helps to explain the impact of the variance and the amount of samples. Among other things, our theoretical analysis gives insights into (i) why increasing the amount of samples drawn for the estimation of adversarial examples increases the attack’s strength, (ii) why decreasing sample size during inference hardly inﬂuences the robustness, and (iii) why a higher prediction variance between realizations relates to a higher robustness. We verify the validity of our theoretical ﬁndings by an extensive empirical analysis.","",""
0,"Zi-JiangYANG, KiyoshiWADA","An Experimental Study on Adaptive Robust PCA Neural Network",2017,"","","","",61,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,2,5,"In this paper, we show the experimental study on adaptive robust neural network Principal Component Analysis (PCA) b出 edon a reconstruction error model . Firstly we explain the traditional batch PCA method which is based on eigenvalue decomposition and discuss its problems of computational complexity and poor robustness. To overcome such problems, the adaptive robust neural network Principal Component Analysis wil b巴 introduced.This adaptive robust approach is based on the structure of single-layer neural network with modification of the reconstruction error model. From the experト ments, it can be seen that this method can reduce the efect of outliers existing in the training sample set.","",""
2,"Gurpreet Singh, Soumyajit Gupta, Matt Lease, Clint N. Dawson","TIME: A Transparent, Interpretable, Model-Adaptive and Explainable Neural Network for Dynamic Physical Processes",2020,"","","","",62,"2022-07-13 09:26:01","","","","",,,,,2,1.00,1,4,2,"Partial Differential Equations are infinite dimensional encoded representations of physical processes. However, imbibing multiple observation data towards a coupled representation presents significant challenges. We present a fully convolutional architecture that captures the invariant structure of the domain to reconstruct the observable system. The proposed architecture is significantly low-weight compared to other networks for such problems. Our intent is to learn coupled dynamic processes interpreted as deviations from true kernels representing isolated processes for model-adaptivity. Experimental analysis shows that our architecture is robust and transparent in capturing process kernels and system anomalies. We also show that high weights representation is not only redundant but also impacts network interpretability. Our design is guided by domain knowledge, with isolated process representations serving as ground truths for verification. These allow us to identify redundant kernels and their manifestations in activation maps to guide better designs that are both interpretable and explainable unlike traditional deep-nets.","",""
1,"Anubhab Ghosh, Antoine Honor'e, Dong Liu, G. Henter, S. Chatterjee","Normalizing Flow based Hidden Markov Models for Classification of Speech Phones with Explainability",2021,"","","","",63,"2022-07-13 09:26:01","","","","",,,,,1,1.00,0,5,1,"In pursuit of explainability, we develop generative models for sequential data. The proposed models provide state-of-the-art classification results and robust performance for speech phone classification. We combine modern neural networks (normalizing flows) and traditional generative models (hidden Markov models HMMs). Normalizing flow-based mixture models (NMMs) are used to model the conditional probability distribution given the hidden state in the HMMs. Model parameters are learned through judicious combinations of time-tested Bayesian learning methods and contemporary neural network learning methods. We mainly combine expectation-maximization (EM) and mini-batch gradient descent. The proposed generative models can compute likelihood of a data and hence directly suitable for maximum-likelihood (ML) classification approach. Due to structural flexibility of HMMs, we can use different normalizing flow models. This leads to different types of HMMs providing diversity in data modeling capacity. The diversity provides an opportunity for easy decision fusion from different models. For a standard speech phone classification setup involving 39 phones (classes) and the TIMIT dataset, we show that the use of standard features called mel-frequency-cepstral-coeffcients (MFCCs), the proposed generative models, and the decision fusion together can achieve 86.6% accuracy by generative training only. This result is close to state-of-the-art results, for examples, 86.2% accuracy of PyTorch-Kaldi toolkit [1], and 85.1% accuracy using light gated recurrent units [2]. We do not use any discriminative learning approach and related sophisticated features in this article.","",""
0,"F. Palmieri, Mario Baldi, A. Buonanno, Giovanni Di Gennaro, Francesco Ospedale","Probing a Deep Neural Network",2020,"","","","",64,"2022-07-13 09:26:01","","10.1007/978-981-13-8950-4_19","","",,,,,0,0.00,0,5,2,"","",""
19,"M. Azad","Predicting mobile banking adoption in Bangladesh: a neural network approach",2016,"","","","",65,"2022-07-13 09:26:01","","10.1080/19186444.2016.1233726","","",,,,,19,3.17,19,1,6,"Abstract Rapid development in technology amplifies the need of examining technology adoption. Financial sectors, nowadays, are providing a long list of both financial and nonfinancial services to attract their potential customers using mobile banking (m-banking). Thus, m-banking is becoming a part of modern life style. This paper critically predicts the key factors of m-banking adoption in Bangladesh from the user perspective. A three layer neural network is used with 10-fold cross validation as a prediction model. For robustness, factor analysis is run using principal component analysis and verimax rotation technique. After pilot study, a structured questionnaire was used and a total of 314 respondents successfully returned their filled survey questionnaire. The results revealed that Perceived Ease of Use is the most influencing factor. One significant finding is that gender has no significance on m-banking adoption in Bangladesh explaining both men and women are flexible in technology adoption. Policy makers can find significant results in this paper for implementing future service design. Limitations and future research scope are also discussed.","",""
90,"Chandan Singh, W. James Murdoch, Bin Yu","Hierarchical interpretations for neural network predictions",2018,"","","","",66,"2022-07-13 09:26:01","","","","",,,,,90,22.50,30,3,4,"Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.","",""
42,"G. Basalyga, E. Salinas","When Response Variability Increases Neural Network Robustness to Synaptic Noise",2005,"","","","",67,"2022-07-13 09:26:01","","10.1162/neco.2006.18.6.1349","","",,,,,42,2.47,21,2,17,"Cortical sensory neurons are known to be highly variable, in the sense that responses evoked by identical stimuli often change dramatically from trial to trial. The origin of this variability is uncertain, but it is usually interpreted as detrimental noise that reduces the computational accuracy of neural circuits. Here we investigate the possibility that such response variability might in fact be beneficial, because it may partially compensate for a decrease in accuracy due to stochastic changes in the synaptic strengths of a network. We study the interplay between two kinds of noise, response (or neuronal) noise and synaptic noise, by analyzing their joint influence on the accuracy of neural networks trained to perform various tasks. We find an interesting, generic interaction: when fluctuations in the synaptic connections are proportional to their strengths (multiplicative noise), a certain amount of response noise in the input neurons can significantly improve network performance, compared to the same network without response noise. Performance is enhanced because response noise and multiplicative synaptic noise are in some ways equivalent. So if the algorithm used to find the optimal synaptic weights can take into account the variability of the model neurons, it can also take into account the variability of the synapses. Thus, the connection patterns generated with response noise are typically more resistant to synaptic degradation than those obtained without response noise. As a consequence of this interplay, if multiplicative synaptic noise is present, it is better to have response noise in the network than not to have it. These results are demonstrated analytically for the most basic network consisting of two input neurons and one output neuron performing a simple classification task, but computer simulations show that the phenomenon persists in a wide range of architectures, including recurrent (attractor) networks and sensorimotor networks that perform coordinate transformations. The results suggest that response variability could play an important dynamic role in networks that continuously learn.","",""
9,"Adam Noack, Isaac Ahern, D. Dou, Boyang Li","Does Interpretability of Neural Networks Imply Adversarial Robustness?",2019,"","","","",68,"2022-07-13 09:26:01","","","","",,,,,9,3.00,2,4,3,"The success of deep neural networks is clouded by two issues: (1) a vulnerability to adversarial examples and (2) a tendency to be uninterpretable. Interestingly, recent empirical evidence in the literature as well as theoretical analysis on simple models suggest these two seemingly disparate issues are actually connected. In particular, robust models tend to be more interpretable than non-robust models.  In this paper, we provide evidence for the claim that this relationship is bidirectional. Viz., models that are optimized to have interpretable gradients are more robust to adversarial examples than models trained in a standard manner. With further analysis and experiments on standard image classification datasets, we identify two factors behind this phenomenon---namely the suppression of the gradient's magnitude and the selective use of features guided by high-quality interpretations---which explain model behaviors under various regularization and target interpretation settings.","",""
7,"Collin Burns, J. Steinhardt","Limitations of Post-Hoc Feature Alignment for Robustness",2021,"","","","",69,"2022-07-13 09:26:01","","10.1109/CVPR46437.2021.00255","","",,,,,7,7.00,4,2,1,"Feature alignment is an approach to improving robustness to distribution shift that matches the distribution of feature activations between the training distribution and test distribution. A particularly simple but effective approach to feature alignment involves aligning the batch normalization statistics between the two distributions in a trained neural network. This technique has received renewed interest lately because of its impressive performance on robustness benchmarks. However, when and why this method works is not well understood. We investigate the approach in more detail and identify several limitations. We show that it only significantly helps with a narrow set of distribution shifts and we identify several settings in which it even degrades performance. We also explain why these limitations arise by pinpointing why this approach can be so effective in the first place. Our findings call into question the utility of this approach and Unsupervised Domain Adaptation more broadly for improving robustness in practice.","",""
4,"M. Hamdi, C. Aloui, S. Nanda","Comparing Functional Link Artificial Neural Network And Multilayer Feedforward Neural Network Model To Forecast Crude Oil Prices",2016,"","","","",70,"2022-07-13 09:26:01","","","","",,,,,4,0.67,1,3,6,"In this paper a trigonometric functional link artificial neural network (FLANN) model using backpropagation rule is applied to predict the next day's spot price of US crude oil. The daily observations of these variables: US dollar index, S&P 500 stock price index, gold spot price, heating oil spot price and US crude oil spot price are employed as inputs of the proposed model. By comparing with multilayer backpropagation feedforward neural network (FNN), more accurate predictions were shown by applying the FLANN model. In fact, several performance criteria are used to assess the forecasting power of the proposed model such as the Root Mean Squared Error (RMSE), the Mean Absolute Error (MAE) and the hit rate. For checking the forecasting robustness of the proposed model, in addition to the other input variables, the US crude oil and biofuels production are also used to predict the next month's spot price of crude oil. Comparatively, similar conclusion was deduced and the FLANN model performs better than the standard FNN. These findings can be explained by the simplicity of FLANN structure since it consists of a single layer with only one neuron at the output thus a lower computational load on the network.","",""
2,"Sulaiman Khan, Hazrat Ali, Z. Ullah, N. Minallah, S. Maqsood, Abdul Hafeez","Higher Accurate Recognition of Handwritten Pashto Letters through Zoning Feature by using K-Nearest Neighbour and Artificial Neural Network",2019,"","","","",71,"2022-07-13 09:26:01","","10.14569/IJACSA.2018.091070","","",,,,,2,0.67,0,6,3,"This paper presents a recognition system for handwritten Pashto letters. However, handwritten character recognition is a challenging task. These letters not only differ in shape and style but also vary among individuals. The recognition becomes further daunting due to the lack of standard datasets for inscribed Pashto letters. In this work, we have designed a database of moderate size, which encompasses a total of 4488 images, stemming from 102 distinguishing samples for each of the 44 letters in Pashto. The recognition framework uses zoning feature extractor followed by K-Nearest Neighbour (KNN) and Neural Network (NN) classifiers for classifying individual letter. Based on the evaluation of the proposed system, an overall classification accuracy of approximately 70.05% is achieved by using KNN while 72% is achieved by using NN. Keywords—KNN, deep neural network, OCR, zoning technique, Pashto, character recognition, classification sectionIntroduction In this modern technological and digital age, optical character recognition (OCR) systems play a vital role in machine learning and automatic recognition problems. OCR is a section of software tool that converts printed text and images to machine readable form and enables the machine to recognize images or text like humans. OCR systems are commercially available for isolated languages, which include Chinese, English, Japanese, and others. However, few OCR systems are available for cursive languages such as Persian and Arabic and are not highly robust. To the best of our knowledge, there is no such commercial OCR system available for carved Pashto letters recognition; however, such systems exist in research labs. Handwritten letters recognition is a daunting task mainly because of variations in writing styles of different users. Handwritten letters recognition can be done either offline or online. Online character recognition is simpler and easier to implement due to the temporal based information such as velocity, time, number of strokes, and direction for writing. In addition, the trace of the pen is a few pixels wide so this does not require thinning techniques for classification. On the other hand, offline character recognition system implementation is even laborious due to high variations in writing and font styles of every user. In our paper, we present inscribed of handwritten Pashto letters. Pashto is a major language of Pashtun tribe in Pakistan and the official language of Afghanistan. In censes 2007 2009, it was estimated that about 40 60 millions of people around the world are native speakers of this language. Pashto letters can be shaped into six different formats, which make the recognition process challenging. Furthermore, the count of character dots and occurrence of these dots that varies from letter to letter make the problem challenging. In order to address these problems, research shows the use of high level features based on the structural information of letters. An OCR based system using deep learning network model that incorporates Biand Multi-dimensional long short term memory for printed Pashto text recognition has been suggested [1]. A web-based survey shows that Pashto script contains a huge number of unique ligature [2]. Such ligature makes the implementation of OCR system for carved Pashto challenging. As printed letters contain a constant shape/style and font size; thus, the said technique fails in our case due to higher higher variations in style and font in case of inscribed letters. Riaz et al. [3] has presented the development of an OCR system for cursive Pashto script using scale invariant feature transform and principle component analysis. In order to address this issue, we present a system for handwritten Pashto letters recognition, which has the following key contributions: • As there is no standard handwritten Pashto letters database for testing an algorithm; thus, one of the contribution of this work is to develop and present a medium-sized database of 4488 (102 samples for each letter) for further research work. • The second contribution of this research work is to provide a base result as a benchmark for Pashto language. For this purpose, the performance results of the state-of-the-art classifiers−KNN and deep Neural Network are used based on zoning features. • Our proposed handwritten Pashto letters recognition system is efficient, simple, and cost-effective. • We provide comprehensive results for analyzing the proposed system for handwritten Pashto letters recognition, which may help the researchers to further explore this area. This paper is divided in seven sections: Section I explains the related work. Section II captures the background informawww.ijacsa.thesai.org 1 | P a g e ar X iv :1 90 4. 03 39 1v 1 [ cs .C V ] 6 A pr 2 01 9 (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 9, No. 10, 2018 tion about the classifiers and feature extraction algorithm used in this research work. Section III delineates the methodology. Section IV discusses about the feature extraction, which is very important in the area of pattern recognition and machine learning while section V demonstrates the experimental results followed by the conclusions and future work in Section VI.","",""
4,"Anis Hamza, N. B. Yahia","Intelligent Neural Network Control for Active Heavy Truck Suspension",2018,"","","","",72,"2022-07-13 09:26:01","","10.1007/978-3-030-19781-0_2","","",,,,,4,1.00,2,2,4,"","",""
1,"Anna-Kathrin Kopetzki, Stephan Günnemann","Reachable Sets of Classifiers & Regression Models: (Non-)Robustness Analysis and Robust Training",2020,"","","","",73,"2022-07-13 09:26:01","","10.1007/S10994-021-05973-0","","",,,,,1,0.50,1,2,2,"","",""
4,"A. Solodovnikov, M. Reed","Robustness of a Neural Network Model for Differencing",2001,"","","","",74,"2022-07-13 09:26:01","","10.1023/A:1012897716913","","",,,,,4,0.19,2,2,21,"","",""
1,"Peter Kok-Yiu Wong, Han Luo, Mingzhu Wang, Jack C. P. Cheng","Enriched and discriminative convolutional neural network features for pedestrian re‐identification and trajectory modeling",2021,"","","","",75,"2022-07-13 09:26:01","","10.1111/mice.12750","","",,,,,1,1.00,0,4,1,"Understanding pedestrian flow patterns in urban areas could support the decision‐making for infrastructure planning. By incorporating computer vision techniques into surveillance video processing, human walking trajectories in a wide area could be identified by pedestrian re‐identification (ReID) across multiple cameras. Recent ReID methods mostly use convolutional neural networks equipped with deep learning techniques to extract discriminative human features from images for identity matching. However, they still suffer from realistic challenges such as occlusion and appearance variation. This paper develops a ReID‐based framework for pedestrian trajectory recognition across multiple cameras. Specifically, a generic approach of explainable model design is presented, which intuitively analyzes existing baseline models based on feature visualization. Hence, a new model named OSNet + BDB is developed that extracts discriminative‐and‐distributed features. Additionally, an incremental feature aggregation strategy is designed for more robust identity matching. Our ReID method notably outperforms its baselines by 4% identification F1 accuracy in public benchmarks. Practically, pedestrian flow statistics in a real building are extracted for behavioral modeling. Simulations of several what‐if layouts are then conducted for facility performance evaluation.","",""
140,"R. Wai, Rajkumar Muthusamy","Fuzzy-Neural-Network Inherited Sliding-Mode Control for Robot Manipulator Including Actuator Dynamics",2013,"","","","",76,"2022-07-13 09:26:01","","10.1109/TNNLS.2012.2228230","","",,,,,140,15.56,70,2,9,"This paper presents the design and analysis of an intelligent control system that inherits the robust properties of sliding-mode control (SMC) for an n-link robot manipulator, including actuator dynamics in order to achieve a high-precision position tracking with a firm robustness. First, the coupled higher order dynamic model of an n-link robot manipulator is briefy introduced. Then, a conventional SMC scheme is developed for the joint position tracking of robot manipulators. Moreover, a fuzzy-neural-network inherited SMC (FNNISMC) scheme is proposed to relax the requirement of detailed system information and deal with chattering control efforts in the SMC system. In the FNNISMC strategy, the FNN framework is designed to mimic the SMC law, and adaptive tuning algorithms for network parameters are derived in the sense of projection algorithm and Lyapunov stability theorem to ensure the network convergence as well as stable control performance. Numerical simulations and experimental results of a two-link robot manipulator actuated by DC servo motors are provided to justify the claims of the proposed FNNISMC system, and the superiority of the proposed FNNISMC scheme is also evaluated by quantitative comparison with previous intelligent control schemes.","",""
3,"Aníbal Pedraza, O. Déniz-Suárez, Gloria Bueno","On the Relationship between Generalization and Robustness to Adversarial Examples",2021,"","","","",77,"2022-07-13 09:26:01","","10.3390/sym13050817","","",,,,,3,3.00,1,3,1,"One of the most intriguing phenomenons related to deep learning is the so-called adversarial examples. These samples are visually equivalent to normal inputs, undetectable for humans, yet they cause the networks to output wrong results. The phenomenon can be framed as a symmetry/asymmetry problem, whereby inputs to a neural network with a similar/symmetric appearance to regular images, produce an opposite/asymmetric output. Some researchers are focused on developing methods for generating adversarial examples, while others propose defense methods. In parallel, there is a growing interest in characterizing the phenomenon, which is also the focus of this paper. From some well known datasets of common images, like CIFAR-10 and STL-10, a neural network architecture is first trained in a normal regime, where training and validation performances increase, reaching generalization. Additionally, the same architectures and datasets are trained in an overfitting regime, where there is a growing disparity in training and validation performances. The behaviour of these two regimes against adversarial examples is then compared. From the results, we observe greater robustness to adversarial examples in the overfitting regime. We explain this simultaneous loss of generalization and gain in robustness to adversarial examples as another manifestation of the well-known fitting-generalization trade-off.","",""
11,"Zhengshi Yang, X. Zhuang, K. Sreenivasan, V. Mishra, D. Cordes","Robust Motion Regression of Resting-State Data Using a Convolutional Neural Network Model",2019,"","","","",78,"2022-07-13 09:26:01","","10.3389/fnins.2019.00169","","",,,,,11,3.67,2,5,3,"Resting-state functional magnetic resonance imaging (rs-fMRI) based on the blood-oxygen-level-dependent (BOLD) signal has been widely used in healthy individuals and patients to investigate brain functions when the subjects are in a resting or task-negative state. Head motion considerably confounds the interpretation of rs-fMRI data. Nuisance regression is commonly used to reduce motion-related artifacts with six motion parameters estimated from rigid-body realignment as regressors. To further compensate for the effect of head movement, the first-order temporal derivatives of motion parameters and squared motion parameters were proposed previously as possible motion regressors. However, these additional regressors may not be sufficient to model the impact of head motion because of the complexity of motion artifacts. In addition, while using more motion-related regressors could explain more variance in the data, the neural signal may also be removed with increasing number of motion regressors. To better model how in-scanner motion affects rs-fMRI data, a robust and automated convolutional neural network (CNN) model is developed in this study to obtain optimal motion regressors. The CNN network consists of two temporal convolutional layers and the output from the network are the derived motion regressors used in the following nuisance regression. The temporal convolutional layer in the network can non-parametrically model the prolonged effect of head motion. The set of regressors derived from the neural network is compared with the same number of regressors used in a traditional nuisance regression approach. It is demonstrated that the CNN-derived regressors can more effectively reduce motion-related artifacts.","",""
125,"Jie Hou, B. Adhikari, Jianlin Cheng","DeepSF: deep convolutional neural network for mapping protein sequences to folds",2017,"","","","",79,"2022-07-13 09:26:01","","10.1093/bioinformatics/btx780","","",,,,,125,25.00,42,3,5,"Motivation Protein fold recognition is an important problem in structural bioinformatics. Almost all traditional fold recognition methods use sequence (homology) comparison to indirectly predict the fold of a target protein based on the fold of a template protein with known structure, which cannot explain the relationship between sequence and fold. Only a few methods had been developed to classify protein sequences into a small number of folds due to methodological limitations, which are not generally useful in practice. Results We develop a deep 1D‐convolution neural network (DeepSF) to directly classify any protein sequence into one of 1195 known folds, which is useful for both fold recognition and the study of sequence‐structure relationship. Different from traditional sequence alignment (comparison) based methods, our method automatically extracts fold‐related features from a protein sequence of any length and maps it to the fold space. We train and test our method on the datasets curated from SCOP1.75, yielding an average classification accuracy of 75.3%. On the independent testing dataset curated from SCOP2.06, the classification accuracy is 73.0%. We compare our method with a top profile‐profile alignment method—HHSearch on hard template‐based and template‐free modeling targets of CASP9‐12 in terms of fold recognition accuracy. The accuracy of our method is 12.63‐26.32% higher than HHSearch on template‐free modeling targets and 3.39‐17.09% higher on hard template‐based modeling targets for top 1, 5 and 10 predicted folds. The hidden features extracted from sequence by our method is robust against sequence mutation, insertion, deletion and truncation, and can be used for other protein pattern recognition problems such as protein clustering, comparison and ranking. Availability and implementation The DeepSF server is publicly available at: http://iris.rnet.missouri.edu/DeepSF/. Supplementary information Supplementary data are available at Bioinformatics online.","",""
4,"Adam Ivankay, Ivan Girardi, Chiara Marchiori, P. Frossard","FAR: A General Framework for Attributional Robustness",2020,"","","","",80,"2022-07-13 09:26:01","","","","",,,,,4,2.00,1,4,2,"Attribution maps have gained popularity as tools for explaining neural networks predictions. By assigning an importance value to each input dimension that represents their influence towards the outcome, they give an intuitive explanation of the decision process. However, recent work has discovered vulnerability of these maps to imperceptible, carefully crafted changes in the input that lead to significantly different attributions, rendering them meaningless. By borrowing notions of traditional adversarial training - a method to achieve robust predictions - we propose a novel framework for attributional robustness (FAR) to mitigate this vulnerability. Central assumption is that similar inputs should yield similar attribution maps, while keeping the prediction of the network constant. Specifically, we define a new generic regularization term and training objective that minimizes the maximal dissimilarity of attribution maps in a local neighbourhood of the input. We then show how current state-of-the-art methods can be recovered through principled instantiations of these objectives. Moreover, we propose two new training methods, AAT and AdvAAT, derived from the framework, that directly optimize for robust attributions and predictions. We showcase the effectivity of our training methods by comparing them to current state-of-the-art attributional robustness approaches on widely used vision datasets. Experiments show that they perform better or comparably to current methods in terms of attributional robustness, while being applicable to any attribution method and input data domain. We finally show that our methods mitigate undesired dependencies of attributional robustness and some training and estimation parameters, which seem to critically affect other methods.","",""
3,"A. Sarkar, Anirban Sarkar, V. Balasubramanian","Enhanced Regularizers for Attributional Robustness",2020,"","","","",81,"2022-07-13 09:26:01","","","","",,,,,3,1.50,1,3,2,"Deep neural networks are the default choice of learning models for computer vision tasks. Extensive work has been carried out in recent years on explaining deep models for vision tasks such as classification. However, recent work has shown that it is possible for these models to produce substantially different attribution maps even when two very similar images are given to the network, raising serious questions about trustworthiness. To address this issue, we propose a robust attribution training strategy to improve attributional robustness of deep neural networks. Our method carefully analyzes the requirements for attributional robustness and introduces two new regularizers that preserve a model’s attribution map during attacks. Our method surpasses state-of-the-art attributional robustness methods by a margin of approximately 3% to 9% in terms of attribution robustness measures on several datasets including MNIST, FMNIST, Flower and GTSRB.","",""
104,"Liu Yang, Hanxin Chen","Fault diagnosis of gearbox based on RBF-PF and particle swarm optimization wavelet neural network",2019,"","","","",82,"2022-07-13 09:26:01","","10.1007/s00521-018-3525-y","","",,,,,104,34.67,52,2,3,"","",""
41,"Mengchen Liu, Shixia Liu, Hang Su, Kelei Cao, Jun Zhu","Analyzing the Noise Robustness of Deep Neural Networks",2018,"","","","",83,"2022-07-13 09:26:01","","10.1109/VAST.2018.8802509","","",,,,,41,10.25,8,5,4,"Deep neural networks (DNNs) are vulnerable to maliciously generated adversarial examples. These examples are intentionally designed by making imperceptible perturbations and often mislead a DNN into making an incorrect prediction. This phenomenon means that there is significant risk in applying DNNs to safety-critical applications, such as driverless cars. To address this issue, we present a visual analytics approach to explain the primary cause of the wrong predictions introduced by adversarial examples. The key is to analyze the datapaths of the adversarial examples and compare them with those of the normal examples. A datapath is a group of critical neurons and their connections. To this end, we formulate the datapath extraction as a subset selection problem and approximately solve it based on back-propagation. A multi-level visualization consisting of a segmented DAG (layer level), an Euler diagram (feature map level), and a heat map (neuron level), has been designed to help experts investigate datapaths from the high-level layers to the detailed neuron activations. Two case studies are conducted that demonstrate the promise of our approach in support of explaining the working mechanism of adversarial examples.","",""
3,"Jennifer Hammelman, Daniel Lobo, M. Levin","Artificial Neural Networks as Models of Robustness in Development and Regeneration: Stability of Memory During Morphological Remodeling",2016,"","","","",84,"2022-07-13 09:26:01","","10.1007/978-3-319-28495-8_3","","",,,,,3,0.50,1,3,6,"","",""
6,"Kaveri A. Thakoor, Sharath C. Koorathota, D. Hood, P. Sajda","Robust and Interpretable Convolutional Neural Networks to Detect Glaucoma in Optical Coherence Tomography Images",2020,"","","","",85,"2022-07-13 09:26:01","","10.1109/tbme.2020.3043215","","",,,,,6,3.00,2,4,2,"Recent studies suggest that deep learning systems can now achieve performance on par with medical experts in diagnosis of disease. A prime example is in the field of ophthalmology, where convolutional neural networks (CNNs) have been used to detect retinal and ocular diseases. However, this type of artificial intelligence (AI) has yet to be adopted clinically due to questions regarding robustness of the algorithms to datasets collected at new clinical sites and a lack of explainability of AI-based predictions, especially relative to those of human expert counterparts. In this work, we develop CNN architectures that demonstrate robust detection of glaucoma in optical coherence tomography (OCT) images and test with concept activation vectors (TCAVs) to infer what image concepts CNNs use to generate predictions. Furthermore, we compare TCAV results to eye fixations of clinicians, to identify common decision-making features used by both AI and human experts. We find that employing fine-tuned transfer learning and CNN ensemble learning create end-to-end deep learning models with superior robustness compared to previously reported hybrid deep-learning/machine-learning models, and TCAV/eye-fixation comparison suggests the importance of three OCT report sub-images that are consistent with areas of interest fixated upon by OCT experts to detect glaucoma. The pipeline described here for evaluating CNN robustness and validating interpretable image concepts used by CNNs with eye movements of experts has the potential to help standardize the acceptance of new AI tools for use in the clinic.","",""
430,"Amirata Ghorbani, Abubakar Abid, James Y. Zou","Interpretation of Neural Networks is Fragile",2017,"","","","",86,"2022-07-13 09:26:01","","10.1609/aaai.v33i01.33013681","","",,,,,430,86.00,143,3,5,"In order for machine learning to be trusted in many applications, it is critical to be able to reliably explain why the machine learning algorithm makes certain predictions. For this reason, a variety of methods have been developed recently to interpret neural network predictions by providing, for example, feature importance maps. For both scientific robustness and security reasons, it is important to know to what extent can the interpretations be altered by small systematic perturbations to the input data, which might be generated by adversaries or by measurement biases. In this paper, we demonstrate how to generate adversarial perturbations that produce perceptively indistinguishable inputs that are assigned the same predicted label, yet have very different interpretations. We systematically characterize the robustness of interpretations generated by several widely-used feature importance interpretation methods (feature importance maps, integrated gradients, and DeepLIFT) on ImageNet and CIFAR-10. In all cases, our experiments show that systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly susceptible to adversarial attack. Our analysis of the geometry of the Hessian matrix gives insight on why robustness is a general challenge to current interpretation approaches.","",""
0,"A. Agogino, Ritchie Lee, D. Giannakopoulou","Machine Learning Explainability and Transferability for Path Navigation",2020,"","","","",87,"2022-07-13 09:26:01","","10.2514/6.2021-1885","","",,,,,0,0.00,0,3,2,"Deep neural networks are powerful tools for machine perception. Unfortunately their decisions are difficult to explain due to the complexity and size of the networks. Previously we have alleviated this issue by using the representational portion of a deep neural network and combining it with a :-nearest neighbor (KNN) classifier. Through inspection of the decisions made by the KNN, we can directly see the training data responsible for the decisions, allowing us to determine the quality of the overall decision and the quality of the representational layer of the deep NN. While the technique worked well, it requires tens of thousands of latent vectors to be stored for classification. In addition, it lacks the ability to show how parts of an image influence the classification decision. Here we address these issues by 1) Using a radial basis function network (RBFN) in place of the KNN allowing far fewer images to be used in deployment and 2) Using an autoencoder network for explainability. In addition to these techniques, we examine the effects of transfer learning to determine that results are robust. All results are tested on a domain where an unmanned aerial vehicle (UAV) navigates a forest trail through a single camera.","",""
692,"W. Samek, Alexander Binder, G. Montavon, S. Lapuschkin, K. Müller","Evaluating the Visualization of What a Deep Neural Network Has Learned",2015,"","","","",88,"2022-07-13 09:26:01","","10.1109/TNNLS.2016.2599820","","",,,,,692,98.86,138,5,7,"Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the “importance” of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.","",""
5,"Zakaria Senousy, M. Abdelsamea, Mona Mostafa Mohamed, M. Gaber","3E-Net: Entropy-Based Elastic Ensemble of Deep Convolutional Neural Networks for Grading of Invasive Breast Carcinoma Histopathological Microscopic Images",2021,"","","","",89,"2022-07-13 09:26:01","","10.3390/e23050620","","",,,,,5,5.00,1,4,1,"Automated grading systems using deep convolution neural networks (DCNNs) have proven their capability and potential to distinguish between different breast cancer grades using digitized histopathological images. In digital breast pathology, it is vital to measure how confident a DCNN is in grading using a machine-confidence metric, especially with the presence of major computer vision challenging problems such as the high visual variability of the images. Such a quantitative metric can be employed not only to improve the robustness of automated systems, but also to assist medical professionals in identifying complex cases. In this paper, we propose Entropy-based Elastic Ensemble of DCNN models (3E-Net) for grading invasive breast carcinoma microscopy images which provides an initial stage of explainability (using an uncertainty-aware mechanism adopting entropy). Our proposed model has been designed in a way to (1) exclude images that are less sensitive and highly uncertain to our ensemble model and (2) dynamically grade the non-excluded images using the certain models in the ensemble architecture. We evaluated two variations of 3E-Net on an invasive breast carcinoma dataset and we achieved grading accuracy of 96.15% and 99.50%.","",""
0,"Renjue Li, Pengfei Yang, Cheng-Chao Huang, Bai Xue, Lijun Zhang","Probabilistic Robustness Analysis for DNNs based on PAC Learning",2021,"","","","",90,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,5,1,"This paper proposes a black box based approach for analysing deep neural networks (DNNs). We view a DNN as a function f from inputs to outputs, and consider the local robustness property for a given input. Based on scenario optimization technique in robust control design, we learn the score difference function fi − fl with respect to the target label l and attacking label i. We use a linear template over the input pixels, and learn the corresponding coefficients of the score difference function, based on a reduction to a linear programming (LP) problems. To make it scalable, we propose optimizations including components based learning and focused learning. The learned function offers a probably approximately correct (PAC) guarantee for the robustness property. Since the score difference function is an approximation of the local behaviour of the DNN, it can be used to generate potential adversarial examples, and the original network can be used to check whether they are spurious or not. Finally, we focus on the input pixels with large absolute coefficients, and use them to explain the attacking scenario. We have implemented our approach in a prototypical tool DeepPAC. Our experimental results show that our framework can handle very large neural networks like ResNet152 with 6.5M neurons, and often generates adversarial examples which are very close to the decision boundary.","",""
13,"Richard Y. Zhang","On the Tightness of Semidefinite Relaxations for Certifying Robustness to Adversarial Examples",2020,"","","","",91,"2022-07-13 09:26:01","","","","",,,,,13,6.50,13,1,2,"The robustness of a neural network to adversarial examples can be provably certified by solving a convex relaxation. If the relaxation is loose, however, then the resulting certificate can be too conservative to be practically useful. Recently, a less conservative robustness certificate was proposed, based on a semidefinite programming (SDP) relaxation of the ReLU activation function. In this paper, we describe a geometric technique that determines whether this SDP certificate is exact, meaning whether it provides both a lower-bound on the size of the smallest adversarial perturbation, as well as a globally optimal perturbation that attains the lower-bound. Concretely, we show, for a least-squares restriction of the usual adversarial attack problem, that the SDP relaxation amounts to the nonconvex projection of a point onto a hyperbola. The resulting SDP certificate is exact if and only if the projection of the point lies on the major axis of the hyperbola. Using this geometric technique, we prove that the certificate is exact over a single hidden layer under mild assumptions, and explain why it is usually conservative for several hidden layers. We experimentally confirm our theoretical insights using a general-purpose interior-point method and a custom rank-2 Burer-Monteiro algorithm.","",""
3,"Malhar Jere, Maghav Kumar, F. Koushanfar","A Singular Value Perspective on Model Robustness",2020,"","","","",92,"2022-07-13 09:26:01","","","","",,,,,3,1.50,1,3,2,"Convolutional Neural Networks (CNNs) have made significant progress on several computer vision benchmarks, but are fraught with numerous non-human biases such as vulnerability to adversarial samples. Their lack of explainability makes identification and rectification of these biases difficult, and understanding their generalization behavior remains an open problem. In this work we explore the relationship between the generalization behavior of CNNs and the Singular Value Decomposition (SVD) of images. We show that naturally trained and adversarially robust CNNs exploit highly different features for the same dataset. We demonstrate that these features can be disentangled by SVD for ImageNet and CIFAR-10 trained networks. Finally, we propose Rank Integrated Gradients (RIG), the first rankbased feature attribution method to understand the dependence of CNNs on image rank.","",""
6,"Joel Dapello, J. Feather, Hang Le, Tiago Marques, D. Cox, Josh H. McDermott, J. DiCarlo, SueYeon Chung","Neural Population Geometry Reveals the Role of Stochasticity in Robust Perception",2021,"","","","",93,"2022-07-13 09:26:01","","","","",,,,,6,6.00,1,8,1,"Adversarial examples are often cited by neuroscientists and machine learning researchers as an example of how computational models diverge from biological sensory systems. Recent work has proposed adding biologically-inspired components to visual neural networks as a way to improve their adversarial robustness. One surprisingly effective component for reducing adversarial vulnerability is response stochasticity, like that exhibited by biological neurons. Here, using recently developed geometrical techniques from computational neuroscience, we investigate how adversarial perturbations influence the internal representations of standard, adversarially trained, and biologically-inspired stochastic networks. We find distinct geometric signatures for each type of network, revealing different mechanisms for achieving robust representations. Next, we generalize these results to the auditory domain, showing that neural stochasticity also makes auditory models more robust to adversarial perturbations. Geometric analysis of the stochastic networks reveals overlap between representations of clean and adversarially perturbed stimuli, and quantitatively demonstrates that competing geometric effects of stochasticity mediate a tradeoff between adversarial and clean performance. Our results shed light on the strategies of robust perception utilized by adversarially trained and stochastic networks, and help explain how stochasticity may be beneficial to machine and biological computation.1","",""
2,"Chirag Agarwal, Bo Dong, D. Schonfeld, A. Hoogs","An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks",2018,"","","","",94,"2022-07-13 09:26:01","","","","",,,,,2,0.50,1,4,4,"Deep Neural Networks(DNN) have excessively advanced the field of computer vision by achieving state of the art performance in various vision tasks. These results are not limited to the field of vision but can also be seen in speech recognition and machine translation tasks. Recently, DNNs are found to poorly fail when tested with samples that are crafted by making imperceptible changes to the original input images. This causes a gap between the validation and adversarial performance of a DNN. An effective and generalizable robustness metric for evaluating the performance of DNN on these adversarial inputs is still missing from the literature. In this paper, we propose Noise Sensitivity Score (NSS), a metric that quantifies the performance of a DNN on a specific input under different forms of fix-directional attacks. An insightful mathematical explanation is provided for deeply understanding the proposed metric. By leveraging the NSS, we also proposed a skewness based dataset robustness metric for evaluating a DNN's adversarial performance on a given dataset. Extensive experiments using widely used state of the art architectures along with popular classification datasets, such as MNIST, CIFAR-10, CIFAR-100, and ImageNet, are used to validate the effectiveness and generalization of our proposed metrics. Instead of simply measuring a DNN's adversarial robustness in the input domain, as previous works, the proposed NSS is built on top of insightful mathematical understanding of the adversarial attack and gives a more explicit explanation of the robustness.","",""
2,"Alexandre Dey, Marc Velay, Jean-Philippe Fauvelle, Sylvain Navers","Adversarial vs behavioural-based defensive AI with joint, continual and active learning: automated evaluation of robustness to deception, poisoning and concept drift",2020,"","","","",95,"2022-07-13 09:26:01","","","","",,,,,2,1.00,1,4,2,"Recent advancements in Artificial Intelligence (AI) have brought new capabilities to behavioural analysis (UEBA) for cyber-security consisting in the detection of hostile action based on the unusual nature of events observed on the Information this http URL our previous work (presented at C\&ESAR 2018 and FIC 2019), we have associated deep neural networks auto-encoders for anomaly detection and graph-based events correlation to address major limitations in UEBA systems. This resulted in reduced false positive and false negative rates, improved alert explainability, while maintaining real-time performances and scalability. However, we did not address the natural evolution of behaviours through time, also known as concept drift. To maintain effective detection capabilities, an anomaly-based detection system must be continually trained, which opens a door to an adversary that can conduct the so-called ""frog-boiling"" attack by progressively distilling unnoticed attack traces inside the behavioural models until the complete attack is considered normal. In this paper, we present a solution to effectively mitigate this attack by improving the detection process and efficiently leveraging human expertise. We also present preliminary work on adversarial AI conducting deception attack, which, in term, will be used to help assess and improve the defense system. These defensive and offensive AI implement joint, continual and active learning, in a step that is necessary in assessing, validating and certifying AI-based defensive solutions.","",""
0,"Marek Sarvaš","Interpretation of Deep Neural Networks in Speech Classification",2021,"","","","",96,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,1,1,"The growing problem of the popularity of using deep neural networks is their black box representation. The lack of transparency is raising questions about their reliability, credibility, or vulnerability to adversarial attacks. This caused rising demand for neural network explainability. The goal of this paper is to replicate existing experiments on a gender classification model and extend these experiments to analyze and uncover vulnerabilities of a network trained for gender classification on audio signal spectrograms. The easiest way to explain something is through visualization. For this, a layer-wise relevance propagation technique was chosen in this work because it produces easy-to-understand heatmaps of features relevant to a neural network. The heatmaps are produced by back-propagating relevances through a network from the output to the input layer. Two neural network models with AlexNet and ResNet architecture were used. Experiments with AlexNet model show that the network’s predictions are highly dependent on a small number of time-frequency (TF) bins. By augmenting the training data using obtained relevance maps, I managed to lower the dependency on these bins. As a result, the prediction accuracy, when these bins were not present, was increased by 15%. The proposed approach can potentially lead to increased robustness of models, preventing or reducing the impact of adversarial attacks. Interpretation of ResNet model showed dependencies on lower frequencies and time. Producing interpretable heatmaps of the ResNet model required the implementation of more robust LRP rules.","",""
0,"Guoxuan Xia, Sangwon Ha, Tiago Azevedo, Partha P. Maji","An Underexplored Dilemma between Confidence and Calibration in Quantized Neural Networks",2021,"","","","",97,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,4,1,"Modern convolutional neural networks (CNNs) are known to be overconﬁdent in terms of their calibration on unseen input data. That is to say, they are more conﬁdent than they are accurate. This is undesirable if the probabilities predicted are to be used for downstream decision making. When considering accuracy, CNNs are also surprisingly robust to compression techniques, such as quantization, which aim to reduce computational and memory costs. We show that this robustness can be partially explained by the calibration behavior of modern CNNs, and may be improved with overconﬁdence. This is due to an intuitive result: low conﬁdence predictions are more likely to change post-quantization, whilst being less accurate. High conﬁdence predictions will be more accurate, but more difﬁcult to change. Thus, a minimal drop in post-quantization accuracy is incurred. This presents a potential conﬂict in neural network design: worse calibration from overconﬁdence may lead to better robustness to quantization. We perform experiments applying post-training quantization to a variety of CNNs, on the CIFAR-100 and ImageNet datasets, and make our code publicly available. 1","",""
0,"Rémi Bernhard, Pierre-Alain Moëllic, J. Dutertre","Luring Transferable Adversarial Perturbations for Deep Neural Networks",2021,"","","","",98,"2022-07-13 09:26:01","","10.1109/IJCNN52387.2021.9534397","","",,,,,0,0.00,0,3,1,"The growing interest for adversarial examples, i.e. maliciously modified examples which fool a classifier, has resulted in many defenses intended to detect them, render them inoffensive or make the model more robust against them. In this paper, we pave the way towards a new approach to improve the robustness of a model against black-box transfer attacks. A removable additional neural network is included in the target model, and is designed to induce the luring effect, which tricks the adversary into choosing false directions to fool the target model. Training the additional model is achieved thanks to a loss function acting on the logits sequence order. Our deception-based method only needs to have access to the predictions of the target model and does not require a labeled data set. We explain the luring effect thanks to the notion of robust and non-robust useful features and perform experiments on MNIST, SVHN and CIFAR10 to characterize and evaluate this phenomenon. Additionally, we scale the luring effect to ImageNet, experiment practical use of it and discuss its complementarity with other defense schemes.","",""
2,"A. A. Abello, R. Hirata, Zhangyang Wang","Dissecting the High-Frequency Bias in Convolutional Neural Networks",2021,"","","","",99,"2022-07-13 09:26:01","","10.1109/CVPRW53098.2021.00096","","",,,,,2,2.00,1,3,1,"For convolutional neural networks (CNNs), a common hypothesis that explains both their generalization capability and their characteristic brittleness is that these models are implicitly regularized to rely on imperceptible high-frequency patterns, more than humans would do. This hypothesis has seen some empirical validation, but most works do not rigorously divide the image frequency spectrum. We present a model to divide the spectrum in disjointed discs based on the distribution of energy and apply simple feature importance procedures to test whether high-frequencies are more important than lower ones. We find evidence that mid or high-level frequencies are disproportionately important for CNNs. The evidence is robust across different datasets and networks. Moreover, we find the diverse effects of the network’s attributes, such as architecture and depth, on frequency bias and robustness in general. Code for reproducing our experiments is available at: https://github.com/Abello966/FrequencyBiasExperiments","",""
0,"Dakarai Crowder, Girik Malik","Robustness of Humans and Machines on Object Recognition with Extreme Image Transformations",2022,"","","","",100,"2022-07-13 09:26:01","","10.48550/arXiv.2205.05167","","",,,,,0,0.00,0,2,1,"Recent neural network architectures have claimed to explain data from the human visual cortex. Their demonstrated performance is however still limited by the depen-dence on exploiting low-level features for solving visual tasks. This strategy limits their performance in case of out-of-distribution/adversarial data. Humans, meanwhile learn abstract concepts and are mostly unaffected by even extreme image distortions. Humans and networks employ strikingly different strategies to solve visual tasks. To probe this, we introduce a novel set of image transforms and evaluate humans and networks on an object recognition task. We found performance for a few common networks quickly decreases while humans are able to recognize objects with a high accuracy.","",""
1,"Haoyu Chu, Shikui Wei, Yao Zhao","Towards Natural Robustness Against Adversarial Examples",2020,"","","","",101,"2022-07-13 09:26:01","","","","",,,,,1,0.50,0,3,2,"Recent studies have shown that deep neural networks are vulnerable to adversarial examples, but most of the methods proposed to defense adversarial examples cannot solve this problem fundamentally. In this paper, we theoretically prove that there is an upper bound for neural networks with identity mappings to constrain the error caused by adversarial noises. However, in actual computations, this kind of neural network no longer holds any upper bound and is therefore susceptible to adversarial examples. Following similar procedures, we explain why adversarial examples can fool other deep neural networks with skip connections. Furthermore, we demonstrate that a new family of deep neural networks called Neural ODEs (Chen et al., 2018) holds a weaker upper bound. This weaker upper bound prevents the amount of change in the result from being too large. Thus, Neural ODEs have natural robustness against adversarial examples. We evaluate the performance of Neural ODEs compared with ResNet under three white-box adversarial attacks (FGSM, PGD, DI2-FGSM) and one black-box adversarial attack (Boundary Attack). Finally, we show that the natural robustness of Neural ODEs is even better than the robustness of neural networks that are trained with adversarial training methods, such as TRADES and YOPO.","",""
1,"Cong Xu, Dan Li, Min Yang","Improve Adversarial Robustness via Weight Penalization on Classification Layer",2020,"","","","",102,"2022-07-13 09:26:01","","","","",,,,,1,0.50,0,3,2,"It is well-known that deep neural networks are vulnerable to adversarial attacks. Recent studies show that well-designed classification parts can lead to better robustness. However, there is still much space for improvement along this line. In this paper, we first prove that, from a geometric point of view, the robustness of a neural network is equivalent to some angular margin condition of the classifier weights. We then explain why ReLU type function is not a good choice for activation under this framework. These findings reveal the limitations of the existing approaches and lead us to develop a novel light-weight-penalized defensive method, which is simple and has a good scalability. Empirical results on multiple benchmark datasets demonstrate that our method can effectively improve the robustness of the network without requiring too much additional computation, while maintaining a high classification precision for clean data.","",""
22,"Adam Kortylewski, Qing Liu, Angtian Wang, Yihong Sun, A. Yuille","Compositional Convolutional Neural Networks: A Robust and Interpretable Model for Object Recognition under Occlusion",2020,"","","","",103,"2022-07-13 09:26:01","","10.1007/s11263-020-01401-3","","",,,,,22,11.00,4,5,2,"","",""
181,"Yuwei Cui, Chetan Surpur, Subutai Ahmad, J. Hawkins","Continuous Online Sequence Learning with an Unsupervised Neural Network Model",2015,"","","","",104,"2022-07-13 09:26:01","","10.1162/NECO_a_00893","","",,,,,181,25.86,45,4,7,"Abstract The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods—autoregressive integrated moving average; feedforward neural networks—time delay neural network and online sequential extreme learning machine; and recurrent neural networks—long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams.","",""
27,"P. Babakhani, J. Bridge, R. Doong, T. Phenrat","Parameterization and prediction of nanoparticle transport in porous media: A reanalysis using artificial neural network",2017,"","","","",105,"2022-07-13 09:26:01","","10.1002/2016WR020358","","",,,,,27,5.40,7,4,5,"The continuing rapid expansion of industrial and consumer processes based on nanoparticles (NP) necessitates a robust model for delineating their fate and transport in groundwater. An ability to reliably specify the full parameter set for prediction of NP transport using continuum models is crucial. In this paper we report the reanalysis of a data set of 493 published column experiment outcomes together with their continuum modeling results. Experimental properties were parameterized into 20 factors which are commonly available. They were then used to predict five key continuum model parameters as well as the effluent concentration via artificial neural network (ANN)‐based correlations. The Partial Derivatives (PaD) technique and Monte Carlo method were used for the analysis of sensitivities and model‐produced uncertainties, respectively. The outcomes shed light on several controversial relationships between the parameters, e.g., it was revealed that the trend of Katt with average pore water velocity was positive. The resulting correlations, despite being developed based on a “black‐box” technique (ANN), were able to explain the effects of theoretical parameters such as critical deposition concentration (CDC), even though these parameters were not explicitly considered in the model. Porous media heterogeneity was considered as a parameter for the first time and showed sensitivities higher than those of dispersivity. The model performance was validated well against subsets of the experimental data and was compared with current models. The robustness of the correlation matrices was not completely satisfactory, since they failed to predict the experimental breakthrough curves (BTCs) at extreme values of ionic strengths.","",""
1,"He Zhang, Bang Wu, Xingliang Yuan, Shirui Pan, Hanghang Tong, Jian Pei","Trustworthy Graph Neural Networks: Aspects, Methods and Trends",2022,"","","","",106,"2022-07-13 09:26:01","","10.48550/arXiv.2205.07424","","",,,,,1,1.00,0,6,1,"—Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications like recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects like vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterised by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehen- sively summarise existing efforts for trustworthy GNNs from six aspects, including robustness, explainability, privacy, fairness, accountability, and environmental well-being. Additionally, we highlight the intricate cross-aspect relations between the above six aspects of trustworthy GNNs. Finally, we present a thorough overview of trending directions for facilitating the research and industrialisation of trustworthy GNNs.","",""
2131,"Nicolas Papernot, P. Mcdaniel, Xi Wu, S. Jha, A. Swami","Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks",2015,"","","","",107,"2022-07-13 09:26:01","","10.1109/SP.2016.41","","",,,,,2131,304.43,426,5,7,"Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.","",""
18,"E. Egrioglu, U. Yolcu, E. Bas, Ali Z. Dalar","Median-Pi artificial neural network for forecasting",2019,"","","","",108,"2022-07-13 09:26:01","","10.1007/s00521-017-3002-z","","",,,,,18,6.00,5,4,3,"","",""
0,"Xinyue Wang, Jianhao Liang, Haitao Sun","The Network of Tumor Microtubes: An Improperly Reactivated Neural Cell Network With Stemness Feature for Resistance and Recurrence in Gliomas",2022,"","","","",109,"2022-07-13 09:26:01","","10.3389/fonc.2022.921975","","",,,,,0,0.00,0,3,1,"Gliomas are known as an incurable brain tumor for the poor prognosis and robust recurrence. In recent years, a cellular subpopulation with tumor microtubes (TMs) was identified in brain tumors, which may provide a new angle to explain the invasion, resistance, recurrence, and heterogeneity of gliomas. Recently, it was demonstrated that the cell subpopulation also expresses neural stem cell markers and shares a lot of features with both immature neurons and cancer stem cells and may be seen as an improperly reactivated neural cell network with a stemness feature at later time points of life. TMs may also provide a new angle to understand the resistance and recurrence mechanisms of glioma stem cells. In this review, we innovatively focus on the common features between TMs and sprouting axons in morphology, formation, and function. Additionally, we summarized the recent progress in the resistance and recurrence mechanisms of gliomas with TMs and explained the incurability and heterogeneity in gliomas with TMs. Moreover, we discussed the recently discovered overlap between cancer stem cells and TM-positive glioma cells, which may contribute to the understanding of resistant glioma cell subpopulation and the exploration of the new potential therapeutic target for gliomas.","",""
7,"Laura Rieger, L. K. Hansen","Aggregating explanation methods for stable and robust explainability.",2019,"","","","",110,"2022-07-13 09:26:01","","","","",,,,,7,2.33,4,2,3,"Despite a growing literature on explaining neural networks, no consensus has been reached on how to explain a neural network decision or how to evaluate an explanation. Our contributions in this paper are twofold. First, we investigate schemes to combine explanation methods and reduce model uncertainty to obtain a single aggregated explanation. We provide evidence that the aggregation is better at identifying important features, than on individual methods. Adversarial attacks on explanations is a recent active research topic. As our second contribution, we present evidence that aggregate explanations are much more robust to attacks than individual explanation methods.","",""
0,"Junhee Lee, Hyeonseong Cho, Yun Jang Pyun, Suk‐Ju Kang, H. Nam","Heatmap Assisted Accuracy Score Evaluation Method for Machine-Centric Explainable Deep Neural Networks",2022,"","","","",111,"2022-07-13 09:26:01","","10.1109/access.2022.3184453","","",,,,,0,0.00,0,5,1,"There have existed many studies about the explainable artificial intelligence (XAI) that explains the logic behind the complex deep neural network called a black box. At the same time, researchers have tried to evaluate the explainability performance of various XAIs. However, most previous evaluation methods are human-centric, that is, subjective, where they rely on how much the results of explanation are similar to what people’s decision is based on rather than what features actually affect the decision in the model. Their XAI selections are also dependent of datasets. Furthermore, they are focusing only on the output variation of a target class. On the other hand, this paper proposes a robust heatmap assisted accuracy score (HAAS) scheme over datasets that helps selecting machine-centric explanation algorithms to show what actually leads to the decision of a given classification network. The proposed method modifies the input image with the heatmap scores obtained by a given explanation algorithm and then puts the resultant heatmap assisted (HA) images into the network to estimate the accuracy change. The resultant metric (<inline-formula> <tex-math notation=""LaTeX"">$HAAS$ </tex-math></inline-formula>) is computed as a ratio of accuracies of the given network over HA and original images. The proposed evaluation scheme is verified in the image classification models of LeNet-5 for MNIST and VGG-16 for CIFAR-10, STL-10, and ILSVRC2012 over totally 11 XAI algorithms of saliency map, deconvolution, and 9 layer-wise relevance propagation (LRP) configurations. Consequently, for LRP1 and LRP3, MINST showed largest <inline-formula> <tex-math notation=""LaTeX"">$HAAS$ </tex-math></inline-formula> values of 1.0088 and 1.0079, CIFAR-10 achieved 1.1160 and 1.1254, STL-10 had 1.0906 and 1.0918, and ILSVRC2012 got 1.3207 and 1.3469. While LRP1 consists of <inline-formula> <tex-math notation=""LaTeX"">$\epsilon $ </tex-math></inline-formula>-rules for input, convolutional, and fully-connected layers, LRP3 adopts a bounded-rule for an input layer and the same <inline-formula> <tex-math notation=""LaTeX"">$\epsilon $ </tex-math></inline-formula>-rules for other layers as LRP1. The consistency of evaluation results of HAAS and AOPC has been compared by means of Kullback-Leibler divergence, ensuring that HAAS is the more robust evaluation method than AOPC independently of datasets since HAAS has much lower average divergence of 0.0251 than AOPC of 0.3048. In addition, the validity of the proposed HAAS scheme is further investigated through the inverted HA test that employs inverted HA images made up with inverted heatmap scores and estimates the accuracy degradation caused by applying them to the network. The XAI algorithms with largest <inline-formula> <tex-math notation=""LaTeX"">$HAAS$ </tex-math></inline-formula> results experience biggest accuracy degradation in the inverted HA test.","",""
0,"K.T.Yasas Mahima, Mohamed Ayoob, Guhanathan Poravi","An Assessment of Robustness for Adversarial Attacks and Physical Distortions on Image Classification using Explainable AI",2021,"","","","",112,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,3,1,"Introducing defence mechanisms to overcome the vulnerability of adversarial attacks is a highly focused research area. However recent research highlights that introducing defence approaches for man-made adversarial attacks is not sufficient, because the deep learning models are vulnerable to the perturbations outside the scope of the training set and the physical world itself acts as an adversarial sample generator. Given this caveat, there is a necessity to introduce general defence approaches for both man-made and physical world adversarial samples. Prior to that, a brief explanation of how the model’s decision-making process happens in the inference phase under the various adversarial perturbations is required. However, the deep learning models act as black boxes in the inference phase where the decision-making is not interpretable. As a result, research on model interpretability and explainability has been carried out in the domain which is collectively known as Explainable AI. Using a set of Explainable AI techniques, this study is investigating the deep learning networks’ robustness; i.e., the decision-making process in neural networks and important pixel attributes for the predictions that are captured when the deep learning model inference gets adversarial inputs. These adversarial inputs are perturbed by adversarial attacks or the physical world adversaries using the deep learning network trained on the CIFAR10 dataset. The study reveals, that when the inference gets adversarial samples, the necessary pixel attributes for the prediction captured by the network vary everywhere in the image. However, when the network is re-trained using adversarial training or data transformation-based augmentation, it will be able to capture pixel attributes within the particular object or reduce the capture of negative pixel attributes. Based on the deductions gained from the findings, this paper states some potential research approaches to introduce a general adversarial defence method.","",""
11,"Xu Cheng, Shengyong Chen, Chen Diao, Liu Mengna, Guoyuan Li, Houxiang Zhang","Simplifying Neural Network Based Model for Ship Motion Prediction: A Comparative Study of Sensitivity Analysis",2017,"","","","",113,"2022-07-13 09:26:01","","10.1115/OMAE2017-61474","","",,,,,11,2.20,2,6,5,"This paper presents a comparative study of sensitivity analysis (SA) and simplification on artificial neural network (ANN) based model used for ship motion prediction. Considering traditional structural complexity of ANN usually results in slow convergence, SA, as an efficient tool for correlation analysis, can help to reconstruct the ANN model for ship motion prediction. An ANN-Garson method and an ANN-EFAST method are proposed, both of which utilize the ANN for modeling but select the input parameters in a local and a global fashion, respectively. Through the benchmark tests, ANN-EFAST exhibits superior performance in both linear and nonlinear systems. Further test on ANN-EFAST via a case study of ship heading prediction shows its cost-effective and timely in compacting the ANN based prediction model. INTRODUCTION With the development and prosperity of the world’s shipping industry, the maritime transportation has become more and more busy. In order to ensure the safety of navigation, great concern has been put toward the ship motion prediction. Furthermore, some special operations, such as submarine cable laying, marine survey, etc., need more accurate ship motion prediction and control precisely. Therefore, how to establish an efficient ship motion prediction model has great theoretical and practical value in the maritime applications. However, mathematical model based ship motion prediction is challenging due to the nonlinear and ∗Corresponding author, Email: guoyuan.li@ntnu.no. Xu Cheng and Guoyuan Li have equal contribution to this paper. time-varying dynamical model of ship, as well as complex dynamic nature of sea [1, 2]. Our partner in Norway therefore started to collect on-board ship sensor data long time ago and intended to create robust predictive models for ship maneuvering technologies. There would be a possibility to combine those ship sensor data with modeling methods to design and implement ship motion prediction model. To date, a variety of novel intelligent approximation-based techniques and algorithms like fuzzy logic, Kalman filtering, Bayesian network, regression analysis and ANN have been applied to create predictive models [3–6]. Those methods have their own pros and cons at specific aspects. For example, regression analysis is not suitable for complex, high dimensional and non-linear system; Fuzzy logic relies more on mathematical model; Kalman filtering works only for Gaussian noise process; The performance of Bayesian network in high dimensional data set is poor. None of them except ANN are suitable for modeling the ship motion, as situations in which lack precise mathematical model and only input-output sample data are available. Indeed, an ANN is a “black box” and has the ability to explicitly identify possible causal relationships from the inputoutput sample data. However, there is no standard to construct a compact ANN for prediction purposes. Input parameters and hidden units are the main factors to obtain an optimized model [7]. If there are too few inputs, the network cannot represent the input-output mapping of system with sufficient accuracy. If there are too many inputs, the network dimension will increase, which in turn aggravates computational complexity. Both cases will deteriorate the generalization capability of the network. Therefore, selection of input parameters is a key issue when applying ANN 1 Copyright c © 2017 by ASME to ship motion prediction. SA investigates how the variation in the output of a numerical model can be attributed to variations of its input factors, and it plays an important role in prediction model construction and simplification, and thus the generalization ability of prediction model. The main purpose of SA is to estimate the contribution of each model input, either main or interaction contribution, on the model output and to identify the main contributors to the output. SA has been widely used in areas such as engineering, economics, and sociology [8]. Taking advantages of SA’s characteristics, it is possible to use it to select the input parameters of an ANN based model used for ship motion prediction. The rest of the paper is organized as follows. The related work section is a brief recall of some of the existing methods in ANN and SA. In the next section, we describe the input selection procedure and the case ship, then the methods we used in this paper is introduced and the calculation of local sensitivity analysis (LSA) and global sensitivity analysis (GSA) are explained. After that, the proposed algorithm is tested using two analytical models and a case study of SA on heading of ship motion prediction model is described in detail. The results are shown and the calculated first order sensitivity index are compared with analytical results. A comparison of the performance of the LSA and GSA is also presented in this section. Finally conclusions are given. RELATED WORK Artificial Neural Network Inspired by biological neural network, ANN could build up the mathematical relationship between the input parameters and the output parameters, with the advantage that it can be modeled without prior knowledge. An ANN facilitates the ability to learn complex nonlinear relationships between input and output parameters. Thanks to the powerful potential (massive parallelism, generalization capacity and fault-tolerance), ANN has been widely used in fields like pattern recognition, reliability analysis, classification, ship motion control and prediction. The basic architecture of ANN consists of single input, hidden and output layer, with each layer containing one or more neurons, in addition to bias neurons connected to the hidden and output layers. The back-propagation (BP) algorithm is the most widely used learning algorithm for ANN, which is a self-adapted learning procedure that minimizes the error between the desired and the predicted outputs. The learning process consists of two parts: feed-forward and backward pass. The output of ANN is calculated in the process of feed-forward pass, with the output error propagated backward to adjust the weights and bias of the ANN. The number of hidden layer nodes and the maximum iteration number should be carefully chosen to overcome the over-fitting and under-fitting problems. Over-fitting means that a trained ANN has weak capability of generalization. An over-fitted ANN usually has a good prediction capability over train samples, but has a bad prediction capability over test samples. Under-fitting means that a trained ANN is too simple to be capable of representing the relationship between input parameters and output targets. An under-fitted ANN usually has bad prediction capabilities over both training and testing samples. Sensitivity Analysis SA could be implemented in either local or global manner. The LSA explore the response of the model output to a small change of the parameter from its nominal value. Garson algorithm is one of the popular LSA algorithms [9]. This method has shown to be computational efficient and conceptually simple when quantifying the relative importance of input parameters. It has been used in some ship motion prediction applications, such as the work in [5, 6, 10]. Local sensitivity index is calculated at the nominal point or a fixed point, which is not representative for all inputs in the whole parameter space. In addition, the LSA do not explore the interactions between input parameters. In contrast, a GSA estimates the effect of input parameters across the whole input parameter space. GSA is generally divided into four categories: Traditional methods, Analysis of Variance methods (ANOVA) methods, Derivative-Based Methods and Surrogate-Based Methods. ANOVA methods are also called variance-based methods, which makes ANOVA decomposition of model response variances into the contributions from individual parameters and their interactions. Cukier, et al. presented Fourier Amplitude Sensitivity Test (FAST) [11]. Later, Salteli et al. introduces a global, quantitative, model independent SA method for calculating both main effect and total effect indices based on the FAST — extended FAST (EFAST) [12]. EFAST is model independent, which can be used in ANN based prediction. Currently, most of the study only focuses on studying either LSA or GSA in ANN based ship prediction model. There is not a systematic comparison between them. In this study, efforts are made to combine the ANN with the Garson algorithm and the EFAST algorithm respectively, aiming to find out which one is preferable for nonlinear ship motion prediction. SIMPLIFICATION OF ANN MODEL VIA SENSITIVITY ANALYSIS System Structure This paper aims to construct a compact ANN model for ship motion prediction using the SA approach. The main idea is to use the SA method to evaluate the importance of each input and select the inputs according to their importance. The input selection procedure consists of four components: data cleaning, surrogate model, SA and result visualization. Data cleaning is to minimize the affection of noisy, redundant information of sensor data on further analysis and modeling. In general, it is difficult to estimate the contribution of each input parameter and the interaction 2 Copyright c © 2017 by ASME","",""
0,"Zhuotong Chen, Qianxiao Li, Zheng Zhang","Self-Healing Robust Neural Networks via Closed-Loop Control",2022,"","","","",114,"2022-07-13 09:26:01","","10.48550/arXiv.2206.12963","","",,,,,0,0.00,0,3,1,"Despite the wide applications of neural networks, there have been increasing concerns about their vulnerability issue. While numerous attack and defense techniques have been developed, this work investigates the robustness issue from a new angle: can we design a self-healing neural network that can automatically detect and ﬁx the vulnerability issue by itself? A typical self-healing mechanism is the immune system of a human body. This biology-inspired idea has been used in many engineering designs, but is rarely investigated in deep learning. This paper considers the post-training self-healing of a neural network, and proposes a closed-loop control formulation to automatically detect and ﬁx the errors caused by various attacks or perturbations. We provide a margin-based analysis to explain how this formulation can improve the robustness of a classiﬁer. To speed up the inference of the proposed self-healing network, we solve the control problem via improving the Pontryagin’s Maximum Principle-based solver. Lastly, we present an error estimation of the proposed framework for neural networks with nonlinear activation functions. We validate the performance on several network architectures against various perturbations. Since the self-healing method does not need a-priori information about data perturbations/attacks, it can handle a broad class of unforeseen perturbations. 1 .","",""
5,"Yuming Qiu, Jingyong Cai, Xiaolin Qin, Ju Zhang","Inferring Skin Lesion Segmentation With Fully Connected CRFs Based on Multiple Deep Convolutional Neural Networks",2020,"","","","",115,"2022-07-13 09:26:01","","10.1109/access.2020.3014787","","",,,,,5,2.50,1,4,2,"This article presents a method to infer skin lesion segmentation based on multiple deep convolutional neural network (DCNN) models by employing fully connected conditional random fields (CRFs). This method is on the strength of the synergism between ensemble learning which is responsible for introducing diversity from multiple DCNN models and CRFs inference which is in charge of probabilistic inference based on random fields over dermoscopy images. Contrasting to single DCNN models, the proposed method can gain better segmentation by comprehensively utilizing the advances and performance preferences of multiple different DCNN models. In comparison with simple ensemble schemes, it can effectively and precisely refine the fuzzy lesion boundary by utilizing the information in test images to maximize label agreement between similar pixels. Further, an engineering bonus is the feasibility of parallelization for the heavy operation, predicting on multiple DCNN models. In experiments, we tested the effectiveness and robustness of the proposed method on the mainstream datasets ISIC 2017 and PH2, and the results were competitive with the state-of-art methods. we also confirmed that the proposed method can capture the local information in fuzzy dermoscopy images being able to find more accurate lesion borders with a good boost on Boundary Recall (BR) metric. Moreover, since the hyper-parameters in CRFs are explainable, it is possible to adjust them manually to reach better results case by case, being attractive in practice. This work is of value on integration between the deep learning technologies and probabilistic inference in resolving lesion segmentation, and has great potential to be applied in similar tasks.","",""
3,"Thomas Fel, Mélanie Ducoffe, David Vigouroux, Remi Cadene, Mikael Capelle, C. Nicodeme, Thomas Serre","Don't Lie to Me! Robust and Efficient Explainability with Verified Perturbation Analysis",2022,"","","","",116,"2022-07-13 09:26:01","","","","",,,,,3,3.00,0,7,1,"A variety of methods have been proposed to try to explain how deep neural networks make their decisions. Key to those approaches is the need to sample the pixel space efficiently in order to derive importance maps. However, it has been shown that the sampling methods used to date introduce biases and other artifacts, leading to inaccurate estimates of the importance of individual pixels and severely limit the reliability of current explainability methods. Unfortunately, the alternative – to exhaustively sample the image space is computationally prohibitive. In this paper, we introduce EVA (Explaining using Verified perturbation Analysis) – the first explainability method guarantee to have an exhaustive exploration of a perturbation space. Specifically, we leverage the beneficial properties of verified perturbation analysis – time efficiency, tractability and guaranteed complete coverage of a manifold – to efficiently characterize the input variables that are most likely to drive the model decision. We evaluate the approach systematically and demonstrate state-of-the-art results on multiple benchmarks.","",""
28,"U. Oparaji, R. Sheu, M. Bankhead, J. Austin, E. Patelli","Robust artificial neural network for reliability and sensitivity analyses of complex non-linear systems",2017,"","","","",117,"2022-07-13 09:26:01","","10.1016/j.neunet.2017.09.003","","",,,,,28,5.60,6,5,5,"","",""
145,"Haohan Wang, Xindi Wu, Pengcheng Yin, E. Xing","High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks",2019,"","","","",118,"2022-07-13 09:26:01","","10.1109/cvpr42600.2020.00871","","",,,,,145,48.33,36,4,3,"We investigate the relationship between the frequency spectrum of image data and the generalization behavior of convolutional neural networks (CNN). We first notice CNN's ability in capturing the high-frequency components of images. These high-frequency components are almost imperceptible to a human. Thus the observation leads to multiple hypotheses that are related to the generalization behaviors of CNN, including a potential explanation for adversarial examples, a discussion of CNN's trade-off between robustness and accuracy, and some evidence in understanding training heuristics.","",""
38,"Rafael Pinot, Laurent Meunier, Alexandre Araujo, H. Kashima, F. Yger, C. Gouy-Pailler, J. Atif","Theoretical evidence for adversarial robustness through randomization",2019,"","","","",119,"2022-07-13 09:26:01","","","","",,,,,38,12.67,5,7,3,"This paper investigates the theory of robustness against adversarial attacks. It focuses on the family of randomization techniques that consist in injecting noise in the network at inference time. These techniques have proven effective in many contexts, but lack theoretical arguments. We close this gap by presenting a theoretical analysis of these approaches, hence explaining why they perform well in practice. More precisely, we make two new contributions. The first one relates the randomization rate to robustness to adversarial attacks. This result applies for the general family of exponential distributions, and thus extends and unifies the previous approaches. The second contribution consists in devising a new upper bound on the adversarial generalization gap of randomized neural networks. We support our theoretical claims with a set of experiments.","",""
1,"André Ferreira, S. Madeira, M. Gromicho, M. Carvalho, S. Vinga, Alexandra M. Carvalho","Predictive Medicine Using Interpretable Recurrent Neural Networks",2020,"","","","",120,"2022-07-13 09:26:01","","10.1007/978-3-030-68763-2_14","","",,,,,1,0.50,0,6,2,"","",""
1,"Zhen Gao, Xiaohui Wei, Han Zhang, Wenshuo Li, Guangjun Ge, Yu Wang, P. Reviriego","Reliability Evaluation of Pruned Neural Networks against Errors on Parameters",2020,"","","","",121,"2022-07-13 09:26:01","","10.1109/DFT50435.2020.9250812","","",,,,,1,0.50,0,7,2,"Convolutional Neural Networks (CNNs) are widely used in image classification tasks. To fit the application of CNNs on resource-limited embedded systems, pruning is a popular technique to reduce the complexity of the network. In this paper, the robustness of the pruned network against errors on the network parameters is examined with VGG16 as a case study. The effects of errors on the weights, bias, and batch normalization (BN) parameters are evaluated for the network with different pruning rates based on error injection experiments. The results show that in general networks with more weights pruned are more robust for a given error rate. The effect of multiple errors on bias or BN parameters is almost the same for the networks with different pruning rates that are lower than 90%. Further experiments are performed to explain the bimodal phenomenon of the network performance with errors on the parameters, to find that only errors on 6% of the parameter bits will cause large degradation of the neural network performance.","",""
252,"Shiqi Wang, Kexin Pei, J. Whitehouse, Junfeng Yang, S. Jana","Efficient Formal Safety Analysis of Neural Networks",2018,"","","","",122,"2022-07-13 09:26:01","","","","",,,,,252,63.00,50,5,4,"Neural networks are increasingly deployed in real-world safety-critical domains such as autonomous driving, aircraft collision avoidance, and malware detection. However, these networks have been shown to often mispredict on inputs with minor adversarial or even accidental perturbations. Consequences of such errors can be disastrous and even potentially fatal as shown by the recent Tesla autopilot crash. Thus, there is an urgent need for formal analysis systems that can rigorously check neural networks for violations of different safety properties such as robustness against adversarial perturbations within a certain L-norm of a given image. An effective safety analysis system for a neural network must be able to either ensure that a safety property is satisfied by the network or find a counterexample, i.e., an input for which the network will violate the property. Unfortunately, most existing techniques for performing such analysis struggle to scale beyond very small networks and the ones that can scale to larger networks suffer from high false positives and cannot produce concrete counterexamples in case of a property violation. In this paper, we present a new efficient approach for rigorously checking different safety properties of neural networks that significantly outperforms existing approaches by multiple orders of magnitude. Our approach can check different safety properties and find concrete counterexamples for networks that are 10x larger than the ones supported by existing analysis techniques. We believe that our approach to estimating tight output bounds of a network for a given input range can also help improve the explainability of neural networks and guide the training process of more robust neural networks.","",""
17,"Yi-Shan Lin, Wen-Chuan Lee, Z. B. Celik","What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors",2020,"","","","",123,"2022-07-13 09:26:01","","10.1145/3447548.3467213","","",,,,,17,8.50,6,3,2,"EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the input parts deemed important to arrive at a decision for a specific target. However, it remains challenging to quantify the correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for the systematic evaluation of explanations that an XAI method generates. We evaluate seven state-of-the-art model-free and model-specific post-hoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We found six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region. We made our code available at https://github.com/yslin013/evalxai.","",""
0,"Junhyung Kwon, Sangkyun Lee","Improving the Robustness of Model Compression by On-Manifold Adversarial Training",2021,"","","","",124,"2022-07-13 09:26:01","","10.3390/fi13120300","","",,,,,0,0.00,0,2,1,"Despite the advance in deep learning technology, assuring the robustness of deep neural networks (DNNs) is challenging and necessary in safety-critical environments, including automobiles, IoT devices in smart factories, and medical devices, to name a few. Furthermore, recent developments allow us to compress DNNs to reduce the size and computational requirements of DNNs to fit them into small embedded devices. However, how robust a compressed DNN can be has not been well studied in addressing its relationship to other critical factors, such as prediction performance and model sizes. In particular, existing studies on robust model compression have been focused on the robustness against off-manifold adversarial perturbation, which does not explain how a DNN will behave against perturbations that follow the same probability distribution as the training data. This aspect is relevant for on-device AI models, which are more likely to experience perturbations due to noise from the regular data observation environment compared with off-manifold perturbations provided by an external attacker. Therefore, this paper investigates the robustness of compressed deep neural networks, focusing on the relationship between the model sizes and the prediction performance on noisy perturbations. Our experiment shows that on-manifold adversarial training can be effective in building robust classifiers, especially when the model compression rate is high.","",""
65,"Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, Cho-Jui Hsieh","Neural SDE: Stabilizing Neural ODE Networks with Stochastic Noise",2019,"","","","",125,"2022-07-13 09:26:01","","","","",,,,,65,21.67,11,6,3,"Neural Ordinary Differential Equation (Neural ODE) has been proposed as a continuous approximation to the ResNet architecture. Some commonly used regularization mechanisms in discrete neural networks (e.g. dropout, Gaussian noise) are missing in current Neural ODE networks. In this paper, we propose a new continuous neural network framework called Neural Stochastic Differential Equation (Neural SDE) network, which naturally incorporates various commonly used regularization mechanisms based on random noise injection. Our framework can model various types of noise injection frequently used in discrete networks for regularization purpose, such as dropout and additive/multiplicative noise in each block. We provide theoretical analysis explaining the improved robustness of Neural SDE models against input perturbations/adversarial attacks. Furthermore, we demonstrate that the Neural SDE network can achieve better generalization than the Neural ODE and is more resistant to adversarial and non-adversarial input perturbations.","",""
184,"D. Querlioz, O. Bichler, C. Gamrat","Simulation of a memristor-based spiking neural network immune to device variations",2011,"","","","",126,"2022-07-13 09:26:01","","10.1109/IJCNN.2011.6033439","","",,,,,184,16.73,61,3,11,"We propose a design methodology to exploit adaptive nanodevices (memristors), virtually immune to their variability. Memristors are used as synapses in a spiking neural network performing unsupervised learning. The memristors learn through an adaptation of spike timing dependent plasticity. Neurons' threshold is adjusted following a homeostasis-type rule. System level simulations on a textbook case show that performance can compare with traditional supervised networks of similar complexity. They also show the system can retain functionality with extreme variations of various memristors' parameters, thanks to the robustness of the scheme, its unsupervised nature, and the power of homeostasis. Additionally the network can adjust to stimuli presented with different coding schemes.","",""
122,"Huaguang Zhang, Jinhai Liu, Dazhong Ma, Zhanshan Wang","Data-Core-Based Fuzzy Min–Max Neural Network for Pattern Classification",2011,"","","","",127,"2022-07-13 09:26:01","","10.1109/TNN.2011.2175748","","",,,,,122,11.09,31,4,11,"A fuzzy min-max neural network based on data core (DCFMN) is proposed for pattern classification. A new membership function for classifying the neuron of DCFMN is defined in which the noise, the geometric center of the hyperbox, and the data core are considered. Instead of using the contraction process of the FMNN described by Simpson, a kind of overlapped neuron with new membership function based on the data core is proposed and added to neural network to represent the overlapping area of hyperboxes belonging to different classes. Furthermore, some algorithms of online learning and classification are presented according to the structure of DCFMN. DCFMN has strong robustness and high accuracy in classification taking onto account the effect of data core and noise. The performance of DCFMN is checked by some benchmark datasets and compared with some traditional fuzzy neural networks, such as the fuzzy min-max neural network (FMNN), the general FMNN, and the FMNN with compensatory neuron. Finally the pattern classification of a pipeline is evaluated using DCFMN and other classifiers. All the results indicate that the performance of DCFMN is excellent.","",""
13,"L. Wandera, K. Mallick, G. Kiely, O. Roupsard, M. Peichl, V. Magliulo","Upscaling instantaneous to daily evapotranspiration using modelled daily shortwave radiation for remote sensing applications: an artificial neural network approach",2016,"","","","",128,"2022-07-13 09:26:01","","10.5194/HESS-21-197-2017","","",,,,,13,2.17,2,6,6,"Abstract. Upscaling instantaneous evapotranspiration retrieved at any specific time-of-day (ETi) to daily evapotranspiration (ETd) is a key challenge in mapping regional ET using polar orbiting sensors. Various studies have unanimously cited the shortwave incoming radiation (RS) to be the most robust reference variable explaining the ratio between ETd and ETi. This study aims to contribute in ETi upscaling for global studies using the ratio between daily and instantaneous incoming shortwave radiation (RSd ∕ RSi) as a factor for converting ETi to ETd. This paper proposes an artificial neural network (ANN) machine-learning algorithm first to predict RSd from RSi followed by using the RSd ∕ RSi ratio to convert ETi to ETd across different terrestrial ecosystems. Using RSi and RSd observations from multiple sub-networks of the FLUXNET database spread across different climates and biomes (to represent inputs that would typically be obtainable from remote sensors during the overpass time) in conjunction with some astronomical variables (e.g. solar zenith angle, day length, exoatmospheric shortwave radiation), we developed the ANN model for reproducing RSd and further used it to upscale ETi to ETd. The efficiency of the ANN is evaluated for different morning and afternoon times of day, under varying sky conditions, and also at different geographic locations. RS-based upscaled ETd produced a significant linear relation (R2 =  0.65 to 0.69), low bias (−0.31 to −0.56 MJ m−2 d−1; approx. 4 %), and good agreement (RMSE 1.55 to 1.86 MJ m−2 d−1; approx. 10 %) with the observed ETd, although a systematic overestimation of ETd was also noted under persistent cloudy sky conditions. Inclusion of soil moisture and rainfall information in ANN training reduced the systematic overestimation tendency in predominantly overcast days. An intercomparison with existing upscaling method at daily, 8-day, monthly, and yearly temporal resolution revealed a robust performance of the ANN-driven RS-based ETi upscaling method and was found to produce lowest RMSE under cloudy conditions. Sensitivity analysis revealed variable sensitivity of the method to biome selection and high ETd prediction errors in forest ecosystems are primarily associated with greater rainfall and cloudiness. The overall methodology appears to be promising and has substantial potential for upscaling ETi to ETd for field and regional-scale evapotranspiration mapping studies using polar orbiting satellites.","",""
3,"Jeremy Bernstein, I. Dasgupta, D. Rolnick, H. Sompolinsky","Markov Transitions between Attractor States in a Recurrent Neural Network",2017,"","","","",129,"2022-07-13 09:26:01","","","","",,,,,3,0.60,1,4,5,"Stochasticity is an essential part of explaining the world. Increasingly, neuroscientists and cognitive scientists are identifying mechanisms whereby the brain uses probabilistic reasoning in representational, predictive, and generative settings. But stochasticity is not always useful: robust perception and memory retrieval require representations that are immune to corruption by stochastic noise. In an effort to combine these robust representations with stochastic computation, we present an architecture that generalizes traditional recurrent attractor networks to follow probabilistic Markov dynamics between stable and noise-resistant fixed points.","",""
7,"Yedi Zhang, Zhe Zhao, Guangke Chen, Fu Song, Taolue Chen","BDD4BNN: A BDD-based Quantitative Analysis Framework for Binarized Neural Networks",2021,"","","","",130,"2022-07-13 09:26:01","","10.1007/978-3-030-81685-8_8","","",,,,,7,7.00,1,5,1,"","",""
9,"Yashaswi Pathak, Sarvesh Mehta, U. D. Priyakumar","Learning Atomic Interactions through Solvation Free Energy Prediction Using Graph Neural Networks",2021,"","","","",131,"2022-07-13 09:26:01","","10.1021/acs.jcim.0c01413","","",,,,,9,9.00,3,3,1,"Solvation free energy is a fundamental property that influences various chemical and biological processes, such as reaction rates, protein folding, drug binding, and bioavailability of drugs. In this work, we present a deep learning method based on graph networks to accurately predict solvation free energies of small organic molecules. The proposed model, comprising three phases, namely, message passing, interaction, and prediction, is able to predict solvation free energies in any generic organic solvent with a mean absolute error of 0.16 kcal/mol. In terms of accuracy, the current model outperforms all of the proposed machine learning-based models so far. The atomic interactions predicted in an unsupervised manner are able to explain the trends of free energies consistent with chemical wisdom. Further, the robustness of the machine learning-based model has been tested thoroughly, and its capability to interpret the predictions has been verified with several examples.","",""
3,"Mohammed Amer, T. Maul","Weight Map Layer for Noise and Adversarial Attack Robustness",2019,"","","","",132,"2022-07-13 09:26:01","","","","",,,,,3,1.00,2,2,3,"Convolutional neural networks (CNNs) are known for their good performance and generalization in vision-related tasks and have become state-of-the-art in both application and research-based domains. However, just like other neural network models, they suffer from a susceptibility to noise and adversarial attacks. An adversarial defence aims at reducing a neural network's susceptibility to adversarial attacks through learning or architectural modifications. We propose a weight map layer (WM) as a generic architectural addition to CNNs and show that it can increase their robustness to noise and adversarial attacks. We further explain the enhanced robustness of the two WM variants introduced via an adaptive noise-variance amplification (ANVA) hypothesis and provide evidence and insights in support of it. We show that the WM layer can be integrated into scaled up models to increase their noise and adversarial attack robustness, while achieving the same or similar accuracy levels.","",""
3,"Chao Ma, Lexing Ying","On Linear Stability of SGD and Input-Smoothness of Neural Networks",2021,"","","","",133,"2022-07-13 09:26:01","","","","",,,,,3,3.00,2,2,1,"The multiplicative structure of parameters and input data in the ﬁrst layer of neural networks is explored to build connection between the landscape of the loss function with respect to parameters and the landscape of the model function with respect to input data. By this connection, it is shown that ﬂat minima regularize the gradient of the model function, which explains the good generalization performance of ﬂat minima. Then, we go beyond the ﬂatness and consider high-order moments of the gradient noise, and show that Stochastic Gradient Descent (SGD) tends to impose constraints on these moments by a linear stability analysis of SGD around global minima. Together with the multiplicative structure, we identify the Sobolev regularization effect of SGD, i.e. SGD regularizes the Sobolev seminorms of the model function with respect to the input data. Finally, bounds for generalization error and adversarial robustness are provided for solutions found by SGD under assumptions of the data distribution. a corollary, we provide theoretical insights of why ﬂat minimum generalizes better. To achieve the goal, we explore the multiplicative structure of the neural network’s input layer, and build connection between the model’s gradient with respect to the parameters and the gradient with respect to the input data. We show that as long as the landscape on the parameter space is mild, the landscape of the model function with respect to the input data is also mild, hence the ﬂatness (as well as higher order linear stability conditions) has the effect of Sobolev regularization. Our study reveals the signiﬁcance of the multiplication structure between data (or features in intermediate layers) and parameters. It is an","",""
2,"Kuntal Ghosh","A neural network based model of M and P LGN cells",2016,"","","","",134,"2022-07-13 09:26:01","","10.1109/BSB.2016.7552165","","",,,,,2,0.33,2,1,6,"A new excitatory-inhibitory neural network model for the extended classical receptive field (ECRF) of Parvo (P), and Magno (M) cells in the lateral geniculate nucleus (LGN) is proposed. The model is based upon various well-known findings in neurophysiology, anatomy and psychophysics. The top-down linking of the proposed model to the feed-forward pathways, that is able to explain the simple, yet intriguing problem of brightness perception, may have implication in developing robust visual capturing and display systems, as well as in overall accurate representation of images as has been demonstrated in recent works.","",""
18,"Francesco Donnarumma, R. Prevete, F. Chersi, G. Pezzulo","A Programmer-Interpreter Neural Network Architecture for Prefrontal Cognitive Control",2015,"","","","",135,"2022-07-13 09:26:01","","10.1142/S0129065715500173","","",,,,,18,2.57,5,4,7,"There is wide consensus that the prefrontal cortex (PFC) is able to exert cognitive control on behavior by biasing processing toward task-relevant information and by modulating response selection. This idea is typically framed in terms of top-down influences within a cortical control hierarchy, where prefrontal-basal ganglia loops gate multiple input-output channels, which in turn can activate or sequence motor primitives expressed in (pre-)motor cortices. Here we advance a new hypothesis, based on the notion of programmability and an interpreter-programmer computational scheme, on how the PFC can flexibly bias the selection of sensorimotor patterns depending on internal goal and task contexts. In this approach, multiple elementary behaviors representing motor primitives are expressed by a single multi-purpose neural network, which is seen as a reusable area of ""recycled"" neurons (interpreter). The PFC thus acts as a ""programmer"" that, without modifying the network connectivity, feeds the interpreter networks with specific input parameters encoding the programs (corresponding to network structures) to be interpreted by the (pre-)motor areas. Our architecture is validated in a standard test for executive function: the 1-2-AX task. Our results show that this computational framework provides a robust, scalable and flexible scheme that can be iterated at different hierarchical layers, supporting the realization of multiple goals. We discuss the plausibility of the ""programmer-interpreter"" scheme to explain the functioning of prefrontal-(pre)motor cortical hierarchies.","",""
1,"Maxwell Hogan, Duarte Rondão, N. Aouf, O. Dubois-matra","Using Convolutional Neural Networks for Relative Pose Estimation of a Non-Cooperative Spacecraft with Thermal Infrared Imagery",2021,"","","","",136,"2022-07-13 09:26:01","","","","",,,,,1,1.00,0,4,1,"Recent interest in on-orbit servicing and Active Debris Removal (ADR) missions have driven the need for technologies to enable non-cooperative rendezvous manoeuvres. Such manoeuvres put heavy burden on the perception capabilities of a chaser spacecraft. This paper demonstrates Convolutional Neural Networks (CNNs) capable of providing an initial coarse pose estimation of a target from a passive thermal infrared camera feed. Thermal cameras offer a promising alternative to visible cameras, which struggle in low light conditions and are susceptible to overexposure. Often, thermal information on the target is not available a priori; this paper therefore proposes using visible images to train networks. The robustness of the models is demonstrated on two different targets, first on synthetic data, and then in a laboratory environment for a realistic scenario that might be faced during an ADR mission. Given that there is much concern over the use of CNNs in critical applications due to their black box nature, we use innovative techniques to explain what is important to our network and fault conditions.","",""
1,"Camilo A. Garcia Trillos, Nicolás García Trillos","On the regularized risk of distributionally robust learning over deep neural networks",2021,"","","","",137,"2022-07-13 09:26:01","","","","",,,,,1,1.00,1,2,1,". In this paper we explore the relation between distributionally robust learning and diﬀerent forms of regularization to enforce robustness of deep neural networks. In particular, starting from a concrete min-max distributionally robust problem, and using tools from optimal transport theory, we derive ﬁrst order and second order approximations to the distributionally robust problem in terms of appropriate regularized risk minimization problems. In the context of deep ResNet models, we identify the structure of the resulting regularization problems as mean-ﬁeld optimal control problems where the number and dimension of state variables is within a dimension-free factor of the dimension of the original unrobust problem. Using the Pontryagin maximum principles associated to these problems we motivate a family of scalable algorithms for the training of robust neural networks. Our analysis recovers some results and algorithms known in the literature (in settings explained throughout the paper) and provides many other theoretical and algorithmic insights that to our knowledge are novel. In our analysis we employ tools that we deem useful for a future analysis of more general adversarial learning problems.","",""
32,"D. Gopinath, Hayes Converse, C. Pasareanu, Ankur Taly","Property Inference for Deep Neural Networks",2019,"","","","",138,"2022-07-13 09:26:01","","10.1109/ASE.2019.00079","","",,,,,32,10.67,8,4,3,"We present techniques for automatically inferring formal properties of feed-forward neural networks. We observe that a significant part (if not all) of the logic of feed forward networks is captured in the activation status (on or off) of its neurons. We propose to extract patterns based on neuron decisions as preconditions that imply certain desirable output property e.g., the prediction being a certain class. We present techniques to extract input properties, encoding convex predicates on the input space that imply given output properties and layer properties, representing network properties captured in the hidden layers that imply the desired output behavior. We apply our techniques on networks for the MNIST and ACASXU applications. Our experiments highlight the use of the inferred properties in a variety of tasks, such as explaining predictions, providing robustness guarantees, simplifying proofs, and network distillation.","",""
1,"M. Dezfooli, Seyed Mohsen","Geometry of adversarial robustness of deep networks: methods and applications",2019,"","","","",139,"2022-07-13 09:26:01","","10.5075/EPFL-THESIS-9579","","",,,,,1,0.33,1,2,3,"We are witnessing a rise in the popularity of using artificial neural networks in many fields of science and technology. Deep neural networks in particular have shown impressive classification performance on a number of challenging benchmarks, generally in well controlled settings. However it is equally important that these classifiers satisfy robustness guarantees when they are deployed in uncontrolled (noise-prone) and possibly hostile environments. In other words, small perturbations applied to the samples should not yield significant loss to the performance of the classifier. Unfortunately, deep neural network classifiers are shown to be intriguingly vulnerable to perturbations and it is relatively easy to design noise that can change the estimated label of the classifier. The study of this high-dimensional phenomenon is a challenging task, and requires the development of new algorithmic tools, as well as theoretical and experimental analysis in order to identify the key factors driving the robustness properties of deep networks. This is exactly the focus of this PhD thesis. First, we propose a computationally efficient yet accurate method to generate minimal perturbations that fool deep neural networks. It permits to reliably quantify the robustness of classifiers and compare different architectures. We further propose a systematic algorithm for computing universal (image-agnostic) and very small perturbation vectors that cause natural images to be misclassified with high probability. The vulnerability to universal perturbations is particularly important in security-critical applications of deep neural networks, and our algorithm shows that these systems are quite vulnerable to noise that is designed with only limited knowledge about test samples or classification architectures. Next, we study the geometry of the classifier’s decision boundary in order to explain the adversarial vulnerability of deep networks. Specifically, we establish precise theoretical bounds on the robustness of classifiers in a novel semi-random noise regime that generalizes both the adversarial and the random perturbation regimes. We show in particular that the robustness of deep networks to universal perturbations is driven by a key property of the curvature of their decision boundaries. Our analysis therefore suggests ways to improve the robustness properties of these classifiers to adversarial perturbations. Finally, we build on the geometric insights derived in this thesis in order to improve the robustness properties of state-of-the-art image classifiers. We leverage a fundamental property in the curvature of the decision boundary of deep networks, and propose a method to detect small adversarial perturbations in images, and to recover the labels vii","",""
40,"Patrick Esser, Robin Rombach, B. Ommer","A Disentangling Invertible Interpretation Network for Explaining Latent Representations",2020,"","","","",140,"2022-07-13 09:26:01","","10.1109/cvpr42600.2020.00924","","",,,,,40,20.00,13,3,2,"Neural networks have greatly boosted performance in computer vision by learning powerful representations of input data. The drawback of end-to-end training for maximal overall performance are black-box models whose hidden representations are lacking interpretability: Since distributed coding is optimal for latent layers to improve their robustness, attributing meaning to parts of a hidden feature vector or to individual neurons is hindered. We formulate interpretation as a translation of hidden representations onto semantic concepts that are comprehensible to the user. The mapping between both domains has to be bijective so that semantic modifications in the target domain correctly alter the original representation. The proposed invertible interpretation network can be transparently applied on top of existing architectures with no need to modify or retrain them. Consequently, we translate an original representation to an equivalent yet interpretable one and backwards without affecting the expressiveness and performance of the original. The invertible interpretation network disentangles the hidden representation into separate, semantically meaningful concepts. Moreover, we present an efficient approach to define semantic concepts by only sketching two images and also an unsupervised strategy. Experimental evaluation demonstrates the wide applicability to interpretation of existing classification and image generation networks as well as to semantically guided image manipulation.","",""
0,"Yunzhe Xue, Meiyan Xie, Zhibo Yang, Usman Roshan","Defending against black-box adversarial attacks with gradient-free trained sign activation neural networks",2021,"","","","",141,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,4,1,"While machine learning models today can achieve high accuracies on classification tasks, they can be deceived by minor imperceptible distortions to the data. These are known as adversarial attacks and can be lethal in the black-box setting which does not require knowledge of the target model type or its parameters. Binary neural networks that have sign activation and are trained with gradient descent have been shown to be harder to attack than conventional sigmoid activation networks but their improvements are marginal. We instead train sign activation networks with a novel gradient-free stochastic coordinate descent algorithm and propose an ensemble of such networks as a defense model. We evaluate the robustness of our model (a hard problem in itself) on image, text, and medical ECG data and find it to be more robust than ensembles of binary, full precision, and convolutional neural networks, and than random forests while attaining comparable clean test accuracy. In order to explain our model's robustness we show that an adversary targeting a single network in our ensemble fails to attack (and thus non-transferable to) other networks in the ensemble. Thus a datapoint requires a large distortion to fool the majority of networks in our ensemble and is likely to be detected in advance. This property of non-transferability arises naturally from the non-convexity of sign activation networks and randomization in our gradient-free training algorithm without any adversarial defense effort.","",""
8,"F. List, N. Rodd, G. Lewis","Dim but not entirely dark: Extracting the Galactic Center Excess' source-count distribution with neural nets",2021,"","","","",142,"2022-07-13 09:26:01","","10.1103/PhysRevD.104.123022","","",,,,,8,8.00,3,3,1,"The two leading hypotheses for the Galactic Center Excess (GCE) in the Fermi data are an unresolved population of faint millisecond pulsars (MSPs) and dark-matter (DM) annihilation. The dichotomy between these explanations is typically reflected by modeling them as two separate emission components. However, point-sources (PSs) such as MSPs become statistically degenerate with smooth Poisson emission in the ultra-faint limit (formally where each source is expected to contribute much less than one photon on average), leading to an ambiguity that can render questions such as whether the emission is PS-like or Poissonian in nature ill-defined. We present a conceptually new approach that describes the PS and Poisson emission in a unified manner and only afterwards derives constraints on the Poissonian component from the so obtained results. For the implementation of this approach, we leverage deep learning techniques, centered around a neural network-based method for histogram regression that expresses uncertainties in terms of quantiles. We demonstrate that our method is robust against a number of systematics that have plagued previous approaches, in particular DM / PS misattribution. In the Fermi data, we find a faint GCE described by a median source-count distribution (SCD) peaked at a flux of ∼ 4 × 10−11 counts cm−2 s−1 (corresponding to ∼ 3− 4 expected counts per PS), which would require N ∼ O(10) sources to explain the entire excess (median value N = 29,300 across the sky). Although faint, this SCD allows us to derive the constraint ηP ≤ 66% for the Poissonian fraction of the GCE flux ηP at 95% confidence, suggesting that a substantial amount of the GCE flux is due to PSs.","",""
3,"Mauro Rodrigo Larrat Frota e Silva, Glaucio H. S. Carvalho, D. Monteiro, L. S. Machado","Distributed Target Location in Wireless Sensors Network: An Approach Using FPGA and Artificial Neural Network",2015,"","","","",143,"2022-07-13 09:26:01","","10.4236/WSN.2015.75005","","",,,,,3,0.43,1,4,7,"This paper analyzes the implementation of an algorithm into a FPGA embedded and distributed target location method using the Received Signal Strength Indicator (RSSI). The objective is to show a method in which an embedded feedforward Artificial Neural Network (ANN) can estimate target location in a distributed fashion against anchor failure. We discuss the lack of FPGA implementation of equivalent methods and the benefits of using a robust platform. We introduce the description of the implementation and we explain the operation of the proposed method, followed by the calculated errors due to inherent Elliott function approximation and the discretization of decimal values used as free parameters in ANN. Furthermore, we show some target location estimation points in function of different numbers of anchor failures. Our contribution is to show that an FPGA embedded ANN implementation, with a few layers, can rapidly estimate target location in a distributed fashion and in presence of failures of anchor nodes considering accuracy, precision and execution time.","",""
7,"Thorben Funke, Megha Khosla, Avishek Anand","Zorro: Valid, Sparse, and Stable Explanations in Graph Neural Networks",2021,"","","","",144,"2022-07-13 09:26:01","","","","",,,,,7,7.00,2,3,1,"With the ever-increasing popularity and applications of graph neural networks, several proposals have been made to explain and understand the decisions of a graph neural network. Explanations for graph neural networks differ in principle from other input settings. It is important to attribute the decision to input features and other related instances connected by the graph structure. We find that the previous explanation generation approaches that maximize the mutual information between the label distribution produced by the model and the explanation to be restrictive. Specifically, existing approaches do not enforce explanations to be valid, sparse, or robust to input perturbations. In this paper, we lay down some of the fundamental principles that an explanation method for graph neural networks should follow and introduce a metric RDT-Fidelity as a measure of the explanation’s effectiveness. We propose a novel approach Zorro based on the principles from rate-distortion theory that uses a simple combinatorial procedure to optimize for RDT-Fidelity. Extensive experiments on real and synthetic datasets reveal that Zorro produces sparser, stable, and more faithful explanations than existing graph neural network explanation approaches.","",""
6,"Chandra Mohan Dasari, Raju Bhukya","Explainable deep neural networks for novel viral genome prediction",2021,"","","","",145,"2022-07-13 09:26:01","","10.1007/s10489-021-02572-3","","",,,,,6,6.00,3,2,1,"","",""
11,"Uduak A. Umoh, E. Nwachukwu","Fuzzy-neural network model for effective control of profitability in a paper recycling plant",2011,"","","","",146,"2022-07-13 09:26:01","","10.5251/AJSIR.2011.2.4.552.558","","",,,,,11,1.00,6,2,11,"This research develops a fuzzy-neural network model, using AI technologies and applies the model for effective control of profitability in paper recycling to improve production accuracy, reliability, robustness and to maximize profit generated by an industry, despite varying cost of production upon which ultimately profit, in an industry depend. Recycling reduces greenhouse gas emissions, conserves the natural resources on Earth, and saves space in the landfills for future generations of people. A sustainable future requires a high degree of recycling. However, Recycling industries face serious economic problems that increase the cost of recycling. Fuzzy logic has emerged as a tool to deal with uncertain, imprecise, partial truth or qualitative decisionmaking problems, to achieve robustness, tractability, and low cost, but it cannot automatically acquire the rules it uses to make those decisions. Neural networks have the ability to learn, generalize and process large amount of numerical data, but they are not good at explaining how they reach their decisions. The hybrid fuzzy-neural system has the ability to overcome the limitations of individual technique and enhances their strengths to handle financial trading. In order to achieve our objective, a study of a knowledge based system for effective control of profitability in paper recycling is carried out. The Mamdani’s Max-Min technique is employed to infer data from the rules developed. This resulted in the establishment of some degrees of influence of input variables on the output. Fuzzy-Neural network model is developed using back propagation and supervised learning methods respectively. The outputs of Fuzzy logic serve as input to the neural network. To reinforce the proposed approach, we apply it to a case study performed on Paper recycling industry in Nigeria. A computer simulation is designed to assist the experimental decision for the best control action. The system is developed using MySQL, NetBeans, Java, MS Excel 2003, MatLab, etc. The obtained simulation and implementation fuzzy-neural results are investigated, compared and discussed.","",""
6,"E. R. Rene, M. E. López, M. Veiga, C. Kennes","Artificial Neural Network Modelling for Waste: Gas and Wastewater Treatment Applications",2011,"","","","",147,"2022-07-13 09:26:01","","10.4018/978-1-60960-551-3.CH010","","",,,,,6,0.55,2,4,11,"Due to their inherent robustness, artificial neural network models have proven to be successful and have been used extensively in biological wastewater treatment applications. However, only recently, with the scientific advancements made in biological waste gas treatment systems, the application of neural networks have slowly gained the practical momentum for performance monitoring in this field. Simple neural models, after vigorous training and testing, are able to generalize the results of a wide range of operating conditions, with high prediction accuracy. This chapter gives a fundamental insight and overview of the process mechanism of different biological waste gas (biofilters, biotrickling filters, continuous stirred tank bioreactors and monolith bioreactors), and wastewater treatment systems (activated sludge process, trickling filter and sequencing batch reactors). The basic theory of artificial neural networks is explained with a clear understanding of the back propagation algorithm. A generalized neural network modelling procedure for waste treatment applications is outlined, and the role of back propagation algorithm network parameters is discussed. Anew, the application of neural networks for solving specific environmental problems is presented in the form of a literature review. DOI: 10.4018/978-1-60960-551-3.ch010","",""
0,"Alessandro Tibo, M. Jaeger, K. Larsen","A general framework for defining and optimizing robustness",2020,"","","","",148,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,3,2,"Robustness of neural networks has recently attracted a great amount of interest. The many investigations in this area lack a precise common foundation of robustness concepts. Therefore, in this paper, we propose a rigorous and flexible framework for defining different types of robustness that also help to explain the interplay between adversarial robustness and generalization. The different robustness objectives directly lead to an adjustable family of loss functions. For two robustness concepts of particular interest we show effective ways to minimize the corresponding loss functions. One loss is designed to strengthen robustness against adversarial off-manifold attacks, and another to improve generalization under the given data distribution. Empirical results show that we can effectively train under different robustness objectives, obtaining higher robustness scores and better generalization, for the two examples respectively, compared to the state-of-the-art data augmentation and regularization techniques.","",""
11,"Wenzhe Guo, M. Fouda, A. Eltawil, K. Salama","Neural Coding in Spiking Neural Networks: A Comparative Study for Robust Neuromorphic Systems",2021,"","","","",149,"2022-07-13 09:26:01","","10.3389/fnins.2021.638474","","",,,,,11,11.00,3,4,1,"Various hypotheses of information representation in brain, referred to as neural codes, have been proposed to explain the information transmission between neurons. Neural coding plays an essential role in enabling the brain-inspired spiking neural networks (SNNs) to perform different tasks. To search for the best coding scheme, we performed an extensive comparative study on the impact and performance of four important neural coding schemes, namely, rate coding, time-to-first spike (TTFS) coding, phase coding, and burst coding. The comparative study was carried out using a biological 2-layer SNN trained with an unsupervised spike-timing-dependent plasticity (STDP) algorithm. Various aspects of network performance were considered, including classification accuracy, processing latency, synaptic operations (SOPs), hardware implementation, network compression efficacy, input and synaptic noise resilience, and synaptic fault tolerance. The classification tasks on Modified National Institute of Standards and Technology (MNIST) and Fashion-MNIST datasets were applied in our study. For hardware implementation, area and power consumption were estimated for these coding schemes, and the network compression efficacy was analyzed using pruning and quantization techniques. Different types of input noise and noise variations in the datasets were considered and applied. Furthermore, the robustness of each coding scheme to the non-ideality-induced synaptic noise and fault in analog neuromorphic systems was studied and compared. Our results show that TTFS coding is the best choice in achieving the highest computational performance with very low hardware implementation overhead. TTFS coding requires 4x/7.5x lower processing latency and 3.5x/6.5x fewer SOPs than rate coding during the training/inference process. Phase coding is the most resilient scheme to input noise. Burst coding offers the highest network compression efficacy and the best overall robustness to hardware non-idealities for both training and inference processes. The study presented in this paper reveals the design space created by the choice of each coding scheme, allowing designers to frame each scheme in terms of its strength and weakness given a designs’ constraints and considerations in neuromorphic systems.","",""
5,"Francesco Sovrano, F. Vitali","An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability",2021,"","","","",150,"2022-07-13 09:26:01","","","","",,,,,5,5.00,3,2,1,"Numerous government initiatives (e.g. the EU with GDPR) are coming to the conclusion that the increasing complexity of modern software systems must be contrasted with some Rights to Explanation and metrics for the Impact Assessment of these tools, that allow humans to understand and oversee the output of Automated Decision Making systems. Explainable AI was born as a pathway to allow humans to explore and understand the inner working of complex systems. But establishing what is an explanation and objectively evaluating explainability, are not trivial tasks. With this paper, we present a new model-agnostic metric to measure the Degree of eXplainability of (correct) information in an objective way, exploiting a specific theoretical model from Ordinary Language Philosophy called the Achinstein’s Theory of Explanations, implemented with an algorithm relying on deep language models for knowledge graph extraction and information retrieval. In order to understand whether this metric is actually behaving as explainability is expected to, we have devised a few experiments and user-studies involving more than 160 participants evaluating two realistic AI-based systems for healthcare and finance using famous AI technology including Artificial Neural Networks and TreeSHAP. The results we obtained are very encouraging, suggesting that our proposed metric for measuring the Degree of eXplainability is robust on several scenarios and it can be eventually exploited for a lawful Impact Assessment of an Automated Decision Making system.","",""
5,"Kshitij Dwivedi, Radoslaw Martin Cichy, G. Roig","Unraveling Representations in Scene-selective Brain Regions Using Scene-Parsing Deep Neural Networks",2020,"","","","",151,"2022-07-13 09:26:01","","10.1162/jocn_a_01624","","",,,,,5,2.50,2,3,2,"Abstract Visual scene perception is mediated by a set of cortical regions that respond preferentially to images of scenes, including the occipital place area (OPA) and parahippocampal place area (PPA). However, the differential contribution of OPA and PPA to scene perception remains an open research question. In this study, we take a deep neural network (DNN)-based computational approach to investigate the differences in OPA and PPA function. In a first step, we search for a computational model that predicts fMRI responses to scenes in OPA and PPA well. We find that DNNs trained to predict scene components (e.g., wall, ceiling, floor) explain higher variance uniquely in OPA and PPA than a DNN trained to predict scene category (e.g., bathroom, kitchen, office). This result is robust across several DNN architectures. On this basis, we then determine whether particular scene components predicted by DNNs differentially account for unique variance in OPA and PPA. We find that variance in OPA responses uniquely explained by the navigation-related floor component is higher compared to the variance explained by the wall and ceiling components. In contrast, PPA responses are better explained by the combination of wall and floor, that is, scene components that together contain the structure and texture of the scene. This differential sensitivity to scene components suggests differential functions of OPA and PPA in scene processing. Moreover, our results further highlight the potential of the proposed computational approach as a general tool in the investigation of the neural basis of human scene perception.","",""
5,"Anis Hamza, N. B. Yahia","Heavy trucks with intelligent control of active suspension based on artificial neural networks",2020,"","","","",152,"2022-07-13 09:26:01","","10.1177/0959651820958516","","",,,,,5,2.50,3,2,2,"The active control of a suspension system is meant to provide an isolated behavior of the system spring-mass (for example, increased comfort and performance). During this article, we are going to explain the importance of developing an intelligent control approach for active truck suspensions based on the artificial neural network. From where the main objective of this article is to obtain a mathematical model for active suspension systems then build a hydraulic model for active suspension control for trucks using an artificial neural network. In this article, a corresponding artificial neural network nonlinear active suspension controller has been designed and optimized for approximate road profiles, using simulation according to International Organization for Standardization 2631-5 and International Organization for Standardization 8608 standardizations. The model developed with MATLAB Toolbox, estimated and validated from data collected during tests carried out with a truck in other research work. To model the system, the laws of physics are used to describe the system and experimental data or information supplied about the system to determine the parameters of the system. The statement of the problem of this research is to develop a robust artificial neural network controller for the nonlinear active suspension system of the heavy truck that can improve the performances and its verifications using graphical and simulation output. The results of the simulation show that the methodology offers excellent performance. In addition, the robustness of the artificial neural network hydraulic controller is demonstrated for a variety of road profiles that increase the capabilities of the proposed methodology and prove its effectiveness.","",""
5,"Haichao Huang, Jingya Chen, Xinting Huo, Yufei Qiao, Lei Ma","Effect of Multi-Scale Decomposition on Performance of Neural Networks in Short-Term Traffic Flow Prediction",2021,"","","","",153,"2022-07-13 09:26:01","","10.1109/ACCESS.2021.3068652","","",,,,,5,5.00,1,5,1,"Numerous studies employ multi-scale decomposition to improve the prediction performance of neural networks, but the grounds for selecting the decomposition algorithm are not explained, and the effects of decomposition algorithms on other performance of neural networks are also lacking further study. This paper studies the influence of commonly used multi-scale decomposition algorithms including EMD (Empirical Mode Decomposition), EEMD(Ensemble Empirical Mode Decomposition), CEEMDAN (Complete Ensemble Empirical Mode Decomposition with Adaptive Noise), VMD (Variational Mode Decomposition), WD (Wavelet Decomposition), and WPD (Wavelet Packet Decomposition) on the performance of Neural Networks. Decomposition algorithms are adopted to decompose traffic flow data into component signals, and then K-means is used to cluster component signals into volatility components, periodic components, and residual components. A Bi-directional LSTM (BiLSTM) neural network is adopted as the standard model for training and forecasting. Finally, three metrics, including prediction performance, robustness, and generalization performance are proposed to evaluate the influence of the multi-scale decomposition algorithm for neural networks comprehensively. By comparing the evaluation results of different hybrid models, this study provides some useful suggestions on proper multi-scale decomposition algorithm selection in short-time traffic flow prediction.","",""
130,"David Sussillo, P. Nuyujukian, Joline M. Fan, J. Kao, S. Stavisky, S. Ryu, K. Shenoy","A recurrent neural network for closed-loop intracortical brain-machine interface decoders.",2012,"","","","",154,"2022-07-13 09:26:01","","10.1088/1741-2560/9/2/026027","","",,,,,130,13.00,19,7,10,"Recurrent neural networks (RNNs) are useful tools for learning nonlinear relationships in time series data with complex temporal dependences. In this paper, we explore the ability of a simplified type of RNN, one with limited modifications to the internal weights called an echostate network (ESN), to effectively and continuously decode monkey reaches during a standard center-out reach task using a cortical brain-machine interface (BMI) in a closed loop. We demonstrate that the RNN, an ESN implementation termed a FORCE decoder (from first order reduced and controlled error learning), learns the task quickly and significantly outperforms the current state-of-the-art method, the velocity Kalman filter (VKF), using the measure of target acquire time. We also demonstrate that the FORCE decoder generalizes to a more difficult task by successfully operating the BMI in a randomized point-to-point task. The FORCE decoder is also robust as measured by the success rate over extended sessions. Finally, we show that decoded cursor dynamics are more like naturalistic hand movements than those of the VKF. Taken together, these results suggest that RNNs in general, and the FORCE decoder in particular, are powerful tools for BMI decoder applications.","",""
8,"S. Pontes-Filho, M. Liwicki","Bidirectional Learning for Robust Neural Networks",2018,"","","","",155,"2022-07-13 09:26:01","","10.1109/IJCNN.2019.8852120","","",,,,,8,2.00,4,2,4,"A multilayer perceptron can behave as a generative classifier by applying bidirectional learning (BL). It consists of training an undirected neural network to map input to output and vice-versa; therefore it can produce a classifier in one direction, and a generator in the opposite direction for the same data. The learning process of BL tries to reproduce the neuroplasticity stated in Hebbian theory using only backward propagation of errors. In this paper, two learning techniques are independently introduced which use BL for improving robustness to white noise static and adversarial examples. The first method is bidirectional propagation of errors, which the error propagation occurs in backward and forward directions. Motivated by the fact that its generative model receives as input a constant vector per class, we introduce as a second method the novel hybrid adversarial networks (HAN). Its generative model receives a random vector as input and its training is based on generative adversarial networks (GAN). To assess the performance of BL, we perform experiments using several architectures with fully and convolutional layers, with and without bias. Experimental results show that both methods improve robustness to white noise static and adversarial examples, and even increase accuracy, but have different behavior depending on the architecture and task, being more beneficial to use the one or the other. Nevertheless, HAN using a convolutional architecture with batch normalization presents outstanding robustness, reaching state-of-the-art accuracy on adversarial examples of hand-written digits.","",""
356,"Travis A. Jarrell, Yi Wang, Adam Bloniarz, C. Brittin, Meng Xu, J. N. Thomson, D. Albertson, D. Hall, S. W. Emmons","The Connectome of a Decision-Making Neural Network",2012,"","","","",156,"2022-07-13 09:26:01","","10.1126/science.1221762","","",,,,,356,35.60,40,9,10,"The Male Wiring Diagram The function of the nervous system is thought to represent an emergent property of its network connectivity. However, there are few complete descriptions of all the physical connections between neurons within a real nervous system. Working in nematodes, Jarrell et al. (p. 437; see the Perspective by Chklovskii and Bargmann) identified the complete connectome—every single chemical and gap junction synapse—of the tail ganglia, which govern mating behavior. The complete wiring structure of the synaptic network governing mating behavior of male nematodes is revealed. In order to understand the nervous system, it is necessary to know the synaptic connections between the neurons, yet to date, only the wiring diagram of the adult hermaphrodite of the nematode Caenorhabditis elegans has been determined. Here, we present the wiring diagram of the posterior nervous system of the C. elegans adult male, reconstructed from serial electron micrograph sections. This region of the male nervous system contains the sexually dimorphic circuits for mating. The synaptic connections, both chemical and gap junctional, form a neural network with four striking features: multiple, parallel, short synaptic pathways directly connecting sensory neurons to end organs; recurrent and reciprocal connectivity among sensory neurons; modular substructure; and interneurons acting in feedforward loops. These features help to explain how the network robustly and rapidly selects and executes the steps of a behavioral program on the basis of the inputs from multiple sensory neurons.","",""
2,"Yingying Yan, Daguang Yang","A Stock Trend Forecast Algorithm Based on Deep Neural Networks",2021,"","","","",157,"2022-07-13 09:26:01","","10.1155/2021/7510641","","",,,,,2,2.00,1,2,1,"As a recognized complex dynamic system, the stock market has many influencing factors, such as nonstationarity, nonlinearity, high noise, and long memory. It is difficult to explain it simply through mathematical models. Therefore, the analysis and prediction of the stock market have been a very challenging job since long time. Therefore, this paper adopts an encoder-decoder model of attention mechanism, adding attention mechanism from two aspects of feature and time. Both encoder and decoder use LSTM neural network. This method solves two problems in time series prediction; the first problem is that multiple input features have different degrees of influence on the target sequence, the feature attention mechanism is used to deal with this problem, and the weights of different input features can be obtained. A more robust feature association relationship is obtained; the second problem is that the data before and after the sequence have a strong time correlation. The time attention mechanism is used to deal with this problem, and the weights at different time points can be obtained to obtain more robustness and good timing dependencies. The simulation and experimental results show that the introduction of the attention mechanism can obtain lower forecast errors, which proves the effectiveness of the model in dealing with stock forecasting problems.","",""
1,"Benjamin Filtjens, P. Ginis, A. Nieuwboer, M. R. Afzal, J. Spildooren, B. Vanrumste, P. Slaets","Modelling and identification of characteristic kinematic features preceding freezing of gait with convolutional neural networks and layer-wise relevance propagation",2021,"","","","",158,"2022-07-13 09:26:01","","10.1186/s12911-021-01699-0","","",,,,,1,1.00,0,7,1,"","",""
6,"R. Mbuvha, Illyes Boulkaibet, T. Marwala","Automatic Relevance Determination Bayesian Neural Networks for Credit Card Default Modelling",2019,"","","","",159,"2022-07-13 09:26:01","","","","",,,,,6,2.00,2,3,3,"Credit risk modelling is an integral part of the global financial system. While there has been great attention paid to neural network models for credit default prediction, such models often lack the required interpretation mechanisms and measures of the uncertainty around their predictions. This work develops and compares Bayesian Neural Networks(BNNs) for credit card default modelling. This includes a BNNs trained by Gaussian approximation and the first implementation of BNNs trained by Hybrid Monte Carlo(HMC) in credit risk modelling. The results on the Taiwan Credit Dataset show that BNNs with Automatic Relevance Determination(ARD) outperform normal BNNs without ARD. The results also show that BNNs trained by Gaussian approximation display similar predictive performance to those trained by the HMC. The results further show that BNN with ARD can be used to draw inferences about the relative importance of different features thus critically aiding decision makers in explaining model output to consumers. The robustness of this result is reinforced by high levels of congruence between the features identified as important using the two different approaches for training BNNs.","",""
1,"Beatriz Garcia Santa Cruz, A. Husch, F. Hertel","The effect of dataset confounding on predictions of deep neural networks for medical imaging",2022,"","","","",160,"2022-07-13 09:26:01","","10.7557/18.6302","","",,,,,1,1.00,0,3,1,"The use of Convolutional Neural Networks (CNN) in medical imaging has often outperformed previous solutions and even specialists, becoming a promising technology for Computer-aidedDiagnosis (CAD) systems. However, recent works suggested that CNN may have poor generalisation on new data, for instance, generated in different hospitals. Uncontrolled confounders have been proposed as a common reason. In this paper, we experimentally demonstrate the impact of confounding data in unknown scenarios. We assessed the effect of four confounding configurations: total, strong, light and balanced. We found the confounding effect is especially prominent in total confounder scenarios, while the effect on light and strong confounding scenarios may depend on the dataset robustness. Our findings indicate that the confounding effect is independent of the architecture employed. These findings might explain why models can report good metrics during the development stage but fail to translate to real-world settings. We highlight the need for thorough consideration of these commonly unattended aspects, to develop safer CNN-based CAD systems.","",""
3,"Nurit Sternberg, R. Luria, G. Sheppes","For whom is social-network usage associated with anxiety? The moderating role of neural working-memory filtering of Facebook information",2018,"","","","",161,"2022-07-13 09:26:01","","10.3758/s13415-018-0627-z","","",,,,,3,0.75,1,3,4,"","",""
0,"Kudzai Sauka, Gunkwon Shin, Dong-Wook Kim, Myung-Mook Han","Adversarial Robust and Explainable Network Intrusion Detection Systems Based on Deep Learning",2022,"","","","",162,"2022-07-13 09:26:01","","10.3390/app12136451","","",,,,,0,0.00,0,4,1,"The ever-evolving cybersecurity environment has given rise to sophisticated adversaries who constantly explore new ways to attack cyberinfrastructure. Recently, the use of deep learning-based intrusion detection systems has been on the rise. This rise is due to deep neural networks (DNN) complexity and efficiency in making anomaly detection activities more accurate. However, the complexity of these models makes them black-box models, as they lack explainability and interpretability. Not only is the DNN perceived as a black-box model, but recent research evidence has also shown that they are vulnerable to adversarial attacks. This paper developed an adversarial robust and explainable network intrusion detection system based on deep learning by applying adversarial training and implementing explainable AI techniques. In our experiments with the NSL-KDD dataset, the PGD adversarial-trained model was a more robust model than DeepFool adversarial-trained and FGSM adversarial-trained models, with a ROC-AUC of 0.87. The FGSM attack did not affect the PGD adversarial-trained model’s ROC-AUC, while the DeepFool attack caused a minimal 9.20% reduction in PGD adversarial-trained model’s ROC-AUC. PGD attack caused a 15.12% reduction in the DeepFool adversarial-trained model’s ROC-AUC and a 12.79% reduction in FGSM trained model’s ROC-AUC.","",""
36,"E. Magosso, Cristiano Cuppini, M. Ursino","A Neural Network Model of Ventriloquism Effect and Aftereffect",2012,"","","","",163,"2022-07-13 09:26:01","","10.1371/journal.pone.0042503","","",,,,,36,3.60,12,3,10,"Presenting simultaneous but spatially discrepant visual and auditory stimuli induces a perceptual translocation of the sound towards the visual input, the ventriloquism effect. General explanation is that vision tends to dominate over audition because of its higher spatial reliability. The underlying neural mechanisms remain unclear. We address this question via a biologically inspired neural network. The model contains two layers of unimodal visual and auditory neurons, with visual neurons having higher spatial resolution than auditory ones. Neurons within each layer communicate via lateral intra-layer synapses; neurons across layers are connected via inter-layer connections. The network accounts for the ventriloquism effect, ascribing it to a positive feedback between the visual and auditory neurons, triggered by residual auditory activity at the position of the visual stimulus. Main results are: i) the less localized stimulus is strongly biased toward the most localized stimulus and not vice versa; ii) amount of the ventriloquism effect changes with visual-auditory spatial disparity; iii) ventriloquism is a robust behavior of the network with respect to parameter value changes. Moreover, the model implements Hebbian rules for potentiation and depression of lateral synapses, to explain ventriloquism aftereffect (that is, the enduring sound shift after exposure to spatially disparate audio-visual stimuli). By adaptively changing the weights of lateral synapses during cross-modal stimulation, the model produces post-adaptive shifts of auditory localization that agree with in-vivo observations. The model demonstrates that two unimodal layers reciprocally interconnected may explain ventriloquism effect and aftereffect, even without the presence of any convergent multimodal area. The proposed study may provide advancement in understanding neural architecture and mechanisms at the basis of visual-auditory integration in the spatial realm.","",""
0,"A. Saadallah, K. Morik","Meta-Adversarial Training of Neural Networks for Binary Classification",2021,"","","","",164,"2022-07-13 09:26:01","","10.1109/IJCNN52387.2021.9534247","","",,,,,0,0.00,0,2,1,"We propose a novel framework for classification using neural networks via an adversarial training procedure, in which we simultaneously train a main classifier—a neural network that solves the original classification task, i.e classifying instances into two main categories—and two meta-classifiers which act as discriminators and aim to detect false positives and negatives predicted by the original classifier. Our framework operates in two stages: In a first stage, both main and meta classifiers are pre-trained using the cross-entropy loss. The second stage consists of an adversarial training stage in which both main and meta classifiers are placed in a min-max game. Therefore, we switch to our new loss function so that the goal for the main classifier becomes to maximize the probability of failure of the adversarial meta-classifiers. Our training procedure can be explained by the fact that the meta-classifiers are more accurate when the main classifier is weak i.e., instances misclassified by the main classifier are naturally easy to separate and assign to the correct class membership. Opposingly, if the main classifier is robust enough, then the meta-classifiers are supposed to distinguish between instances that are naturally hard to classify, making thus more mistakes. In this work, both main and meta-classifiers are defined by Multi-Layer Perceptrons (MLP) and the entire training system is performed using backpropagation with gradient descent optimization. Experiments demonstrate the potential of our framework in outperforming the traditional learning scheme in improving the classification accuracy.","",""
1,"H. Bouzari, M. Srámek, G. Mistelbauer, E. Bouzari","Robust Adaptive Wavelet Neural Network Control of Buck Converters",2011,"","","","",165,"2022-07-13 09:26:01","","10.5772/16843","","",,,,,1,0.09,0,4,11,"Robustness is of crucial importance in control system design because the real engineering systems are vulnerable to external disturbance and measurement noise and there are always differences between mathematical models used for design and the actual system. Typically, it is required to design a controller that will stabilize a plant, if it is not stable originally, and to satisfy certain performance levels in the presence of disturbance signals, noise interference, unmodelled plant dynamics and plant-parameter variations. These design objectives are best realized via the feedback control mechanism (Fig. 1), although it introduces in the issues of high cost (the use of sensors), system complexity (implementation and safety) and more concerns on stability (thus internal stability and stabilizing controllers) (Gu, Petkov, & Konstantinov, 2005). In abstract, a control system is robust if it remains stable and achieves certain performance criteria in the presence of possible uncertainties. The robust design is to find a controller, for a given system, such that the closed-loop system is robust. In this chapter, the basic concepts and representations of a robust adaptive wavelet neural network control for the case study of buck converters will be discussed. The remainder of the chapter is organized as follows: In section 2 the advantages of neural network controllers over conventional ones will be discussed, considering the efficiency of introduction of wavelet theory in identifying unknown dependencies. Section 3 presents an overview of the buck converter models. In section 4, a detailed overview of WNN methods is presented. Robust control is introduced in section 5 to increase the robustness against noise by implementing the error minimization. Section 6 explains the stability analysis which is based on adaptive bound estimation. The implementation procedure and results of AWNN controller are explained in section 7. The results show the effectiveness of the proposed method in comparison to other previous works. The final section concludes the chapter.","",""
57,"Jonathan Peck, J. Roels, B. Goossens, Y. Saeys","Lower bounds on the robustness to adversarial perturbations",2017,"","","","",166,"2022-07-13 09:26:01","","","","",,,,,57,11.40,14,4,5,"The input-output mappings learned by state-of-the-art neural networks are significantly discontinuous. It is possible to cause a neural network used for image recognition to misclassify its input by applying very specific, hardly perceptible perturbations to the input, called adversarial perturbations. Many hypotheses have been proposed to explain the existence of these peculiar samples as well as several methods to mitigate them. A proven explanation remains elusive, however. In this work, we take steps towards a formal characterization of adversarial perturbations by deriving lower bounds on the magnitudes of perturbations necessary to change the classification of neural networks. The bounds are experimentally verified on the MNIST and CIFAR-10 data sets.","",""
9,"Darius Afchar, Romain Hennequin","Making Neural Networks Interpretable with Attribution: Application to Implicit Signals Prediction",2020,"","","","",167,"2022-07-13 09:26:01","","10.1145/3383313.3412253","","",,,,,9,4.50,5,2,2,"Explaining recommendations enables users to understand whether recommended items are relevant to their needs and has been shown to increase their trust in the system. More generally, if designing explainable machine learning models is key to check the sanity and robustness of a decision process and improve their efficiency, it however remains a challenge for complex architectures, especially deep neural networks that are often deemed ”black-box”. In this paper, we propose a novel formulation of interpretable deep neural networks for the attribution task. Differently to popular post-hoc methods, our approach is interpretable by design. Using masked weights, hidden features can be deeply attributed, split into several input-restricted sub-networks and trained as a boosted mixture of experts. Experimental results on synthetic data and real-world recommendation tasks demonstrate that our method enables to build models achieving close predictive performances to their non-interpretable counterparts, while providing informative attribution interpretations.","",""
1,"J. Hyatt, Michael Lee","Requirements for Developing Robust Neural Networks",2019,"","","","",168,"2022-07-13 09:26:01","","","","",,,,,1,0.33,1,2,3,"Validation accuracy is a necessary, but not sufficient, measure of a neural network classifier's quality. High validation accuracy during development does not guarantee that a model is free of serious flaws, such as vulnerability to adversarial attacks or a tendency to misclassify (with high confidence) data it was not trained on. The model may also be incomprehensible to a human or base its decisions on unreasonable criteria. These problems, which are not unique to classifiers, have been the focus of a substantial amount of recent research. However, they are not prioritized during model development, which almost always optimizes on validation accuracy to the exclusion of everything else. The product of this approach is likely to fail in unexpected ways outside of the training environment. We believe that, in addition to validation accuracy, the model development process must give added weight to other performance metrics such as explainability, resistance to adversarial attacks, and overconfidence on out-of-distribution data.","",""
1,"Rohit Gandikota, Deepak Mishra","How You See Me: Understanding Convolutional Neural Networks",2019,"","","","",169,"2022-07-13 09:26:01","","10.1109/TENCON.2019.8929603","","",,,,,1,0.33,1,2,3,"Convolutional Neural networks(CNN) are one of the most powerful tools in the present era of science. There has been a lot of research done to improve their performance and robustness while their internal working was left unexplored to much extent. They are often defined as black boxes that can map non-linear data effectively. This paper answers the question, “How does a CNN look at an image?”. Visual results are also provided to strongly support the proposed method. The proposed algorithm exploits the basic math behind CNN to backtrack the important pixels. This is a generic approach which can be applied to any architecture of a neural network. This doesn't require any additional training or architectural changes. In literature, few attempts have been made to explain how learning happens in CNN internally, by exploiting the convolution filter maps. This is a simple algorithm as it does not involve any cost functions, filter exploitation, gradient calculations or probability scores. Further, we demonstrate that the proposed scheme can be used in some important computer vision tasks such as object detection, salient region proposal, etc.","",""
22,"Masaki Kobayashi","Noise Robust Projection Rule for Hyperbolic Hopfield Neural Networks",2020,"","","","",170,"2022-07-13 09:26:01","","10.1109/TNNLS.2019.2899914","","",,,,,22,11.00,22,1,2,"A complex-valued Hopfield neural network (CHNN) is a multistate Hopfield model. Low noise tolerance is the main disadvantage of CHNNs. The hyperbolic Hopfield neural network (HHNN) is a noise robust multistate Hopfield model. In HHNNs employing the projection rule, noise tolerance rapidly worsened as the number of training patterns increased. This result was caused by the self-loops. The projection rule for CHNNs improves noise tolerance by removing the self-loops, however, that for HHNNs cannot remove them. In this brief, we extended the stability condition for the self-loops of HHNNs and modified the projection rule. Thus, the HHNNs had improved noise tolerance.","",""
19,"Hande Dong, Jiawei Chen, Fuli Feng, Xiangnan He, Shuxian Bi, Zhaolin Ding, Peng Cui","On the Equivalence of Decoupled Graph Convolution Network and Label Propagation",2020,"","","","",171,"2022-07-13 09:26:01","","10.1145/3442381.3449927","","",,,,,19,9.50,3,7,2,"The original design of Graph Convolution Network (GCN) couples feature transformation and neighborhood aggregation for node representation learning. Recently, some work shows that coupling is inferior to decoupling, which supports deep graph propagation better and has become the latest paradigm of GCN (e.g., APPNP [16] and SGCN [32]). Despite effectiveness, the working mechanisms of the decoupled GCN are not well understood. In this paper, we explore the decoupled GCN for semi-supervised node classification from a novel and fundamental perspective — label propagation. We conduct thorough theoretical analyses, proving that the decoupled GCN is essentially the same as the two-step label propagation: first, propagating the known labels along the graph to generate pseudo-labels for the unlabeled nodes, and second, training normal neural network classifiers on the augmented pseudo-labeled data. More interestingly, we reveal the effectiveness of decoupled GCN: going beyond the conventional label propagation, it could automatically assign structure- and model- aware weights to the pseudo-label data. This explains why the decoupled GCN is relatively robust to the structure noise and over-smoothing, but sensitive to the label noise and model initialization. Based on this insight, we propose a new label propagation method named Propagation then Training Adaptively (PTA), which overcomes the flaws of the decoupled GCN with a dynamic and adaptive weighting strategy. Our PTA is simple yet more effective and robust than decoupled GCN. We empirically validate our findings on four benchmark datasets, demonstrating the advantages of our method. The code is available at https://github.com/DongHande/PT_propagation_then_training.","",""
64,"Andras Rozsa, Manuel Günther, T. Boult","Towards Robust Deep Neural Networks with BANG",2016,"","","","",172,"2022-07-13 09:26:01","","10.1109/WACV.2018.00093","","",,,,,64,10.67,21,3,6,"Machine learning models, including state-of-the-art deep neural networks, are vulnerable to small perturbations that cause unexpected classification errors. This unexpected lack of robustness raises fundamental questions about their generalization properties and poses a serious concern for practical deployments. As such perturbations can remain imperceptible – the formed adversarial examples demonstrate an inherent inconsistency between vulnerable machine learning models and human perception – some prior work casts this problem as a security issue. Despite the significance of the discovered instabilities and ensuing research, their cause is not well understood and no effective method has been developed to address the problem. In this paper, we present a novel theory to explain why this unpleasant phenomenon exists in deep neural networks. Based on that theory, we introduce a simple, efficient, and effective training approach, Batch Adjusted Network Gradients (BANG), which significantly improves the robustness of machine learning models. While the BANG technique does not rely on any form of data augmentation or the utilization of adversarial images for training, the resultant classifiers are more resistant to adversarial perturbations while maintaining or even enhancing the overall classification performance.","",""
24,"Ç. Aladag, E. Egrioglu, U. Yolcu","Robust multilayer neural network based on median neuron model",2014,"","","","",173,"2022-07-13 09:26:01","","10.1007/s00521-012-1315-5","","",,,,,24,3.00,8,3,8,"","",""
0,"Jenn-Bing Ong, W. Ng, C.-C. Jay Kuo","Convolutional Neural Networks with Transformed Input based on Robust Tensor Network Decomposition",2018,"","","","",174,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,3,4,"Tensor network decomposition, originated from quantum physics to model entangled many-particle quantum systems, turns out to be a promising mathematical technique to efficiently represent and process big data in parsimonious manner. In this study, we show that tensor networks can systematically partition structured data, e.g. color images, for distributed storage and communication in privacy-preserving manner. Leveraging the sea of big data and metadata privacy, empirical results show that neighbouring subtensors with implicit information stored in tensor network formats cannot be identified for data reconstruction. This technique complements the existing encryption and randomization techniques which store explicit data representation at one place and highly susceptible to adversarial attacks such as side-channel attacks and de-anonymization. Furthermore, we propose a theory for adversarial examples that mislead convolutional neural networks to misclassification using subspace analysis based on singular value decomposition (SVD). The theory is extended to analyze higher-order tensors using tensor-train SVD (TT-SVD); it helps to explain the level of susceptibility of different datasets to adversarial attacks, the structural similarity of different adversarial attacks including global and localized attacks, and the efficacy of different adversarial defenses based on input transformation. An efficient and adaptive algorithm based on robust TT-SVD is then developed to detect strong and static adversarial attacks.","",""
2,"Edmundo Lopez-Sola, R. Sanchez-Todo, Èlia Lleal, Elif Köksal-Ersöz, M. Yochum, Julia Makhalova, Borja Mercadal, R. Salvador, D. Lozano-Soldevilla, J. Modolo, F. Bartolomei, F. Wendling, P. Benquet, G. Ruffini","A personalizable autonomous neural mass model of epileptic seizures",2021,"","","","",175,"2022-07-13 09:26:01","","10.1101/2021.12.24.474090","","",,,,,2,2.00,0,14,1,"Work in the last two decades has shown that neural mass models (NMM) can realistically reproduce and explain epileptic seizure transitions as recorded by electrophysiological methods (EEG, SEEG). In previous work, advances were achieved by increasing excitation and heuristically varying network inhibitory coupling parameters in the models. Based on these early studies, we provide a laminar NMM capable of realistically reproducing the electrical activity recorded by SEEG in the epileptogenic zone during interictal to ictal states. With the exception of the external noise input into the pyramidal cell population, the model dynamics are autonomous. By setting the system at a point close to bifurcation, seizure-like transitions are generated, including pre-ictal spikes, low voltage fast activity, and ictal rhythmic activity. A novel element in the model is a physiologically motivated algorithm for chloride dynamics: the gain of GABAergic post-synaptic potentials is modulated by the pathological accumulation of chloride in pyramidal cells due to high inhibitory input and/or dysfunctional chloride transport. In addition, in order to simulate SEEG signals for comparison with real seizure recordings, the NMM is embedded first in a layered model of the neocortex and then in a realistic physical model. We compare modelling results with data from four epilepsy patient cases. By including key pathophysiological mechanisms, the proposed framework captures succinctly the electrophysiological phenomenology observed in ictal states, paving the way for robust personalization methods based on NMMs.","",""
2,"Christian Fiedler, M. Fornasier, T. Klock, Michael Rauchensteiner","Stable Recovery of Entangled Weights: Towards Robust Identification of Deep Neural Networks from Minimal Samples",2021,"","","","",176,"2022-07-13 09:26:01","","","","",,,,,2,2.00,1,4,1,"In this paper we approach the problem of unique and stable identifiability of generic deep artificial neural networks with pyramidal shape and smooth activation functions from a finite number of input-output samples. More specifically we introduce the so-called entangled weights, which compose weights of successive layers intertwined with suitable diagonal and invertible matrices depending on the activation functions and their shifts. We prove that entangled weights are completely and stably approximated by an efficient and robust algorithm as soon as O(D ×m) nonadaptive input-output samples of the network are collected, where D is the input dimension and m is the number of neurons of the network. Moreover, we empirically observe that the approach applies to networks with up to O(D × mL) neurons, wheremL is the number of output neurons at layer L. Provided knowledge of layer assignments of entangled weights and of remaining scaling and shift parameters, which may be further heuristically obtained by least squares, the entangled weights identify the network completely and uniquely. To highlight the relevance of the theoretical result of stable recovery of entangled weights, we present numerical experiments, which demonstrate that multilayered networks with generic weights can be robustly identified and therefore uniformly approximated by the presented algorithmic pipeline. In contrast backpropagation cannot generalize stably very well in this setting, being always limited by relatively large uniform error. In terms of practical impact, our study shows that we can relate input-output information uniquely and stably to network parameters, providing a form of explainability. Moreover, our method paves the way for compression of overparametrized networks and for the training of minimal complexity networks.","",""
3,"Arun K. Sharma, D. Singh, V. Singh, N. Verma","Aerodynamic Modeling of ATTAS Aircraft Using Mamdani Fuzzy Inference Network",2020,"","","","",177,"2022-07-13 09:26:01","","10.1109/TAES.2020.2975447","","",,,,,3,1.50,1,4,2,"This article presents aerodynamic modeling of the fixed-wing aircraft using the Mamdani fuzzy inference network (MFIN). A Mamdani fuzzy inference system with a Gaussian membership function has been used as a nonlinear regression functional node to create a multilayer network, called MFIN. The multilayered MFIN incorporates the nonlinear function approximation capability of the multilayered neural network in addition to robustness against uncertainties and measurement noises. The limited-memory Broyden–Fletcher–Goldfarb–Shanno optimization technique has been used to optimize network parameters to learn the nonlinear yawing moment dynamics of the Advanced Technology Testing Aircraft System (ATTAS) aircraft. Since every node in the network learns the nonlinearity of the dynamics, the proposed MFIN becomes capable of learning highly nonlinear dynamics. The adequacy of the proposed network is validated using the recorded flight data from the ATTAS aircraft of the DLR German Aerospace Centre in two cases: 1) trimmed low-angle-of-attack flight condition and 2) quasi-steady stall high-angle-of-attack (highly nonlinear complex) flight condition. The simulated time history tracking performance, mean square error, $R^2$ score, and explained variance score of the proposed network are compared with state-of-the-art methods. Also, the robustness of the proposed approach is demonstrated by evaluating its performance against test data corrupted with additive white Gaussian noise.","",""
1,"N. M. Thoiyab, P. Muruganantham, N. Gunasekaran","Global Robust Stability Analysis for Hybrid BAM Neural Networks",2021,"","","","",178,"2022-07-13 09:26:01","","10.1109/CMI50323.2021.9362980","","",,,,,1,1.00,0,3,1,"In this paper, we study some new sufficient criteria on global stability analysis for the hybrid bidirectional associative memory (BAM) neural networks with multiple time delays. The ultimate focus of this paper is to derive some new generalized sufficient criteria for the global asymptotic robust stability (GARS) of equilibrium point of the time-delayed BAM neural networks. The obtained sufficient conditions are always independent on the delay of system parameters of hybrid BAM neural networks. Finally, numerical example has been given to explain the effectiveness of our results in terms of network parameters.","",""
1,"S. Matveev, I. Oseledets, E. Ponomarev, A. Chertkov","Overview of Visualization Methods for Artificial Neural Networks",2021,"","","","",179,"2022-07-13 09:26:01","","10.1134/S0965542521050134","","",,,,,1,1.00,0,4,1,"","",""
1,"E. Heitz, K. Vanhoey, T. Chambon, Laurent Belcour","Pitfalls of the Gram Loss for Neural Texture Synthesis in Light of Deep Feature Histograms",2020,"","","","",180,"2022-07-13 09:26:01","","","","",,,,,1,0.50,0,4,2,"Neural texture synthesis and style transfer are both powered by the Gram matrix as a means to measure deep feature statistics. Despite its ubiquity, this second-order feature descriptor has several shortcomings resulting in visual artifacts, ill-defined interpolation, or inability to capture spatial constraints. Many previous works acknowledge these shortcomings but do not really explain why they occur. Fixing them is thus usually approached by adding new losses, which require parameter tuning and make the problem even more ill-defined, or architecturing complex and/or adversarial networks. In this paper, we propose a comprehensive study of these problems in the light of the multi-dimensional histograms of deep features. With the insights gained from our analysis, we show how to compute a well-defined and efficient textural loss based on histogram transformations. Our textural loss outperforms the Gram matrix in terms of quality, robustness, spatial control, and interpolation. It does not require additional learning or parameter tuning, and can be implemented in a few lines of code.","",""
1,"Zuzanna Klawikowska, Agnieszka Mikołajczyk, M. Grochowski","Explainable AI for Inspecting Adversarial Attacks on Deep Neural Networks",2020,"","","","",181,"2022-07-13 09:26:01","","10.1007/978-3-030-61401-0_14","","",,,,,1,0.50,0,3,2,"","",""
8,"Seyed Iman Mirzadeh, Arslan Chaudhry, Huiyi Hu, Razvan Pascanu, Dilan Gorur, Mehrdad Farajtabar","Wide Neural Networks Forget Less Catastrophically",2021,"","","","",182,"2022-07-13 09:26:01","","","","",,,,,8,8.00,1,6,1,"A growing body of research in continual learning is devoted to overcoming the “Catastrophic Forgetting” of neural networks by designing new algorithms that are more robust to the distribution shifts. While the recent progress in continual learning literature is encouraging, our understanding of what properties of neural networks contribute to catastrophic forgetting is still limited. To address this, instead of focusing on continual learning algorithms, in this work, we focus on the model itself and study the impact of “width” of the neural network architecture on catastrophic forgetting, and show that width has a surprisingly significant effect on forgetting. To explain this effect, we study the learning dynamics of the network from various perspectives such as gradient norm and sparsity, orthogonalization, and lazy training regime. We provide potential explanations that are consistent with the empirical results across different architectures and continual learning benchmarks.","",""
0,"Justin A. Goodwin, Olivia M. Brown, Victoria Helus","Fast Training of Deep Neural Networks Robust to Adversarial Perturbations",2020,"","","","",183,"2022-07-13 09:26:01","","10.1109/HPEC43674.2020.9286256","","",,,,,0,0.00,0,3,2,"Despite their promising performance, deep neural networks have shown sensitivities to perturbations of their inputs (e.g., adversarial examples) and their learned feature representations are often difficult to interpret, raising concerns about their true capability and trustworthiness. Recent work in adversarial training, a form of robust optimization in which the model is optimized against adversarial examples, demonstrates the ability to improve performance sensitivities to perturbations and yield feature representations that are more interpretable. Adversarial training, however, comes with an increased computational cost over that of standard (i.e., nonrobust) training, rendering it impractical for use in large-scale problems. Recent work suggests that a fast approximation to adversarial training shows promise for reducing training time and maintaining robustness in the presence of perturbations bounded by the infinity norm. In this work, we demonstrate that this approach extends to the Euclidean norm and preserves the human-aligned feature representations that are common for robust models. Additionally, we show that using a distributed training scheme can further reduce the time to train robust deep networks. Fast adversarial training is a promising approach that will provide increased security and explainability in machine learning applications for which robust optimization was previously thought to be impractical.","",""
0,"Alexandru-Răzvan Florea, M. Roman","Artificial neural networks applied for predicting and explaining the education level of Twitter users",2021,"","","","",184,"2022-07-13 09:26:01","","10.1007/s13278-021-00832-1","","",,,,,0,0.00,0,2,1,"","",""
352,"H. Venkatesh, W. Morishita, A. Geraghty, Dana Silverbush, Shawn M. Gillespie, M. Arzt, L. Tam, Cedric Espenel, Anitha Ponnuswami, Lijun Ni, Pamelyn J. Woo, Kathryn R. Taylor, A. Agarwal, A. Regev, D. Brang, H. Vogel, S. Hervey-Jumper, D. Bergles, M. Suvà, R. Malenka, M. Monje","Electrical and synaptic integration of glioma into neural circuits",2019,"","","","",185,"2022-07-13 09:26:01","","10.1038/s41586-019-1563-y","","",,,,,352,117.33,35,21,3,"","",""
5,"Hui Liu, Wei Kong, Tianshuang Qiu, Guo-Li Li","A Neural Network Based on Rough Set (RSNN) for Prediction of Solitary Pulmonary Nodules",2009,"","","","",186,"2022-07-13 09:26:01","","10.1109/IJCBS.2009.105","","",,,,,5,0.38,1,4,13,"Although algorithms based on rough set (RS) theory can extract useful decision rules with the effectiveness in dealing with inexact, uncertain or vague information, the deterministic mechanism for the description of error is very simple and the rules generated by RS are often unstable and have low classification accuracy. Neural networks (NN) are considered the most powerful classifier for their low classification error rates and robustness to noise. But NN usually require long time to train the huge amount of data of large databases and lack explanation facilities for their knowledge. Therefore, we combine RS and NN for autonomous decision-making, with high accuracy, robustness to noise, efficiency, and good understandability. First, generate the decision rules based on RS, then construct the NN with the hidden layer representing decision rules, and learn the arguments of the NN with BP algorithm. With the direction of RS, NN needn’t long time for training, and the knowledge buried in their structures and weights can be well explained by decision rule on RS. The proposed algorithm has been tested on a medical data set for patients with solitary pulmonary nodules (SPN).","",""
9,"Chongzhi Zhang, Aishan Liu, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing Ma, Tianlin Li","Interpreting and Improving Adversarial Robustness with Neuron Sensitivity",2019,"","","","",187,"2022-07-13 09:26:01","","","","",,,,,9,3.00,1,7,3,"Deep neural networks (DNNs) are vulnerable to adversarial examples where inputs with imperceptible perturbations mislead DNNs to incorrect results. Despite the potential risk they bring, adversarial examples are also valuable for providing insights into the weakness and blind-spots of DNNs. Thus, the interpretability of a DNN in adversarial setting aims to explain the rationale behind its decision-making process and makes deeper understanding which results in better practical applications. To address this issue, we try to explain adversarial robustness for deep models from a new perspective of neuron sensitivity which is measured by neuron behavior variation intensity against benign and adversarial examples. In this paper, we first draw the close connection between adversarial robustness and neuron sensitivities, as sensitive neurons make the most non-trivial contributions to model predictions in adversarial setting. Based on that, we further propose to improve adversarial robustness by constraining the similarities of sensitive neurons between benign and adversarial examples which stabilizes the behaviors of sensitive neurons in adversarial setting. Moreover, we demonstrate that state-of-the-art adversarial training methods improve model robustness by reducing neuron sensitivities which in turn confirms the strong connections between adversarial robustness and neuron sensitivity as well as the effectiveness of using sensitive neurons to build robust models. Extensive experiments on various datasets demonstrate that our algorithm effectively achieve excellent results.","",""
19,"B. Pandey, Tarun Jain, V. Kothari, Tarush Grover, Tata Consultancy, Services Limited","Evolutionary Modular Neural Network Approach for Breast Cancer Diagnosis",2012,"","","","",188,"2022-07-13 09:26:01","","","","",,,,,19,1.90,3,6,10,"Knowledge Discovery paradigms especially Soft Computing techniques like Artificial Neural Networks have been at the fore front of research aimed at solving the problem areas involved in many diverse fields of application. Automated diagnosis of deadly diseases is one of such fields that have seen much effort from researchers in the last few years. One area where this effort has been most felt is the diagnosis of breast cancer in women. However, development of a computationally efficient, detection-wise effective and robust framework for the diagnosis of breast cancer has still not materialized. The major problem here is the presence of a number of decision variables involved that makes this problem of diagnosis much more complex and intricate. This makes it difficult to be tackled by traditional computing paradigms efficiently. In this paper, we explain how the paradigms of modularity and optimization using evolutionary technique could be used to solve the aforesaid problem with significant success. Here, to take benefit of modularity, we make of use modular neural network instead of the traditional monolithic neural network for the recognition of input vectors implying breast cancer. Also, to make the architecture more optimal, we make use of genetic algorithms to achieve optimal connections (weights) among the neurons in each of the individual experts of the modular neural network. Experimental results show that the proposed approach has been significantly successful in dealing with aforesaid problem of breast cancer diagnosis with a training accuracy of 95.97% and testing accuracy of 96.5%. That is well above what shown by traditional approaches as described later on.","",""
0,"A. Nande, V. Dubinkina, Riccardo Ravasio, Grace H. Zhang, Gordon J. Berman","Bottlenecks, Modularity, and the Neural Control of Behavior",2022,"","","","",189,"2022-07-13 09:26:01","","10.3389/fnbeh.2022.835753","","",,,,,0,0.00,0,5,1,"In almost all animals, the transfer of information from the brain to the motor circuitry is facilitated by a relatively small number of neurons, leading to a constraint on the amount of information that can be transmitted. Our knowledge of how animals encode information through this pathway, and the consequences of this encoding, however, is limited. In this study, we use a simple feed-forward neural network to investigate the consequences of having such a bottleneck and identify aspects of the network architecture that enable robust information transfer. We are able to explain some recently observed properties of descending neurons—that they exhibit a modular pattern of connectivity and that their excitation leads to consistent alterations in behavior that are often dependent upon the desired behavioral state of the animal. Our model predicts that in the presence of an information bottleneck, such a modular structure is needed to increase the efficiency of the network and to make it more robust to perturbations. However, it does so at the cost of an increase in state-dependent effects. Despite its simplicity, our model is able to provide intuition for the trade-offs faced by the nervous system in the presence of an information processing constraint and makes predictions for future experiments.","",""
4,"S. Tan, R. Du, J. A. Perucho, S. S. Chopra, V. Vardhanabhuti, L. Lim","Dropout in Neural Networks Simulates the Paradoxical Effects of Deep Brain Stimulation on Memory",2020,"","","","",190,"2022-07-13 09:26:01","","10.3389/fnagi.2020.00273","","",,,,,4,2.00,1,6,2,"Neuromodulation techniques such as Deep Brain Stimulation (DBS) are a promising treatment for memory-related disorders including anxiety, addiction, and dementia. However, the outcome of these treatments appears to be paradoxical, as the use of these techniques can both disrupt and enhance memory even when applied to the same brain target. In this paper, we hypothesize that disruption and enhancement of memory through neuromodulation can be explained by the dropout of engram nodes. We used a convolutional neural network to classify handwritten digits and letters, applying dropout at different stages to simulate DBS effects on engrams. We showed that dropout applied during training improves the accuracy of prediction, whereas dropout applied during testing dramatically decreases accuracy of prediction, which mimics enhancement and disruption of memory, respectively. We further showed that transfer learning of neural networks with dropout had increased accuracy and rate of learning. Dropout during training provided a more robust “skeleton” network where transfer learning can be applied, mimicking the effects of chronic DBS on memory. Overall, we show that dropout of nodes can be a potential mechanism by which neuromodulation techniques such as DBS can both disrupt and enhance memory and provides a unique perspective on this paradox.","",""
4,"M. O'Brien, W. Goble, G. Hager, J. Bukowski","Dependable Neural Networks for Safety Critical Tasks",2019,"","","","",191,"2022-07-13 09:26:01","","10.1007/978-3-030-62144-5_10","","",,,,,4,1.33,1,4,3,"","",""
1,"Kshitij Dwivedi, Radoslaw Martin Cichy, G. Roig","Unravelling Representations in Scene-selective Brain Regions Using Scene Parsing Deep Neural Networks",2020,"","","","",192,"2022-07-13 09:26:01","","10.1101/2020.03.10.985309","","",,,,,1,0.50,0,3,2,"Visual scene perception is mediated by a set of cortical regions that respond preferentially to images of scenes, including the occipital place area (OPA) and parahippocampal place area (PPA). However, the differential contribution of OPA and PPA to scene perception remains an open research question. In this study, we take a deep neural network (DNN)-based computational approach to investigate the differences in OPA and PPA function. In a first step we search for a computational model that predicts fMRI responses to scenes in OPA and PPA well. We find that DNNs trained to predict scene components (e.g., wall, ceiling, floor) explain higher variance uniquely in OPA and PPA than a DNN trained to predict scene category (e.g., bathroom, kitchen, office). This result is robust across several DNN architectures. On this basis, we then determine whether particular scene components predicted by DNNs differentially account for unique variance in OPA and PPA. We find that variance in OPA responses uniquely explained by the navigation-related floor component is higher compared to the variance explained by the wall and ceiling components. In contrast, PPA responses are better explained by the combination of wall and floor, that is scene components that together contain the structure and texture of the scene. This differential sensitivity to scene components suggests differential functions of OPA and PPA in scene processing. Moreover, our results further highlight the potential of the proposed computational approach as a general tool in the investigation of the neural basis of human scene perception.","",""
31,"Q. Song, J. Spall, Y. Soh, Jie-ke Ni","Robust Neural Network Tracking Controller Using Simultaneous Perturbation Stochastic Approximation",2008,"","","","",193,"2022-07-13 09:26:01","","10.1109/TNN.2007.912315","","",,,,,31,2.21,8,4,14,"This paper considers the design of robust neural network tracking controllers for nonlinear systems. The neural network is used in the closed-loop system to estimate the nonlinear system function. We introduce the conic sector theory to establish a robust neural control system, with guaranteed boundedness for both the input/output (I/O) signals and the weights of the neural network. The neural network is trained by the simultaneous perturbation stochastic approximation (SPSA) method instead of the standard backpropagation (BP) algorithm. The proposed neural control system guarantees closed-loop stability of the estimation system, and a good tracking performance. The performance improvement of the proposed system over existing systems can be quantified in terms of preventing weight shifts, fast convergence, and robustness against system disturbance.","",""
5,"Matthew Holden, M. Pereyra, K. Zygalakis","Bayesian Imaging With Data-Driven Priors Encoded by Neural Networks: Theory, Methods, and Algorithms",2021,"","","","",194,"2022-07-13 09:26:01","","10.1137/21m1406313","","",,,,,5,5.00,2,3,1,"This paper proposes a new methodology for performing Bayesian inference in imaging inverse problems where the prior knowledge is available in the form of training data. Following the manifold hypothesis and adopting a generative modelling approach, we construct a data-driven prior that is supported on a sub-manifold of the ambient space, which we can learn from the training data by using a variational autoencoder or a generative adversarial network. We establish the existence and well-posedness of the associated posterior distribution and posterior moments under easily verifiable conditions, providing a rigorous underpinning for Bayesian estimators and uncertainty quantification analyses. Bayesian computation is performed by using a parallel tempered version of the preconditioned Crank-Nicolson algorithm on the manifold, which is shown to be ergodic and robust to the non-convex nature of these data-driven models. In addition to point estimators and uncertainty quantification analyses, we derive a model misspecification test to automatically detect situations where the data-driven prior is unreliable, and explain how to identify the dimension of the latent space directly from the training data. The proposed approach is illustrated with a range of experiments with the MNIST dataset, where it outperforms alternative image reconstruction approaches from the state of the art. A model accuracy analysis suggests that the Bayesian probabilities reported by the data-driven models are also remarkably accurate under a frequentist definition of probability.","",""
0,"Rub'en Ballester, Xavier Arnal Clemente, C. Casacuberta, M. Madadi, C. Corneanu, S. Escalera","Towards explaining the generalization gap in neural networks using topological data analysis",2022,"","","","",195,"2022-07-13 09:26:01","","10.48550/arXiv.2203.12330","","",,,,,0,0.00,0,6,1,"Understanding how neural networks generalize on unseen data is crucial for designing more robust and reliable models. In this paper, we study the generalization gap of neural networks using methods from topological data analysis. For this purpose, we compute homological persistence diagrams of weighted graphs constructed from neuron activation correlations after a training phase, aiming to capture patterns that are linked to the generalization capacity of the network. We compare the usefulness of different numerical summaries from persistence diagrams and show that a combination of some of them can accurately predict and partially explain the generalization gap without the need of a test set. Evaluation on two computer vision recognition tasks (CIFAR10 and SVHN) shows competitive generalization gap prediction when compared against state-of-the-art methods.","",""
38,"Hassan Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, Pierre-Alain Muller","Accurate and interpretable evaluation of surgical skills from kinematic data using fully convolutional neural networks",2019,"","","","",196,"2022-07-13 09:26:01","","10.1007/s11548-019-02039-4","","",,,,,38,12.67,8,5,3,"","",""
1,"GholamAbbas Fallah, M. Mousavi, M. Habiby","Using artificial neural network(ANN) for seasonal rainfall forecasting based on tele-connection patterns",2008,"","","","",197,"2022-07-13 09:26:01","","","","",,,,,1,0.07,0,3,14,"Long-term rainfall prediction is very important to counteies thriving on agro-based economy. In general, climate and rainfall are highly non-linear phenomena in nature giving rise to what is known as ""butterfly effect"". The parameters that are required to predicted rainfall are enormous even for a short period. artificial neural network is an innovative approach to construct computationally intelligent systems that are supposed to possess humanlike expertise within a specific domain, adapt themselves and learn to do better in changing environments, and explain how they make decisions. Unlike conventional artificial intelligence techniques the guiding principle of soft computing is to exploit tolerance for imprecision, uncertainty, robustness, partial truth ot achieve tractability, and better rapport with reality. rain is one of the nature’s greatest gifts and in third world countries like Iran; the entire agriculture depends upon rain. It is thus a major concern to identify any trends for rainfall to deviate from its periodicity, which would disrupt the economy of the country. This fear has been aggravated due to threat by the global warming and green house effect. The geographical configuration of Iran with the three sea, namely the Persian Gulf and Oman Sea, Caspian Sea and the Mediteranian Sea gives her a climate system with hot and cold weather seasons. The parameters that are required to predict the rainfall are enormously complex and subtle so that the uncertainty in a prediction using all these parameters even for a short period.in this paper, we analysed 33 years of rainfall datd in khorasan state, the northeastern part of Iran situated at latitude-logitude pairs (31 ̊-38 ̊N , 74 ̊80 ̊E). We attempted to train artificial neural network based on prediction models and tele-connection patterns with 33 years of rainfall data. For performance evaluation, network predicted outputs were compared with the actual rainfall data. Simulation results reveal that artificial neural network techniques are promising and efficient.","",""
0,"S. Metc, Neural Networ, M. Essawy, M. Bodruzzaman, A. Shamsi, S. Noel","C-96 / 7237 Title : Iterative Prediction of Chaotic Time Series Using a Recurrent Neural Network",2008,"","","","",198,"2022-07-13 09:26:01","","","","",,,,,0,0.00,0,6,14,"Chaotic systems are known for their unpredictability due to their sensitive dependence on initial conditions. When only time series measurements from such systems are available, neural network based models are preferred due to their simplicity, availability, and robustness. However, the type of neural network used should be capable of modeling the highly non-linear behavior and the multiattractor nature of such systems. In this paper we use a special type of recurrent neural network called the ""Dynamic System' Imitator (DSI)"", that has been proven to be capable of modeling very complex dynamic behaviors. The DSI is a fully recurrent neural network that is specially designed to model a wide variety of dynamic systems. The prediction method presented in this paper is based upon predicting one step ahead in the time series, and using that predicted value to iteratively predict the following steps. This method was applied to chaotic time series generated from the logistic, Henon, and the cubic equations, in addition to experimental pressure drop time series measured from a Fluidized Bed Reactor (FBR), which is known to exhibit chaotic behavior. The time behavior and state space attractor of the actual and network synthetic chaotic time series were analyzed and compared. The correlation dimension and the Kolmogorov entropy for both the original and network synthetic data were computed. They were found to resemble each other, confirming the success of the DSI based chaotic system modeling. J","",""
209,"F. Abdollahi, H. Talebi, Rajnikant V. Patel","A stable neural network-based observer with application to flexible-joint manipulators",2006,"","","","",199,"2022-07-13 09:26:01","","10.1109/TNN.2005.863458","","",,,,,209,13.06,70,3,16,"A stable neural network (NN)-based observer for general multivariable nonlinear systems is presented in this paper. Unlike most previous neural network observers, the proposed observer uses a nonlinear-in-parameters neural network (NLPNN). Therefore, it can be applied to systems with higher degrees of nonlinearity without any a priori knowledge about system dynamics. The learning rule for the neural network is a novel approach based on the modified backpropagation (BP) algorithm. An e-modification term is added to guarantee robustness of the observer. No strictly positive real (SPR) or any other strong assumption is imposed on the proposed approach. The stability of the recurrent neural network observer is shown by Lyapunov's direct method. Simulation results for a flexible-joint manipulator are presented to demonstrate the enhanced performance achieved by utilizing the proposed neural network observer.","",""
179,"Hwanjun Song, Minseok Kim, Dongmin Park, Jae-Gil Lee","Learning from Noisy Labels with Deep Neural Networks: A Survey",2020,"","","","",200,"2022-07-13 09:26:01","","10.1109/TNNLS.2022.3152527","","",,,,,179,89.50,45,4,2,"Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we first describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 62 state-of-the-art robust training methods, all of which are categorized into five groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we perform an in-depth analysis of noise rate estimation and summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies.","",""
