Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
65,"Jonas Rauber, Roland S. Zimmermann, M. Bethge, Wieland Brendel","Foolbox Native: Fast adversarial attacks to benchmark the robustness of machine learning models in PyTorch, TensorFlow, and JAX",2020,"","","","",1,"2022-07-13 09:22:49","","10.21105/JOSS.02607","","",,,,,65,32.50,16,4,2,"Machine learning has made enormous progress in recent years and is now being used in many real-world applications. Nevertheless, even state-of-the-art machine learning models can be fooled by small, maliciously crafted perturbations of their input data. Foolbox is a popular Python library to benchmark the robustness of machine learning models against these adversarial perturbations. It comes with a huge collection of state-of-the-art adversarial attacks to find adversarial perturbations and thanks to its framework-agnostic design it is ideally suited for comparing the robustness of many different models implemented in different frameworks. Foolbox 3 aka Foolbox Native has been rewritten from scratch to achieve native performance on models developed in PyTorch (Paszke et al., 2019), TensorFlow (Abadi et al., 2016), and JAX (Bradbury et al., 2018), all with one codebase without code duplication.","",""
7,"L. Salmaso, L. Pegoraro, R. A. Giancristofaro, R. Ceccato, Alberto Bianchi, Silvio Restello, Davide Scarabottolo","Design of experiments and machine learning to improve robustness of predictive maintenance with application to a real case study",2019,"","","","",2,"2022-07-13 09:22:49","","10.1080/03610918.2019.1656740","","",,,,,7,2.33,1,7,3,"Abstract When deploying predictive analytics in a Big Data context, some concerns may arise regarding the validity of the results obtained. The reason for this is linked to flaws which are intrinsic to the nature of the Big Data Analytics methods themselves. In this article a new approach is proposed with the aim of mitigating new problems which arise. This novel method consists of a two-step workflow in which a Design of Experiments (DOE) study is conducted prior to the usual Big Data Analytics and machine learning modeling phase. The advantages of the new approach are presented and an industrial application of the method in predictive maintenance is described in detail.","",""
10,"M. Campi, S. Garatti","Scenario optimization with relaxation: a new tool for design and application to machine learning problems",2020,"","","","",3,"2022-07-13 09:22:49","","10.1109/CDC42340.2020.9303914","","",,,,,10,5.00,5,2,2,"Scenario optimization is by now a well established technique to perform designs in the presence of uncertainty. It relies on domain knowledge integrated with first-hand information that comes from data and generates solutions that are also accompanied by precise statements of reliability. In this paper, following recent developments in [22], we venture beyond the traditional set-up of scenario optimization by analyzing the concept of constraints relaxation. By a solid theoretical underpinning, this new paradigm furnishes fundamental tools to perform designs that meet a proper compromise between robustness and performance. After suitably expanding the scope of constraints relaxation as proposed in [22], we focus on various classical Support Vector methods in machine learning – including SVM (Support Vector Machine), SVR (Support Vector Regression) and SVDD (Support Vector Data Description) – and derive new results that attest the ability of these methods to generalize.","",""
16,"B. Koçak, Ece Ates Kus, O. Kilickesmez","How to read and review papers on machine learning and artificial intelligence in radiology: a survival guide to key methodological concepts",2020,"","","","",4,"2022-07-13 09:22:49","","10.1007/s00330-020-07324-4","","",,,,,16,8.00,5,3,2,"","",""
18,"J. Badra, Fethi Khaled, Meng Tang, Y. Pei, Janardhan Kodavasal, P. Pal, Opeoluwa Owoyele, C. Fuetterer, Brenner Mattia, F. Aamir","Engine Combustion System Optimization Using Computational Fluid Dynamics and Machine Learning: A Methodological Approach",2021,"","","","",5,"2022-07-13 09:22:49","","10.1115/1.4047978","","",,,,,18,18.00,2,10,1,"  Gasoline compression ignition (GCI) engines are considered an attractive alternative to traditional spark-ignition and diesel engines. In this work, a Machine Learning-Grid Gradient Ascent (ML-GGA) approach was developed to optimize the performance of internal combustion engines. ML offers a pathway to transform complex physical processes that occur in a combustion engine into compact informational processes. The developed ML-GGA model was compared with a recently developed Machine Learning-Genetic Algorithm (ML-GA). Detailed investigations of optimization solver parameters and variable limit extension were performed in the present ML-GGA model to improve the accuracy and robustness of the optimization process. Detailed descriptions of the different procedures, optimization tools, and criteria that must be followed for a successful output are provided here. The developed ML-GGA approach was used to optimize the operating conditions (case 1) and the piston bowl design (case 2) of a heavy-duty diesel engine running on a gasoline fuel with a research octane number (RON) of 80. The ML-GGA approach yielded >2% improvements in the merit function, compared with the optimum obtained from a thorough computational fluid dynamics (CFD) guided system optimization. The predictions from the ML-GGA approach were validated with engine CFD simulations. This study demonstrates the potential of ML-GGA to significantly reduce the time needed for optimization problems, without loss in accuracy compared with traditional approaches.","",""
6,"A. Ulhaq, O. Burmeister","COVID-19 Imaging Data Privacy by Federated Learning Design: A Theoretical Framework",2020,"","","","",6,"2022-07-13 09:22:49","","","","",,,,,6,3.00,3,2,2,"To address COVID-19 healthcare challenges, we need frequent sharing of health data, knowledge and resources at a global scale. However, in this digital age, data privacy is a big concern that requires the secure embedding of privacy assurance into the design of all technological solutions that use health data. In this paper, we introduce differential privacy by design (dPbD) framework and discuss its embedding into the federated machine learning system. To limit the scope of our paper, we focus on the problem scenario of COVID-19 imaging data privacy for disease diagnosis by computer vision and deep learning approaches. We discuss the evaluation of the proposed design of federated machine learning systems and discuss how differential privacy by design (dPbD) framework can enhance data privacy in federated learning systems with scalability and robustness. We argue that scalable differentially private federated learning design is a promising solution for building a secure, private and collaborative machine learning model such as required to combat COVID19 challenge.","",""
10,"Sunkyu Yu, Xianji Piao, N. Park","Machine learning identifies scale-free properties in disordered materials",2020,"","","","",7,"2022-07-13 09:22:49","","10.1038/s41467-020-18653-9","","",,,,,10,5.00,3,3,2,"","",""
7,"Flávio Luis de Mello","A Survey on Machine Learning Adversarial Attacks",2020,"","","","",8,"2022-07-13 09:22:49","","10.17648/jisc.v7i1.76","","",,,,,7,3.50,7,1,2,"It is becoming notorious several types of adversaries based on their threat model leverage vulnerabilities to compromise a machine learning system. Therefore, it is important to provide robustness to machine learning algorithms and systems against these adversaries. However, there are only a few strong countermeasures, which can be used in all types of attack scenarios to design a robust artificial intelligence system. This paper is structured and comprehensive overview of the research on attacks to machine learning systems and it tries to call the attention from developers and software houses to the security issues concerning machine learning.","",""
189,"A. Mosavi, M. Salimi, Sina Faizollahzadeh Ardabili, T. Rabczuk, Shahaboddin Shamshirband, A. Várkonyi-Kóczy","State of the Art of Machine Learning Models in Energy Systems, a Systematic Review",2019,"","","","",9,"2022-07-13 09:22:49","","10.3390/EN12071301","","",,,,,189,63.00,32,6,3,"Machine learning (ML) models have been widely used in the modeling, design and prediction in energy systems. During the past two decades, there has been a dramatic increase in the advancement and application of various types of ML models for energy systems. This paper presents the state of the art of ML models used in energy systems along with a novel taxonomy of models and applications. Through a novel methodology, ML models are identified and further classified according to the ML modeling technique, energy type, and application area. Furthermore, a comprehensive review of the literature leads to an assessment and performance evaluation of the ML models and their applications, and a discussion of the major challenges and opportunities for prospective research. This paper further concludes that there is an outstanding rise in the accuracy, robustness, precision and generalization ability of the ML models in energy systems using hybrid ML models. Hybridization is reported to be effective in the advancement of prediction models, particularly for renewable energy systems, e.g., solar energy, wind energy, and biofuels. Moreover, the energy demand prediction using hybrid models of ML have highly contributed to the energy efficiency and therefore energy governance and sustainability.","",""
36,"Jianing Lu, Xuben Hou, Cheng Wang, Yingkai Zhang","Incorporating Explicit Water Molecules and Ligand Conformation Stability in Machine-Learning Scoring Functions",2019,"","","","",10,"2022-07-13 09:22:49","","10.1021/acs.jcim.9b00645","","",,,,,36,12.00,9,4,3,"Structure-based drug design is critically dependent on accuracy of molecular docking scoring functions, and there is of significant interest to advance scoring functions with machine learning approaches. In this work, by judiciously expanding the training set, exploring new features related to explicit mediating water molecules as well as ligand conformation stability, and apply extreme gradient boosting (XGBoost) with Δ-Vina parameterization, we have improved robustness and applicability of machine-learning scoring functions. The new scoring function ΔvinaXGB can not only perform consistently among the top compared to classical scoring functions for the CASF-2016 benchmark, but also achieves significantly better prediction accuracy in different types of structures that mimic real docking applications.","",""
34,"Sai Gokul Subraveti, Zukui Li, V. Prasad, A. Rajendran","Machine Learning-Based Multiobjective Optimization of Pressure Swing Adsorption",2019,"","","","",11,"2022-07-13 09:22:49","","10.1021/acs.iecr.9b04173","","",,,,,34,11.33,9,4,3,"The transient, cyclic nature and the flexibility in process design makes the optimization of pressure-swing adsorption (PSA) computationally intensive. Two hybrid approaches incorporating machine learning methods into the optimization routines are described. The first optimization approach uses artificial neural networks as surrogate models for function evaluations. The surrogates are constructed in the course of the initial optimization and utilized for function evaluations in subsequent optimization. In the second optimization approach, important design variables are identified to reduce the high dimensional search space to a lower dimension based on partial least squares regression. The accuracy, robustness and reliability of these approaches are demonstrated by considering a complex 8-step PSA process for pre-combustion CO2 capture as a case study. The machine learning-based optimization ∼10x reduction in computational efforts while achieving the same performance as that of the detailed models.","",""
28,"C. Geng, A. Vangone, G. Folkers, L. Xue, A. Bonvin","iSEE: Interface structure, evolution, and energy‐based machine learning predictor of binding affinity changes upon mutations",2018,"","","","",12,"2022-07-13 09:22:49","","10.1002/prot.25630","","",,,,,28,7.00,6,5,4,"Quantitative evaluation of binding affinity changes upon mutations is crucial for protein engineering and drug design. Machine learning‐based methods are gaining increasing momentum in this field. Due to the limited number of experimental data, using a small number of sensitive predictive features is vital to the generalization and robustness of such machine learning methods. Here we introduce a fast and reliable predictor of binding affinity changes upon single point mutation, based on a random forest approach. Our method, iSEE, uses a limited number of interface Structure, Evolution, and Energy‐based features for the prediction. iSEE achieves, using only 31 features, a high prediction performance with a Pearson correlation coefficient (PCC) of 0.80 and a root mean square error of 1.41 kcal/mol on a diverse training dataset consisting of 1102 mutations in 57 protein‐protein complexes. It competes with existing state‐of‐the‐art methods on two blind test datasets. Predictions for a new dataset of 487 mutations in 56 protein complexes from the recently published SKEMPI 2.0 database reveals that none of the current methods perform well (PCC < 0.42), although their combination does improve the predictions. Feature analysis for iSEE underlines the significance of evolutionary conservations for quantitative prediction of mutation effects. As an application example, we perform a full mutation scanning of the interface residues in the MDM2–p53 complex.","",""
20,"Kaustuv Datta, A. Larkoski, B. Nachman","Automating the construction of jet observables with machine learning",2019,"","","","",13,"2022-07-13 09:22:49","","10.1103/PhysRevD.100.095016","","",,,,,20,6.67,7,3,3,"Machine-learning assisted jet substructure tagging techniques have the potential to significantly improve searches for new particles and Standard Model measurements in hadronic final states. Techniques with simple analytic forms are particularly useful for establishing robustness and gaining physical insight. We introduce procedures to automate the construction of a large class of observables that are chosen to completely specify M-body phase space. The procedures are validated on the task of distinguishing H→bb¯ from g→bb¯, where M=3 and previous brute-force approaches to construct an optimal product observable for the M-body phase space have established the baseline performance. We then use the new methods to design tailored observables for the boosted Z′ search, where M=4 and brute-force methods are intractable. The new classifiers outperform standard two-prong tagging observables, illustrating the power of the new optimization method for improving searches and measurement at the LHC and beyond.","",""
29,"Alexander Wei, Fred Zhang","Optimal Robustness-Consistency Trade-offs for Learning-Augmented Online Algorithms",2020,"","","","",14,"2022-07-13 09:22:49","","","","",,,,,29,14.50,15,2,2,"We study the problem of improving the performance of online algorithms by incorporating machine-learned predictions. The goal is to design algorithms that are both consistent and robust, meaning that the algorithm performs well when predictions are accurate and maintains worst-case guarantees. Such algorithms have been studied in a recent line of works due to Lykouris and Vassilvitskii (ICML '18) and Purohit et al (NeurIPS '18). They provide robustness-consistency trade-offs for a variety of online problems. However, they leave open the question of whether these trade-offs are tight, i.e., to what extent to such trade-offs are necessary. In this paper, we provide the first set of non-trivial lower bounds for competitive analysis using machine-learned predictions. We focus on the classic problems of ski-rental and non-clairvoyant scheduling and provide optimal trade-offs in various settings.","",""
9,"J. Badra, Fethi Khaled, Meng Tang, Y. Pei, Janardhan Kodavasal, P. Pal, Opeoluwa Owoyele, C. Fuetterer, M. Brenner, A. Farooq","Engine Combustion System Optimization Using CFD and Machine Learning: A Methodological Approach",2019,"","","","",15,"2022-07-13 09:22:49","","10.1115/icef2019-7238","","",,,,,9,3.00,1,10,3,"  Gasoline compression ignition (GCI) engines are considered an attractive alternative to traditional spark-ignition and diesel engines. In this work, a Machine Learning-Grid Gradient Algorithm (ML-GGA) approach was developed to optimize the performance of internal combustion engines. Machine learning (ML) offers a pathway to transform complex physical processes that occur in a combustion engine into compact informational processes. The developed ML-GGA model was compared with a recently developed Machine learning Genetic Algorithm (ML-GA). Detailed investigations of optimization solver parameters and variables limits extension were performed in the present ML-GGA model to improve the accuracy and robustness of the optimization process. Detailed descriptions of the different procedures, optimization tools and criteria that must be followed for a successful output are provided here. The developed ML-GGA approach was used to optimize the operating conditions (case 1) and the piston bowl design (case 2) of a heavy-duty diesel engine running on a gasoline fuel with a Research Octane Number (RON) of 80. The ML-GGA approach yielded > 2% improvements in the merit function, compared to the optimum obtained from a thorough computational fluid dynamics (CFD) guided system optimization. The predictions from the ML-GGA approach were validated with engine CFD simulations. This study demonstrates the potential of ML-GGA to significantly reduce the time needed for optimization problems, without loss in accuracy compared to traditional approaches.","",""
3,"Haoyu Yang, Wen Chen, P. Pathak, Frank Gennari, Ya-Chieh Lai, Bei Yu","Automatic Layout Generation with Applications in Machine Learning Engine Evaluation",2019,"","","","",16,"2022-07-13 09:22:49","","10.1109/MLCAD48534.2019.9142121","","",,,,,3,1.00,1,6,3,"Machine learning-based lithography hotspot detection has been deeply studied recently, from varies feature extraction techniques to efficient learning models. It has been observed that such machine learning-based frameworks are providing satisfactory metal layer hotspot prediction results on known public metal layer benchmarks. In this work, we seek to evaluate how these machine learning-based hotspot detectors generalize to complicated patterns. We first introduce a automatic layout generation tool that can synthesize varies layout patterns given a set of design rules. The tool currently supports both metal layer and via layer generation. As a case study, we conduct hotspot detection on the generated via layer layouts with representative machine learning-based hotspot detectors, which shows that continuous study on model robustness and generality is necessary to prototype and integrate the learning engines in DFM flows. The source code of the layout generation tool will be available at https://github.com/phdyang007/layout-generation.","",""
6,"Manjari Pradhan, B. Bhattacharya, K. Chakrabarty, B. Bhattacharya","Predicting ${X}$ -Sensitivity of Circuit-Inputs on Test-Coverage: A Machine-Learning Approach",2019,"","","","",17,"2022-07-13 09:22:49","","10.1109/TCAD.2018.2878169","","",,,,,6,2.00,2,4,3,"Digital circuits are often prone to suffer from uncertain timing, inadequate sensor feedback, limited controllability of past states or inability of initializing memory-banks, and erroneous behavior of analog-to-digital converters, which may produce an unknown (<inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>) logic value at various circuit nodes. Additionally, many design bugs that are identified during the post-silicon validation phase manifest themselves as <inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>-values. The presence of such <inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>-sources on certain primary or secondary inputs of a logic circuit may cause loss of fault-coverage of a test set, which, in turn, may impact its reliability and robustness. In this paper, we provide a mechanism for predicting the sensitivity of <inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>-sources in terms of loss of fault-coverage, on the basis of learning only a few structural features of the circuit that are easy to extract from the netlist. We show that the <inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>-sources can be graded satisfactorily according to their sensitivity using support vector regression, thereby obviating the need for costly explicit simulation. Experimental results on several benchmark circuits demonstrate the efficacy, speed, and accuracy of prediction.","",""
23,"Zi-kui Liu","Ocean of Data: Integrating First-Principles Calculations and CALPHAD Modeling with Machine Learning",2018,"","","","",18,"2022-07-13 09:22:49","","10.1007/S11669-018-0654-Z","","",,,,,23,5.75,23,1,4,"","",""
87,"Kexin Pei, Yinzhi Cao, Junfeng Yang, S. Jana","Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems",2017,"","","","",19,"2022-07-13 09:22:49","","","","",,,,,87,17.40,22,4,5,"Due to the increasing usage of machine learning (ML) techniques in security- and safety-critical domains, such as autonomous systems and medical diagnosis, ensuring correct behavior of ML systems, especially for different corner cases, is of growing importance. In this paper, we propose a generic framework for evaluating security and robustness of ML systems using different real-world safety properties. We further design, implement and evaluate VeriVis, a scalable methodology that can verify a diverse set of safety properties for state-of-the-art computer vision systems with only blackbox access. VeriVis leverage different input space reduction techniques for efficient verification of different safety properties. VeriVis is able to find thousands of safety violations in fifteen state-of-the-art computer vision systems including ten Deep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving system with thousands of neurons as well as five commercial third-party vision APIs including Google vision and Clarifai for twelve different safety properties. Furthermore, VeriVis can successfully verify local safety properties, on average, for around 31.7% of the test images. VeriVis finds up to 64.8x more violations than existing gradient-based methods that, unlike VeriVis, cannot ensure non-existence of any violations. Finally, we show that retraining using the safety violations detected by VeriVis can reduce the average number of violations up to 60.2%.","",""
5,"Hanbin Hu, Qingran Zheng, Ya Wang, P. Li","HFMV: Hybridizing Formal Methods and Machine Learning for Verification of Analog and Mixed-Signal Circuits",2018,"","","","",20,"2022-07-13 09:22:49","","10.1145/3195970.3196059","","",,,,,5,1.25,1,4,4,"With increasing design complexity and robustness requirement, analog and mixed-signal (AMS) verification manifests itself as a key bottleneck. While formal methods and machine learning have been proposed for AMS verification, these two techniques suffer from their own limitations, with the former being specifically limited by scalability and the latter by the inherent uncertainty in learning-based models. We present a new direction in AMS verification by proposing a hybrid formal/machine-learning verification technique (HFMV) to combine the best of the two worlds. HFMV adds formalism on the top of a probabilistic learning model while providing a sense of coverage for extremely rare failure detection. HFMV intelligently and iteratively reduces uncertainty of the learning model by a proposed formally-guided active learning strategy and discovers potential rare failure regions in complex high-dimensional parameter spaces. It leads to reliable failure prediction in the case of a failing circuit, or a high-confidence pass decision in the case of a good circuit. We demonstrate that HFMV is able to employ a modest amount of data to identify hard-to-find rare failures which are completely missed by state-of-the-art sampling methods even with high volume sampling data.","",""
15,"Indrajit Mandal","Machine learning algorithms for the creation of clinical healthcare enterprise systems",2016,"","","","",21,"2022-07-13 09:22:49","","10.1080/17517575.2016.1251617","","",,,,,15,2.50,15,1,6,"ABSTRACT Clinical recommender systems are increasingly becoming popular for improving modern healthcare systems. Enterprise systems are persuasively used for creating effective nurse care plans to provide nurse training, clinical recommendations and clinical quality control. A novel design of a reliable clinical recommender system based on multiple classifier system (MCS) is implemented. A hybrid machine learning (ML) ensemble based on random subspace method and random forest is presented. The performance accuracy and robustness of proposed enterprise architecture are quantitatively estimated to be above 99% and 97%, respectively (above 95% confidence interval). The study then extends to experimental analysis of the clinical recommender system with respect to the noisy data environment. The ranking of items in nurse care plan is demonstrated using machine learning algorithms (MLAs) to overcome the drawback of the traditional association rule method. The promising experimental results are compared against the sate-of-the-art approaches to highlight the advancement in recommendation technology. The proposed recommender system is experimentally validated using five benchmark clinical data to reinforce the research findings.","",""
62,"E. Zahedinejad, J. Ghosh, B. Sanders","Designing High-Fidelity Single-Shot Three-Qubit Gates: A Machine Learning Approach",2015,"","","","",22,"2022-07-13 09:22:49","","10.1103/PHYSREVAPPLIED.6.054005","","",,,,,62,8.86,21,3,7,"Three-qubit quantum gates are key ingredients for quantum error correction and quantum information processing. We generate quantum-control procedures to design three types of three-qubit gates, namely Toffoli, Controlled-Not-Not and Fredkin gates. The design procedures are applicable to a system comprising three nearest-neighbor-coupled superconducting artificial atoms. For each three-qubit gate, the numerical simulation of the proposed scheme achieves 99.9% fidelity, which is an accepted threshold fidelity for fault-tolerant quantum computing. We test our procedure in the presence of decoherence-induced noise as well as show its robustness against random external noise generated by the control electronics. The three-qubit gates are designed via the machine learning algorithm called Subspace-Selective Self-Adaptive Differential Evolution (SuSSADE).","",""
34,"A. Sargolzaei, C. Crane, Alireza Abbaspour, S. Noei","A Machine Learning Approach for Fault Detection in Vehicular Cyber-Physical Systems",2016,"","","","",23,"2022-07-13 09:22:49","","10.1109/ICMLA.2016.0112","","",,,,,34,5.67,9,4,6,"A network of vehicular cyber-physical systems (VCPSs) can use wireless communications to interact with each other and the surrounding environment to improve transportation safety, mobility, and sustainability. However, cloud-oriented architectures are vulnerable to cyber attacks, which may endanger passenger and pedestrian safety and privacy, and cause severe property damage. For instance, a hacker can use message falsification attack to affect functionality of a particular application in a platoon of VCPSs. In this paper, a neural network-based fault detection technique is applied to detect and track fault data injection attacks on the cooperative adaptive cruise control layer of a platoon of connected vehicles in real time. A decision support system was developed to reduce the probability and severity of any consequent accident. A case study with its design specifications is demonstrated in detail. The simulation results show that the proposed method can improve system reliability, robustness, and safety.","",""
7,"Ghislain Takam Tchendjou, Rshdee Alhakim, E. Simeu, F. Lebowsky","Evaluation of machine learning algorithms for image quality assessment",2016,"","","","",24,"2022-07-13 09:22:49","","10.1109/IOLTS.2016.7604697","","",,,,,7,1.17,2,4,6,"In this article, we apply different machine learning (ML) techniques for building objective models, that permit to automatically assess the image quality in agreement with human visual perception. The six ML methods proposed are discriminant analysis, k-nearest neighbors, artificial neural network, non-linear regression, decision tree and fuzzy logic. Both the stability and the robustness of designed models are evaluated by using Monte-Carlo cross-validation approach (MCCV). The simulation results demonstrate that fuzzy logic model provides the best prediction accuracy.","",""
8,"Yusuke Sakemi, K. Morino, T. Morie, K. Aihara","A Supervised Learning Algorithm for Multilayer Spiking Neural Networks Based on Temporal Coding Toward Energy-Efficient VLSI Processor Design",2020,"","","","",25,"2022-07-13 09:22:49","","10.1109/TNNLS.2021.3095068","","",,,,,8,4.00,2,4,2,"Spiking neural networks (SNNs) are brain-inspired mathematical models with the ability to process information in the form of spikes. SNNs are expected to provide not only new machine-learning algorithms but also energy-efficient computational models when implemented in very-large-scale integration (VLSI) circuits. In this article, we propose a novel supervised learning algorithm for SNNs based on temporal coding. A spiking neuron in this algorithm is designed to facilitate analog VLSI implementations with analog resistive memory, by which ultrahigh energy efficiency can be achieved. We also propose several techniques to improve the performance on recognition tasks and show that the classification accuracy of the proposed algorithm is as high as that of the state-of-the-art temporal coding SNN algorithms on the MNIST and Fashion-MNIST datasets. Finally, we discuss the robustness of the proposed SNNs against variations that arise from the device manufacturing process and are unavoidable in analog VLSI implementation. We also propose a technique to suppress the effects of variations in the manufacturing process on the recognition performance.","",""
26,"Theja Tulabandhula, C. Rudin","Robust Optimization using Machine Learning for Uncertainty Sets",2014,"","","","",26,"2022-07-13 09:22:49","","","","",,,,,26,3.25,13,2,8,"Our goal is to build robust optimization problems for making decisions based on complex data from the past. In robust optimization (RO) generally, the goal is to create a policy for decision-making that is robust to our uncertainty about the future. In particular, we want our policy to best handle the the worst possible situation that could arise, out of an uncertainty set of possible situations. Classically, the uncertainty set is simply chosen by the user, or it might be estimated in overly simplistic ways with strong assumptions; whereas in this work, we learn the uncertainty set from data collected in the past. The past data are drawn randomly from an (unknown) possibly complicated high-dimensional distribution. We propose a new uncertainty set design and show how tools from statistical learning theory can be employed to provide probabilistic guarantees on the robustness of the policy.","",""
3,"Ihsen Alouani, Anouar Ben Khalifa, Farhad Merchant, R. Leupers","An Investigation on Inherent Robustness of Posit Data Representation",2021,"","","","",27,"2022-07-13 09:22:49","","10.1109/VLSID51830.2021.00052","","",,,,,3,3.00,1,4,1,"As the dimensions and operating voltages of computer electronics shrink to cope with consumers' demand for higher performance and lower power consumption, circuit sensitivity to soft errors increases dramatically. Recently, a new data-type is proposed in the literature called posit data type. Posit arithmetic has absolute advantages such as higher numerical accuracy, speed, and simpler hardware design than IEEE 754–2008 technical standard-compliant arithmetic. In this paper, we propose a comparative robustness study between 32-bit posit and 32-bit IEEE 754–2008 compliant representations. At first, we propose a theoretical analysis for IEEE 754 compliant numbers and posit numbers for single bit flip and double bit flips. Then, we conduct exhaustive fault injection experiments that show a considerable inherent resilience in posit format compared to classical IEEE 754 compliant representation. To show a relevant use-case of fault-tolerant applications, we perform experiments on a set of machine-learning applications. In more than 95% of the exhaustive fault injection exploration, posit representation is less impacted by faults than the IEEE 754 compliant floating-point representation. Moreover, in 100% of the tested machine-learning applications, the accuracy of posit-implemented systems is higher than the classical floating-point-based ones.","",""
47,"A. Joseph, P. Laskov, F. Roli, J. Tygar, Blaine Nelson","Machine Learning Methods for Computer Security (Dagstuhl Perspectives Workshop 12371)",2012,"","","","",28,"2022-07-13 09:22:49","","10.4230/DagMan.3.1.1","","",,,,,47,4.70,9,5,10,"The study of learning in adversarial environments is an emerging discipline at the juncture between machine learning and computer security that raises new questions within both fields.    The interest in learning-based methods for security and system design applications comes from the high degree of complexity of phenomena underlying the security and reliability of computer systems. As it becomes increasingly difficult to reach the desired properties by design alone, learning methods are being used to obtain a better understanding of various data collected from these complex systems.    However, learning approaches can be co-opted or evaded by adversaries, who change to counter them. To-date, there has been limited research into learning techniques that are resilient to attacks with provable robustness guarantees making the task of designing secure learning-based systems a lucrative open  research area with many challenges.    The Perspectives Workshop, ``Machine Learning Methods for Computer Security'' was convened to bring together interested researchers from both the computer security and machine learning communities to discuss techniques, challenges, and future research directions for secure learning and learning-based security applications.    This workshop featured twenty-two invited talks from leading researchers within the secure learning community covering topics in adversarial learning, game-theoretic learning, collective classification, privacy-preserving learning, security evaluation metrics, digital forensics, authorship identification, adversarial advertisement detection, learning for offensive security, and data sanitization. The workshop also featured workgroup sessions  organized into three topic: machine learning for computer security, secure learning, and future applications of secure learning.","",""
11,"Chuteng Zhou, Prad Kadambi, Matthew Mattina, P. Whatmough","Noisy Machines: Understanding Noisy Neural Networks and Enhancing Robustness to Analog Hardware Errors Using Distillation",2020,"","","","",29,"2022-07-13 09:22:49","","","","",,,,,11,5.50,3,4,2,"The success of deep learning has brought forth a wave of interest in computer hardware design to better meet the high demands of neural network inference. In particular, analog computing hardware has been heavily motivated specifically for accelerating neural networks, based on either electronic, optical or photonic devices, which may well achieve lower power consumption than conventional digital electronics. However, these proposed analog accelerators suffer from the intrinsic noise generated by their physical components, which makes it challenging to achieve high accuracy on deep neural networks. Hence, for successful deployment on analog accelerators, it is essential to be able to train deep neural networks to be robust to random continuous noise in the network weights, which is a somewhat new challenge in machine learning. In this paper, we advance the understanding of noisy neural networks. We outline how a noisy neural network has reduced learning capacity as a result of loss of mutual information between its input and output. To combat this, we propose using knowledge distillation combined with noise injection during training to achieve more noise robust networks, which is demonstrated experimentally across different networks and datasets, including ImageNet. Our method achieves models with as much as two times greater noise tolerance compared with the previous best attempts, which is a significant step towards making analog hardware practical for deep learning.","",""
17,"Hadian S. G. Asep, Y. Bandung","A Design of Continuous User Verification for Online Exam Proctoring on M-Learning",2019,"","","","",30,"2022-07-13 09:22:49","","10.1109/ICEEI47359.2019.8988786","","",,,,,17,5.67,9,2,3,"The use of m-learning or other remote education continue to increase due to its ability to reach people who don't have access to campus. Exams are important components of educational programs as well as on an online learning program. In an exam, a proctoring method to detect and reduce the cheating possibility is very important to ensure that the students have learned the material given. Various methods had been proposed to provide an efficient, comfortable online exam proctoring. Start with implementing an exam design with hard constraints in a no proctoring exam, a remote proctoring using a webcam, a machine based proctoring and finally research on automated online proctoring. A visual verification for the whole exam session is needed in an online exam, therefore a face verification is needed. A remaining problem in face recognition area is the system robustness for pose and lighting variations. In this paper, we proposed a method to enhance the robustness for pose and lighting variations by doing an incremental training process using the training data set obtained from m-learning online lecture sessions. As a result, the design of the proposed method is presented in this paper.","",""
15,"Qiaolin Ye, Peng Huang, Zhao Zhang, Yuhui Zheng, L. Fu, Wankou Yang","Multiview Learning With Robust Double-Sided Twin SVM.",2021,"","","","",31,"2022-07-13 09:22:49","","10.1109/TCYB.2021.3088519","","",,,,,15,15.00,3,6,1,"Multiview learning (MVL), which enhances the learners' performance by coordinating complementarity and consistency among different views, has attracted much attention. The multiview generalized eigenvalue proximal support vector machine (MvGSVM) is a recently proposed effective binary classification method, which introduces the concept of MVL into the classical generalized eigenvalue proximal support vector machine (GEPSVM). However, this approach cannot guarantee good classification performance and robustness yet. In this article, we develop multiview robust double-sided twin SVM (MvRDTSVM) with SVM-type problems, which introduces a set of double-sided constraints into the proposed model to promote classification performance. To improve the robustness of MvRDTSVM against outliers, we take L1-norm as the distance metric. Also, a fast version of MvRDTSVM (called MvFRDTSVM) is further presented. The reformulated problems are complex, and solving them are very challenging. As one of the main contributions of this article, we design two effective iterative algorithms to optimize the proposed nonconvex problems and then conduct theoretical analysis on the algorithms. The experimental results verify the effectiveness of our proposed methods.","",""
51,"Liang Tong, Bo Li, Chen Hajaj, Chaowei Xiao, Ning Zhang, Y. Vorobeychik","Improving Robustness of ML Classifiers against Realizable Evasion Attacks Using Conserved Features",2017,"","","","",32,"2022-07-13 09:22:49","","","","",,,,,51,10.20,9,6,5,"Machine learning (ML) techniques are increasingly common in security applications, such as malware and intrusion detection. However, ML models are often susceptible to evasion attacks, in which an adversary makes changes to the input (such as malware) in order to avoid being detected. A conventional approach to evaluate ML robustness to such attacks, as well as to design robust ML, is by considering simplified feature-space models of attacks, where the attacker changes ML features directly to effect evasion, while minimizing or constraining the magnitude of this change. We investigate the effectiveness of this approach to designing robust ML in the face of attacks that can be realized in actual malware (realizable attacks). We demonstrate that in the context of structure-based PDF malware detection, such techniques appear to have limited effectiveness, but they are effective with content-based detectors. In either case, we show that augmenting the feature space models with conserved features (those that cannot be unilaterally modified without compromising malicious functionality) significantly improves performance. Finally, we show that feature space models enable generalized robustness when faced with a variety of realizable attacks, as compared to classifiers which are tuned to be robust to a specific realizable attack.","",""
31,"A. Elbir, S. Coleri","Federated Learning for Channel Estimation in Conventional and RIS-Assisted Massive MIMO",2020,"","","","",33,"2022-07-13 09:22:49","","10.1109/twc.2021.3128392","","",,,,,31,15.50,16,2,2,"Machine learning (ML) has attracted a great research interest for physical layer design problems, such as channel estimation, thanks to its low complexity and robustness. Channel estimation via ML requires model training on a dataset, which usually includes the received pilot signals as input and channel data as output. In previous works, model training is mostly done via centralized learning (CL), where the whole training dataset is collected from the users at the base station (BS). This approach introduces huge communication overhead for data collection. In this paper, to address this challenge, we propose a federated learning (FL) framework for channel estimation. We design a convolutional neural network (CNN) trained on the local datasets of the users without sending them to the BS. We develop FL-based channel estimation schemes for both conventional and RIS (intelligent reflecting surface) assisted massive MIMO (multiple-input multiple-output) systems, where a single CNN is trained for two different datasets for both scenarios. We evaluate the performance for noisy and quantized model transmission and show that the proposed approach provides approximately 16 times lower overhead than CL, while maintaining satisfactory performance close to CL. Furthermore, the proposed architecture exhibits lower estimation error than the state-of-the-art ML-based schemes.","",""
6,"Wenqi Wei, Ling Liu","Robust Deep Learning Ensemble Against Deception",2020,"","","","",34,"2022-07-13 09:22:49","","10.1109/TDSC.2020.3024660","","",,,,,6,3.00,3,2,2,"Deep neural network (DNN) models are known to be vulnerable to maliciously crafted adversarial examples and to out-of-distribution inputs drawn sufficiently far away from the training data. How to protect a machine learning model against deception of both types of destructive inputs remains an open challenge. This article presents XEnsemble, a diversity ensemble verification methodology for enhancing the adversarial robustness of DNN models against deception caused by either adversarial examples or out-of-distribution inputs. XEnsemble by design has three unique capabilities. First, XEnsemble builds diverse input denoising verifiers by leveraging different data cleaning techniques. Second, XEnsemble develops a disagreement-diversity ensemble learning methodology for guarding the output of the prediction model against deception. Third, XEnsemble provides a suite of algorithms to combine input verification and output verification to protect the DNN prediction models from both adversarial examples and out of distribution inputs. Evaluated using 11 popular adversarial attacks and two representative out-of-distribution datasets, we show that XEnsemble achieves a high defense success rate against adversarial examples and a high detection success rate against out-of-distribution data inputs, and outperforms existing representative defense methods with respect to robustness and defensibility.","",""
4,"Jan Ackmann, P. Düben, T. Palmer, P. Smolarkiewicz","Machine-Learned Preconditioners for Linear Solvers in Geophysical Fluid Flows",2020,"","","","",35,"2022-07-13 09:22:49","","10.5194/EGUSPHERE-EGU21-5507","","",,,,,4,2.00,1,4,2,"It is tested whether machine learning methods can be used for preconditioning to increase the performance of the linear solver -- the backbone of the semi-implicit, grid-point model approach for weather and climate models. Embedding the machine-learning method within the framework of a linear solver circumvents potential robustness issues that machine learning approaches are often criticized for, as the linear solver ensures that a sufficient, pre-set level of accuracy is reached. The approach does not require prior availability of a conventional preconditioner and is highly flexible regarding complexity and machine learning design choices. Several machine learning methods are used to learn the optimal preconditioner for a shallow-water model with semi-implicit timestepping that is conceptually similar to more complex atmosphere models. The machine-learning preconditioner is competitive with a conventional preconditioner and provides good results even if it is used outside of the dynamical range of the training dataset.","",""
5,"Wenbo Zheng, Lan Yan, Chao Gou, Fei-Yue Wang","Fighting fire with fire: A spatial–frequency ensemble relation network with generative adversarial learning for adversarial image classification",2021,"","","","",36,"2022-07-13 09:22:49","","10.1002/int.22372","","",,,,,5,5.00,1,4,1,"Adversarial images generated by generative adversarial networks are not close to any existing benign images, and contain nonrobust features that have been identified as critical to the robustness of a machine learning model. Since adversarial images have an underlying distribution that differs from normal images, these kinds of images can offer valuable features for training a robust model. To deal with these special features, we focus on a novel machine learning task of adversarial images classification, where adversarial images can be used to investigate the problem of classifying adversarial images themselves. In the setting of this novel task, adversarial images are the ONLY kind of data used in training and testing, rather than not just a set of testing images as usual. To this end, we propose a novel spatial–frequency ensemble relation network with generative adversarial learning. First, we present a spatial–frequency ensemble representation learning to extract the feature of training images. Second, we design a meta‐learning‐based relation model to gain the relationship between images. Third, to achieve a robust model, we utilize generative adversarial learning and transform the relationship into a Jacobian matrix. Finally, we design a discriminator model that determines whether an adversarial image is from the matching category or not. Experimental results demonstrate that our approach achieves significantly higher performance compared with other state‐of‐the‐arts.","",""
10,"Lei Cui, Youyang Qu, Gang Xie, Deze Zeng, Ruidong Li, Shigen Shen, Shui Yu","Security and Privacy-Enhanced Federated Learning for Anomaly Detection in IoT Infrastructures",2021,"","","","",37,"2022-07-13 09:22:49","","10.1109/tii.2021.3107783","","",,,,,10,10.00,1,7,1,"Internet of Things (IoT) anomaly detection is significant due to its fundamental roles of securing modern critical infrastructures, such as falsified data injection detection and transmission line faults diagnostic in smart grids. Researchers have proposed various detection methods fostered by machine learning (ML) techniques. Federated learning (FL), as a promising distributed ML paradigm, has been employed recently to improve detection performance due to its advantages of privacy-preserving and lower latency. However, existing FL-based methods still suffer from efficiency, robustness, and security challenges. To address these problems, in this article, we initially introduce a blockchain-empowered decentralized and asynchronous FL framework for anomaly detection in IoT systems, which ensures data integrity and prevents single-point failure while improving the efficiency. Further, we design an improved differentially private FL based on generative adversarial nets, aiming to optimize data utility throughout the training process. To the best of our knowledge, it is the first system to employ a decentralized FL approach with privacy-preserving for IoT anomaly detection. Simulation results on the real-world dataset demonstrate the superior performance from aspects of robustness, accuracy, and fast convergence while maintaining high level of privacy and security protection.","",""
76,"Ningning Jia, E. Lam","Machine learning for inverse lithography: using stochastic gradient descent for robust photomask synthesis",2010,"","","","",38,"2022-07-13 09:22:49","","10.1088/2040-8978/12/4/045601","","",,,,,76,6.33,38,2,12,"Inverse lithography technology (ILT) synthesizes photomasks by solving an inverse imaging problem through optimization of an appropriate functional. Much effort on ILT is dedicated to deriving superior masks at a nominal process condition. However, the lower k1 factor causes the mask to be more sensitive to process variations. Robustness to major process variations, such as focus and dose variations, is desired. In this paper, we consider the focus variation as a stochastic variable, and treat the mask design as a machine learning problem. The stochastic gradient descent approach, which is a useful tool in machine learning, is adopted to train the mask design. Compared with previous work, simulation shows that the proposed algorithm is effective in producing robust masks.","",""
13,"Tenavi Nakamura-Zimmerer, Q. Gong, W. Kang","QRnet: Optimal Regulator Design With LQR-Augmented Neural Networks",2020,"","","","",39,"2022-07-13 09:22:49","","10.1109/LCSYS.2020.3034415","","",,,,,13,6.50,4,3,2,"In this letter we propose a new computational method for designing optimal regulators for high-dimensional nonlinear systems. The proposed approach leverages physics-informed machine learning to solve high-dimensional Hamilton-Jacobi-Bellman equations arising in optimal feedback control. Concretely, we augment linear quadratic regulators with neural networks to handle nonlinearities. We train the augmented models on data generated without discretizing the state space, enabling application to high-dimensional problems. We use the proposed method to design a candidate optimal regulator for an unstable Burgers’ equation, and through this example, demonstrate improved robustness and accuracy compared to existing neural network formulations.","",""
13,"Di Chai, Leye Wang, Kai Chen, Qiang Yang","FedEval: A Benchmark System with a Comprehensive Evaluation Model for Federated Learning",2020,"","","","",40,"2022-07-13 09:22:49","","","","",,,,,13,6.50,3,4,2,"As an innovative solution for privacy-preserving machine learning (ML), federated learning (FL) is attracting much attention from research and industry areas. While new technologies proposed in the past few years do evolve the FL area, unfortunately, the evaluation results presented in these works fall short in integrity and are hardly comparable because of the inconsistent evaluation metrics and the lack of a common platform. In this paper, we propose a comprehensive evaluation framework for FL systems. Specifically, we first introduce the ACTPR model, which defines five metrics that cannot be excluded in FL evaluation, including Accuracy, Communication, Time efficiency, Privacy, and Robustness. Then we design and implement a benchmarking system called FedEval, which enables the systematic evaluation and comparison of existing works under consistent experimental conditions. We then provide an in-depth benchmarking study between the two most widely-used FL mechanisms, FedSGD and FedAvg. The benchmarking results show that FedSGD and FedAvg both have advantages and disadvantages under the ACTPR model. For example, FedSGD is barely influenced by the none independent and identically distributed (non-IID) data problem, but FedAvg suffers from a decline in accuracy of up to 9% in our experiments. On the other hand, FedAvg is more efficient than FedSGD regarding time consumption and communication. Lastly, we excavate a set of take-away conclusions, which are very helpful for researchers in the FL area.","",""
13,"S. Mugunthan, T. Vijayakumar","Design of Improved Version of Sigmoidal Function with Biases for Classification Task in ELM Domain",2021,"","","","",41,"2022-07-13 09:22:49","","10.36548/JSCP.2021.2.002","","",,,,,13,13.00,7,2,1,"Extreme Learning Machine (ELM) is one of the latest trends in learning algorithm, which can provide a good recognition rate within less computation time. Therefore, the algorithm can sustain for a faster response application by utilizing a feed-forward neural network. In this research article, the ELM method has been designed with the presence of sigmoidal function of biases in the hidden nodes to perform the classification task. The classification task is very challenging with the existing learning algorithm and increased computation time due to the huge amount of dataset. While handling of the stochastic matrix for hidden layer, output provides the lower performance for learning rate and robustness in the determination. To address these issues, the modified version of ELM has been developed to obtain better accuracy and minimize the classification error. This research article includes the mathematical proof of sigmoidal activation function with biases of the hidden nodes present in the networks. The output matrix maintains the column rank in order to improve the speed of the training output weights (β). The proposed improved version of ELM leverages better accuracy and efficacy in classification and regression problems as well. Due to the inclusion of matrix Journal of Soft Computing Paradigm (JSCP) (2021) Vol.03/ No.02 Pages: 70-82 http://irojournals.com/jscp/ DOI: https://doi.org/10.36548/jscp.2021.2.002 71 ISSN: 2582-2640 (online) Submitted: 26.03.2021 Revised: 15.04.2021 Accepted: 4.05.2021 Published: 25.05.2021 column ranking in mathematical proof, the proposed improved version of ELM solves the slow training speed and over-fitting problems present in the existing learning approach.","",""
63,"Judy Hoffman, Daniel A. Roberts, Sho Yaida","Robust Learning with Jacobian Regularization",2019,"","","","",42,"2022-07-13 09:22:49","","","","",,,,,63,21.00,21,3,3,"Design of reliable systems must guarantee stability against input perturbations. In machine learning, such guarantee entails preventing overfitting and ensuring robustness of models against corruption of input data. In order to maximize stability, we analyze and develop a computationally efficient implementation of Jacobian regularization that increases classification margins of neural networks. The stabilizing effect of the Jacobian regularizer leads to significant improvements in robustness, as measured against both random and adversarial input perturbations, without severely degrading generalization properties on clean data.","",""
55,"He Fang, Xianbin Wang, L. Hanzo","Learning-Aided Physical Layer Authentication as an Intelligent Process",2018,"","","","",43,"2022-07-13 09:22:49","","10.1109/TCOMM.2018.2881117","","",,,,,55,13.75,18,3,4,"Performance of the existing physical layer authentication schemes could be severely affected by the imperfect estimates and variations of the communication link attributes used. The commonly adopted static hypothesis testing for physical layer authentication faces significant challenges in time-varying communication channels due to the changing propagation and interference conditions, which are typically unknown at the design stage. To circumvent this impediment, we propose an adaptive physical layer authentication scheme based on machine-learning as an intelligent process to learn and utilize the complex time-varying environment, and hence to improve the reliability and robustness of physical layer authentication. Explicitly, a physical layer attribute fusion model based on a kernel machine is designed for dealing with multiple attributes without requiring the knowledge of their statistical properties. By modeling the physical layer authentication as a linear system, the proposed technique directly reduces the authentication scope from a combined  $N$ -dimensional feature space to a single-dimensional (scalar) space, hence leading to reduced authentication complexity. By formulating the learning (training) objective of the physical layer authentication as a convex problem, an adaptive algorithm based on kernel least mean square is then proposed as an intelligent process to learn and track the variations of multiple attributes, and therefore to enhance the authentication performance. Both the convergence and the authentication performance of the proposed intelligent authentication process are theoretically analyzed. Our simulations demonstrate that our solution significantly improves the authentication performance in time-varying environments.","",""
36,"J. Starzyk, Haibo He","Spatio–Temporal Memories for Machine Learning: A Long-Term Memory Organization",2009,"","","","",44,"2022-07-13 09:22:49","","10.1109/TNN.2009.2012854","","",,,,,36,2.77,18,2,13,"Design of artificial neural structures capable of reliable and flexible long-term spatio-temporal memory is of paramount importance in machine intelligence. To this end, we propose a novel, biologically inspired, long-term memory (LTM) architecture. We intend to use it as a building block of a neuron-level architecture that is able to mimic natural intelligence through learning, anticipation, and goal-driven behavior. A mutual input enhancement and blocking structure is proposed, and its operation is discussed in detail. The paper focuses on a hierarchical memory organization, storage, recognition, and recall mechanisms. Simulation results of the proposed memory show its effectiveness, adaptability, and robustness. Accuracy of the proposed method is compared to other methods including Levenshtein distance method and a Markov chain.","",""
8,"A. Mitrokhin, P. Sutor, Douglas Summers-Stay, C. Fermüller, Y. Aloimonos","Symbolic Representation and Learning With Hyperdimensional Computing",2020,"","","","",45,"2022-07-13 09:22:49","","10.3389/frobt.2020.00063","","",,,,,8,4.00,2,5,2,"It has been proposed that machine learning techniques can benefit from symbolic representations and reasoning systems. We describe a method in which the two can be combined in a natural and direct way by use of hyperdimensional vectors and hyperdimensional computing. By using hashing neural networks to produce binary vector representations of images, we show how hyperdimensional vectors can be constructed such that vector-symbolic inference arises naturally out of their output. We design the Hyperdimensional Inference Layer (HIL) to facilitate this process and evaluate its performance compared to baseline hashing networks. In addition to this, we show that separate network outputs can directly be fused at the vector symbolic level within HILs to improve performance and robustness of the overall model. Furthermore, to the best of our knowledge, this is the first instance in which meaningful hyperdimensional representations of images are created on real data, while still maintaining hyperdimensionality.","",""
38,"L. T. Phong, T. Phuong","Privacy-Preserving Deep Learning via Weight Transmission",2018,"","","","",46,"2022-07-13 09:22:49","","10.1109/TIFS.2019.2911169","","",,,,,38,9.50,19,2,4,"This paper considers the scenario that multiple data owners wish to apply a machine learning method over the combined dataset of all owners to obtain the best possible learning output but do not want to share the local datasets owing to privacy concerns. We design systems for the scenario that the stochastic gradient descent (SGD) algorithm is used as the machine learning method, because SGD (or its variants) is at the heart of recent deep learning techniques over neural networks. Our systems differ from the existing systems in the following features: 1) any activation function can be used, meaning that no privacy-preserving-friendly approximation is required; 2) gradients computed by SGD are not shared but the weight parameters are shared instead; and 3) robustness against colluding parties even in the extreme case that only one honest party exists. One of our systems requires a shared symmetric key among the data owners (trainers) to ensure the secrecy of the weight parameters against a central server. We prove that our systems, while privacy preserving, achieve the same learning accuracy as SGD and, hence, retain the merit of deep learning with respect to accuracy. Finally, we conduct several experiments using benchmark datasets and show that our systems outperform the previous system in terms of learning accuracies.","",""
4,"A. Wheeldon, A. Yakovlev, R. Shafik, Jordan Morris","Low-Latency Asynchronous Logic Design for Inference at the Edge",2020,"","","","",47,"2022-07-13 09:22:49","","10.23919/DATE51398.2021.9474126","","",,,,,4,2.00,1,4,2,"Modern internet of things (IoT) devices leverage machine learning inference using sensed data on-device rather than offloading them to the cloud. Commonly known as inference at-the-edge, this gives many benefits to the users, including personalization and security. However, such applications demand high energy efficiency and robustness. In this paper we propose a method for reduced area and power overhead of self-timed early-propagative asynchronous inference circuits, designed using the principles of learning automata. Due to natural resilience to timing as well as logic underpinning, the circuits are tolerant to variations in environment and supply voltage whilst enabling the lowest possible latency. Our method is exemplified through an inference datapath for a low power machine learning application. The circuit builds on the Tsetlin machine algorithm further enhancing its energy efficiency. Average latency of the proposed circuit is reduced by 10× compared with the synchronous implementation whilst maintaining similar area. Robustness of the proposed circuit is proven through post-synthesis simulation with 0.25 V to 1.2 V supply. Functional correctness is maintained and latency scales with gate delay as voltage is decreased.","",""
8,"Yufei Han, Xiangliang Zhang","Robust Federated Training via Collaborative Machine Teaching using Trusted Instances",2019,"","","","",48,"2022-07-13 09:22:49","","","","",,,,,8,2.67,4,2,3,"Federated learning performs distributed model training using local data hosted by agents. It shares only model parameter updates for iterative aggregation at the server. Although it is privacy-preserving by design, federated learning is vulnerable to noise corruption of local agents, as demonstrated in the previous study on adversarial data poisoning threat against federated learning systems. Even a single noise-corrupted agent can bias the model training. In our work, we propose a collaborative and privacy-preserving machine teaching paradigm with multiple distributed teachers, to improve robustness of the federated training process against local data corruption. We assume that each local agent (teacher) have the resources to verify a small portions of trusted instances, which may not by itself be adequate for learning. In the proposed collaborative machine teaching method, these trusted instances guide the distributed agents to jointly select a compact while informative training subset from data hosted by their own. Simultaneously, the agents learn to add changes of limited magnitudes into the selected data instances, in order to improve the testing performances of the federally trained model despite of the training data corruption. Experiments on toy and real data demonstrate that our approach can identify training set bugs effectively and suggest appropriate changes to the labels. Our algorithm is a step toward trustworthy machine learning.","",""
128,"Abhradeep Thakurta, Adam D. Smith","Differentially Private Feature Selection via Stability Arguments, and the Robustness of the Lasso",2013,"","","","",49,"2022-07-13 09:22:49","","","","",,,,,128,14.22,64,2,9,"We design differentially private algorithms for statistical model selection. Given a data set and a large, discrete collection of “models”, each of which is a family of probability distributions, the goal is to determine the model that best “fits” the data. This is a basic problem in many areas of statistics and machine learning. We consider settings in which there is a well-defined answer, in the following sense: Suppose that there is a nonprivate model selection proceduref which is the reference to which we compare our performance. Our differentially private algorithms output the correct valuef(D) wheneverf is stable on the input data setD. We work with two notions, perturbation stability and subsampling stability. We give two classes of results: generic ones, that apply to any function with discrete output set; and specific algorithms for the problem of sparse linear regression. The algorithms we describe are efficient and in some cases match the optimal nonprivate asymptotic sample complexity. Our algorithms for sparse linear regression require analyzing the stability properties of the popular LASSO estimator. We give sufficient conditions for the LASSO estimator to be robust to small changes in the data set, and show that these conditions hold with high probability under essentially the same stochastic assumptions that are used in the literature to analyze convergence of the LASSO.","",""
83,"Evangelos Stromatias, Daniel Neil, Michael Pfeiffer, F. Galluppi, S. Furber, Shih-Chii Liu","Robustness of spiking Deep Belief Networks to noise and reduced bit precision of neuro-inspired hardware platforms",2015,"","","","",50,"2022-07-13 09:22:49","","10.3389/fnins.2015.00222","","",,,,,83,11.86,14,6,7,"Increasingly large deep learning architectures, such as Deep Belief Networks (DBNs) are the focus of current machine learning research and achieve state-of-the-art results in different domains. However, both training and execution of large-scale Deep Networks require vast computing resources, leading to high power requirements and communication overheads. The on-going work on design and construction of spike-based hardware platforms offers an alternative for running deep neural networks with significantly lower power consumption, but has to overcome hardware limitations in terms of noise and limited weight precision, as well as noise inherent in the sensor signal. This article investigates how such hardware constraints impact the performance of spiking neural network implementations of DBNs. In particular, the influence of limited bit precision during execution and training, and the impact of silicon mismatch in the synaptic weight parameters of custom hybrid VLSI implementations is studied. Furthermore, the network performance of spiking DBNs is characterized with regard to noise in the spiking input signal. Our results demonstrate that spiking DBNs can tolerate very low levels of hardware bit precision down to almost two bits, and show that their performance can be improved by at least 30% through an adapted training mechanism that takes the bit precision of the target platform into account. Spiking DBNs thus present an important use-case for large-scale hybrid analog-digital or digital neuromorphic platforms such as SpiNNaker, which can execute large but precision-constrained deep networks in real time.","",""
7,"Huy Ha, Shubham Agrawal, S. Song","Fit2Form: 3D Generative Model for Robot Gripper Form Design",2020,"","","","",51,"2022-07-13 09:22:49","","","","",,,,,7,3.50,2,3,2,"The 3D shape of a robot's end-effector plays a critical role in determining it's functionality and overall performance. Many industrial applications rely on task-specific gripper designs to ensure the system's robustness and accuracy. However, the process of manual hardware design is both costly and time-consuming, and the quality of the resulting design is dependent on the engineer's experience and domain expertise, which can easily be out-dated or inaccurate. The goal of this work is to use machine learning algorithms to automate the design of task-specific gripper fingers. We propose Fit2Form, a 3D generative design framework that generates pairs of finger shapes to maximize design objectives (i.e., grasp success, stability, and robustness) for target grasp objects. We model the design objectives by training a Fitness network to predict their values for pairs of gripper fingers and their corresponding grasp objects. This Fitness network then provides supervision to a 3D Generative network that produces a pair of 3D finger geometries for the target grasp object. Our experiments demonstrate that the proposed 3D generative design framework generates parallel jaw gripper finger shapes that achieve more stable and robust grasps compared to other general-purpose and task-specific gripper design algorithms. Video can be found at this https URL.","",""
7,"Peng Zhou, Fan Ye, Liang Du","Unsupervised Robust Multiple Kernel Learning via Extracting Local and Global Noises",2019,"","","","",52,"2022-07-13 09:22:49","","10.1109/ACCESS.2019.2904727","","",,,,,7,2.33,2,3,3,"Kernel-based clustering methods can capture the non-linear structure and identify arbitrarily shaped clusters, so they have been widely used in machine learning tasks. Since the performance of kernel methods critically depends on the choices of kernels, multiple kernel learning methods are proposed to alleviate the effort for kernel designing. The conventional multiple kernel learning methods learn a consensus kernel by linearly combining all candidate kernels, whereas ignoring the influence of the noises. To improve the robustness of multiple kernel learning methods, in this paper, we analyze the local and global noises and design regularized terms to characterize them respectively. Then, we propose a novel local and global de-noising multiple kernel learning method which can explicitly extract the local and global noises to recover the clean kernels for multiple kernel learning. After that, a block coordinate descent algorithm is presented to solve the optimization problem. Finally, the extensive experiments on benchmark data sets will demonstrate the effectiveness of the proposed algorithm.","",""
26,"Tahmina Zebin, Patricia Jane Scully, Niels Peek, A. Casson, K. Ozanyan","Design and Implementation of a Convolutional Neural Network on an Edge Computing Smartphone for Human Activity Recognition",2019,"","","","",53,"2022-07-13 09:22:49","","10.1109/ACCESS.2019.2941836","","",,,,,26,8.67,5,5,3,"Edge computing aims to integrate computing into everyday settings, enabling the system to be context-aware and private to the user. With the increasing success and popularity of deep learning methods, there is an increased demand to leverage these techniques in mobile and wearable computing scenarios. In this paper, we present an assessment of a deep human activity recognition system’s memory and execution time requirements, when implemented on a mid-range smartphone class hardware and the memory implications for embedded hardware. This paper presents the design of a convolutional neural network (CNN) in the context of human activity recognition scenario. Here, layers of CNN automate the feature learning and the influence of various hyper-parameters such as the number of filters and filter size on the performance of CNN. The proposed CNN showed increased robustness with better capability of detecting activities with temporal dependence compared to models using statistical machine learning techniques. The model obtained an accuracy of 96.4% in a five-class static and dynamic activity recognition scenario. We calculated the proposed model memory consumption and execution time requirements needed for using it on a mid-range smartphone. Per-channel quantization of weights and per-layer quantization of activation to 8-bits of precision post-training produces classification accuracy within 2% of floating-point networks for dense, convolutional neural network architecture. Almost all the size and execution time reduction in the optimized model was achieved due to weight quantization. We achieved more than four times reduction in model size when optimized to 8-bit, which ensured a feasible model capable of fast on-device inference.","",""
15,"S. Ganjefar, M. Alizadeh","A novel adaptive power system stabilizer design using the self-recurrent wavelet neural networks via adaptive learning rates",2013,"","","","",54,"2022-07-13 09:22:49","","10.1002/ETEP.1616","","",,,,,15,1.67,8,2,9,"SUMMARY    In this article, the self-recurrent wavelet neural network (SRWNN) is used as a controller in both direct and indirect adaptive control structures to damp the low-frequency power system oscillations when only the inputs and outputs of synchronous generator are accessible for measurement. The gradient descent method using adaptive learning rates (ALRs) is applied to train all weights of SRWNN. The ALRs are derived from the discrete Lyapunov stability theorem, which was applied to guarantee the convergence of the proposed control schemes. Finally, the proposed control schemes are evaluated on a single machine infinite bus power system under different operating conditions and disturbances to demonstrate their effectiveness and robustness. Copyright © 2012 John Wiley & Sons, Ltd.","",""
187,"Hamed Masnadi-Shirazi, N. Vasconcelos","On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost",2008,"","","","",55,"2022-07-13 09:22:49","","","","",,,,,187,13.36,94,2,14,"The machine learning problem of classifier design is studied from the perspective of probability elicitation, in statistics. This shows that the standard approach of proceeding from the specification of a loss, to the minimization of conditional risk is overly restrictive. It is shown that a better alternative is to start from the specification of a functional form for the minimum conditional risk, and derive the loss function. This has various consequences of practical interest, such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary, and 2) many new losses can be derived for classification problems. These points are illustrated by the derivation of a new loss which is not convex, but does not compromise the computational tractability of classifier design, and is robust to the contamination of data with outliers. A new boosting algorithm, SavageBoost, is derived for the minimization of this loss. Experimental results show that it is indeed less sensitive to outliers than conventional methods, such as Ada, Real, or LogitBoost, and converges in fewer iterations.","",""
21,"Manasi A. Pethe, Aliza B. Rubenstein, S. Khare","Data-driven supervised learning of a viral protease specificity landscape from deep sequencing and molecular simulations",2018,"","","","",56,"2022-07-13 09:22:49","","10.1073/pnas.1805256116","","",,,,,21,5.25,7,3,4,"Significance Substrate specificity landscape of a protease enzyme is the set of all substrate sequences that are recognized/cut (and, as importantly, not recognized/cut) by the enzyme. Accurate and rapid elucidation of these landscapes for any given protease is key for the design of novel targeted proteases to prevent unwarranted off-target cleavage, and provides insight into the functional robustness of naturally occurring proteases. We developed a structure-guided approach for predicting protease substrate specificity landscapes, in which data from experiments in yeast and molecular simulations are combined using machine learning. Using this approach, we comprehensively map the sequence−energetics−function landscape of the hepatitis C virus NS3/4A protease and its drug-resistant variants. Biophysical interactions between proteins and peptides are key determinants of molecular recognition specificity landscapes. However, an understanding of how molecular structure and residue-level energetics at protein−peptide interfaces shape these landscapes remains elusive. We combine information from yeast-based library screening, next-generation sequencing, and structure-based modeling in a supervised machine learning approach to report the comprehensive sequence−energetics−function mapping of the specificity landscape of the hepatitis C virus (HCV) NS3/4A protease, whose function—site-specific cleavages of the viral polyprotein—is a key determinant of viral fitness. We screened a library of substrates in which five residue positions were randomized and measured cleavability of ∼30,000 substrates (∼1% of the library) using yeast display and fluorescence-activated cell sorting followed by deep sequencing. Structure-based models of a subset of experimentally derived sequences were used in a supervised learning procedure to train a support vector machine to predict the cleavability of 3.2 million substrate variants by the HCV protease. The resulting landscape allows identification of previously unidentified HCV protease substrates, and graph-theoretic analyses reveal extensive clustering of cleavable and uncleavable motifs in sequence space. Specificity landscapes of known drug-resistant variants are similarly clustered. The described approach should enable the elucidation and redesign of specificity landscapes of a wide variety of proteases, including human-origin enzymes. Our results also suggest a possible role for residue-level energetics in shaping plateau-like functional landscapes predicted from viral quasispecies theory.","",""
15,"D. Linkens, J. Shieh, J. Peacock","Machine-learning rule-based fuzzy logic control for depth of anaesthesia",1994,"","","","",57,"2022-07-13 09:22:49","","10.1049/CP:19940104","","",,,,,15,0.54,5,3,28,"A machine-learning rule-based fuzzy logic controller for depth of anaesthesia which is similar to the way an anaesthetist works is presented in this paper. The results of discussions with anaesthetists to obtain a rule base and the application of fuzzy logic to predict the primary depth of anaesthesia (PDOA) and to control drug administration are very promising. By using simple rules from machine learning trials, similar results for the prediction of PDOA were obtained and can be used to design a drug infusion controller. The robustness of the self-organising fuzzy logic control (SOFLC) algorithm is good and can supplement the anaesthetist's experience for administering drug to patients when the system is dynamic and time-varying. Using these results, the design of a hierarchical architecture for the determination of the level of depth of anaesthesia is being investigated, which will include the use of clinical signs and refinements in the control of drug administered to patients. >","",""
11,"V. Bulitko, Lihong Li, Greg Lee, R. Greiner, I. Levner","Adaptive Image Interpretation : A Spectrum Of Machine Learning Problems",2003,"","","","",58,"2022-07-13 09:22:49","","","","",,,,,11,0.58,2,5,19,"Automated image interpretation is an important task in numerous applications ranging from security systems to natural resource inventorization based on remote-sensing. Recently, a second generation of adaptive machine-learned image interpretation systems have shown expertlevel performance in several challenging domains. While demonstrating an unprecedented improvement over hand-engineered or first generation machine learned systems in terms of cross-domain portability, design cycle time, and robustness, such systems are still severely limited. In this paper we inspect the anatomy of a state-of-the-art adaptive image interpretation system and discuss the range of the corresponding machine learning problems. We then report on the novel machine learning approaches engaged and the resulting improvements.","",""
239,"F. Lotte, F. Larrue, C. Mühl","Flaws in current human training protocols for spontaneous Brain-Computer Interfaces: lessons learned from instructional design",2013,"","","","",59,"2022-07-13 09:22:49","","10.3389/fnhum.2013.00568","","",,,,,239,26.56,80,3,9,"While recent research on Brain-Computer Interfaces (BCI) has highlighted their potential for many applications, they remain barely used outside laboratories. The main reason is their lack of robustness. Indeed, with current BCI, mental state recognition is usually slow and often incorrect. Spontaneous BCI (i.e., mental imagery-based BCI) often rely on mutual learning efforts by the user and the machine, with BCI users learning to produce stable ElectroEncephaloGraphy (EEG) patterns (spontaneous BCI control being widely acknowledged as a skill) while the computer learns to automatically recognize these EEG patterns, using signal processing. Most research so far was focused on signal processing, mostly neglecting the human in the loop. However, how well the user masters the BCI skill is also a key element explaining BCI robustness. Indeed, if the user is not able to produce stable and distinct EEG patterns, then no signal processing algorithm would be able to recognize them. Unfortunately, despite the importance of BCI training protocols, they have been scarcely studied so far, and used mostly unchanged for years. In this paper, we advocate that current human training approaches for spontaneous BCI are most likely inappropriate. We notably study instructional design literature in order to identify the key requirements and guidelines for a successful training procedure that promotes a good and efficient skill learning. This literature study highlights that current spontaneous BCI user training procedures satisfy very few of these requirements and hence are likely to be suboptimal. We therefore identify the flaws in BCI training protocols according to instructional design principles, at several levels: in the instructions provided to the user, in the tasks he/she has to perform, and in the feedback provided. For each level, we propose new research directions that are theoretically expected to address some of these flaws and to help users learn the BCI skill more efficiently.","",""
50,"M. Musil, Hannes Konegger, J. Hon, D. Bednář, J. Damborský","Computational Design of Stable and Soluble Biocatalysts",2019,"","","","",60,"2022-07-13 09:22:49","","10.1021/ACSCATAL.8B03613","","",,,,,50,16.67,10,5,3,"Natural enzymes are delicate biomolecules possessing only marginal thermodynamic stability. Poorly stable, misfolded, and aggregated proteins lead to huge economic losses in the biotechnology and biopharmaceutical industries. Consequently, there is a need to design optimized protein sequences that maximize stability, solubility, and activity over a wide range of temperatures and pH values in buffers of different composition and in the presence of organic cosolvents. This has created great interest in using computational methods to enhance biocatalysts’ robustness and solubility. Suitable methods include (i) energy calculations, (ii) machine learning, (iii) phylogenetic analyses, and (iv) combinations of these approaches. We have witnessed impressive progress in the design of stable enzymes over the last two decades, but predictions of protein solubility and expressibility are scarce. Stabilizing mutations can be predicted accurately using available force fields, and the number of sequences available for p...","",""
10,"L. T. Phong, T. Phuong","Privacy-Preserving Deep Learning for any Activation Function",2018,"","","","",61,"2022-07-13 09:22:49","","10.1109/TIFS.2019.2911169","","",,,,,10,2.50,5,2,4,"This paper considers the scenario that multiple data owners wish to apply a machine learning method over the combined dataset of all owners to obtain the best possible learning output but do not want to share the local datasets owing to privacy concerns. We design systems for the scenario that the stochastic gradient descent (SGD) algorithm is used as the machine learning method because SGD (or its variants) is at the heart of recent deep learning techniques over neural networks. Our systems differ from existing systems in the following features: {\bf (1)} any activation function can be used, meaning that no privacy-preserving-friendly approximation is required; {\bf (2)} gradients computed by SGD are not shared but the weight parameters are shared instead; and {\bf (3)} robustness against colluding parties even in the extreme case that only one honest party exists. We prove that our systems, while privacy-preserving, achieve the same learning accuracy as SGD and hence retain the merit of deep learning with respect to accuracy. Finally, we conduct several experiments using benchmark datasets, and show that our systems outperform previous system in terms of learning accuracies.","",""
191,"Ilias Diakonikolas, Gautam Kamath, D. Kane, Jerry Li, J. Steinhardt, Alistair Stewart","Sever: A Robust Meta-Algorithm for Stochastic Optimization",2018,"","","","",62,"2022-07-13 09:22:49","","","","",,,,,191,47.75,32,6,4,"In high dimensions, most machine learning methods are brittle to even a small fraction of structured outliers. To address this, we introduce a new meta-algorithm that can take in a base learner such as least squares or stochastic gradient descent, and harden the learner to be resistant to outliers. Our method, Sever, possesses strong theoretical guarantees yet is also highly scalable -- beyond running the base learner itself, it only requires computing the top singular vector of a certain $n \times d$ matrix. We apply Sever on a drug design dataset and a spam classification dataset, and find that in both cases it has substantially greater robustness than several baselines. On the spam dataset, with $1\%$ corruptions, we achieved $7.4\%$ test error, compared to $13.4\%-20.5\%$ for the baselines, and $3\%$ error on the uncorrupted dataset. Similarly, on the drug design dataset, with $10\%$ corruptions, we achieved $1.42$ mean-squared error test error, compared to $1.51$-$2.33$ for the baselines, and $1.23$ error on the uncorrupted dataset.","",""
16,"Hadi Salman, Andrew Ilyas, Logan Engstrom, Sai Vemprala, A. Madry, Ashish Kapoor","Unadversarial Examples: Designing Objects for Robust Vision",2020,"","","","",63,"2022-07-13 09:22:49","","","","",,,,,16,8.00,3,6,2,"We study a class of realistic computer vision settings wherein one can inﬂuence the design of the objects being recognized. We develop a framework that leverages this capability to signiﬁcantly improve vision models’ performance and robustness. This framework exploits the sensitivity of modern machine learning algorithms to input perturbations in order to design “robust objects,” i.e., objects that are explicitly optimized to be conﬁdently detected or classiﬁed. We demonstrate the efﬁcacy of the framework on a wide variety of vision-based tasks ranging from standard benchmarks, to (in-simulation) robotics, to real-world experiments. Our code can be found at https://git.io/unadversarial .","",""
41,"R. Jadrich, B. A. Lindquist, Thomas M Truskett","Probabilistic inverse design for self-assembling materials",2017,"","","","",64,"2022-07-13 09:22:49","","10.1063/1.4981796","","",,,,,41,8.20,14,3,5,"One emerging approach for the fabrication of complex architectures on the nanoscale is to utilize particles customized to intrinsically self-assemble into a desired structure. Inverse methods of statistical mechanics have proven particularly effective for the discovery of interparticle interactions suitable for this aim. Here we evaluate the generality and robustness of a recently introduced inverse design strategy [B. A. Lindquist et al., J. Chem. Phys. 145, 111101 (2016)] by applying this simulation-based machine learning method to optimize for interparticle interactions that self-assemble particles into a variety of complex microstructures as follows: cluster fluids, porous mesophases, and crystalline lattices. Using the method, we discover isotropic pair interactions that lead to the self-assembly of each of the desired morphologies, including several types of potentials that were not previously understood to be capable of stabilizing such systems. One such pair potential led to the assembly of the hi...","",""
13,"Adam D. Smith","Differentially Private Model Selection via Stability Arguments and the Robustness of the Lasso",2013,"","","","",65,"2022-07-13 09:22:49","","","","",,,,,13,1.44,13,1,9,"We design differentially private algorithms for statistical model selection. Given a data set and a large, discrete collection of “models”, each of which is a family of probability distributions, the goal is to determine the model that best “fits” the data. This is a basic problem in many areas of statistics and machine learning. We consider settings in which there is a well-defined answer, in the following sense: Suppose that there is a nonprivate model selection procedure f which is the reference to which we compare our performance. Our differentially private algorithms output the correct value f(D) whenever f is stable on the input data set D. We work with two notions, perturbation stability and subsampling stability. We give two classes of results: generic ones, that apply to any function with discrete output set; and specific algorithms for the problem of sparse linear regression. The algorithms we describe are efficient and in some cases match the optimal nonprivate asymptotic sample complexity. Our algorithms for sparse linear regression require analyzing the stability properties of the popular LASSO estimator. We give sufficient conditions for the LASSO estimator to be robust to small changes in the data set, and show that these conditions hold with high probability under essentially the same stochastic assumptions that are used in the literature to analyze convergence of the LASSO.","",""
78,"J. Gillula, C. Tomlin","Guaranteed Safe Online Learning via Reachability: tracking a ground target using a quadrotor",2012,"","","","",66,"2022-07-13 09:22:49","","10.1109/ICRA.2012.6225136","","",,,,,78,7.80,39,2,10,"While machine learning techniques have become popular tools in the design of autonomous systems, the asymptotic nature of their performance guarantees means that they should not be used in scenarios in which safety and robustness are critical for success. By pairing machine learning algorithms with rigorous safety analyses, such as Hamilton-Jacobi-Isaacs (HJI) reachability, this limitation can be overcome. Guaranteed Safe Online Learning via Reachability (GSOLR) is a framework which combines HJI reachability with general machine learning techniques, allowing for the design of robotic systems which demonstrate both high performance and guaranteed safety. In this paper we show how the GSOLR framework can be applied to a target tracking problem, in which an observing quadrotor helicopter must keep a target ground vehicle with unknown (but bounded) dynamics inside its field of view at all times, while simultaneously attempting to build a motion model of the target. The resulting algorithm was implemented on board the Stanford Testbed of Autonomous Rotorcraft for Multi-Agent Control, and was compared to a naive safety-only algorithm and a learning-only algorithm. Experimental results illustrate the success of the GSOLR algorithm, even under scenarios in which the machine learning algorithm performed poorly (and would otherwise lead to unsafe actions), thus demonstrating the power of this technique.","",""
11,"A. Hanuka, X. Huang, J. Shtalenkova, D. Kennedy, A. Edelen, V. Lalchand, D. Ratner, J. Duris","Physics-informed Gaussian Process for Online Optimization of Particle Accelerators",2020,"","","","",67,"2022-07-13 09:22:49","","10.1103/PhysRevAccelBeams.24.072802","","",,,,,11,5.50,1,8,2,"High-dimensional optimization is a critical challenge for operating large-scale scientific facilities. We apply a physics-informed Gaussian process (GP) optimizer to tune a complex system by conducting efficient global search. Typical GP models learn from past observations to make predictions, but this reduces their applicability to new systems where archive data is not available. Instead, here we use a fast approximate model from physics simulations to design the GP model. The GP is then employed to make inferences from sequential online observations in order to optimize the system. Simulation and experimental studies were carried out to demonstrate the method for online control of a storage ring. We show that the physics-informed GP outperforms current routinely used online optimizers in terms of convergence speed, and robustness on this task. The ability to inform the machine-learning model with physics may have wide applications in science.","",""
7,"D. Winklehner, J. Conrad, D. Schoen, M. Yampolskaya, A. Adelmann, Sonali Mayani, Sriramkrishnan Muralikrishnan","Order-of-magnitude beam current improvement in compact cyclotrons",2021,"","","","",68,"2022-07-13 09:22:49","","10.1088/1367-2630/ac5001","","",,,,,7,7.00,1,7,1,"There is great need for high intensity proton beams from compact particle accelerators in particle physics, medical isotope production, and materials- and energy-research. To address this need, we present, for the first time, a design for a compact isochronous cyclotron that will be able to deliver 10 mA of 60 MeV protons—an order of magnitude higher than on-market compact cyclotrons and a factor four higher than research machines. A key breakthrough is that vortex motion is incorporated in the design of a cyclotron, leading to clean extraction. Beam losses on the septa of the electrostatic extraction channels stay below 120 W (40% below the required safety limit), while maintaining good beam quality. We present a set of highly accurate particle-in-cell simulations, and an uncertainty quantification of select beam input parameters using machine learning, showing the robustness of the design. This design can be utilized for beams for experiments in particle and nuclear physics, materials science and medical physics as well as for industrial applications.","",""
28,"Qinglong Wang, Wenbo Guo, Kaixuan Zhang, Alexander Ororbia, Xinyu Xing, C. Lee Giles, Xue Liu","Learning Adversary-Resistant Deep Neural Networks",2016,"","","","",69,"2022-07-13 09:22:49","","","","",,,,,28,4.67,4,7,6,"Deep neural networks (DNNs) have proven to be quite effective in a vast array of machine learning tasks, with recent examples in cyber security and autonomous vehicles. Despite the superior performance of DNNs in these applications, it has been recently shown that these models are susceptible to a particular type of attack that exploits a fundamental flaw in their design. This attack consists of generating particular synthetic examples referred to as adversarial samples. These samples are constructed by slightly manipulating real data-points in order to ""fool"" the original DNN model, forcing it to mis-classify previously correctly classified samples with high confidence. Addressing this flaw in the model is essential if DNNs are to be used in critical applications such as those in cyber security.  Previous work has provided various learning algorithms to enhance the robustness of DNN models, and they all fall into the tactic of ""security through obscurity"". This means security can be guaranteed only if one can obscure the learning algorithms from adversaries. Once the learning technique is disclosed, DNNs protected by these defense mechanisms are still susceptible to adversarial samples. In this work, we investigate this issue shared across previous research work and propose a generic approach to escalate a DNN's resistance to adversarial samples. More specifically, our approach integrates a data transformation module with a DNN, making it robust even if we reveal the underlying learning algorithm. To demonstrate the generality of our proposed approach and its potential for handling cyber security applications, we evaluate our method and several other existing solutions on datasets publicly available. Our results indicate that our approach typically provides superior classification performance and resistance in comparison with state-of-art solutions.","",""
12,"R. Pflugfelder","Siamese Learning Visual Tracking: A Survey",2017,"","","","",70,"2022-07-13 09:22:49","","","","",,,,,12,2.40,12,1,5,"The aim of this survey is an attempt to review the kind of machine learning and stochastic techniques and the ways existing work currently uses machine learning and stochastic methods for the challenging problem of visual tracking. It is not intended to study the whole tracking literature of the last decades as this seems impossible by the incredible vast number of published papers. This first draft version of the article focuses very targeted on recent literature that suggests Siamese networks for the learning of tracking. This approach promise a step forward in terms of robustness, accuracy and computational efficiency. For example, the representative tracker SINT performs currently best on the popular OTB-2013 benchmark with AuC/IoU/prec. 65.5/62.5/84.8 % for the one-pass experiment (OPE). The CVPR'17 work CVNet by the Oxford group shows the approach's large potential of HW/SW co-design with network memory needs around 600 kB and frame-rates of 75 fps and beyond. Before a detailed description of this approach is given, the article recaps the definition of tracking, the current state-of-the-art view on designing algorithms and the state-of-the-art of trackers by summarising insights from existing literature. In future, the article will be extended by the review of two alternative approaches, the one using very general recurrent networks such as the Long Shortterm Memory (LSTM) networks and the other most obvious approach of applying sole convolutional networks (CNN), the earliest approach since the idea of deep learning tracking appeared at NIPS'13.","",""
176,"Q. Mao, Melissa Jay, J. Hoffman, J. Calvert, C. Barton, David Shimabukuro, L. Shieh, U. Chettipally, Grant S. Fletcher, Yaniv Kerem, Yifan Zhou, R. Das","Multicentre validation of a sepsis prediction algorithm using only vital sign data in the emergency department, general ward and ICU",2018,"","","","",71,"2022-07-13 09:22:49","","10.1136/bmjopen-2017-017833","","",,,,,176,44.00,18,12,4,"Objectives We validate a machine learning-based sepsis-prediction algorithm (InSight) for the detection and prediction of three sepsis-related gold standards, using only six vital signs. We evaluate robustness to missing data, customisation to site-specific data using transfer learning and generalisability to new settings. Design A machine-learning algorithm with gradient tree boosting. Features for prediction were created from combinations of six vital sign measurements and their changes over time. Setting A mixed-ward retrospective dataset from the University of California, San Francisco (UCSF) Medical Center (San Francisco, California, USA) as the primary source, an intensive care unit dataset from the Beth Israel Deaconess Medical Center (Boston, Massachusetts, USA) as a transfer-learning source and four additional institutions’ datasets to evaluate generalisability. Participants 684 443 total encounters, with 90 353 encounters from June 2011 to March 2016 at UCSF. Interventions None. Primary and secondary outcome measures Area under the receiver operating characteristic (AUROC) curve for detection and prediction of sepsis, severe sepsis and septic shock. Results For detection of sepsis and severe sepsis, InSight achieves an AUROC curve of 0.92 (95% CI 0.90 to 0.93) and 0.87 (95% CI 0.86 to 0.88), respectively. Four hours before onset, InSight predicts septic shock with an AUROC of 0.96 (95% CI 0.94 to 0.98) and severe sepsis with an AUROC of 0.85 (95% CI 0.79 to 0.91). Conclusions InSight outperforms existing sepsis scoring systems in identifying and predicting sepsis, severe sepsis and septic shock. This is the first sepsis screening system to exceed an AUROC of 0.90 using only vital sign inputs. InSight is robust to missing data, can be customised to novel hospital data using a small fraction of site data and retains strong discrimination across all institutions.","",""
17,"Chen Dan, Yuting Wei, Pradeep Ravikumar","Sharp Statistical Guarantees for Adversarially Robust Gaussian Classification",2020,"","","","",72,"2022-07-13 09:22:49","","","","",,,,,17,8.50,6,3,2,"Adversarial robustness has become a fundamental requirement in modern machine learning applications. Yet, there has been surprisingly little statistical understanding so far. In this paper, we provide the first result of the optimal minimax guarantees for the excess risk for adversarially robust classification, under Gaussian mixture model proposed by \cite{schmidt2018adversarially}. The results are stated in terms of the Adversarial Signal-to-Noise Ratio (AdvSNR), which generalizes a similar notion for standard linear classification to the adversarial setting. For the Gaussian mixtures with AdvSNR value of $r$, we establish an excess risk lower bound of order $\Theta(e^{-(\frac{1}{8}+o(1)) r^2} \frac{d}{n})$ and design a computationally efficient estimator that achieves this optimal rate. Our results built upon minimal set of assumptions while cover a wide spectrum of adversarial perturbations including $\ell_p$ balls for any $p \ge 1$.","",""
15,"J. Zhang, Yuhang Wang","An ensemble method to improve prediction of earthquake-induced soil liquefaction: a multi-dataset study",2020,"","","","",73,"2022-07-13 09:22:49","","10.1007/s00521-020-05084-2","","",,,,,15,7.50,8,2,2,"","",""
17,"R. Maulik, Bethany Lusch, Prasanna Balaprakash","Non-autoregressive time-series methods for stable parametric reduced-order models",2020,"","","","",74,"2022-07-13 09:22:49","","10.1063/5.0019884","","",,,,,17,8.50,6,3,2,"Advection-dominated dynamical systems, characterized by partial differential equations, are found in applications ranging from weather forecasting to engineering design where accuracy and robustness are crucial. There has been significant interest in the use of techniques borrowed from machine learning to reduce the computational expense and/or improve the accuracy of predictions for these systems. These rely on the identification of a basis that reduces the dimensionality of the problem and the subsequent use of time series and sequential learning methods to forecast the evolution of the reduced state. Often, however, machine-learned predictions after reduced-basis projection are plagued by issues of stability stemming from incomplete capture of multiscale processes as well as due to error growth for long forecast durations. To address these issues, we have developed a non-autoregressive time series approach for predicting linear reduced-basis time histories of forward models. In particular, we demonstrate that non-autoregressive counterparts of sequential learning methods such as long short-term memory (LSTM) considerably improve the stability of machine-learned reduced-order models. We evaluate our approach on the inviscid shallow water equations and show that a non-autoregressive variant of the standard LSTM approach that is bidirectional in the principal component directions obtains the best accuracy for recreating the nonlinear dynamics of partial observations. Moreover—and critical for many applications of these surrogates—inference times are reduced by three orders of magnitude using our approach, compared with both the equation-based Galerkin projection method and the standard LSTM approach.","",""
126,"A. Rahmani, Bryan Donyanavard, T. Mück, Kasra Moazzemi, A. Jantsch, O. Mutlu, N. Dutt","SPECTR",2018,"","","","",75,"2022-07-13 09:22:49","","10.1145/3296957.3173199","","",,,,,126,31.50,18,7,4,"Resource management strategies for many-core systems need to enable sharing of resources such as power, processing cores, and memory bandwidth while coordinating the priority and significance of system- and application-level objectives at runtime in a scalable and robust manner. State-of-the-art approaches use heuristics or machine learning for resource management, but unfortunately lack formalism in providing robustness against unexpected corner cases. While recent efforts deploy classical control-theoretic approaches with some guarantees and formalism, they lack scalability and autonomy to meet changing runtime goals. We present SPECTR, a new resource management approach for many-core systems that leverages formal supervisory control theory (SCT) to combine the strengths of classical control theory with state-of-the-art heuristic approaches to efficiently meet changing runtime goals. SPECTR is a scalable and robust control architecture and a systematic design flow for hierarchical control of many-core systems. SPECTR leverages SCT techniques such as gain scheduling to allow autonomy for individual controllers. It facilitates automatic synthesis of the high-level supervisory controller and its property verification. We implement SPECTR on an Exynos platform containing ARM»s big.LITTLE-based heterogeneous multi-processor (HMP) and demonstrate that SPECTR»s use of SCT is key to managing multiple interacting resources (e.g., chip power and processing cores) in the presence of competing objectives (e.g., satisfying QoS vs. power capping). The principles of SPECTR are easily applicable to any resource type and objective as long as the management problem can be modeled using dynamical systems theory (e.g., difference equations), discrete-event dynamic systems, or fuzzy dynamics.","",""
12,"S. Ali, Rajbir Kaur, D. Persis, Raiswa Saha, Murugan Pattusamy, V. Sreedharan","Developing a hybrid evaluation approach for the low carbon performance on sustainable manufacturing environment",2020,"","","","",76,"2022-07-13 09:22:49","","10.1007/s10479-020-03877-1","","",,,,,12,6.00,2,6,2,"","",""
10,"Kang Liu, Benjamin Tan, R. Karri, S. Garg","Poisoning the (Data) Well in ML-Based CAD: A Case Study of Hiding Lithographic Hotspots",2020,"","","","",77,"2022-07-13 09:22:49","","10.23919/DATE48585.2020.9116489","","",,,,,10,5.00,3,4,2,"Machine learning (ML) provides state-of-the-art performance in many parts of computer-aided design (CAD) flows. However, deep neural networks (DNNs) are susceptible to various adversarial attacks, including data poisoning to compromise training to insert backdoors. Sensitivity to training data integrity presents a security vulnerability, especially in light of malicious insiders who want to cause targeted neural network misbehavior. In this study, we explore this threat in lithographic hotspot detection via training data poisoning, where hotspots in a layout clip can be ""hidden"" at inference time by including a trigger shape in the input. We show that training data poisoning attacks are feasible and stealthy, demonstrating a backdoored neural network that performs normally on clean inputs but misbehaves on inputs when a backdoor trigger is present. Furthermore, our results raise some fundamental questions about the robustness of ML-based systems in CAD.","",""
92,"L. Dery, B. Nachman, F. Rubbo, A. Schwartzman","Weakly supervised classification in high energy physics",2017,"","","","",78,"2022-07-13 09:22:49","","10.1007/JHEP05(2017)145","","",,,,,92,18.40,23,4,5,"","",""
11,"J. Chen, Mohamed Baker Alawieh, Yibo Lin, Maolin Zhang, Jun Zhang, Yufeng Guo, D. Pan","Powernet: SOI Lateral Power Device Breakdown Prediction With Deep Neural Networks",2020,"","","","",79,"2022-07-13 09:22:49","","10.1109/ACCESS.2020.2970966","","",,,,,11,5.50,2,7,2,"The breakdown performance is a critical metric for power device design. This paper explores the feasibility of efficiently predicting the breakdown performance of silicon on insulator (SOI) lateral power device using multi-layer neural networks as an alternative to expensive technology computer-aided design (TCAD) simulation. In this work, we propose the first breakdown performance prediction framework, PowerNet, for SOI lateral power devices, based on deep learning methods. The framework can provide breakdown location prediction and breakdown voltage (BV) prediction by utilizing a two-stage machine learning method. In addition, it demonstrates 97.67% accuracy on breakdown location prediction and less than 4% average error on the BV prediction compared with TCAD simulation. The proposed method can be used to measure changes in performance caused by random variability in structural parameters during manufacturing process, allowing designers to avoid unstable structural parameters and enhance design robustness. More importantly, it can significantly reduce the computational cost when compared with the TCAD simulation. We believe the proposed machine learning technique can significantly speedup the design space exploration for power devices, eventually reducing the overall product-to-market time.","",""
34,"Shouvanik Chakrabarti, Yiming Huang, Tongyang Li, S. Feizi, Xiaodi Wu","Quantum Wasserstein Generative Adversarial Networks",2019,"","","","",80,"2022-07-13 09:22:49","","","","",,,,,34,11.33,7,5,3,"The study of quantum generative models is well-motivated, not only because of its importance in quantum machine learning and quantum chemistry but also because of the perspective of its implementation on near-term quantum machines. Inspired by previous studies on the adversarial training of classical and quantum generative models, we propose the first design of quantum Wasserstein Generative Adversarial Networks (WGANs), which has been shown to improve the robustness and the scalability of the adversarial training of quantum generative models even on noisy quantum hardware. Specifically, we propose a definition of the Wasserstein semimetric between quantum data, which inherits a few key theoretical merits of its classical counterpart. We also demonstrate how to turn the quantum Wasserstein semimetric into a concrete design of quantum WGANs that can be efficiently implemented on quantum machines. Our numerical study, via classical simulation of quantum systems, shows the more robust and scalable numerical performance of our quantum WGANs over other quantum GAN proposals. As a surprising application, our quantum WGAN has been used to generate a 3-qubit quantum circuit of ~50 gates that well approximates a 3-qubit 1-d Hamiltonian simulation circuit that requires over 10k gates using standard techniques.","",""
79,"Sakshi Udeshi, Pryanshu Arora, Sudipta Chattopadhyay","Automated Directed Fairness Testing",2018,"","","","",81,"2022-07-13 09:22:49","","10.1145/3238147.3238165","","",,,,,79,19.75,26,3,4,"Fairness is a critical trait in decision making. As machine-learning models are increasingly being used in sensitive application domains (e.g. education and employment) for decision making, it is crucial that the decisions computed by such models are free of unintended bias. But how can we automatically validate the fairness of arbitrary machine-learning models? For a given machine-learning model and a set of sensitive input parameters, our Aequitas approach automatically discovers discriminatory inputs that highlight fairness violation. At the core of Aequitas are three novel strategies to employ probabilistic search over the input space with the objective of uncovering fairness violation. Our Aequitas approach leverages inherent robustness property in common machine-learning models to design and implement scalable test generation methodologies. An appealing feature of our generated test inputs is that they can be systematically added to the training set of the underlying model and improve its fairness. To this end, we design a fully automated module that guarantees to improve the fairness of the model. We implemented Aequitas and we have evaluated it on six state-of-the-art classifiers. Our subjects also include a classifier that was designed with fairness in mind. We show that Aequitas effectively generates inputs to uncover fairness violation in all the subject classifiers and systematically improves the fairness of respective models using the generated test inputs. In our evaluation, Aequitas generates up to 70% discriminatory inputs (w.r.t. the total number of inputs generated) and leverages these inputs to improve the fairness up to 94%.","",""
9,"M. Sommer, Sven Tomforde, J. Hähner","Learning a Dynamic Re-combination Strategy of Forecast Techniques at Runtime",2015,"","","","",82,"2022-07-13 09:22:49","","10.1109/ICAC.2015.70","","",,,,,9,1.29,3,3,7,"Traffic experts try to optimise the signalisation of traffic light controllers during design-time based on historic traffic flow data. Traffic exhibits dynamic behaviour. Due to changing traffic demands, new and flexible traffic management systems are needed that optimise themselves during runtime. Organic Traffic Control is such a decentralised, self-organising system that adapts the green times of traffic lights to the current traffic conditions. Forecasts of future traffic conditions may result in a faster adaptation, higher robustness and flexibility. The combination of several forecasting techniques leads to fewer forecast errors. This paper presents three novel combination strategies from the machine learning domain using an Artificial Neural Network, Historic Load Curves and an Extended Classifier System.","",""
74,"Fei Deng, S. Pu, Xuehong Chen, Yusheng Shi, Ting Yuan, Shengyan Pu","Hyperspectral Image Classification with Capsule Network Using Limited Training Samples",2018,"","","","",83,"2022-07-13 09:22:49","","10.3390/s18093153","","",,,,,74,18.50,12,6,4,"Deep learning techniques have boosted the performance of hyperspectral image (HSI) classification. In particular, convolutional neural networks (CNNs) have shown superior performance to that of the conventional machine learning algorithms. Recently, a novel type of neural networks called capsule networks (CapsNets) was presented to improve the most advanced CNNs. In this paper, we present a modified two-layer CapsNet with limited training samples for HSI classification, which is inspired by the comparability and simplicity of the shallower deep learning models. The presented CapsNet is trained using two real HSI datasets, i.e., the PaviaU (PU) and SalinasA datasets, representing complex and simple datasets, respectively, and which are used to investigate the robustness or representation of every model or classifier. In addition, a comparable paradigm of network architecture design has been proposed for the comparison of CNN and CapsNet. Experiments demonstrate that CapsNet shows better accuracy and convergence behavior for the complex data than the state-of-the-art CNN. For CapsNet using the PU dataset, the Kappa coefficient, overall accuracy, and average accuracy are 0.9456, 95.90%, and 96.27%, respectively, compared to the corresponding values yielded by CNN of 0.9345, 95.11%, and 95.63%. Moreover, we observed that CapsNet has much higher confidence for the predicted probabilities. Subsequently, this finding was analyzed and discussed with probability maps and uncertainty analysis. In terms of the existing literature, CapsNet provides promising results and explicit merits in comparison with CNN and two baseline classifiers, i.e., random forests (RFs) and support vector machines (SVMs).","",""
23,"K. Mandal, G. Gong","PrivFL: Practical Privacy-preserving Federated Regressions on High-dimensional Data over Mobile Networks",2019,"","","","",84,"2022-07-13 09:22:49","","10.1145/3338466.3358926","","",,,,,23,7.67,12,2,3,"Federated Learning (FL) enables a large number of users to jointly learn a shared machine learning (ML) model, coordinated by a centralized server, where the data is distributed across multiple devices. This approach enables the server or users to train and learn an ML model using gradient descent, while keeping all the training data on users' devices. We consider training an ML model over a mobile network where user dropout is a common phenomenon. Although federated learning was aimed at reducing data privacy risks, the ML model privacy has not received much attention. In this work, we present PrivFL, a privacy-preserving system for training (predictive) linear and logistic regression models and oblivious predictions in the federated setting, while guaranteeing data and model privacy as well as ensuring robustness to users dropping out in the network. We design two privacy-preserving protocols for training linear and logistic regression models based on an additive homomorphic encryption (HE) scheme and an aggregation protocol. Exploiting the training algorithm of federated learning, at the core of our training protocols is a secure multiparty global gradient computation on alive users' data. We analyze the security of our training protocols against semi-honest adversaries. As long as the aggregation protocol is secure under the aggregation privacy game and the additive HE scheme is semantically secure, PrivFL guarantees the users' data privacy against the server, and the server's regression model privacy against the users. We demonstrate the performance of PrivFL on real-world datasets and show its applicability in the federated learning system.","",""
59,"Durga Prasad Sahoo, Debdeep Mukhopadhyay, R. Chakraborty, Phuong Ha Nguyen","A Multiplexer-Based Arbiter PUF Composition with Enhanced Reliability and Security",2018,"","","","",85,"2022-07-13 09:22:49","","10.1109/TC.2017.2749226","","",,,,,59,14.75,15,4,4,"Arbiter Physically Unclonable Functions (APUFs), while being relatively lightweight, are extremely vulnerable to modeling attacks. Hence, various compositions of APUFs such as XOR APUF and Lightweight Secure PUF have been proposed to be secure alternatives. Previous research has demonstrated that PUF compositions have two major challenges to overcome: vulnerability against modeling and statistical attacks, and lack of reliability. In this paper, we introduce a multiplexer-based composition of APUFs, denoted as MPUF, to simultaneously overcome these challenges. In addition to the basic MPUF design, we propose two MPUF variants namely cMPUF and rMPUF to improve the robustness against cryptanalysis and reliability-based modeling attack, respectively. An rMPUF demonstrates enhanced robustness against the reliability-based modeling attack, while even the well-known XOR APUF, otherwise robust to machine learning based modeling attacks, has been modeled using the same technique with linear data and time complexities. The rMPUF can provide a good trade-off between security and hardware overhead while maintaining a significantly higher reliability level than any practical XOR APUF instance. Moreover, MPUF variants are the first APUF compositions, to the best of our knowledge, that can achieve Strict Avalanche Criterion without using any additional input network (or hardware) for challenge transformation. Finally, we validate our theoretical findings using Matlab-based simulations of MPUFs.","",""
60,"Sanjaya Lohani, R. Glasser","Turbulence correction with artificial neural networks.",2018,"","","","",86,"2022-07-13 09:22:49","","10.1364/OL.43.002611","","",,,,,60,15.00,30,2,4,"We design an optical feedback network making use of machine learning (ML) techniques and demonstrate via simulations its ability to correct for the effects of turbulent propagation on optical modes. This artificial neural network scheme relies only on measuring the intensity profile of the distorted modes, making the approach simple and robust. The network results in the generation of various mode profiles at the transmitter that, after propagation through turbulence, closely resemble the desired target mode. The corrected optical mode profiles at the receiver are found to be nearly identical to the desired profiles, with near-zero mean square error indices. We are hopeful that the present results combining the fields of ML and optical communications will greatly enhance the robustness of free-space optical links.","",""
158,"M. Elgendi","Fast QRS Detection with an Optimized Knowledge-Based Method: Evaluation on 11 Standard ECG Databases",2013,"","","","",87,"2022-07-13 09:22:49","","10.1371/journal.pone.0073557","","",,,,,158,17.56,158,1,9,"The current state-of-the-art in automatic QRS detection methods show high robustness and almost negligible error rates. In return, the methods are usually based on machine-learning approaches that require sufficient computational resources. However, simple-fast methods can also achieve high detection rates. There is a need to develop numerically efficient algorithms to accommodate the new trend towards battery-driven ECG devices and to analyze long-term recorded signals in a time-efficient manner. A typical QRS detection method has been reduced to a basic approach consisting of two moving averages that are calibrated by a knowledge base using only two parameters. In contrast to high-accuracy methods, the proposed method can be easily implemented in a digital filter design.","",""
17,"Ziqi Yang, Hung Dang, E. Chang","Effectiveness of Distillation Attack and Countermeasure on Neural Network Watermarking",2019,"","","","",88,"2022-07-13 09:22:49","","","","",,,,,17,5.67,6,3,3,"The rise of machine learning as a service and model sharing platforms has raised the need of traitor-tracing the models and proof of authorship. Watermarking technique is the main component of existing methods for protecting copyright of models. In this paper, we show that distillation, a widely used transformation technique, is a quite effective attack to remove watermark embedded by existing algorithms. The fragility is due to the fact that distillation does not retain the watermark embedded in the model that is redundant and independent to the main learning task. We design ingrain in response to the destructive distillation. It regularizes a neural network with an ingrainer model, which contains the watermark, and forces the model to also represent the knowledge of the ingrainer. Our extensive evaluations show that ingrain is more robust to distillation attack and its robustness against other widely used transformation techniques is comparable to existing methods.","",""
43,"A. Rahmani, Bryan Donyanavard, T. Mück, Kasra Moazzemi, A. Jantsch, O. Mutlu, N. Dutt","SPECTR: Formal Supervisory Control and Coordination for Many-core Systems Resource Management",2018,"","","","",89,"2022-07-13 09:22:49","","10.1145/3173162.3173199","","",,,,,43,10.75,6,7,4,"Resource management strategies for many-core systems need to enable sharing of resources such as power, processing cores, and memory bandwidth while coordinating the priority and significance of system- and application-level objectives at runtime in a scalable and robust manner. State-of-the-art approaches use heuristics or machine learning for resource management, but unfortunately lack formalism in providing robustness against unexpected corner cases. While recent efforts deploy classical control-theoretic approaches with some guarantees and formalism, they lack scalability and autonomy to meet changing runtime goals. We present SPECTR, a new resource management approach for many-core systems that leverages formal supervisory control theory (SCT) to combine the strengths of classical control theory with state-of-the-art heuristic approaches to efficiently meet changing runtime goals. SPECTR is a scalable and robust control architecture and a systematic design flow for hierarchical control of many-core systems. SPECTR leverages SCT techniques such as gain scheduling to allow autonomy for individual controllers. It facilitates automatic synthesis of the high-level supervisory controller and its property verification. We implement SPECTR on an Exynos platform containing ARM»s big.LITTLE-based heterogeneous multi-processor (HMP) and demonstrate that SPECTR»s use of SCT is key to managing multiple interacting resources (e.g., chip power and processing cores) in the presence of competing objectives (e.g., satisfying QoS vs. power capping). The principles of SPECTR are easily applicable to any resource type and objective as long as the management problem can be modeled using dynamical systems theory (e.g., difference equations), discrete-event dynamic systems, or fuzzy dynamics.","",""
19,"Jie Zhang, Hai Wang, Z. Cao, Jinchuan Zheng, Ming Yu, A. Yazdani, F. Shahnia","Fast nonsingular terminal sliding mode control for permanent-magnet linear motor via ELM",2020,"","","","",90,"2022-07-13 09:22:49","","10.1007/s00521-019-04502-4","","",,,,,19,9.50,3,7,2,"","",""
159,"Zi Chu, I. Widjaja, Haining Wang","Detecting Social Spam Campaigns on Twitter",2012,"","","","",91,"2022-07-13 09:22:49","","10.1007/978-3-642-31284-7_27","","",,,,,159,15.90,53,3,10,"","",""
11,"E. Kerrigan","Co-design of hardware and algorithms for real-time optimization",2014,"","","","",92,"2022-07-13 09:22:49","","10.1109/ECC.2014.6862630","","",,,,,11,1.38,11,1,8,"It is difficult or impossible to separate the performance of an optimization solver from the architecture of the computing system on which the algorithm is implemented. This is particularly true if measurements from a physical system are used to update and solve a sequence of mathematical optimization problems in real-time, such as in control, automation, signal processing and machine learning. In these real-time optimization applications the designer has to trade off computing time, space and energy against each other, while satisfying constraints on the performance and robustness of the resulting cyber-physical system. This paper is an informal introduction to the issues involved when designing the computing hardware and a real-time optimization algorithm at the same time, which can result in systems with efficiencies and performances that are unachievable when designing the sub-systems independently. The co-design process can, in principle, be formulated as a sequence of uncertain and non-smooth optimization problems. In other words, optimizers might be used to design optimizers. Before this can become a reality, new systems theory and numerical methods will have to be developed to solve these co-design problems effectively and reliably.","",""
23,"A. Mahabuba, M. A. Khan","Small signal stability enhancement of a multi-machine power system using robust and adaptive fuzzy neural network-based power system stabilizer",2009,"","","","",93,"2022-07-13 09:22:49","","10.1002/ETEP.276","","",,,,,23,1.77,12,2,13,"This paper presents a design procedure for a robust and adaptive fuzzy neural network-based power system stabilizer (RAFNNPSS) and investigates the robustness and adaptive feature of the RAFNNPSS for a single machine connected to an infinite bus system and multi-machine power systems in order to enhance the dynamic stability (small signal stability of the system). The parameters of RAFNNPSS are tuned by adaptive neural network (NN). This RAFNNPSS uses adaptive network-based fuzzy inference system (ANFIS) network, which provides a natural framework of multi-layered feed forward adaptive network using fuzzy logic inference system. In this approach, the hybrid-learning algorithm tunes the fuzzy rules and the membership functions of the RAFNNPSS. Speed deviation of synchronous generator and its derivative are chosen as the input signals to the RAFNNPSS. The dynamic performance of single-machine infinite bus (SMIB) system, a two-area, five-machine, eight-bus power system and a large power system (10-machine, 39-bus New England system) with the proposed RAFNNPSS under different operating conditions and change in system parameters have been investigated. The simulation results obtained from the conventional PSS (CPSS) and Fuzzy logic-based PSS (FPSS) are compared with the proposed RAFNNPSS. The simulation results demonstrate that the proposed RAFNNPSS performs well in damping and quicker response when compared with the other two PSSs. Copyright © 2008 John Wiley & Sons, Ltd.","",""
28,"Xiu-Shen Wei, Chen-Lin Zhang, Yao Li, Chen-Wei Xie, Jianxin Wu, Chunhua Shen, Zhi-Hua Zhou","Deep Descriptor Transforming for Image Co-Localization",2017,"","","","",94,"2022-07-13 09:22:49","","10.24963/ijcai.2017/425","","",,,,,28,5.60,4,7,5,"Reusable model design becomes desirable with the rapid expansion of machine learning applications. In this paper, we focus on the reusability of pre-trained deep convolutional models. Specifically, different from treating pre-trained models as feature extractors, we reveal more treasures beneath convolutional layers, i.e., the convolutional activations could act as a detector for the common object in the image co-localization problem. We propose a simple but effective method, named Deep Descriptor Transforming (DDT), for evaluating the correlations of descriptors and then obtaining the category-consistent regions, which can accurately locate the common object in a set of images. Empirical studies validate the effectiveness of the proposed DDT method. On benchmark image co-localization datasets, DDT consistently outperforms existing state-of-the-art methods by a large margin. Moreover, DDT also demonstrates good generalization ability for unseen categories and robustness for dealing with noisy data.","",""
52,"Edward H. Lee, S. Wong","24.2 A 2.5GHz 7.7TOPS/W switched-capacitor matrix multiplier with co-designed local memory in 40nm",2016,"","","","",95,"2022-07-13 09:22:49","","10.1109/ISSCC.2016.7418085","","",,,,,52,8.67,26,2,6,"Matrix multiplication, enabled by multiply-and-accumulate hardware, is ubiquitous in signal processing, computer graphics, machine learning, and optimization. Many important applications with inherent robustness to reduced precision for matrix multiplication, e.g. inference for neural networks [1], can take advantage of analog signal processing for energy efficiency. This work presents a 64-cycle programmable passive Switched-Capacitor Matrix Multiplier (SCMM) with co-designed bitline-less memory. The design exploits 300aF unit fringe capacitors for high speed and low energy charge-domain processing and contains the input DAC, multiply-and-accumulate SAR ADC, and local memory. Two applications of the SCMM are demonstrated: 1) an analog front-end for an image classifier system, which reduces A/D conversions by 21x and multiply-and-accumulate compute energy by 11x over a conventional system, and 2) a co-processing accelerator to solve Stochastic Gradient Descent optimization, which achieves a measured 7.7 TOPS/W at 2.5GHz.","",""
69,"S. Amsuess, P. Goebel, B. Graimann, D. Farina","A Multi-Class Proportional Myocontrol Algorithm for Upper Limb Prosthesis Control: Validation in Real-Life Scenarios on Amputees",2015,"","","","",96,"2022-07-13 09:22:49","","10.1109/TNSRE.2014.2361478","","",,,,,69,9.86,17,4,7,"Functional replacement of upper limbs by means of dexterous prosthetic devices remains a technological challenge. While the mechanical design of prosthetic hands has advanced rapidly, the human-machine interfacing and the control strategies needed for the activation of multiple degrees of freedom are not reliable enough for restoring hand function successfully. Machine learning methods capable of inferring the user intent from EMG signals generated by the activation of the remnant muscles are regarded as a promising solution to this problem. However, the lack of robustness of the current methods impedes their routine clinical application. In this study, we propose a novel algorithm for controlling multiple degrees of freedom sequentially, inherently proportionally and with high robustness, allowing a good level of prosthetic hand function. The control algorithm is based on the spatial linear combinations of amplitude-related EMG signal features. The weighting coefficients in this combination are derived from the optimization criterion of the common spatial patterns filters which allow for maximal discriminability between movements. An important component of the study is the validation of the method which was performed on both able-bodied and amputee subjects who used physical prostheses with customized sockets and performed three standardized functional tests mimicking daily-life activities of varying difficulty. Moreover, the new method was compared in the same conditions with one clinical/industrial and one academic state-of-the-art method. The novel algorithm outperformed significantly the state-of-the-art techniques in both subject groups for tests that required the activation of more than one degree of freedom. Because of the evaluation in real time control on both able-bodied subjects and final users (amputees) wearing physical prostheses, the results obtained allow for the direct extrapolation of the benefits of the proposed method for the end users. In conclusion, the method proposed and validated in real-life use scenarios, allows the practical usability of multifunctional hand prostheses in an intuitive way, with significant advantages with respect to previous systems.","",""
33,"J. Carrasco, S. García, M. Rueda, F. Herrera","rNPBST: An R Package Covering Non-parametric and Bayesian Statistical Tests",2017,"","","","",97,"2022-07-13 09:22:49","","10.1007/978-3-319-59650-1_24","","",,,,,33,6.60,8,4,5,"","",""
7,"Sina Mohseni, Akshay V. Jagadeesh, Zhangyang Wang","Predicting Model Failure using Saliency Maps in Autonomous Driving Systems",2019,"","","","",98,"2022-07-13 09:22:49","","","","",,,,,7,2.33,2,3,3,"While machine learning systems show high success rate in many complex tasks, research shows they can also fail in very unexpected situations. Rise of machine learning products in safety-critical industries cause an increase in attention in evaluating model robustness and estimating failure probability in machine learning systems. In this work, we propose a design to train a student model -- a failure predictor -- to predict the main model's error for input instances based on their saliency map. We implement and review the preliminary results of our failure predictor model on an autonomous vehicle steering control system as an example of safety-critical applications.","",""
6,"Robin Roussel, Marie-Paule Cani, J. Léon, N. Mitra","Designing chain reaction contraptions from causal graphs",2019,"","","","",99,"2022-07-13 09:22:49","","10.1145/3306346.3322977","","",,,,,6,2.00,2,4,3,"Chain reaction contraptions, commonly referred to as Rube Goldberg machines, achieve simple tasks in an intentionally complex fashion via a cascading sequence of events. They are fun, engaging and satisfying to watch. Physically realizing them, however, involves hours or even days of manual trial-and-error effort. The main difficulties lie in predicting failure factors over long chains of events and robustly enforcing an expected causality between parallel chains, especially under perturbations of the layout. We present a computational framework to help design the layout of such contraptions by optimizing their robustness to possible assembly errors. Inspired by the active learning paradigm in machine learning, we propose a generic sampling-based method to progressively approximate the success probability distribution of a given scenario over the design space of possible scene layouts. The success or failure of any given simulation is determined from a user-specified causal graph enforcing a time ordering between expected events. Our method scales to complex causal graphs and high dimensional design spaces by dividing the graph and scene into simpler sub-scenarios. The aggregated success probability distribution is subsequently used to optimize the entire layout. We demonstrate the use of our framework through a range of real world examples of increasing complexity, and report significant improvements over alternative approaches. Code and fabrication diagrams are available on the project page.","",""
18,"Qingquan Song, Haifeng Jin, Xiao Huang, Xia Hu","Multi-label Adversarial Perturbations",2018,"","","","",100,"2022-07-13 09:22:49","","10.1109/ICDM.2018.00166","","",,,,,18,4.50,5,4,4,"Adversarial examples are delicately perturbed inputs, which aim to mislead machine learning models towards incorrect outputs. While existing work focuses on generating adversarial perturbations in multiclass classification problems, many real-world applications fall into the multi-label setting, in which one instance could be associated with more than one label. To analyze the vulnerability and robustness of multi-label learning models, we investigate the generation of multi-label adversarial perturbations. This is a challenging task due to the uncertain number of positive labels associated with one instance, and the fact that multiple labels are usually not mutually exclusive with each other. To bridge the gap, in this paper, we propose a general attacking framework targeting multi-label classification problem and conduct a premier analysis on the perturbations for deep neural networks. Leveraging the ranking relationships among labels, we further design a ranking-based framework to attack multi-label ranking algorithms. Experiments on two different datasets demonstrate the effectiveness of the proposed frameworks and provide insights of the vulnerability of multi-label deep models under diverse targeted attacks.","",""
6,"Robin Roussel, Marie-Paule Cani, J. Léon, N. Mitra","Designing chain reaction contraptions from causal graphs",2019,"","","","",101,"2022-07-13 09:22:49","","10.1145/3306346.3322977","","",,,,,6,2.00,2,4,3,"Chain reaction contraptions, commonly referred to as Rube Goldberg machines, achieve simple tasks in an intentionally complex fashion via a cascading sequence of events. They are fun, engaging and satisfying to watch. Physically realizing them, however, involves hours or even days of manual trial-and-error effort. The main difficulties lie in predicting failure factors over long chains of events and robustly enforcing an expected causality between parallel chains, especially under perturbations of the layout. We present a computational framework to help design the layout of such contraptions by optimizing their robustness to possible assembly errors. Inspired by the active learning paradigm in machine learning, we propose a generic sampling-based method to progressively approximate the success probability distribution of a given scenario over the design space of possible scene layouts. The success or failure of any given simulation is determined from a user-specified causal graph enforcing a time ordering between expected events. Our method scales to complex causal graphs and high dimensional design spaces by dividing the graph and scene into simpler sub-scenarios. The aggregated success probability distribution is subsequently used to optimize the entire layout. We demonstrate the use of our framework through a range of real world examples of increasing complexity, and report significant improvements over alternative approaches. Code and fabrication diagrams are available on the project page.","",""
18,"Qingquan Song, Haifeng Jin, Xiao Huang, Xia Hu","Multi-label Adversarial Perturbations",2018,"","","","",102,"2022-07-13 09:22:49","","10.1109/ICDM.2018.00166","","",,,,,18,4.50,5,4,4,"Adversarial examples are delicately perturbed inputs, which aim to mislead machine learning models towards incorrect outputs. While existing work focuses on generating adversarial perturbations in multiclass classification problems, many real-world applications fall into the multi-label setting, in which one instance could be associated with more than one label. To analyze the vulnerability and robustness of multi-label learning models, we investigate the generation of multi-label adversarial perturbations. This is a challenging task due to the uncertain number of positive labels associated with one instance, and the fact that multiple labels are usually not mutually exclusive with each other. To bridge the gap, in this paper, we propose a general attacking framework targeting multi-label classification problem and conduct a premier analysis on the perturbations for deep neural networks. Leveraging the ranking relationships among labels, we further design a ranking-based framework to attack multi-label ranking algorithms. Experiments on two different datasets demonstrate the effectiveness of the proposed frameworks and provide insights of the vulnerability of multi-label deep models under diverse targeted attacks.","",""
16,"Kangjun Bai, Y. Yi","DFR: An Energy-efficient Analog Delay Feedback Reservoir Computing System for Brain-inspired Computing",2018,"","","","",103,"2022-07-13 09:22:49","","10.1145/3264659","","",,,,,16,4.00,8,2,4,"Neuromorphic computing, which is built on a brain-inspired silicon chip, is uniquely applied to keep pace with the explosive escalation of algorithms and data density on machine learning. Reservoir computing, an emerging computing paradigm based on the recurrent neural network with proven benefits across multifaceted applications, offers an alternative training mechanism only at the readout stage. In this work, we successfully design and fabricate an energy-efficient analog delayed feedback reservoir (DFR) computing system, which is built upon a temporal encoding scheme, a nonlinear transfer function, and a dynamic delayed feedback loop. Measurement results demonstrate its high energy efficiency with rich dynamic behaviors, making the designed system a candidate for low power embedded applications. The system performance, as well as the robustness, are studied and analyzed through the Monte Carlo simulation. The chaotic time series prediction benchmark, NARMA10, is examined through the proposed DFR computing system, and exhibits a 36%−85% reduction on the error rate compared to state-of-the-art DFR computing system designs. To the best of our knowledge, our work represents the first analog integrated circuit (IC) implementation of the DFR computing system.","",""
226,"Piotr Dollár, Z. Tu, Hai Tao, Serge J. Belongie","Feature Mining for Image Classification",2007,"","","","",104,"2022-07-13 09:22:49","","10.1109/CVPR.2007.383046","","",,,,,226,15.07,57,4,15,"The efficiency and robustness of a vision system is often largely determined by the quality of the image features available to it. In data mining, one typically works with immense volumes of raw data, which demands effective algorithms to explore the data space. In analogy to data mining, the space of meaningful features for image analysis is also quite vast. Recently, the challenges associated with these problem areas have become more tractable through progress made in machine learning and concerted research effort in manual feature design by domain experts. In this paper, we propose a feature mining paradigm for image classification and examine several feature mining strategies. We also derive a principled approach for dealing with features with varying computational demands. Our goal is to alleviate the burden of manual feature design, which is a key problem in computer vision and machine learning. We include an in-depth empirical study on three typical data sets and offer theoretical explanations for the performance of various feature mining strategies. As a final confirmation of our ideas, we show results of a system, that utilizing feature mining strategies matches or outperforms the best reported results on pedestrian classification (where considerable effort has been devoted to expert feature design).","",""
23,"Chong Peng, Zhao Kang, Yunhong Hu, Jie Cheng, Q. Cheng","Robust Graph Regularized Nonnegative Matrix Factorization for Clustering",2017,"","","","",105,"2022-07-13 09:22:49","","10.1145/3003730","","",,,,,23,4.60,5,5,5,"Matrix factorization is often used for data representation in many data mining and machine-learning problems. In particular, for a dataset without any negative entries, nonnegative matrix factorization (NMF) is often used to find a low-rank approximation by the product of two nonnegative matrices. With reduced dimensions, these matrices can be effectively used for many applications such as clustering. The existing methods of NMF are often afflicted with their sensitivity to outliers and noise in the data. To mitigate this drawback, in this paper, we consider integrating NMF into a robust principal component model, and design a robust formulation that effectively captures noise and outliers in the approximation while incorporating essential nonlinear structures. A set of comprehensive empirical evaluations in clustering applications demonstrates that the proposed method has strong robustness to gross errors and superior performance to current state-of-the-art methods.","",""
238,"A. Varsek, T. Urbancic, B. Filipič","Genetic algorithms in controller design and tuning",1993,"","","","",106,"2022-07-13 09:22:49","","10.1109/21.260663","","",,,,,238,8.21,79,3,29,"A three-phased framework for learning dynamic system control is presented. A genetic algorithm is employed to derive control rules encoded as decision tables. Next, the rules are automatically transformed into comprehensible form by means of inductive machine learning. Finally, a genetic algorithm is applied again to optimize the numerical parameters of the induced rules. The approach is experimentally verified on a benchmark problem of inverted pendulum control, with special emphasis on robustness and reliability. It is also shown that the proposed framework enables exploiting available domain knowledge. In this case, genetic algorithm makes qualitative control rules operational by providing interpretation of symbols in terms of numerical values. >","",""
72,"Ying-Hong Liao, Chuen-Tsai Sun","An educational genetic algorithms learning tool",2001,"","","","",107,"2022-07-13 09:22:49","","10.1109/13.925863","","",,,,,72,3.43,36,2,21,"During the last thirty years, there has been a rapidly growing interest in a field called genetic algorithms (GAs). The field is at a stage of tremendous growth as evidenced by the increasing number of conferences, workshops and papers concerning it, as well as the emergence of a central journal for the field. With their great robustness, genetic algorithms have proven to be a promising technique for many optimization, design, control, and machine learning applications. Students who take a GAs course study and implement a wide range of difference techniques of GAs. And practical implementation experience plays a very important role in learning computer relative courses. Herein, an educational genetic algorithm learning tool (EGALT) has been developed to help students facilitate GAs course. With the readily available tool students can reduce the mechanical programming aspect of learning and concentrate on principles alone. A friendly graphic user interface was established to help students operate and control not only the structural identification but also the parametric identification of GAs. It outlines how to implemented genetic algorithms, how to set parameters of different kinds of problems, and recommends a set of genetic algorithms, which were suggested in previous studies.","",""
111,"B. Twala","AN EMPIRICAL COMPARISON OF TECHNIQUES FOR HANDLING INCOMPLETE DATA USING DECISION TREES",2009,"","","","",108,"2022-07-13 09:22:49","","10.1080/08839510902872223","","",,,,,111,8.54,111,1,13,"Increasing the awareness of how incomplete data affects learning and classification accuracy has led to increasing numbers of missing data techniques. This article investigates the robustness and accuracy of seven popular techniques for tolerating incomplete training and test data for different patterns of missing data—different proportions and mechanisms of missing data on resulting tree-based models. The seven missing data techniques were compared by artificially simulating different proportions, patterns, and mechanisms of missing data using 21 complete datasets (i.e., with no missing values) obtained from the University of California, Irvine repository of machine-learning databases (Blake and Merz, 1998). A four-way repeated measures design was employed to analyze the data. The simulation results suggest important differences. All methods have their strengths and weaknesses. However, listwise deletion is substantially inferior to the other six techniques, while multiple imputation, that utilizes the expectation maximization algorithm, represents a superior approach to handling incomplete data. Decision tree single imputation and surrogate variables splitting are more severely impacted by missing values distributed among all attributes compared to when they are only on a single attribute. Otherwise, the imputation—versus model-based imputation procedures gave—reasonably good results although some discrepancies remained. Different techniques for addressing missing values when using decision trees can give substantially diverse results, and must be carefully considered to protect against biases and spurious findings. Multiple imputation should always be used, especially if the data contain many missing values. If few values are missing, any of the missing data techniques might be considered. The choice of technique should be guided by the proportion, pattern, and mechanisms of missing data, especially the latter two. However, the use of older techniques like listwise deletion and mean or mode single imputation is no longer justifiable given the accessibility and ease of use of more advanced techniques, such as multiple imputation and supervised learning imputation.","",""
128,"L. Bruzzone, C. Persello","A Novel Context-Sensitive Semisupervised SVM Classifier Robust to Mislabeled Training Samples",2009,"","","","",109,"2022-07-13 09:22:49","","10.1109/TGRS.2008.2011983","","",,,,,128,9.85,64,2,13,"This paper presents a novel context-sensitive semisupervised support vector machine (CS4VM) classifier, which is aimed at addressing classification problems where the available training set is not fully reliable, i.e., some labeled samples may be associated to the wrong information class (mislabeled patterns). Unlike standard context-sensitive methods, the proposed CS4VM classifier exploits the contextual information of the pixels belonging to the neighborhood system of each training sample in the learning phase to improve the robustness to possible mislabeled training patterns. This is achieved according to both the design of a semisupervised procedure and the definition of a novel contextual term in the cost function associated with the learning of the classifier. In order to assess the effectiveness of the proposed CS4VM and to understand the impact of the addressed problem in real applications, we also present an extensive experimental analysis carried out on training sets that include different percentages of mislabeled patterns having different distributions on the classes. In the analysis, we also study the robustness to mislabeled training patterns of some widely used supervised and semisupervised classification algorithms (i.e., conventional support vector machine (SVM), progressive semisupervised SVM, maximum likelihood, and k-nearest neighbor). Results obtained on a very high resolution image and on a medium resolution image confirm both the robustness and the effectiveness of the proposed CS4VM with respect to standard classification algorithms and allow us to derive interesting conclusions on the effects of mislabeled patterns on different classifiers.","",""
11,"He Yan, Qiaolin Ye, Dong-Jun Yu","Efficient and robust TWSVM classification via a minimum L1-norm distance metric criterion",2018,"","","","",110,"2022-07-13 09:22:49","","10.1007/s10994-018-5771-8","","",,,,,11,2.75,4,3,4,"","",""
9,"Emilio Ferrara, Robert Baumgartner","Design of Automatically Adaptable Web Wrappers",2011,"","","","",111,"2022-07-13 09:22:49","","10.5220/0003131802110217","","",,,,,9,0.82,5,2,11,"Nowadays, the huge amount of information distributed through the Web motivates studying techniques to  be adopted in order to extract relevant data in an efﬁcient and reliable way. Both academia and enterprises  developed several approaches of Web data extraction, for example using techniques of artiﬁcial intelligence or  machine learning. Some commonly adopted procedures, namely wrappers, ensure a high degree of precision  of information extracted from Web pages, and, at the same time, have to prove robustness in order not to  compromise quality and reliability of data themselves.  In this paper we focus on some experimental aspects related to the robustness of the data extraction process  and the possibility of automatically adapting wrappers. We discuss the implementation of algorithms for  ﬁnding similarities between two different version of a Web page, in order to handle modiﬁcations, avoiding  the failure of data extraction tasks and ensuring reliability of information extracted. Our purpose is to evaluate  performances, advantages and draw-backs of our novel system of automatic wrapper adaptation.","",""
19,"C. Laurier, O. Meyers, J. Serrà, Martin Blech, P. Herrera","Music Mood Annotator Design and Integration",2009,"","","","",112,"2022-07-13 09:22:49","","10.1109/CBMI.2009.45","","",,,,,19,1.46,4,5,13,"A robust and efficient technique for automatic music mood annotation is presented. A song's mood is expressed by a supervised machine learning approach based on musical features extracted from the raw audio signal. A ground truth, used for training, is created using both social network information systems and individual experts. Tests of 7 different classification configurations have been performed, showing that Support Vector Machines perform best for the task at hand. Moreover, we evaluate the algorithm robustness to different audio compression schemes. This fact, often neglected, is fundamental to build a system that is usable in real conditions. In addition, the integration of a fast and scalable version of this technique with the European Project PHAROS is discussed.","",""
25,"Shiuh-Jer Huang, C.-C. Lin","A Self-Organising Fuzzy Logic Controller for a Coordinate Machine",2002,"","","","",113,"2022-07-13 09:22:49","","10.1007/S001700200084","","",,,,,25,1.25,13,2,20,"","",""
7,"Hitesh Shah, M. Gopal","Reinforcement learning control of robot manipulators in uncertain environments",2009,"","","","",114,"2022-07-13 09:22:49","","10.1109/ICIT.2009.4939504","","",,,,,7,0.54,4,2,13,"Considerable attention has been given to the design of stable controllers for robot manipulators, in the presence of uncertainties. We investigate here the robust tracking performance of reinforcement learning control of manipulators, subjected to parameter variations and extraneous disturbances. Robustness properties in terms of average error, absolute maximum errors and absolute maximum control efforts, have been compared for reinforcement learning systems using various parameterized function approximators, such as fuzzy, neural network, decision tree, and support vector machine. Simulation results show the importance of fuzzy Q-learning control. Further improvements in this control approach through dynamic fuzzy Q-learning have also been highlighted.","",""
8,"Keyang Cai, Hong Wang","Cloud classification of satellite image based on convolutional neural networks",2017,"","","","",115,"2022-07-13 09:22:49","","10.1109/ICSESS.2017.8343049","","",,,,,8,1.60,4,2,5,"Cloud classification of satellite image is the basis of meteorological forecast. Traditional machine learning methods need to manually design and extract a large number of image features, while the utilization of satellite image features is not high. This paper constructs a convolution neural network for cloud classification, which can automatically learn features and obtain classification results. The experimental results on the FY-2C satellite image show that the features extracted by deep convolution neural network are more favorable to the classification of satellite cloud. The performance of cloud classification based on deep convolution neural network is better than that of traditional machine learning methods. The method has high precision and good robustness.","",""
18,"R. McGibbon, Bharath Ramsundar, Mohammad M. Sultan, G. Kiss, V. Pande","Understanding Protein Dynamics with L1-Regularized Reversible Hidden Markov Models",2014,"","","","",116,"2022-07-13 09:22:49","","","","",,,,,18,2.25,4,5,8,"We present a machine learning framework for modeling protein dynamics. Our approach uses L1-regularized, reversible hidden Markov models to understand large protein datasets generated via molecular dynamics simulations. Our model is motivated by three design principles: (1) the requirement of massive scalability; (2) the need to adhere to relevant physical law; and (3) the necessity of providing accessible interpretations, critical for both cellular biology and rational drug design. We present an EM algorithm for learning and introduce a model selection criteria based on the physical notion of convergence in relaxation timescales. We contrast our model with standard methods in biophysics and demonstrate improved robustness. We implement our algorithm on GPUs and apply the method to two large protein simulation datasets generated respectively on the NCSA Bluewaters supercomputer and the Folding@Home distributed computing network. Our analysis identifies the conformational dynamics of the ubiquitin protein critical to cellular signaling, and elucidates the stepwise activation mechanism of the c-Src kinase protein.","",""
44,"L. Zadeh","The roles of soft computing and fuzzy logic in the conception, design and deployment of intelligent systems",1996,"","","","",117,"2022-07-13 09:22:49","","10.1109/FUZZY.1997.616336","","",,,,,44,1.69,44,1,26,"Summary form only given. Soft computing (SC) is a consortium of methodologies which provide a foundation for intelligent systems. The principal methods are fuzzy logic (FL), neurocomputing (NC), genetic computing (GC) and probabilistic computing (PC), with PC subsuming evidential reasoning, uncertainty management and some machine learning theory. The main contribution of FL is a methodology for dealing with imprecision, approximate reasoning, fuzzy information granulation and computing with words; that of NC system identification, learning and adaption; that of CC systematized random research, tuning and optimization; and that of PC decision analysis and uncertainty management. The guiding principle of soft computing is: Exploit the tolerance for imprecision, uncertainty and partial truth to achieve tractability, robustness, low solution cost and better rapport with reality. The 4 methods are complementary rather than competitive. Their use in combination leads to hybrid intelligent systems. The most visible of such systems are neuro-fuzzy systems. The ubiquity of intelligent systems is certain to have a profound impact on the ways in which man-made systems are conceived, designed, manufactured, employed and interacted with. This is the perspective in which basic issues relating to soft computing and intelligent systems are addressed.","",""
11,"Davide Maiorca, Davide Ariu, I. Corona, G. Giacinto","An Evasion Resilient Approach to the Detection of Malicious PDF Files",2015,"","","","",118,"2022-07-13 09:22:49","","10.1007/978-3-319-27668-7_5","","",,,,,11,1.57,3,4,7,"","",""
11,"C. Atkeson","Efficient robust policy optimization",2012,"","","","",119,"2022-07-13 09:22:49","","10.1109/ACC.2012.6315619","","",,,,,11,1.10,11,1,10,"We provide efficient algorithms to calculate first and second order gradients of the cost of a control law with respect to its parameters, to speed up policy optimization. We achieve robustness by simultaneously designing one control law for multiple models with potentially different model structures, which represent model uncertainty and unmodeled dynamics. Providing explicit examples of possible unmodeled dynamics during the control design process is easier for the designer and is more effective than providing simulated perturbations to increase robustness, as is currently done in machine learning. Our approach supports the design of deterministic nonlinear and time varying controllers for both deterministic and stochastic nonlinear and time varying systems, including policies with internal state such as observers or other state estimators. We highlight the benefit of control laws made up of collections of simple policies where only one component policy is active at a time. Controller optimization and learning is particularly fast and effective in this situation because derivatives are decoupled.","",""
123,"S. Matwin, T. Szapiro, K. Haigh","Genetic algorithms approach to a negotiation support system",1991,"","","","",120,"2022-07-13 09:22:49","","10.1109/21.101141","","",,,,,123,3.97,41,3,31,"It is argued that negotiation rules can be learned and invented by means of genetic algorithms. The work presented introduces a method, a system design, and a prototype implementation that uses genetic-based machine learning to acquire negotiation rules. The learned rules support a party involved in a two-party bargaining problem with multiple issues. It is assumed that both parties work towards a compromise deal. The method provides a framework in which genetic-based learning is applied repetitively on a changing problem representation. System design proposes a problem representation that is adequate to express bargaining processes and that is at the same time conducive to genetic-based learning. The authors report results of experiments with the prototype implementation. These results indicate that genetically learned rules, when used in real negotiations, yield results that are better than results obtained by humans in the same negotiation. The experiments indicate considerable robustness of genetically learned rules with respect to varying parameters defining the genetic operations on which the system relies in modeling negotiations. In terms of user support, experimental results show that in the bargaining process, a good rule is one that advises conceding in small steps and bringing new issues into the negotiation process. >","",""
19,"A. Dainotti, A. Pescapé, Hyun-chul Kim","Traffic Classification through Joint Distributions of Packet-Level Statistics",2011,"","","","",121,"2022-07-13 09:22:49","","10.1109/GLOCOM.2011.6134093","","",,,,,19,1.73,6,3,11,"Interest in traffic classification, in both industry and academia, has dramatically grown in the past few years. Research is devoting great efforts to statistical approaches using robust features. In this paper we propose a classification approach based on the joint distribution of Packet Size (PS) and Inter-Packet Time (IPT) and on machine- learning algorithms. Provided results, obtained using different real traffic traces, demonstrate how the proposed approach is able to achieve high (byte) accuracy (till 98%) and how the new features we introduced show properties of robustness, which suggest their use in the design of classification/identification approaches robust to traffic encryption and protocol obfuscation.","",""
11,"D. Moody, S. Brumby, Kary L. Myers, N. Pawley","Sparse classification of rf transients using chirplets and learned dictionaries",2011,"","","","",122,"2022-07-13 09:22:49","","10.1109/ACSSC.2011.6190351","","",,,,,11,1.00,3,4,11,"We assess the performance of a sparse classification approach for radiofrequency (RF) transient signals using dictionaries adapted to the data. We explore two approaches: pursuit-type decompositions over analytical, over-complete dictionaries, and dictionaries learned directly from data. Pursuit-type decompositions over analytical, over-complete dictionaries yield sparse representations by design and can work well for target signals in the same function class as the dictionary atoms. Discriminative dictionaries learned directly from data do not rely on analytical constraints or additional knowledge about the signal characteristics, and provide sparse representations that can perform well when used with a statistical classifier. We present classification results for learned dictionaries on simulated test data, and discuss robustness compared to conventional Fourier methods. We draw from techniques of adaptive feature extraction, statistical machine learning, and image processing.","",""
14,"S. Changyin","LS-SVM predictive control based on PSO for nonlinear systems",2010,"","","","",123,"2022-07-13 09:22:49","","","","",,,,,14,1.17,14,1,12,"For the predictive control of nonlinear systems, we present a single-step predictive control algorithm based on model learning and particle swarm optimization(PSO). The method utilizes least square support vector machine(LS-SVM) to estimate the model of a nonlinear system and forecast the output value, reducing the error in output feedback and error correction. The control values are obtained by the rolling optimization of PSO. This method can be used to design effective controllers for nonlinear systems with unknown mathematical models. For univariate and multivariate nonlinear systems, simulation results show that the predictive control algorithm is effective and has an excellent adaptive ability and robustness.","",""
25,"Ting Kuo, Shu-Yuen Hwang","Using Disruptive Selection to Maintain Diversity in Genetic Algorithms",1997,"","","","",124,"2022-07-13 09:22:49","","10.1023/A:1008276600101","","",,,,,25,1.00,13,2,25,"","",""
4,"T.D. Ross, M. L. Axtell, M. J. Noviskey, D. Gadd","Pattern theory paradigm for system design",1993,"","","","",125,"2022-07-13 09:22:49","","10.1109/MWSCAS.1993.343180","","",,,,,4,0.14,1,4,29,"A recent convergence of ideas from logic minimization, computational complexity, and machine learning theory has resulted in a promising new approach to robust pattern finding, called Pattern Theory. This paper demonstrates the robustness of this approach using experimental and theoretical considerations. The results of experiments in two applications, machine learning and image processing, are summarized.<<ETX>>","",""
18,"Adrian O'Riordan, H. Sorensen","An intelligent agent for high-precision text filtering",1995,"","","","",126,"2022-07-13 09:22:49","","10.1145/221270.221569","","",,,,,18,0.67,9,2,27,"We present here an overview of a research project aimed at reducing information overload for individual computer users. High-precision information filtering software has been developed to disseminate on–line electronic information. While the robustness and scalability of statistical approaches to information retrieval were a major influence on our design, we looked to the AI literature to supply the necessary techniques for the creation of an adaptive system. The system, called INFOrmer, is based on art intelligent agent approach and embodies machine learning, adaptation and relevance feedback techniques in its construction. A weighted graph representation is used for documents, and graph manipulation algorithms are used in the processing.","",""
8,"Dayong Li, Ying-hong Peng, Jilong Yin","Optimization of metal-forming process via a hybrid intelligent optimization technique",2007,"","","","",127,"2022-07-13 09:22:49","","10.1007/S00158-006-0075-1","","",,,,,8,0.53,3,3,15,"","",""
15,"Dong-Hyun Baek, W. Yoon, S. Park","A spatial rule adaptation procedure for reliable production control in a wafer fabrication system",1998,"","","","",128,"2022-07-13 09:22:49","","10.1080/002075498193129","","",,,,,15,0.63,5,3,24,"In conventional approaches to scheduling problems, a single dispatching rule was applied to the all machines in a manufacturing system. However, since the condition of a machine generally differs from those of other machines in the context of overall system operation, it is reasonable to identify a suitable dispatching rule for each machine. This study proposes an adaptive procedure which produces a reliable dispatching rule for each machine. The dispatching rule consists of several criteria of which weights are adaptively determined by learning through repeated runs of simulation. A Taguchi experimental design for the simulation is used to find effective criteria weights with efficiency and robustness. For evaluation, the proposed method was applied to a scheduling problem in a semiconductor wafer fabrication system. The method resulted in reliable performances compared with those of traditional dispatching rules.","",""
4,"Jae-Gil Lee, Yuji Roh, Hwanjun Song, S. E. Whang","Machine Learning Robustness, Fairness, and their Convergence",2021,"","","","",129,"2022-07-13 09:22:49","","10.1145/3447548.3470799","","",,,,,4,4.00,1,4,1,"Responsible AI becomes critical where robustness and fairness must be satisfied together. Traditionally, the two topics have been studied by different communities for different applications. Robust training is designed for noisy or poisoned data where image data is typically considered. In comparison, fair training primarily deals with biased data where structured data is typically considered. Nevertheless, robust training and fair training are fundamentally similar in considering that both of them aim at fixing the inherent flaws of real-world data. In this tutorial, we first cover state-of-the-art robust training techniques where most of the research is on combating various label noises. In particular, we cover label noise modeling, robust training approaches, and real-world noisy data sets. Then, proceeding to the related fairness literature, we discuss pre-processing, in-processing, and post-processing unfairness mitigation techniques, depending on whether the mitigation occurs before, during, or after the model training. Finally, we cover the recent trend emerged to combine robust and fair training in two flavors: the former is to make the fair training more robust (i.e., robust fair training), and the latter is to consider robustness and fairness as two equals to incorporate them into a holistic framework. This tutorial is indeed timely and novel because the convergence of the two topics is increasingly common, but yet to be addressed in tutorials. The tutors have extensive experience publishing papers in top-tier machine learning and data mining venues and developing machine learning platforms.","",""
10,"Rih-Teng Wu, Ting-Wei Liu, M. Jahanshahi, F. Semperlotti","Design of one-dimensional acoustic metamaterials using machine learning and cell concatenation",2021,"","","","",130,"2022-07-13 09:22:49","","10.1007/S00158-020-02819-6","","",,,,,10,10.00,3,4,1,"","",""
9,"Yanbin Li, G. Lei, G. Bramerdorfer, S. Peng, Xiaodong Sun, Jianguo Zhu","Machine Learning for Design Optimization of Electromagnetic Devices: Recent Developments and Future Directions",2021,"","","","",131,"2022-07-13 09:22:49","","10.3390/APP11041627","","",,,,,9,9.00,2,6,1,"This paper reviews the recent developments of design optimization methods for electromagnetic devices, with a focus on machine learning methods. First, the recent advances in multi-objective, multidisciplinary, multilevel, topology, fuzzy, and robust design optimization of electromagnetic devices are overviewed. Second, a review is presented to the performance prediction and design optimization of electromagnetic devices based on the machine learning algorithms, including artificial neural network, support vector machine, extreme learning machine, random forest, and deep learning. Last, to meet modern requirements of high manufacturing/production quality and lifetime reliability, several promising topics, including the application of cloud services and digital twin, are discussed as future directions for design optimization of electromagnetic devices.","",""
6,"A. Soni, Dharamvir Dharmacharya, A. Pal, V. Srivastava, R. Shaw, Ankush Ghosh","Design of a Machine Learning-Based Self-driving Car",2021,"","","","",132,"2022-07-13 09:22:49","","10.1007/978-981-16-0598-7_11","","",,,,,6,6.00,1,6,1,"","",""
27,"Vikash Sehwag, A. Bhagoji, Liwei Song, Chawin Sitawarin, Daniel Cullina, M. Chiang, Prateek Mittal","Analyzing the Robustness of Open-World Machine Learning",2019,"","","","",133,"2022-07-13 09:22:49","","10.1145/3338501.3357372","","",,,,,27,9.00,4,7,3,"When deploying machine learning models in real-world applications, an open-world learning framework is needed to deal with both normal in-distribution inputs and undesired out-of-distribution (OOD) inputs. Open-world learning frameworks include OOD detectors that aim to discard input examples which are not from the same distribution as the training data of machine learning classifiers. However, our understanding of current OOD detectors is limited to the setting of benign OOD data, and an open question is whether they are robust in the presence of adversaries. In this paper, we present the first analysis of the robustness of open-world learning frameworks in the presence of adversaries by introducing and designing øodAdvExamples. Our experimental results show that current OOD detectors can be easily evaded by slightly perturbing benign OOD inputs, revealing a severe limitation of current open-world learning frameworks. Furthermore, we find that øodAdvExamples also pose a strong threat to adversarial training based defense methods in spite of their effectiveness against in-distribution adversarial attacks. To counteract these threats and ensure the trustworthy detection of OOD inputs, we outline a preliminary design for a robust open-world machine learning framework.","",""
148,"Amedeo Sapio, M. Canini, Chen-Yu Ho, J. Nelson, Panos Kalnis, Changhoon Kim, A. Krishnamurthy, M. Moshref, Dan R. K. Ports, Peter Richtárik","Scaling Distributed Machine Learning with In-Network Aggregation",2019,"","","","",134,"2022-07-13 09:22:49","","","","",,,,,148,49.33,15,10,3,"Training complex machine learning models in parallel is an increasingly important workload. We accelerate distributed parallel training by designing a communication primitive that uses a programmable switch dataplane to execute a key step of the training process. Our approach, SwitchML, reduces the volume of exchanged data by aggregating the model updates from multiple workers in the network. We co-design the switch processing with the end-host protocols and ML frameworks to provide a robust, efficient solution that speeds up training by up to 300%, and at least by 20% for a number of real-world benchmark models.","",""
29,"Ajay-Vikram Singh, Daniel Rosenkranz, M. Ansari, Rishabh Singh, Anurag Kanase, Shubham Pratap Singh, Blair Johnston, J. Tentschert, P. Laux, A. Luch","Artificial Intelligence and Machine Learning Empower Advanced Biomedical Material Design to Toxicity Prediction",2020,"","","","",135,"2022-07-13 09:22:49","","10.1002/aisy.202000084","","",,,,,29,14.50,3,10,2,"Materials at the nanoscale exhibit specific physicochemical interactions with their environment. Therefore, evaluating their toxic potential is a primary requirement for regulatory purposes and for the safer development of nanomedicines. In this review, to aid the understanding of nano–bio interactions from environmental and health and safety perspectives, the potential, reality, challenges, and future advances that artificial intelligence (AI) and machine learning (ML) present are described. Herein, AI and ML algorithms that assist in the reporting of the minimum information required for biomaterial characterization and aid in the development and establishment of standard operating procedures are focused. ML tools and ab initio simulations adopted to improve the reproducibility of data for robust quantitative comparisons and to facilitate in silico modeling and meta‐analyses leading to a substantial contribution to safe‐by‐design development in nanotoxicology/nanomedicine are mainly focused. In addition, future opportunities and challenges in the application of ML in nanoinformatics, which is particularly well‐suited for the clinical translation of nanotherapeutics, are highlighted. This comprehensive review is believed that it will promote an unprecedented involvement of AI research in improvements in the field of nanotoxicology and nanomedicine.","",""
22,"F. Plisson, O. Ramírez-Sánchez, Cristina Martínez-Hernández","Machine learning-guided discovery and design of non-hemolytic peptides",2020,"","","","",136,"2022-07-13 09:22:49","","10.1038/s41598-020-73644-6","","",,,,,22,11.00,7,3,2,"","",""
31,"Chongchong Qi, Qiu-song Chen, S. Sonny Kim","Integrated and intelligent design framework for cemented paste backfill: A combination of robust machine learning modelling and multi-objective optimization",2020,"","","","",137,"2022-07-13 09:22:49","","10.1016/j.mineng.2020.106422","","",,,,,31,15.50,10,3,2,"","",""
8,"Daniil Bash, Yongqiang Cai, Vijila Chellappan, S. L. Wong, Yang Xu, Pawan Kumar, J. Tan, Anas Abutaha, J. Cheng, Y. Lim, S. Tian, D. Ren, Flore Mekki-Barrada, W. Wong, J. Kumar, Saif A. Khan, Qianxiao Li, T. Buonassisi, K. Hippalgaonkar","Machine Learning and High-Throughput Robust Design of P3HT-CNT Composite Thin Films for High Electrical Conductivity",2020,"","","","",138,"2022-07-13 09:22:49","","10.26434/chemrxiv.13265288.v1","","",,,,,8,4.00,1,19,2,"Combining high-throughput experiments with machine learning allows quick optimization of parameter spaces towards achieving target properties. In this study, we demonstrate that machine learning, combined with multi-labeled datasets, can additionally be used for scientific understanding and hypothesis testing. We introduce an automated flow system with high-throughput drop-casting for thin film preparation, followed by fast characterization of optical and electrical properties, with the capability to complete one cycle of learning of fully labeled ~160 samples in a single day. We combine regio-regular poly-3-hexylthiophene with various carbon nanotubes to achieve electrical conductivities as high as 1200 S/cm. Interestingly, a non-intuitive local optimum emerges when 10% of double-walled carbon nanotubes are added with long single wall carbon nanotubes, where the conductivity is seen to be as high as 700 S/cm, which we subsequently explain with high fidelity optical characterization. Employing dataset resampling strategies and graph-based regressions allows us to account for experimental cost and uncertainty estimation of correlated multi-outputs, and supports the proving of the hypothesis linking charge delocalization to electrical conductivity. We therefore present a robust machine-learning driven high-throughput experimental scheme that can be applied to optimize and understand properties of composites, or hybrid organic-inorganic materials.","",""
10,"R. Armiento","Database-Driven High-Throughput Calculations and Machine Learning Models for Materials Design",2019,"","","","",139,"2022-07-13 09:22:49","","10.1007/978-3-030-40245-7_17","","",,,,,10,3.33,10,1,3,"","",""
7,"D. Vanpoucke, Onno S. J. van Knippenberg, K. Hermans, K. Bernaerts, S. Mehrkanoon","Small data materials design with machine learning: When the average model knows best",2020,"","","","",140,"2022-07-13 09:22:49","","10.1063/5.0012285","","",,,,,7,3.50,1,5,2,"Machine learning is quickly becoming an important tool in modern materials design. Where many of its successes are rooted in huge datasets, the most common applications in academic and industrial materials design deal with datasets of at best a few tens of data points. Harnessing the power of machine learning in this context is, therefore, of considerable importance. In this work, we investigate the intricacies introduced by these small datasets. We show that individual data points introduce a significant chance factor in both model training and quality measurement. This chance factor can be mitigated by the introduction of an ensemble-averaged model. This model presents the highest accuracy, while at the same time, it is robust with regard to changing the dataset size. Furthermore, as only a single model instance needs to be stored and evaluated, it provides a highly efficient model for prediction purposes, ideally suited for the practical materials scientist.","",""
59,"Yang Long, Jie Ren, Yunhui Li, Hong Chen","Inverse design of photonic topological state via machine learning",2019,"","","","",141,"2022-07-13 09:22:49","","10.1063/1.5094838","","",,,,,59,19.67,15,4,3,"The photonics topological state plays an important role in recent optical physics and has led to devices with robust properties. However, the design of optical structures with the target topological states is a challenge for current research. Here, we propose an approach to achieve this goal by exploiting machine learning technologies. In our work, we focus on Zak phases, which are the topological properties of one-dimensional photonics crystals. After learning the principle between the geometrical parameters and the Zak phases, the neural network can obtain the appropriate structures of photonics crystals by applying the objective Zak phase properties. Our work would give more insights into the application of machine learning on the inverse design of the complex material properties and could be extended to other fields, i.e., advanced phononics devices.","",""
17,"F. Kriebel, Semeen Rehman, Muhammad Abdullah Hanif, Faiq Khalid, M. Shafique","Robustness for Smart Cyber Physical Systems and Internet-of-Things: From Adaptive Robustness Methods to Reliability and Security for Machine Learning",2018,"","","","",142,"2022-07-13 09:22:49","","10.1109/ISVLSI.2018.00111","","",,,,,17,4.25,3,5,4,"In recent years, the exponential growth of internet of things (IoT) and cyber physical systems (CPS) in safety critical applications has imposed severe reliability and security challenges. This is due to the heterogeneity and complex connectivity of the CPS components as well as error-prone and vulnerable nature of the underlying devices, harsh operating environments, and escalating security attacks. Different reliability threats (like soft errors, process variation and the temperature-induced dark silicon problem) have posed diverse challenges, which led to the development of various mitigation techniques on different layers of the CPS/IoT stack. Similarly, security threats (like manipulation of communication channels, hardware components and associated software) led to the development of different detection and protection techniques on different layers of the CPS/IoT stack, e.g., cross-layer and intra-layer connectivity. Towards this, the associated costs and overhead as well as potentially conflicting goals are important to be considered, e.g., most of the soft error mitigation techniques are based on redundancy and most of the security-related techniques require continuous runtime monitoring, obfuscation, attestation, and trusted execution environments. This paper first discusses different existing options for approaching this problem at different system layers, i.e., adaptive reliability and security management. These different solutions will provide a wide variety of options to choose from, as a basis for selection and adaptation, to solve reliability-related problems at design-time and run-time. Due to the exponential increase in the complexity and functional requirements, there is a trend towards employing Machine Learning in CPSs and IoT systems. Therefore, we will show how systems can be protected against different security and reliability threats when Machine Learning sub-systems are employed in CPS/IoT.","",""
19,"T. Le, M. Penna, D. Winkler, I. Yarovsky","Quantitative design rules for protein-resistant surface coatings using machine learning",2019,"","","","",143,"2022-07-13 09:22:49","","10.1038/s41598-018-36597-5","","",,,,,19,6.33,5,4,3,"","",""
17,"Fan Li, Xiaoqi Peng, Zuo Wang, Yi Zhou, Yuxia Wu, Minlin Jiang, Min Xu","Machine Learning (ML)‐Assisted Design and Fabrication for Solar Cells",2019,"","","","",144,"2022-07-13 09:22:49","","10.1002/eem2.12049","","",,,,,17,5.67,2,7,3,"Photovoltaic (PV) technologies have attracted great interest due to their capability of generating electricity directly from sunlight. Machine learning (ML) is a technique for computer to learn how to perform a specific task using known data. It can be used in many areas and has become a hot research topic recently due to the rapid accumulation of data and advancement of computer hardware. The application of ML techniques in the design and fabrication of solar cells started slowly but has recently gained tremendous momentum. An exhaustive compilation of the literatures indicates that all the major aspects in the research and development of solar cells can be effectively assisted by ML techniques. If combined with other tools and fed with additional theoretical and experimental data, more accurate and robust results can be achieved from ML techniques. The aspects can be grouped into four categories: prediction of material properties, optimization of device structures, optimization of fabrication processes, and reconstruction of measurement data. A statistical analysis of the literatures shows that artificial neural network (ANN) and genetic algorithm (GA) are the two most applied ML techniques and the topics in the optimization of device structures and optimization of fabrication processes are more popular. This article can be used as a reference by all PV researchers who are interested in ML techniques.","",""
13,"Chenru Duan, F. Liu, A. Nandy, H. Kulik","Putting Density Functional Theory to the Test in Machine-Learning-Accelerated Materials Discovery",2021,"","","","",145,"2022-07-13 09:22:49","","10.1021/acs.jpclett.1c00631","","",,,,,13,13.00,3,4,1,"Accelerated discovery with machine learning (ML) has begun to provide the advances in efficiency needed to overcome the combinatorial challenge of computational materials design. Nevertheless, ML-accelerated discovery both inherits the biases of training data derived from density functional theory (DFT) and leads to many attempted calculations that are doomed to fail. Many compelling functional materials and catalytic processes involve strained chemical bonds, open-shell radicals and diradicals, or metal-organic bonds to open-shell transition-metal centers. Although promising targets, these materials present unique challenges for electronic structure methods and combinatorial challenges for their discovery. In this Perspective, we describe the advances needed in accuracy, efficiency, and approach beyond what is typical in conventional DFT-based ML workflows. These challenges have begun to be addressed through ML models trained to predict the results of multiple methods or the differences between them, enabling quantitative sensitivity analysis. For DFT to be trusted for a given data point in a high-throughput screen, it must pass a series of tests. ML models that predict the likelihood of calculation success and detect the presence of strong correlation will enable rapid diagnoses and adaptation strategies. These ""decision engines"" represent the first steps toward autonomous workflows that avoid the need for expert determination of the robustness of DFT-based materials discoveries.","",""
12,"Maziar Montazerian, Edgar Dutra Zanotto, J. Mauro","Model-driven design of bioactive glasses: from molecular dynamics through machine learning",2020,"","","","",146,"2022-07-13 09:22:49","","10.1080/09506608.2019.1694779","","",,,,,12,6.00,4,3,2,"ABSTRACT Research in bioactive glasses (BGs) has traditionally been performed through trial-and-error experimentation. However, several modelling techniques will accelerate the discovery of new BGs as part of the ongoing endeavour to ‘decode the glass genome.’ Here, we critically review recent publications applying molecular dynamics simulations, machine learning approaches, and other modelling techniques for understanding BGs. We argue that modelling should be utilised more frequently in the design of BGs to achieve properties such as high bioactivity, high fracture strength and toughness, low density, and controlled morphology. Another challenge is modelling the biological response to biomaterials, such as their ability to foster protein adsorption, cell adhesion, cell proliferation, osteogenesis, angiogenesis, and bactericidal effects. The development of databases integrated with robust computational tools will be indispensable to these efforts. Future challenges are thus envisaged in which the compositional design, synthesis, characterisation, and application of BGs can be greatly accelerated by computational modelling.","",""
9,"S. Aich, Sabyasachi Chakraborty, J. Sim, Dong-Jin Jang, Hee-Cheol Kim","The Design of an Automated System for the Analysis of the Activity and Emotional Patterns of Dogs with Wearable Sensors Using Machine Learning",2019,"","","","",147,"2022-07-13 09:22:49","","10.3390/app9224938","","",,,,,9,3.00,2,5,3,"The safety and welfare of companion animals such as dogs has become a large challenge in the last few years. To assess the well-being of a dog, it is very important for human beings to understand the activity pattern of the dog, and its emotional behavior. A wearable, sensor-based system is suitable for such ends, as it will be able to monitor the dogs in real-time. However, the question remains unanswered as to what kind of data should be used to detect the activity patterns and emotional patterns, as does another: what should be the location of the sensors for the collection of data and how should we automate the system? Yet these questions remain unanswered, because to date, there is no such system that can address the above-mentioned concerns. The main purpose of this study was (1) to develop a system that can detect the activities and emotions based on the accelerometer and gyroscope signals and (2) to automate the system with robust machine learning techniques for implementing it for real-time situations. Therefore, we propose a system which is based on the data collected from 10 dogs, including nine breeds of various sizes and ages, and both genders. We used machine learning classification techniques for automating the detection and evaluation process. The ground truth fetched for the evaluation process was carried out by taking video recording data in frame per second and the wearable sensors data were collected in parallel with the video recordings. Evaluation of the system was performed using an ANN (artificial neural network), random forest, SVM (support vector machine), KNN (k nearest neighbors), and a naïve Bayes classifier. The robustness of our system was evaluated by taking independent training and validation sets. We achieved an accuracy of 96.58% while detecting the activity and 92.87% while detecting emotional behavior, respectively. This system will help the owners of dogs to track their behavior and emotions in real-life situations for various breeds in different scenarios.","",""
12,"E. Talbi","Machine Learning into Metaheuristics",2021,"","","","",148,"2022-07-13 09:22:49","","10.1145/3459664","","",,,,,12,12.00,12,1,1,"During the past few years, research in applying machine learning (ML) to design efficient, effective, and robust metaheuristics has become increasingly popular. Many of those machine learning-supported metaheuristics have generated high-quality results and represent state-of-the-art optimization algorithms. Although various appproaches have been proposed, there is a lack of a comprehensive survey and taxonomy on this research topic. In this article, we will investigate different opportunities for using ML into metaheuristics. We define uniformly the various ways synergies that might be achieved. A detailed taxonomy is proposed according to the concerned search component: target optimization problem and low-level and high-level components of metaheuristics. Our goal is also to motivate researchers in optimization to include ideas from ML into metaheuristics. We identify some open research issues in this topic that need further in-depth investigations.","",""
82,"Qian Yang, Jina Suh, N. Chen, Gonzalo A. Ramos","Grounding Interactive Machine Learning Tool Design in How Non-Experts Actually Build Models",2018,"","","","",149,"2022-07-13 09:22:49","","10.1145/3196709.3196729","","",,,,,82,20.50,21,4,4,"Machine learning (ML) promises data-driven insights and solutions for people from all walks of life, but the skill of crafting these solutions is possessed by only a few. Emerging research addresses this issue by creating ML tools that are easy and accessible to people who are not formally trained in ML (non-experts). This work investigated how non-experts build ML solutions for themselves in real life. Our interviews and surveys revealed unique potentials of non-expert ML, as well several pitfalls that non-experts are susceptible to. For example, many perceived percentage accuracy as a sole measure of performance, thus problematic models proceeded to deployment. These observations suggested that, while challenging, making ML easy and robust should both be important goals of designing novice-facing ML tools. To advance on this insight, we discuss design implications and created a sensitizing concept to demonstrate how designers might guide non-experts to easily build robust solutions.","",""
10,"Chengjun Xu, G. Zhu","Intelligent manufacturing Lie Group Machine Learning: real-time and efficient inspection system based on fog computing",2020,"","","","",150,"2022-07-13 09:22:49","","10.1007/s10845-020-01570-5","","",,,,,10,5.00,5,2,2,"","",""
53,"M. Shafique, Mahum Naseer, T. Theocharides, C. Kyrkou, O. Mutlu, Lois Orosa, Jungwook Choi","Robust Machine Learning Systems: Challenges,Current Trends, Perspectives, and the Road Ahead",2020,"","","","",151,"2022-07-13 09:22:49","","10.1109/MDAT.2020.2971217","","",,,,,53,26.50,8,7,2,"Currently, machine learning (ML) techniques are at the heart of smart cyber-physical systems (CPS) and Internet-of-Things (IoT). This article discusses various challenges and probable solutions for security attacks on these ML-inspired hardware and software techniques. —Partha Pratim Pande, Washington State University","",""
106,"Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, V. Larivière, A. Beygelzimer, Florence d'Alché-Buc, E. Fox, H. Larochelle","Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",2020,"","","","",152,"2022-07-13 09:22:49","","","","",,,,,106,53.00,13,8,2,"One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.","",""
21,"Kevin Fauvel, Daniel Balouek-Thomert, D. Melgar, Pedro Silva, Anthony Simonet, G. Antoniu, Alexandru Costan, Véronique Masson, M. Parashar, I. Rodero, A. Termier","A Distributed Multi-Sensor Machine Learning Approach to Earthquake Early Warning",2020,"","","","",153,"2022-07-13 09:22:49","","10.1609/AAAI.V34I01.5376","","",,,,,21,10.50,2,11,2,"Our research aims to improve the accuracy of Earthquake Early Warning (EEW) systems by means of machine learning. EEW systems are designed to detect and characterize medium and large earthquakes before their damaging effects reach a certain location. Traditional EEW methods based on seismometers fail to accurately identify large earthquakes due to their sensitivity to the ground motion velocity. The recently introduced high-precision GPS stations, on the other hand, are ineffective to identify medium earthquakes due to its propensity to produce noisy data. In addition, GPS stations and seismometers may be deployed in large numbers across different locations and may produce a significant volume of data consequently, affecting the response time and the robustness of EEW systems.In practice, EEW can be seen as a typical classification problem in the machine learning field: multi-sensor data are given in input, and earthquake severity is the classification result. In this paper, we introduce the Distributed Multi-Sensor Earthquake Early Warning (DMSEEW) system, a novel machine learning-based approach that combines data from both types of sensors (GPS stations and seismometers) to detect medium and large earthquakes. DMSEEW is based on a new stacking ensemble method which has been evaluated on a real-world dataset validated with geoscientists. The system builds on a geographically distributed infrastructure, ensuring an efficient computation in terms of response time and robustness to partial infrastructure failures. Our experiments show that DMSEEW is more accurate than the traditional seismometer-only approach and the combined-sensors (GPS and seismometers) approach that adopts the rule of relative strength.","",""
7,"Apostolos Ampountolas, Mark Legg","A segmented machine learning modeling approach of social media for predicting occupancy",2021,"","","","",154,"2022-07-13 09:22:49","","10.1108/IJCHM-06-2020-0611","","",,,,,7,7.00,4,2,1," Purpose This study aims to predict hotel demand through text analysis by investigating keyword series to increase demand predictions’ precision. To do so, this paper presents a framework for modeling hotel demand that incorporates machine learning techniques.   Design/methodology/approach The empirical forecasting is conducted by introducing a segmented machine learning approach of leveraging hierarchical clustering tied to machine learning and deep learning techniques. These features allow the model to yield more precise estimates. This study evaluates an extensive range of social media–derived words with the most significant probability of gradually establishing an understanding of an optimal outcome. Analyzes were performed on a major hotel chain in an urban market setting within the USA.   Findings The findings indicate that while traditional methods, being the naïve approach and ARIMA models, struggled with forecasting accuracy, segmented boosting methods (XGBoost) leveraging social media predict hotel occupancy with greater precision for all examined time horizons. Additionally, the segmented learning approach improved the forecasts’ stability and robustness while mitigating common overfitting issues within a highly dimensional data set.   Research limitations/implications Incorporating social media into a segmented learning framework can augment the current generation of forecasting methods’ accuracy. Moreover, the segmented learning approach mitigates the negative effects of market shifts (e.g. COVID-19) that can reduce in-production forecasts’ life-cycles. The ability to be more robust to market deviations will allow hospitality firms to minimize development time.   Originality/value The results are expected to generate insights by providing revenue managers with an instrument for predicting demand. ","",""
60,"M. Hannan, M. Lipu, A. Hussain, P. Ker, T. Mahlia, M. Mansor, A. Ayob, M. Saad, Z. Dong","Toward Enhanced State of Charge Estimation of Lithium-ion Batteries Using Optimized Machine Learning Techniques",2020,"","","","",155,"2022-07-13 09:22:49","","10.1038/s41598-020-61464-7","","",,,,,60,30.00,7,9,2,"","",""
5,"Tao Chen, M. Ludkovski","A Machine Learning Approach to Adaptive Robust Utility Maximization and Hedging",2019,"","","","",156,"2022-07-13 09:22:49","","10.1137/20m1336023","","",,,,,5,1.67,3,2,3,"We investigate the adaptive robust control framework for portfolio optimization and loss-based hedging under drift and volatility uncertainty. Adaptive robust problems offer many advantages but require handling a double optimization problem (infimum over market measures, supremum over the control) at each instance. Moreover, the underlying Bellman equations are intrinsically multi-dimensional. We propose a novel machine learning approach that solves for the local saddle-point at a chosen set of inputs and then uses a nonparametric (Gaussian process) regression to obtain a functional representation of the value function. Our algorithm resembles control randomization and regression Monte Carlo techniques but also brings multiple innovations, including adaptive experimental design, separate surrogates for optimal control and the local worst-case measure, and computational speed-ups for the sup-inf optimization. Thanks to the new scheme we are able to consider settings that have been previously computationally intractable and provide several new financial insights about learning and optimal trading under unknown market parameters. In particular, we demonstrate the financial advantages of adaptive robust framework compared to adaptive and static robust alternatives.","",""
1920,"A. Kurakin, Ian J. Goodfellow, Samy Bengio","Adversarial Machine Learning at Scale",2016,"","","","",157,"2022-07-13 09:22:49","","","","",,,,,1920,320.00,640,3,6,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.","",""
20,"Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae","Engineering problems in machine learning systems",2019,"","","","",158,"2022-07-13 09:22:49","","10.1007/s10994-020-05872-w","","",,,,,20,6.67,7,3,3,"","",""
10,"Floriant Labarrière, Elizabeth Thomas, Laurine Calistri, V. Optasanu, Mathieu Gueugnon, P. Ornetti, D. Laroche","Machine Learning Approaches for Activity Recognition and/or Activity Prediction in Locomotion Assistive Devices—A Systematic Review",2020,"","","","",159,"2022-07-13 09:22:49","","10.3390/s20216345","","",,,,,10,5.00,1,7,2,"Locomotion assistive devices equipped with a microprocessor can potentially automatically adapt their behavior when the user is transitioning from one locomotion mode to another. Many developments in the field have come from machine learning driven controllers on locomotion assistive devices that recognize/predict the current locomotion mode or the upcoming one. This review synthesizes the machine learning algorithms designed to recognize or to predict a locomotion mode in order to automatically adapt the behavior of a locomotion assistive device. A systematic review was conducted on the Web of Science and MEDLINE databases (as well as in the retrieved papers) to identify articles published between 1 January 2000 to 31 July 2020. This systematic review is reported in accordance with the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guidelines and is registered on Prospero (CRD42020149352). Study characteristics, sensors and algorithms used, accuracy and robustness were also summarized. In total, 1343 records were identified and 58 studies were included in this review. The experimental condition which was most often investigated was level ground walking along with stair and ramp ascent/descent activities. The machine learning algorithms implemented in the included studies reached global mean accuracies of around 90%. However, the robustness of those algorithms seems to be more broadly evaluated, notably, in everyday life. We also propose some guidelines for homogenizing future reports.","",""
242,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",160,"2022-07-13 09:22:49","","10.1145/3308809.3308857","","",,,,,242,48.40,81,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server andm working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of them working machines suffer Byzantine faults - a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks.","",""
6,"Jamie Hayes","Provable trade-offs between private & robust machine learning",2020,"","","","",161,"2022-07-13 09:22:49","","","","",,,,,6,3.00,6,1,2,"Historically, machine learning methods have not been designed with security in mind. In turn, this has given rise to adversarial examples, carefully perturbed input samples aimed to mislead detection at test time, which have been applied to attack spam and malware classification, and more recently to attack image classification. Consequently, an abundance of research has been devoted to designing machine learning methods that are robust to adversarial examples. Unfortunately, there are desiderata besides robustness that a secure and safe machine learning model must satisfy, such as fairness and privacy. Recent work by Song et al. (2019) has shown, empirically, that there exists a trade-off between robust and private machine learning models. Models designed to be robust to adversarial examples often overfit on training data to a larger extent than standard (non-robust) models. If a dataset contains private information, then any statistical test that separates training and test data by observing a model's outputs can represent a privacy breach, and if a model overfits on training data, these statistical tests become easier.  In this work, we identify settings where standard models will provably overfit to a larger extent in comparison to robust models, and as empirically observed in previous works, settings where the opposite behavior occurs. Thus, it is not necessarily the case that privacy must be sacrificed to achieve robustness. The degree of overfitting naturally depends on the amount of data available for training. We go on to formally characterize how the training set size factors into the privacy risks exposed by training a robust model. Finally, we empirically show our findings hold on image classification benchmark datasets, such as CIFAR-10.","",""
24,"A. Hürkamp, S. Gellrich, T. Ossowski, Jan Beuscher, S. Thiede, C. Herrmann, K. Dröder","Combining Simulation and Machine Learning as Digital Twin for the Manufacturing of Overmolded Thermoplastic Composites",2020,"","","","",162,"2022-07-13 09:22:49","","10.3390/jmmp4030092","","",,,,,24,12.00,3,7,2,"The design and development of composite structures requires precise and robust manufacturing processes. Composite materials such as fiber reinforced thermoplastics (FRTP) provide a good balance between manufacturing time, mechanical performance and weight. In this contribution, we investigate the process combination of thermoforming FRTP sheets (organo sheets) and injection overmolding of short FRTP for automotive structures. The limiting factor in those structures is the bond strength between the organo sheet and the overmolded thermoplastic. Within this process chain, even small deviations of the process settings (e.g., temperature) can lead to significant defects in the structure. A cyber physical production system based framework for a digital twin combining simulation and machine learning is presented. Based on parametric Finite-Element-Method (FEM) studies, training data for machine learning methods are generated and a FEM surrogate is developed. A comparison of different data-driven methods yields information on the estimation accuracy of task-specific data-driven methods. Finally, in accordance with experimental cross tension tests, the investigated FEM surrogate model is able to predict the interface bond strength quality in dependence of the process settings. The visualization into different quality domains qualifies the presented approach as decision support.","",""
26,"W. Gou, Chu-wen Ling, Yan He, Zengliang Jiang, Yuanqing Fu, Fengzhe Xu, Z. Miao, Ting-yu Sun, Jie-sheng Lin, Hui-lian Zhu, Hongwei Zhou, Yu-ming Chen, Ju-Sheng Zheng","Interpretable Machine Learning Framework Reveals Robust Gut Microbiome Features Associated With Type 2 Diabetes",2020,"","","","",163,"2022-07-13 09:22:49","","10.2337/dc20-1536","","",,,,,26,13.00,3,13,2,"OBJECTIVE To identify the core gut microbial features associated with type 2 diabetes risk and potential demographic, adiposity, and dietary factors associated with these features. RESEARCH DESIGN AND METHODS We used an interpretable machine learning framework to identify the type 2 diabetes–related gut microbiome features in the cross-sectional analyses of three Chinese cohorts: one discovery cohort (n = 1,832, 270 cases of type 2 diabetes) and two validation cohorts (cohort 1: n = 203, 48 cases; cohort 2: n = 7,009, 608 cases). We constructed a microbiome risk score (MRS) with the identified features. We examined the prospective association of the MRS with glucose increment in 249 participants without type 2 diabetes and assessed the correlation between the MRS and host blood metabolites (n = 1,016). We transferred human fecal samples with different MRS levels to germ-free mice to confirm the MRS–type 2 diabetes relationship. We then examined the prospective association of demographic, adiposity, and dietary factors with the MRS (n = 1,832). RESULTS The MRS (including 14 microbial features) consistently associated with type 2 diabetes, with risk ratio for per 1-unit change in MRS 1.28 (95% CI 1.23–1.33), 1.23 (1.13–1.34), and 1.12 (1.06–1.18) across three cohorts. The MRS was positively associated with future glucose increment (P < 0.05) and was correlated with a variety of gut microbiota–derived blood metabolites. Animal study further confirmed the MRS–type 2 diabetes relationship. Body fat distribution was found to be a key factor modulating the gut microbiome–type 2 diabetes relationship. CONCLUSIONS Our results reveal a core set of gut microbiome features associated with type 2 diabetes risk and future glucose increment.","",""
25,"Nastaran Meftahi, M. Klymenko, A. Christofferson, U. Bach, D. Winkler, S. Russo","Machine learning property prediction for organic photovoltaic devices",2020,"","","","",164,"2022-07-13 09:22:49","","10.1038/s41524-020-00429-w","","",,,,,25,12.50,4,6,2,"","",""
19,"A. Menon, Chetali Gupta, K. Perkins, B. DeCost, Nikita Budwal, Renee T. Rios, Kunpeng Zhang, B. Póczos, N. Washburn","Elucidating multi-physics interactions in suspensions for the design of polymeric dispersants: a hierarchical machine learning approach",2017,"","","","",165,"2022-07-13 09:22:49","","10.1039/C7ME00027H","","",,,,,19,3.80,2,9,5,"A computational method for understanding and optimizing the properties of complex physical systems is presented using polymeric dispersants as an example. Concentrated suspensions are formulated with dispersants to tune rheological parameters, such as yield stress or viscosity, but their competing effects on solution and particle variables have made it impossible to design them based on our knowledge of the interplay of chemistry and function. Here, physical and statistical modeling are integrated into a hierarchical framework of machine learning that provides insight into sparse experimental datasets. A library of 10 polymers having similar molecular weight but incorporating different functional groups commonly found in aqueous dispersants was used as a training set in magnesium oxide slurries. The compositions of these polymers were the experimental variables that determined the complex system responses, but the method leverages knowledge of the constituent “single-physics” interactions that underlie the suspension properties. Integration of domain knowledge is shown to allow robust predictions based on orders of magnitude fewer samples in the training set compared with purely statistical methods that directly correlate dispersant chemistry with changes in rheological properties. Minimization of the resulting function for slurry yield stress resulted in the prediction of a novel dispersant that was synthesized and shown to impart similar reductions as a leading commercial dispersant but with a significantly different composition and molecular architecture.","",""
19,"Sidra Mehtab, Jaydip Sen","A Time Series Analysis-Based Stock Price Prediction Using Machine Learning and Deep Learning Models",2020,"","","","",166,"2022-07-13 09:22:49","","10.1504/IJBFMI.2020.115691","","",,,,,19,9.50,10,2,2,"Prediction of future movement of stock prices has always been a challenging task for the researchers. While the advocates of the efficient market hypothesis (EMH) believe that it is impossible to design any predictive framework that can accurately predict the movement of stock prices, there are seminal work in the literature that have clearly demonstrated that the seemingly random movement patterns in the time series of a stock price can be predicted with a high level of accuracy. Design of such predictive models requires choice of appropriate variables, right transformation methods of the variables, and tuning of the parameters of the models. In this work, we present a very robust and accurate framework of stock price prediction that consists of an agglomeration of statistical, machine learning and deep learning models. We use the daily stock price data, collected at five minutes interval of time, of a very well known company that is listed in the National Stock Exchange (NSE) of India. The granular data is aggregated into three slots in a day, and the aggregated data is used for building and training the forecasting models. We contend that the agglomerative approach of model building that uses a combination of statistical, machine learning, and deep learning approaches, can very effectively learn from the volatile and random movement patterns in a stock price data. We build eight classification and eight regression models based on statistical and machine learning approaches. In addition to these models, a deep learning regression model using a long-and-short-term memory (LSTM) network is also built. Extensive results have been presented on the performance of these models, and the results are critically analyzed.","",""
17,"Liangyi Gong, Zhenhua Li, Feng Qian, Zi-Mei Zhang, Qi Alfred Chen, Zhiyun Qian, Hao Lin, Yunhao Liu","Experiences of landing machine learning onto market-scale mobile malware detection",2020,"","","","",167,"2022-07-13 09:22:49","","10.1145/3342195.3387530","","",,,,,17,8.50,2,8,2,"App markets, being crucial and critical for today's mobile ecosystem, have also become a natural malware delivery channel since they actually ""lend credibility"" to malicious apps. In the past decade, machine learning (ML) techniques have been explored for automated, robust malware detection. Unfortunately, to date, we have yet to see an ML-based malware detection solution deployed at market scales. To better understand the real-world challenges, we conduct a collaborative study with a major Android app market (T-Market) offering us large-scale ground-truth data. Our study shows that the key to successfully developing such systems is manifold, including feature selection/engineering, app analysis speed, developer engagement, and model evolution. Failure in any of the above aspects would lead to the ""wooden barrel effect"" of the entire system. We discuss our careful design choices as well as our first-hand deployment experiences in building such an ML-powered malware detection system. We implement our design and examine its effectiveness in the T-Market for over one year, using a single commodity server to vet ~ 10K apps every day. The evaluation results show that this design achieves an overall precision of 98% and recall of 96% with an average per-app scan time of 1.3 minutes.","",""
15,"E. Talbi","Machine learning into metaheuristics: A survey and taxonomy of data-driven metaheuristics",2020,"","","","",168,"2022-07-13 09:22:49","","","","",,,,,15,7.50,15,1,2,"During the last years, research in applying machine learning (ML) to design efficient, effective and robust metaheuristics became increasingly popular. Many of those data driven metaheuristics have generated high quality results and represent state-of-the-art optimization algorithms. Although various appproaches have been proposed, there is a lack of a comprehensive survey and taxonomy on this research topic. In this paper we will investigate different opportunities for using ML into metaheuristics. We define uniformly the various ways synergies which might be achieved. A detailed taxonomy is proposed according to the concerned search component: target optimization problem, low-level and high-level components of metaheuristics. Our goal is also to motivate researchers in optimization to include ideas from ML into metaheuristics. We identify some open research issues in this topic which needs further in-depth investigations.","",""
16,"Abinet Tesfaye Eseye, M. Lehtonen","Short-Term Forecasting of Heat Demand of Buildings for Efficient and Optimal Energy Management Based on Integrated Machine Learning Models",2020,"","","","",169,"2022-07-13 09:22:49","","10.1109/TII.2020.2970165","","",,,,,16,8.00,8,2,2,"The increasing growth in the energy demand calls for robust actions to design and optimize energy-related assets for efficient and economic energy supply and demand within a smart grid setup. This article proposes a novel integrated machine learning (ML) technique to forecast the heat demand of buildings in a district heating system. The proposed short-term (24h-ahead) heat demand forecasting model is based on the integration of empirical mode decomposition (EMD), imperialistic competitive algorithm (ICA), and support vector machine (SVM). The proposed model also embeds an ML-based feature selection (FS) technique combining binary genetic algorithm and Gaussian process regression to obtain the most important and nonredundant variables that can constitute the input predictor subset to the forecasting model. The model is developed using a two-year (2015–2016) hourly dataset of actual district heat demand obtained from various buildings in the Otaniemi area of Espoo, Finland. Several variables from different domains such as seasonality (calendar), weather, occupancy, and heat demand are used to construct the initial feature space for FS process. Short-term forecasting models are also implemented using the Persistence approach as a reference and other eight ML approaches: artificial neural network (ANN), genetic algorithm combined with ANN (GA-ANN), ICA-ANN, SVM, GA-SVM, ICA-SVM, EMD-GA-ANN, and EMD-ICA-ANN. The performance of the proposed EMD-ICA-SVM-based forecasting model is tested using an out-of-sample one-year (2017) hourly dataset of district heat consumption of various building types. Comparative analysis of the forecasting performance of the models was performed. The obtained results demonstrate that the devised model forecasts the heat demand with improved performance evaluated using various accuracy metrics. Moreover, the devised model achieves outperformed forecasting accuracy enhancement, compared to the other nine evaluated models.","",""
15,"Itzel Nunez, Afshin Marani, M. Nehdi","Mixture Optimization of Recycled Aggregate Concrete Using Hybrid Machine Learning Model",2020,"","","","",170,"2022-07-13 09:22:49","","10.3390/ma13194331","","",,,,,15,7.50,5,3,2,"Recycled aggregate concrete (RAC) contributes to mitigating the depletion of natural aggregates, alleviating the carbon footprint of concrete construction, and averting the landfilling of colossal amounts of construction and demolition waste. However, complexities in the mixture optimization of RAC due to the variability of recycled aggregates and lack of accuracy in estimating its compressive strength require novel and sophisticated techniques. This paper aims at developing state-of-the-art machine learning models to predict the RAC compressive strength and optimize its mixture design. Results show that the developed models including Gaussian processes, deep learning, and gradient boosting regression achieved robust predictive performance, with the gradient boosting regression trees yielding highest prediction accuracy. Furthermore, a particle swarm optimization coupled with gradient boosting regression trees model was developed to optimize the mixture design of RAC for various compressive strength classes. The hybrid model achieved cost-saving RAC mixture designs with lower environmental footprint for different target compressive strength classes. The model could be further harvested to achieve sustainable concrete with optimal recycled aggregate content, least cost, and least environmental footprint.","",""
14,"A. Mahmoud, Salaheldin Elkatatny, D. Al Shehri","Application of Machine Learning in Evaluation of the Static Young’s Modulus for Sandstone Formations",2020,"","","","",171,"2022-07-13 09:22:49","","10.3390/su12051880","","",,,,,14,7.00,5,3,2,"Prediction of the mechanical characteristics of the reservoir formations, such as static Young’s modulus (Estatic), is very important for the evaluation of the wellbore stability and development of the earth geomechanical model. Estatic considerably varies with the change in the lithology. Therefore, a robust model for Estatic prediction is needed. In this study, the predictability of Estatic for sandstone formation using four machine learning models was evaluated. The design parameters of the machine learning models were optimized to improve their predictability. The machine learning models were trained to estimate Estatic based on bulk formation density, compressional transit time, and shear transit time. The machine learning models were trained and tested using 592 well log data points and their corresponding core-derived Estatic values collected from one sandstone formation in well-A and then validated on 38 data points collected from a sandstone formation in well-B. Among the machine learning models developed in this work, Mamdani fuzzy interference system was the highly accurate model to predict Estatic for the validation data with an average absolute percentage error of only 1.56% and R of 0.999. The developed static Young’s modulus prediction models could help the new generation to characterize the formation rock with less cost and safe operation.","",""
19,"J. Mallick, S. AlQadhi, Swapan Talukdar, Majed Alsubih, Mohd. Ahmed, R. A. Khan, N. Kahla, Saud M. Abutayeh","Risk Assessment of Resources Exposed to Rainfall Induced Landslide with the Development of GIS and RS Based Ensemble Metaheuristic Machine Learning Algorithms",2021,"","","","",172,"2022-07-13 09:22:49","","10.3390/SU13020457","","",,,,,19,19.00,2,8,1,"Disastrous natural hazards, such as landslides, floods, and forest fires cause a serious threat to natural resources, assets and human lives. Consequently, landslide risk assessment has become requisite for managing the resources in future. This study was designed to develop four ensemble metaheuristic machine learning algorithms, such as grey wolf optimized based artificial neural network (GW-ANN), grey wolf optimized based random forest (GW-RF), particle swarm optimization optimized based ANN (PSO-ANN), and PSO optimized based RF for modeling rainfall-induced landslide susceptibility (LS) in Aqabat Al-Sulbat, Asir region, Saudi Arabia, which observes landslide frequently. To obtain very high precision and robust prediction from machine learning algorithms, the grey wolf and PSO optimization algorithms were integrated to develop new ensemble machine learning techniques. Subsequently, LS maps produced by training dataset were validated using the receiver operating characteristics (ROC) curve based on the testing dataset. Based on the area under curve (AUC) value of ROC curve, the best method for LS modeling was selected. We developed ROC curve-based sensitivity analysis to investigate the influence of the parameters for LS modeling. The Gumble extreme value distribution was employed to estimate the rainfall at 2, 5, 10, 20, 50, and 100 year return periods. Then, the landslide hazard maps were prepared at different return periods by integrating the best LS model and estimated rainfall at different return periods. The theory of danger pixels was employed to prepare a final risk assessment of the resources, which have been exposed to the landslide. The results showed that 27–42 and 6–15 km2 were predicted as the very high and high LS zones using four ensemble metaheuristic machine learning algorithms. Based on the area under curve (AUC) of ROC, GR-ANN (AUC-0.905) appeared as the best model for LS modeling. The areas under high and very high landslide hazard were gradually increased over the progression of time (26 km2 at the 2 year return period and 40 km2 at the 100 year return period for the high landslide hazard zone, and 6 km2 at the 2 year return period and 20 km2 at the 100 year return period for the very high landslide hazard zone). Similarly, the areas of danger pixel also increased gradually from the 2 to 100 year return periods (37 km2 to 62 km2). Various natural resources, such as scrubland, built up, and sparse vegetation, were identified under risk zone due to landslide hazards. In addition, these resources would be exposed extensively to landslides over the advancement of return periods. Therefore, the outcome of the present study will help planners and scientists to propose high precision management plans for protecting natural resources, which have been exposed to landslides.","",""
9,"Ruben F. Kranenburg, J. Verduin, Y. Weesepoel, M. Alewijn, Marcel Heerschop, G. Koomen, P. Keizers, Frank Bakker, Fionn Wallace, Annette van Esch, Annemieke Hulsbergen, A. V. van Asten","Rapid and robust on‐scene detection of cocaine in street samples using a handheld near‐infrared spectrometer and machine learning algorithms",2020,"","","","",173,"2022-07-13 09:22:49","","10.1002/dta.2895","","",,,,,9,4.50,1,12,2,"Abstract On‐scene drug detection is an increasingly significant challenge due to the fast‐changing drug market as well as the risk of exposure to potent drug substances. Conventional colorimetric cocaine tests involve handling of the unknown material and are prone to false‐positive reactions on common pharmaceuticals used as cutting agents. This study demonstrates the novel application of 740–1070 nm small‐wavelength‐range near‐infrared (NIR) spectroscopy to confidently detect cocaine in case samples. Multistage machine learning algorithms are used to exploit the limited spectral features and predict not only the presence of cocaine but also the concentration and sample composition. A model based on more than 10,000 spectra from case samples yielded 97% true‐positive and 98% true‐negative results. The practical applicability is shown in more than 100 case samples not included in the model design. One of the most exciting aspects of this on‐scene approach is that the model can almost instantly adapt to changes in the illicit‐drug market by updating metadata with results from subsequent confirmatory laboratory analyses. These results demonstrate that advanced machine learning strategies applied on limited‐range NIR spectra from economic handheld sensors can be a valuable procedure for rapid on‐site detection of illicit substances by investigating officers. In addition to forensics, this interesting approach could be beneficial for screening and classification applications in the pharmaceutical, food‐safety, and environmental domains.","",""
9,"Wajid Hassan, T. Chou, Omar Tamer, John Pickard, Patrick Appiah-Kubi, L. Pagliari","Cloud computing survey on services, enhancements and challenges in the era of machine learning and data science",2020,"","","","",174,"2022-07-13 09:22:49","","10.11591/IJICT.V9I2.PP117-139","","",,,,,9,4.50,2,6,2,"Cloud computing has sweeping impact on the human productivity. Today it’s used for Computing, Storage, Predictions and Intelligent Decision Making, among others. Intelligent Decision Making using Machine Learning has pushed for the Cloud Services to be even more fast, robust and accurate. Security remains one of the major concerns which affect the cloud computing growth however there exist various research challenges in cloud computing adoption such as lack of well managed service level agreement (SLA), frequent disconnections, resource scarcity, interoperability, privacy, and reliability. Tremendous amount of work still needs to be done to explore the security challenges arising due to widespread usage of cloud deployment using Containers. We also discuss Impact of Cloud Computing and Cloud Standards. Hence in this research paper, a detailed survey of cloud computing, concepts, architectural principles, key services, and implementation, design and deployment challenges of cloud computing are discussed in detail and important future research directions in the era of Machine Learning and Data Science have been identified.","",""
17,"M. El-Azazy","Factorial Design and Machine Learning Strategies: Impacts on Pharmaceutical Analysis",2017,"","","","",175,"2022-07-13 09:22:49","","10.5772/INTECHOPEN.69891","","",,,,,17,3.40,17,1,5,"Pharmaceutical analysis is going through an expeditious progress as the perception of ‘multivariate data analysis’ (MVA) becomes gradually more assimilated. Pharmaceutical analysis comprises a range of processes that covers both chemical and physical assessment of drugs and their formulations employing different analytical techniques. With the revolution in instrumental analysis and the huge amount of information produced, there must be an up-to-date data processing tool. The role of chemometrics then comes up. Multivariate analysis (MVA) has the capability of effectively drawing a complete picture of the investigated process. Moreover, MVA reproduces the arithmetic influence of variables and their interactions through a smaller number of trials, keeping both efforts and capitals. Spectrophotometry is among the most extensively used techniques in pharmaceutical analysis either direct (single component) or derivative (multicomponent). In addition to these recognized benefits, using chemometrics in conjunction with spectrophotometry affects three vital characteristics: accuracy, precision and robustness. The impact of hyphenation of spectrophotometric analytical techniques to chemometrics (experimental design and support vector machines) on analytical laboratory will be revealed. A theoretical background on the different factorial designs and their relevance is provided. Readers will be able to use this chapter as a guide to select the appropriate design for a problem.","",""
7,"Bingbing Sun, T. Alkhalifah","ML-misfit: Learning a robust misfit function for full-waveform inversion using machine learning",2020,"","","","",176,"2022-07-13 09:22:49","","10.3997/2214-4609.202010466","","",,,,,7,3.50,4,2,2,"Most of the available advanced misfit functions for full waveform inversion (FWI) are hand-crafted, and the performance of those misfit functions is data-dependent. Thus, we propose to learn a misfit function for FWI, entitled ML-misfit, based on machine learning. Inspired by the optimal transport of the matching filter misfit, we design a neural network (NN) architecture for the misfit function in a form similar to comparing the mean and variance for two distributions. To guarantee the resulting learned misfit is a metric, we accommodate the symmetry of the misfit with respect to its input and a Hinge loss regularization term in a meta-loss function to satisfy the ""triangle inequality"" rule. In the framework of meta-learning, we train the network by running FWI to invert for randomly generated velocity models and update the parameters of the NN by minimizing the meta-loss, which is defined as accumulated difference between the true and inverted models. We first illustrate the basic principle of the ML-misfit for learning a convex misfit function for travel-time shifted signals. Further, we train the NN on 2D horizontally layered models, and we demonstrate the effectiveness and robustness of the learned ML-misfit by applying it to the well-known Marmousi model.","",""
16,"J. Väyrynen, M. Lau, K. Haruki, Sara A. Väyrynen, Andressa Dias Costa, J. Borowsky, Melissa Zhao, K. Fujiyoshi, K. Arima, Tyler S. Twombly, J. Kishikawa, Simeng Gu, S. Aminmozaffari, Shanshan Shi, Y. Baba, Naohiko Akimoto, T. Ugai, Annacarolina da Silva, M. Song, Kana Wu, A. Chan, R. Nishihara, C. Fuchs, J. Meyerhardt, M. Giannakis, S. Ogino, J. Nowak","Prognostic Significance of Immune Cell Populations Identified by Machine Learning in Colorectal Cancer Using Routine Hematoxylin and Eosin–Stained Sections",2020,"","","","",177,"2022-07-13 09:22:49","","10.1158/1078-0432.CCR-20-0071","","",,,,,16,8.00,2,27,2,"Purpose: Although high T-cell density is a well-established favorable prognostic factor in colorectal cancer, the prognostic significance of tumor-associated plasma cells, neutrophils, and eosinophils is less well-defined. Experimental Design: We computationally processed digital images of hematoxylin and eosin (H&E)–stained sections to identify lymphocytes, plasma cells, neutrophils, and eosinophils in tumor intraepithelial and stromal areas of 934 colorectal cancers in two prospective cohort studies. Multivariable Cox proportional hazards regression was used to compute mortality HR according to cell density quartiles. The spatial patterns of immune cell infiltration were studied using the GTumor:Immune cell function, which estimates the likelihood of any tumor cell in a sample having at least one neighboring immune cell of the specified type within a certain radius. Validation studies were performed on an independent cohort of 570 colorectal cancers. Results: Immune cell densities measured by the automated classifier demonstrated high correlation with densities both from manual counts and those obtained from an independently trained automated classifier (Spearman's ρ 0.71–0.96). High densities of stromal lymphocytes and eosinophils were associated with better cancer-specific survival [Ptrend < 0.001; multivariable HR (4th vs 1st quartile of eosinophils), 0.49; 95% confidence interval, 0.34–0.71]. High GTumor:Lymphocyte area under the curve (AUC0,20μm; Ptrend = 0.002) and high GTumor:Eosinophil AUC0,20μm (Ptrend < 0.001) also showed associations with better cancer-specific survival. High stromal eosinophil density was also associated with better cancer-specific survival in the validation cohort (Ptrend < 0.001). Conclusions: These findings highlight the potential for machine learning assessment of H&E-stained sections to provide robust, quantitative tumor-immune biomarkers for precision medicine.","",""
24,"Mukul Singh, S. Bansal, Sakshi Ahuja, R. Dubey, B. K. Panigrahi, N. Dey","Transfer learning–based ensemble support vector machine model for automated COVID-19 detection using lung computerized tomography scan data",2021,"","","","",178,"2022-07-13 09:22:49","","10.1007/s11517-020-02299-2","","",,,,,24,24.00,4,6,1,"","",""
18,"J. Lai, F. Tsai","Improving GIS-Based Landslide Susceptibility Assessments with Multi-temporal Remote Sensing and Machine Learning",2019,"","","","",179,"2022-07-13 09:22:49","","10.3390/s19173717","","",,,,,18,6.00,9,2,3,"This study developed a systematic approach with machine learning (ML) to apply the satellite remote sensing images, geographic information system (GIS) datasets, and spatial analysis for multi-temporal and event-based landslide susceptibility assessments at a regional scale. Random forests (RF) algorithm, one of the ML-based methods, was selected to construct the landslide susceptibility models. Different ratios of landslide and non-landslide samples were considered in the experiments. This study also employed a cost-sensitive analysis to adjust the decision boundary of the developed RF models with unbalanced sample ratios to improve the prediction results. Two strategies were investigated for model verification, namely space- and time-robustness. The space-robustness verification was designed for separating samples into training and examining data based on a single event or the same dataset. The time-robustness verification was designed for predicting subsequent landslide events by constructing a landslide susceptibility model based on a specific event or period. A total of 14 GIS-based landslide-related factors were used and derived from the spatial analyses. The developed landslide susceptibility models were tested in a watershed region in northern Taiwan with a landslide inventory of changes detected through multi-temporal satellite images and verified through field investigation. To further examine the developed models, the landslide susceptibility distributions of true occurrence samples and the generated landslide susceptibility maps were compared. The experiments demonstrated that the proposed method can provide more reasonable results, and the accuracies were found to be higher than 93% and 75% in most cases for space- and time-robustness verifications, respectively. In addition, the mapping results revealed that the multi-temporal models did not seem to be affected by the sample ratios included in the analyses.","",""
14,"A. A. Alshaikh, A. Magana-Mora, S. Gharbi, A. Al-yami","Machine Learning for Detecting Stuck Pipe Incidents: Data Analytics and Models Evaluation",2019,"","","","",180,"2022-07-13 09:22:49","","10.2523/IPTC-19394-MS","","",,,,,14,4.67,4,4,3,"  The earlier a stuck pipe incident is predicted and mitigated, the higher the chance of success in freeing the pipe or avoiding severe sticking in the first place. Time is crucial in such cases as an improper reaction to a stuck pipe incident can easily make it worse. In this work, practical machine learning, classification models were developed using real-time drilling data to automatically detect stuck pipe incidents during drilling operations and communicate the observations and alerts, sufficiently ahead of time, to the rig crew for avoidance or remediation actions to be taken.  The models use machine learning algorithms that feed on identified key drilling parameters to detect stuck pipe anomalies. The parameters used in building the system were selected based on published literature and historical data and reports of stuck pipe incidents and were analyzed and ranked to identify the ones of key influence on the accuracy of stuck pipe detection via a nonlinear relationship. The model exceptionally uses the robustness of data-based analysis along with the physics-based analysis.  The model has shown effective detection of the signs observed by experts ahead of time and has helped with providing enhanced stuck pipe detection and risk assessment. Validating and testing the model on several cases showed promising results as anomalies on simple and complex parameters were detected before or near the actual time stuck pipe incidents were reported from the rig crew. This facilitated better understanding of the underlying physics principles and provided awareness of stuck pipe occurrence.  The model improved monitoring and interpreting the drilling data streams. Beside such pipe signs, the model helped with detecting signs of other impeding problems in the downhole conditions of the wellbore, the drilling equipment, and the sensors. The model is designed to be implemented in the real-time drilling data portal to provide an alarm system for all oil and gas rigs based on the observed abnormalities. The alarm is to be populated on the real-time environment and communicated to the rig crew in a timely manner to ensure optimal results, giving them sufficient time ahead to prevent or remediate a potential stuck pipe incident.","",""
13,"N. Zhang, Fehmi Jaafar, Yasir Malik","Low-Rate DoS Attack Detection Using PSD Based Entropy and Machine Learning",2019,"","","","",181,"2022-07-13 09:22:49","","10.1109/CSCloud/EdgeCom.2019.00020","","",,,,,13,4.33,4,3,3,"The Distributed Denial of Service attack is one of the most common attacks and it is hard to mitigate, however, it has become more difficult while dealing with the Low-rate DoS (LDoS) attacks. The LDoS exploits the vulnerability of TCP congestion-control mechanism by sending malicious traffic at the low constant rate and influence the victim machine. Recently, machine learning approaches are applied to detect the complex DDoS attacks and improve the efficiency and robustness of the intrusion detection system. In this research, the algorithm is designed to balance the detection rate and its efficiency. The detection algorithm combines the Power Spectral Density (PSD) entropy function and Support Vector Machine to detect LDoS traffic from normal traffic. In our solution, the detection rate and efficiency are adjustable based on the parameter in the decision algorithm. To have high efficiency, the detection method will always detect the attacks by calculating PSD-entropy first and compare it with the two adaptive thresholds. The thresholds can efficiently filter nearly 19% of the samples with a high detection rate. To minimize the computational cost and look only for the patterns that are most relevant for detection, Support Vector Machine based machine learning model is applied to learn the traffic pattern and select appropriate features for detection algorithm. The experimental results show that the proposed approach can detect 99.19% of the LDoS attacks and has an O (n log n) time complexity in the best case.","",""
28,"P. Panda, I. Chakraborty, K. Roy","Discretization Based Solutions for Secure Machine Learning Against Adversarial Attacks",2019,"","","","",182,"2022-07-13 09:22:49","","10.1109/ACCESS.2019.2919463","","",,,,,28,9.33,9,3,3,"Adversarial examples are perturbed inputs that are designed (from a deep learning network’s (DLN) parameter gradients) to mislead the DLN during test time. Intuitively, constraining the dimensionality of inputs or parameters of a network reduces the “space” in which adversarial examples exist. Guided by this intuition, we demonstrate that discretization greatly improves the robustness of the DLNs against adversarial attacks. Specifically, discretizing the input space (or allowed pixel levels from 256 values or 8<inline-formula> <tex-math notation=""LaTeX"">$bit$ </tex-math></inline-formula> to 4 values or 2<inline-formula> <tex-math notation=""LaTeX"">$bit$ </tex-math></inline-formula>) extensively improves the adversarial robustness of the DLNs for a substantial range of perturbations for minimal loss in test accuracy. Furthermore, we find that binary neural networks (BNNs) and related variants are intrinsically more robust than their full precision counterparts in adversarial scenarios. Combining input discretization with the BNNs furthers the robustness, even waiving the need for adversarial training for the certain magnitude of perturbation values. We evaluate the effect of discretization on MNIST, CIFAR10, CIFAR100, and ImageNet datasets. Across all datasets, we observe maximal adversarial resistance with 2<inline-formula> <tex-math notation=""LaTeX"">$bit$ </tex-math></inline-formula> input discretization that incurs an adversarial accuracy loss of just ~1% – 2% as compared to clean test accuracy against single-step attacks. We also show standalone discretization remains vulnerable to stronger multi-step attack scenarios necessitating the use of adversarial training with discretization as an improved defense strategy.","",""
174,"Andrew F. Zahrt, J. Henle, Brennan T Rose, Yang Wang, William T. Darrow, S. Denmark","Prediction of higher-selectivity catalysts by computer-driven workflow and machine learning",2019,"","","","",183,"2022-07-13 09:22:49","","10.1126/science.aau5631","","",,,,,174,58.00,29,6,3,"Predicting catalyst selectivity Asymmetric catalysis is widely used in chemical research and manufacturing to access just one of two possible mirror-image products. Nonetheless, the process of tuning catalyst structure to optimize selectivity is still largely empirical. Zahrt et al. present a framework for more efficient, predictive optimization. As a proof of principle, they focused on a known coupling reaction of imines and thiols catalyzed by chiral phosphoric acid compounds. By modeling multiple conformations of more than 800 prospective catalysts, and then training machine-learning algorithms on a subset of experimental results, they achieved highly accurate predictions of enantioselectivities. Science, this issue p. eaau5631 A model encompassing multiple conformations of chiral phosphoric acid catalysts accurately predicts enantioselectivities. INTRODUCTION The development of new synthetic methods in organic chemistry is traditionally accomplished through empirical optimization. Catalyst design, wherein experimentalists attempt to qualitatively identify correlations between catalyst structure and catalyst efficiency, is no exception. However, this approach is plagued by numerous deficiencies, including the lack of mechanistic understanding of a new transformation, the inherent limitations of human cognitive abilities to find patterns in large collections of data, and the lack of quantitative guidelines to aid catalyst identification. Chemoinformatics provides an attractive alternative to empiricism for several reasons: Mechanistic information is not a prerequisite, catalyst structures can be characterized by three-dimensional (3D) descriptors (numerical representations of molecular properties derived from the 3D molecular structure) that quantify the steric and electronic properties of thousands of candidate molecules, and the suitability of a given catalyst candidate can be quantified by comparing its properties with a computationally derived model trained on experimental data. The ability to accurately predict a selective catalyst by using a set of less than optimal data remains a major goal for machine learning with respect to asymmetric catalysis. We report a method to achieve this goal and propose a more efficient alternative to traditional catalyst design. RATIONALE The workflow we have created consists of the following components: (i) construction of an in silico library comprising a large collection of conceivable, synthetically accessible catalysts derived from a particular scaffold; (ii) calculation of relevant chemical descriptors for each scaffold; (iii) selection of a representative subset of the catalysts [this subset is termed the universal training set (UTS) because it is agnostic to reaction or mechanism and thus can be used to optimize any reaction catalyzed by that scaffold]; (iv) collection of the training data; and (v) application of machine learning methods to generate models that predict the enantioselectivity of each member of the in silico library. These models are evaluated with an external test set of catalysts (predicting selectivities of catalysts outside of the training data). The validated models can then be used to select the optimal catalyst for a given reaction. RESULTS To demonstrate the viability of our method, we predicted reaction outcomes with substrate combinations and catalysts different from the training data and simulated a situation in which highly selective reactions had not been achieved. In the first demonstration, a model was constructed by using support vector machines and validated with three different external test sets. The first test set evaluated the ability of the model to predict the selectivity of only reactions forming new products with catalysts from the training set. The model performed well, with a mean absolute deviation (MAD) of 0.161 kcal/mol. Next, the same model was used to predict the selectivity of an external test set of catalysts with substrate combinations from the training set. The performance of the model was still highly accurate, with a MAD of 0.211 kcal/mol. Lastly, reactions forming new products with the external test catalysts were predicted with a MAD of 0.236 kcal/mol. In the second study, no reactions with selectivity above 80% enantiomeric excess were used as training data. Deep feed-forward neural networks accurately reproduced the experimental selectivity data, successfully predicting the most selective reactions. More notably, the general trends in selectivity, on the basis of average catalyst selectivity, were correctly identified. Despite omitting about half of the experimental free energy range from the training data, we could still make accurate predictions in this region of selectivity space. CONCLUSION The capability to predict selective catalysts has the potential to change the way chemists select and optimize chiral catalysts from an empirically guided to a mathematically guided approach. Chemoinformatics-guided optimization protocol. (A) Generation of a large in silico library of catalyst candidates. (B) Calculation of robust chemical descriptors. (C) Selection of a UTS. (D) Acquisition of experimental selectivity data. (E) Application of machine learning to use moderate- to low-selectivity reactions to predict high-selectivity reactions. R, any group; X, O or S; Y, OH, SH, or NHTf; PC, principal component; ΔΔG, mean selectivity. Catalyst design in asymmetric reaction development has traditionally been driven by empiricism, wherein experimentalists attempt to qualitatively recognize structural patterns to improve selectivity. Machine learning algorithms and chemoinformatics can potentially accelerate this process by recognizing otherwise inscrutable patterns in large datasets. Herein we report a computationally guided workflow for chiral catalyst selection using chemoinformatics at every stage of development. Robust molecular descriptors that are agnostic to the catalyst scaffold allow for selection of a universal training set on the basis of steric and electronic properties. This set can be used to train machine learning methods to make highly accurate predictive models over a broad range of selectivity space. Using support vector machines and deep feed-forward neural networks, we demonstrate accurate predictive modeling in the chiral phosphoric acid–catalyzed thiol addition to N-acylimines.","",""
101,"Andreas K Triantafyllidis, A. Tsanas","Applications of Machine Learning in Real-Life Digital Health Interventions: Review of the Literature",2019,"","","","",184,"2022-07-13 09:22:49","","10.2196/12286","","",,,,,101,33.67,51,2,3,"Background Machine learning has attracted considerable research interest toward developing smart digital health interventions. These interventions have the potential to revolutionize health care and lead to substantial outcomes for patients and medical professionals. Objective Our objective was to review the literature on applications of machine learning in real-life digital health interventions, aiming to improve the understanding of researchers, clinicians, engineers, and policy makers in developing robust and impactful data-driven interventions in the health care domain. Methods We searched the PubMed and Scopus bibliographic databases with terms related to machine learning, to identify real-life studies of digital health interventions incorporating machine learning algorithms. We grouped those interventions according to their target (ie, target condition), study design, number of enrolled participants, follow-up duration, primary outcome and whether this had been statistically significant, machine learning algorithms used in the intervention, and outcome of the algorithms (eg, prediction). Results Our literature search identified 8 interventions incorporating machine learning in a real-life research setting, of which 3 (37%) were evaluated in a randomized controlled trial and 5 (63%) in a pilot or experimental single-group study. The interventions targeted depression prediction and management, speech recognition for people with speech disabilities, self-efficacy for weight loss, detection of changes in biopsychosocial condition of patients with multiple morbidity, stress management, treatment of phantom limb pain, smoking cessation, and personalized nutrition based on glycemic response. The average number of enrolled participants in the studies was 71 (range 8-214), and the average follow-up study duration was 69 days (range 3-180). Of the 8 interventions, 6 (75%) showed statistical significance (at the P=.05 level) in health outcomes. Conclusions This review found that digital health interventions incorporating machine learning algorithms in real-life studies can be useful and effective. Given the low number of studies identified in this review and that they did not follow a rigorous machine learning evaluation methodology, we urge the research community to conduct further studies in intervention settings following evaluation principles and demonstrating the potential of machine learning in clinical practice.","",""
41,"S. R. van der Voort, F. Incekara, M. Wijnenga, G. Kapsas, M. Gardeniers, J. Schouten, M. Starmans, R. Nandoe Tewarie, G. Lycklama, P. French, H. Dubbink, M. J. van den Bent, A. Vincent, W. Niessen, S. Klein, M. Smits","Predicting the 1p/19q Codeletion Status of Presumed Low-Grade Glioma with an Externally Validated Machine Learning Algorithm",2019,"","","","",185,"2022-07-13 09:22:49","","10.1158/1078-0432.CCR-19-1127","","",,,,,41,13.67,4,16,3,"Purpose: Patients with 1p/19q codeleted low-grade glioma (LGG) have longer overall survival and better treatment response than patients with 1p/19q intact tumors. Therefore, it is relevant to know the 1p/19q status. To investigate whether the 1p/19q status can be assessed prior to tumor resection, we developed a machine learning algorithm to predict the 1p/19q status of presumed LGG based on preoperative MRI. Experimental Design: Preoperative brain MR images from 284 patients who had undergone biopsy or resection of presumed LGG were used to train a support vector machine algorithm. The algorithm was trained on the basis of features extracted from post-contrast T1-weighted and T2-weighted MR images and on patients' age and sex. The performance of the algorithm compared with tissue diagnosis was assessed on an external validation dataset of MR images from 129 patients with LGG from The Cancer Imaging Archive (TCIA). Four clinical experts also predicted the 1p/19q status of the TCIA MR images. Results: The algorithm achieved an AUC of 0.72 in the external validation dataset. The algorithm had a higher predictive performance than the average of the neurosurgeons (AUC 0.52) but lower than that of the neuroradiologists (AUC of 0.81). There was a wide variability between clinical experts (AUC 0.45–0.83). Conclusions: Our results suggest that our algorithm can noninvasively predict the 1p/19q status of presumed LGG with a performance that on average outperformed the oncological neurosurgeons. Evaluation on an independent dataset indicates that our algorithm is robust and generalizable.","",""
52,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",186,"2022-07-13 09:22:49","","10.1145/3219617.3219655","","",,,,,52,10.40,17,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q łe m for an arbitrarily small but fixed constant ε>0. The parameter estimate converges in O(łog N) rounds with an estimation error on the order of max √dq/N, ~√d/N , which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q . The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log 3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.","",""
1203,"Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. B. McMahan, Sarvar Patel, D. Ramage, Aaron Segal, Karn Seth","Practical Secure Aggregation for Privacy-Preserving Machine Learning",2017,"","","","",187,"2022-07-13 09:22:49","","10.1145/3133956.3133982","","",,,,,1203,240.60,134,9,5,"We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.","",""
31,"Kristin V. Presnell, H. Alper","Systems Metabolic Engineering Meets Machine Learning: A New Era for Data‐Driven Metabolic Engineering",2019,"","","","",188,"2022-07-13 09:22:49","","10.1002/biot.201800416","","",,,,,31,10.33,16,2,3,"The recent increase in high‐throughput capacity of ‘omics datasets combined with advances and interest in machine learning (ML) have created great opportunities for systems metabolic engineering. In this regard, data‐driven modeling methods have become increasingly valuable to metabolic strain design. In this review, the nature of ‘omics is discussed and a broad introduction to the ML algorithms combining these datasets into predictive models of metabolism and metabolic rewiring is provided. Next, this review highlights recent work in the literature that utilizes such data‐driven methods to inform various metabolic engineering efforts for different classes of application including product maximization, understanding and profiling phenotypes, de novo metabolic pathway design, and creation of robust system‐scale models for biotechnology. Overall, this review aims to highlight the potential and promise of using ML algorithms with metabolic engineering and systems biology related datasets.","",""
22,"F. Emmert‐Streib, M. Dehmer","A Machine Learning Perspective on Personalized Medicine: An Automized, Comprehensive Knowledge Base with Ontology for Pattern Recognition",2018,"","","","",189,"2022-07-13 09:22:49","","10.3390/MAKE1010009","","",,,,,22,5.50,11,2,4,"Personalized or precision medicine is a new paradigm that holds great promise for individualized patient diagnosis, treatment, and care. However, personalized medicine has only been described on an informal level rather than through rigorous practical guidelines and statistical protocols that would allow its robust practical realization for implementation in day-to-day clinical practice. In this paper, we discuss three key factors, which we consider dimensions that effect the experimental design for personalized medicine: (I) phenotype categories; (II) population size; and (III) statistical analysis. This formalization allows us to define personalized medicine from a machine learning perspective, as an automized, comprehensive knowledge base with an ontology that performs pattern recognition of patient profiles.","",""
16,"Mo Zhou, Yoshimi Fukuoka, Ken Goldberg, E. Vittinghoff, Anil Aswani","Applying machine learning to predict future adherence to physical activity programs",2019,"","","","",190,"2022-07-13 09:22:49","","10.1186/s12911-019-0890-0","","",,,,,16,5.33,3,5,3,"","",""
21,"Mohammadreza Mirzahosseini, Pengcheng Jiao, Kaveh Barri, K. Riding, A. Alavi","New machine learning prediction models for compressive strength of concrete modified with glass cullet",2019,"","","","",191,"2022-07-13 09:22:49","","10.1108/EC-08-2018-0348","","",,,,,21,7.00,4,5,3,"PurposeRecycled waste glasses have been widely used in Portland cement and concrete as aggregate or supplementary cementitious material. Compressive strength is one of the most important properties of concrete containing waste glasses, providing information about the loading capacity, pozzolanic reaction and porosity of the mixture. This study aims to propose highly nonlinear models to predict the compressive strength of concrete containing finely ground glass particles.Design/methodology/approachA robust machine leaning method called genetic programming is used the build the compressive strength prediction models. The models are developed using a number of test results on 50-mm mortar cubes containing glass powder according to ASTM C109. Parametric and sensitivity analyses are conducted to evaluate the effect of the predictor variables on the compressive strength. Furthermore, a comparative study is performed to benchmark the proposed models against classical regression models.FindingsThe derived design equations accurately characterize the compressive strength of concrete with ground glass fillers and remarkably outperform the regression models. A key feature of the proposed models as compared to the previous studies is that they include the simultaneous effect of various parameters such as glass compositions, size distributions, curing age and isothermal temperatures. Parametric and sensitivity analyses indicate that compressive strength is very sensitive to the curing age, curing temperature and particle surface area.Originality/valueThis study presents accurate machine learning models for the prediction of one of the most important mechanical properties of cementitious mixtures modified by waste glass, i.e. compressive strength. In addition, it provides an insight into the effect of several parameters influencing the compressive strength. From a computing perspective, a robust machine learning technique that overcomes the shortcomings of existing soft computing methods is introduced.","",""
7,"Puling Liu, Z. Du, Huimin Li, Ming Deng, Xiaobing Feng, Jian-guo Yang","Thermal error modeling based on BiLSTM deep learning for CNC machine tool",2021,"","","","",192,"2022-07-13 09:22:49","","10.1007/S40436-020-00342-X","","",,,,,7,7.00,1,6,1,"","",""
18,"S. Fleming, A. Goodbody","A Machine Learning Metasystem for Robust Probabilistic Nonlinear Regression-Based Forecasting of Seasonal Water Availability in the US West",2019,"","","","",193,"2022-07-13 09:22:49","","10.1109/ACCESS.2019.2936989","","",,,,,18,6.00,9,2,3,"Hydroelectric power generation, water supplies for municipal, agricultural, manufacturing, and service industry uses including technology-sector requirements, dam safety, flood control, recreational uses, and ecological and legal constraints, all place simultaneous, competing demands on the heavily stressed water management infrastructure of the mostly arid American West. Optimally managing these resources depends on predicting water availability. We built a probabilistic nonlinear regression water supply forecast (WSF) technique for the US Department of Agriculture, which runs the largest stand-alone WSF system in the US West. Design criteria included improved accuracy over the existing system; uncertainty estimates that seamlessly handle complex (heteroscedastic, non-Gaussian) prediction errors; integration of physical hydrometeorological process knowledge and domain-specific expert experience; ability to accommodate nonlinearity, model selection uncertainty and equifinality, and predictor multicollinearity and high dimensionality; and relatively easy, low-cost implementation. Some methods satisfied some of these requirements but none met all, leading us to develop a novel, interdisciplinary, and pragmatic prediction metasystem through a carefully considered synthesis of well-established, off-the-shelf components and approaches, spanning supervised and unsupervised machine learning, nonparametric statistical modeling, ensemble learning, and evolutionary optimization, focusing on maintaining but radically updating the principal components regression framework widely used for WSF. Testing this integrated multi-method prediction engine demonstrated its value for river forecasting; USDA adoption is a landmark for transitioning machine learning from research into practice in this field. Its ability to handle all the foregoing design criteria and requirements, which are not unique to WSF, suggests potential for extension to complex probabilistic prediction problems in other fields.","",""
7,"R. Castaldo, C. Cavaliere, A. Soricelli, M. Salvatore, L. Pecchia, M. Franzese","Radiomic and Genomic Machine Learning Method Performance for Prostate Cancer Diagnosis: Systematic Literature Review",2021,"","","","",194,"2022-07-13 09:22:49","","10.2196/22394","","",,,,,7,7.00,1,6,1,"Background Machine learning algorithms have been drawing attention at the joining of pathology and radiology in prostate cancer research. However, due to their algorithmic learning complexity and the variability of their architecture, there is an ongoing need to analyze their performance. Objective This study assesses the source of heterogeneity and the performance of machine learning applied to radiomic, genomic, and clinical biomarkers for the diagnosis of prostate cancer. One research focus of this study was on clearly identifying problems and issues related to the implementation of machine learning in clinical studies. Methods Following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) protocol, 816 titles were identified from the PubMed, Scopus, and OvidSP databases. Studies that used machine learning to detect prostate cancer and provided performance measures were included in our analysis. The quality of the eligible studies was assessed using the QUADAS-2 (quality assessment of diagnostic accuracy studies–version 2) tool. The hierarchical multivariate model was applied to the pooled data in a meta-analysis. To investigate the heterogeneity among studies, I2 statistics were performed along with visual evaluation of coupled forest plots. Due to the internal heterogeneity among machine learning algorithms, subgroup analysis was carried out to investigate the diagnostic capability of machine learning systems in clinical practice. Results In the final analysis, 37 studies were included, of which 29 entered the meta-analysis pooling. The analysis of machine learning methods to detect prostate cancer reveals the limited usage of the methods and the lack of standards that hinder the implementation of machine learning in clinical applications. Conclusions The performance of machine learning for diagnosis of prostate cancer was considered satisfactory for several studies investigating the multiparametric magnetic resonance imaging and urine biomarkers; however, given the limitations indicated in our study, further studies are warranted to extend the potential use of machine learning to clinical settings. Recommendations on the use of machine learning techniques were also provided to help researchers to design robust studies to facilitate evidence generation from the use of radiomic and genomic biomarkers.","",""
104,"Yangkang Zhang","Automatic microseismic event picking via unsupervised machine learning",2020,"","","","",195,"2022-07-13 09:22:49","","10.1093/GJI/GGX420","","",,,,,104,52.00,104,1,2,"  Effective and efficient arrival picking plays an important role in microseismic and earthquake data processing and imaging. Widely used short-term-average long-term-average ratio (STA/LTA) based arrival picking algorithms suffer from the sensitivity to moderate-to-strong random ambient noise. To make the state-of-the-art arrival picking approaches effective, microseismic data need to be first pre-processed, for example, removing sufficient amount of noise, and second analysed by arrival pickers. To conquer the noise issue in arrival picking for weak microseismic or earthquake event, I leverage the machine learning techniques to help recognizing seismic waveforms in microseismic or earthquake data. Because of the dependency of supervised machine learning algorithm on large volume of well-designed training data, I utilize an unsupervised machine learning algorithm to help cluster the time samples into two groups, that is, waveform points and non-waveform points. The fuzzy clustering algorithm has been demonstrated to be effective for such purpose. A group of synthetic, real microseismic and earthquake data sets with different levels of complexity show that the proposed method is much more robust than the state-of-the-art STA/LTA method in picking microseismic events, even in the case of moderately strong background noise.","",""
10,"Z. Chaker, M. Salanne, J. Delaye, T. Charpentier","NMR shifts in aluminosilicate glasses via machine learning.",2019,"","","","",196,"2022-07-13 09:22:49","","10.1039/c9cp02803j","","",,,,,10,3.33,3,4,3,"Machine learning (ML) approaches are investigated for the prediction of nuclear magnetic resonance (NMR) parameters in aluminosilicate glasses, for which NMR has proven to be a cutting-edge method over the last decade. DFT computations have emerged as a new dimension for complementing these NMR methods although suffering from severe limitations in terms of size, time and computational resources consumption. While previous approaches tend to use DFT-GIPAW calculations for the prediction of NMR parameters in glassy systems, we propose to employ ML methods, characterized by a speed similar to that of classical molecular dynamics while the accuracy of ab initio methods can be reached. We design ML procedures to predict the isotropic magnetic shielding (σiso) for different multicomponent relevant glass compositions. The ML predictions of σiso deviate from DFT-GIPAW calculations, when including relaxed and room-temperature structures, by 0.7 ppm for 29Si (1.0% of the total span of the calculated ) and 1.5 ppm for 17O (1.9%) in SiO2 glasses, 1.4 ppm for 23Na (1.5%) in Na2O-SiO2 and 1.5 ppm for 27Al (2.1%) in Al2O3-Na2O-SiO2 systems. We compare the performances obtained for a set of three descriptors suitable for encoding atomic local environments information (atom-centered representations) together with seven popular ML algorithms with a focus on the simple (but robust) linear ridge regression (LRR) and the popular smooth overlap of atomic positions (SOAP) descriptor.","",""
78,"Xianfang Wang, Peng Gao, Yifeng Liu, Hongfei Li, Fan Lu","Predicting Thermophilic Proteins by Machine Learning",2020,"","","","",197,"2022-07-13 09:22:49","","10.2174/1574893615666200207094357","","",,,,,78,39.00,16,5,2,"  Thermophilic proteins can maintain good activity under high temperature, therefore, it is important to study thermophilic proteins for the thermal stability of proteins.    In order to solve the problem of low precision and low efficiency in predicting thermophilic proteins, a prediction method based on feature fusion and machine learning was proposed in this paper.    For the selected thermophilic data sets, firstly, the thermophilic protein sequence was characterized based on feature fusion by the combination of g-gap dipeptide, entropy density and autocorrelation coefficient. Then, Kernel Principal Component Analysis (KPCA) was used to reduce the dimension of the expressed protein sequence features in order to reduce the training time and improve efficiency. Finally, the classification model was designed by using the classification algorithm.    A variety of classification algorithms was used to train and test on the selected thermophilic dataset. By comparison, the accuracy of the Support Vector Machine (SVM) under the jackknife method was over 92%. The combination of other evaluation indicators also proved that the SVM performance was the best.     Because of choosing an effectively feature representation method and a robust classifier, the proposed method is suitable for predicting thermophilic proteins and is superior to most reported methods. ","",""
74,"Monika A. Myszczynska, P. Ojamies, Alix M. B. Lacoste, Daniel Neil, Amir Saffari, R. Mead, G. Hautbergue, J. Holbrook, L. Ferraiuolo","Applications of machine learning to diagnosis and treatment of neurodegenerative diseases",2020,"","","","",198,"2022-07-13 09:22:49","","10.1038/s41582-020-0377-8","","",,,,,74,37.00,8,9,2,"","",""
12,"Devyani P. Bhamare, P. Suryawanshi","Review on Reliable Pattern Recognition with Machine Learning Techniques",2018,"","","","",199,"2022-07-13 09:22:49","","10.1080/16168658.2019.1611030","","",,,,,12,3.00,6,2,4,"ABSTRACT The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have deserved increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples and performance evaluation. The general problem of recognising complex pattern with arbitrary patterns with arbitrary orientation, location and scale remains unsolved. New and emerging application, such as data mining, web searching, retrieval of multimedia data, face recognition and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarise and review some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field. In the literature, Pattern recognition frameworks have been drawn closer by different machine learning strategies. This part reviews 33 related examinations in the period between 2014 and 2017.","",""
40,"Kevin G Falk, T. Jubery, S. Mirnezami, Kyle A. Parmley, S. Sarkar, Arti Singh, B. Ganapathysubramanian, Asheesh K Singh","Computer vision and machine learning enabled soybean root phenotyping pipeline",2020,"","","","",200,"2022-07-13 09:22:49","","10.1186/s13007-019-0550-5","","",,,,,40,20.00,5,8,2,"","",""
