Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
0,"Rassa Ghavami Modegh, Ahmadali Salimi, H. Rabiee","LAP: An Attention-Based Module for Faithful Interpretation and Knowledge Injection in Convolutional Neural Networks",2022,"","","","",1,"2022-07-13 10:11:42","","","","",,,,,0,0.00,0,3,1,"Despite the state-of-the-art performance of deep convolutional neural networks, they are susceptible to bias and malfunction in unseen situations. The complex computation behind their reasoning is not sufficiently human-understandable to develop trust. External explainer methods have tried to interpret the network decisions in a human-understandable way, but they are accused of fallacies due to their assumptions and simplifications. On the other side, the inherent self-interpretability of models, while being more robust to the mentioned fallacies, cannot be applied to the already trained models. In this work, we propose a new attention-based pooling layer, called Local Attention Pooling (LAP), that accomplishes self-interpretability and the possibility for knowledge injection while improving the model’s performance. Moreover, several weakly-supervised knowledge injection methodologies are provided to enhance the process of training. We verified our claims by evaluating several LAP-extended models on three different datasets, including Imagenet. The proposed framework offers more valid humanunderstandable and more faithful-to-the-model interpretations than the commonly used white-box explainer methods.","",""
2,"Moonis Ali, B. Whitehead, U. Gupta, H. Ferber","Identification and interpretation of patterns in rocket engine data: Artificial intelligence and neural network approaches",1995,"","","","",2,"2022-07-13 10:11:42","","","","",,,,,2,0.07,1,4,27,"This paper describes an expert system which is designed to perform automatic data analysis, identify anomalous events, and determine the characteristic features of these events. We have employed both artificial intelligence and neural net approaches in the design of this expert system. The artificial intelligence approach is useful because it provides (1) the use of human experts' knowledge of sensor behavior and faulty engine conditions in interpreting data; (2) the use of engine design knowledge and physical sensor locations in establishing relationships among the events of multiple sensors; (3) the use of stored analysis of past data of faulty engine conditions; and (4) the use of knowledge-based reasoning in distinguishing sensor failure from actual faults. The neural network approach appears promising because neural nets (1) can be trained on extremely noisy data and produce classifications which are more robust under noisy conditions than other classification techniques; (2) avoid the necessity of noise removal by digital filtering and therefore avoid the need to make assumptions about frequency bands or other signal characteristics of anomalous behavior; (3) can, in effect, generate their own feature detectors based on the characteristics of the sensor data used in training; and (4) are inherently parallel and therefore are potentially implementable in special-purpose parallel hardware.","",""
0,"Junho Kim, Seong-Tae Kim, Seong Tae Kim, Y. Ro","Robust Perturbation for Visual Explanation: Cross-Checking Mask Optimization to Avoid Class Distortion",2021,"","","","",3,"2022-07-13 10:11:42","","10.1109/TIP.2021.3130526","","",,,,,0,0.00,0,4,1,"Along with the outstanding performance of the deep neural networks (DNNs), considerable research efforts have been devoted to finding ways to understand the decision of DNNs structures. In the computer vision domain, visualizing the attribution map is one of the most intuitive and understandable ways to achieve human-level interpretation. Among them, perturbation-based visualization can explain the “black box” property of the given network by optimizing perturbation masks that alter the network prediction of the target class the most. However, existing perturbation methods could make unexpected changes to network predictions after applying a perturbation mask to the input image, resulting in a loss of robustness and fidelity of the perturbation mechanisms. In this paper, we define class distortion as the unexpected changes of the network prediction during the perturbation process. To handle that, we propose a novel visual interpretation framework, Robust Perturbation, which shows robustness against the unexpected class distortion during the mask optimization. With a new cross-checking mask optimization strategy, our proposed framework perturbs the target prediction of the network while upholding the non-target predictions, providing more reliable and accurate visual explanations. We evaluate our framework on three different public datasets through extensive experiments. Furthermore, we propose a new metric for class distortion evaluation. In both quantitative and qualitative experiments, tackling the class distortion problem turns out to enhance the quality and fidelity of the visual explanation in comparison with the existing perturbation-based methods.","",""
9,"Adam Noack, Isaac Ahern, D. Dou, Boyang Li","An Empirical Study on the Relation Between Network Interpretability and Adversarial Robustness",2020,"","","","",4,"2022-07-13 10:11:42","","10.1007/s42979-020-00390-x","","",,,,,9,4.50,2,4,2,"","",""
28,"M. Alam, L. Vidyaratne, K. Iftekharuddin","Sparse Simultaneous Recurrent Deep Learning for Robust Facial Expression Recognition",2018,"","","","",5,"2022-07-13 10:11:42","","10.1109/TNNLS.2017.2776248","","",,,,,28,7.00,9,3,4,"Facial expression recognition is a challenging task that involves detection and interpretation of complex and subtle changes in facial muscles. Recent advances in feed-forward deep neural networks (DNNs) have offered improved object recognition performance. Sparse feature learning in feed-forward DNN models offers further improvement in performance when compared to the earlier handcrafted techniques. However, the depth of the feed-forward DNNs and the computational complexity of the models increase proportional to the challenges posed by the facial expression recognition problem. The feed-forward DNN architectures do not exploit another important learning paradigm, known as recurrency, which is ubiquitous in the human visual system. Consequently, this paper proposes a novel biologically relevant sparse-deep simultaneous recurrent network (S-DSRN) for robust facial expression recognition. The feature sparsity is obtained by adopting dropout learning in the proposed DSRN as opposed to usual handcrafting of additional penalty terms for the sparse representation of data. Theoretical analysis of S-DSRN shows that the dropout learning offers desirable properties such as sparsity, and prevents the model from overfitting. Experimental results also suggest that the proposed method yields better performance accuracy, requires reduced number of parameters, and offers reduced computational complexity than that of the previously reported state-of-the-art feed-forward DNNs using two of the most widely used publicly available facial expression data sets. Furthermore, we show that by combining the proposed neural architecture with a state-of-the-art metric learning technique significantly improves the overall recognition performance. Finally, a graphical processing unit (GPU)-based implementation of S-DSRN is obtained for real-time applications.","",""
16,"Ramyar Saeedi, S. Norgaard, A. Gebremedhin","A closed-loop deep learning architecture for robust activity recognition using wearable sensors",2017,"","","","",6,"2022-07-13 10:11:42","","10.1109/BigData.2017.8257960","","",,,,,16,3.20,5,3,5,"Human activity recognition (HAR) plays a central role in health-care, fitness and sport applications because of its potential to enable context-aware human monitoring. With the increase in popularity of wearable devices, we are witnessing a large influx in availability of human activity data. For effective analysis and interpretation of these heterogeneous and high-volume streaming data, we need powerful algorithms. In particular, there is a strong need for developing algorithms for robust classification of human activity data that specifically address challenges associated with dynamic environments (e.g. different users, signal heterogeneity). We use the term robust here in two, orthogonal senses: 1) leveraging related data in such a way that knowledge is transferred to a new context; and 2) actively reconfiguring machine learning algorithms such that they can be applied in a new context. In this paper, we propose an architecture that combines an active learning approach with a novel deep network. Our deep neural network exploits both Convolutional and Long Short-Term Memory (LSTM) layers in order to learn hierarchical representation of features and capture time dependencies from raw-data. The active learning process allows us to choose the best instances for fine-tuning the deep network to the new setting in which the system operates (i.e. a new subject). We demonstrate the efficacy of the architecture using real data of human activity. We show that the accuracy of activity recognition reaches over 90% by annotating less than 20% of unlabeled data.","",""
7,"A. Spitzer, M. Hassoun, C. Wang, F. Bearden","Signal Decompostion and Diagnostic Classification of the Electromyogram Using a Novel Neural Network Technique.",1990,"","","","",7,"2022-07-13 10:11:42","","","","",,,,,7,0.22,2,4,32,"Abstract  Interpretation of physiologic signals to assist medical diagnosis requires human expertise. Success in automating this process has been limited. We present a three-step method for automated interpretation of the EMG. Signal decomposition and classification steps, which have not been automated using traditional computer methods, utilize neural networks. To deal with poorly described signals, a novel decomposition method, pseudoun-supervised learning, has been developed. The resulting method is considerably more robust than prior methods.","",""
29,"Jerome Paul N. Cruz, Ma Lourdes Dimaala, L. Francisco, E. J. Franco, A. Bandala, E. Dadios","Object recognition and detection by shape and color pattern recognition utilizing Artificial Neural Networks",2013,"","","","",8,"2022-07-13 10:11:42","","10.1109/ICOICT.2013.6574562","","",,,,,29,3.22,5,6,9,"A robust and accurate object recognition tool is presented in this paper. The paper introduced the use of Artificial Neural Networks in evaluating a frame shot of the target image. The system utilizes three major steps in object recognition, namely image processing, ANN processing and interpretation. In image processing stage a frame shot or an image go through a process of extracting numerical values of object's shape and object's color. These values are then fed to the Artificial Neural Network stage, wherein the recognition of the object is done. Since the output of the ANN stage is in numerical form the third process is indispensable for human understanding. This stage simply converts a given value to its equivalent linguistic term. All three components are integrated in an interface for ease of use. Upon the conclusion of the system's development, experimentation and testing procedures are initiated. The study proved that the optimum lighting condition opted for the system is at 674 lumens with an accuracy of 99.99996072%. Another finding that the paper presented is that the optimum distance for recognition is at 40cm with an accuracy of 99.99996072%. Lastly the system contains a very high tolerance in the variations in the objects position or orientation, with the optimum accuracy at upward position with 99.99940181% accuracy rate.","",""
3,"Yingkai Sha, D. Gagne, Gregory West, R. Stull","Deep-learning-based precipitation observation quality control",2021,"","","","",9,"2022-07-13 10:11:42","","10.1175/JTECH-D-20-0081.1","","",,,,,3,3.00,1,4,1,"We present a novel approach for the automated quality control (QC) of precipitation for a sparse station observation network within the complex terrain of British Columbia, Canada. Our QC approach uses Convolutional Neural Networks (CNNs) to classify bad observation values, incorporating a multi-classifier ensemble to achieve better QC performance. We train CNNs using human QC’d labels from 2016 to 2017 with gridded precipitation and elevation analyses as inputs. Based on the classification evaluation metrics, our QC approach shows reliable and robust performance across different geographical environments (e.g., coastal and inland mountains), with 0.927 Area Under Curve (AUC) and type I/type II error lower than 15%. Based on the saliency-map-based interpretation studies, we explain the success of CNN-based QC by showing that it can capture the precipitation patterns around, and upstream of the station locations. This automated QC approach is an option for eliminating bad observations for various applications, including the pre-processing of training datasets for machine learning. It can be used in conjunction with human QC to improve upon what could be accomplished with either method alone.","",""
0,"M. Sarkar","Salt Detection Using Segmentation of Seismic Image",2022,"","","","",10,"2022-07-13 10:11:42","","10.48550/arXiv.2203.13721","","",,,,,0,0.00,0,1,1,"—In this project, state-of-the-art deep convolution neural network (DCNN) is presented to segment seismic image for salt detection below the earth surface. Detection of salt location is very important for starting mining. Hence, a seismic image is used to detect the exact salt location under the earth surface. However, precisely detecting the exact location of salt deposits is difﬁcult. Therefore, professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective, highly variable renderings. Hence, to create the most accurate seismic images and 3D renderings, we need a robust algorithm that automatically and accurately identiﬁes if a surface target is salt or not. Since the performance of DCNN is well-known and well-established for object recognition in image, DCNN is a very good choice for this particular problem and being successfully applied to a dataset of seismic images in which each pixel is labeled as salt or not. The result of this algorithm is promising.","",""
2,"A. Kallipolitis, A. Stratigos, A. Zarras, Ilias Maglogiannis","Explainable Fully Connected Visual Words for the Classification of Skin Cancer Confocal Images: Interpreting the influence of visual words in classifying benign vs malignant pattern",2020,"","","","",11,"2022-07-13 10:11:42","","10.1145/3411408.3411435","","",,,,,2,1.00,1,4,2,"Skin cancer is affecting the lives of million people worldwide. Early detection and treatment of the cause can reduce drastically morbidity. Although the main workflow in dermatology clinics includes invasive skin removal procedures for diagnostic purposes, Reflectance Confocal Microscopy (RCM) provides an ancillary, non-invasive methodology for reviewing areas of interest of the human skin at a high resolution. In this paper, we propose a method for the classification and the interpretation of visual patterns in skin cancer confocal images. Both tasks are based on the formation of a visual vocabulary from Speeded up Robust Features (SURF) and the utilization of simple shallow artificial neural network with fully connected layers. Interpretability of the predictive models is also quite important, since it improves their reliability, accountability, transparency and provides useful insight of how to evolve the predictive model towards better performance. The paper discusses the technical details of both approaches along with some initial results.","",""
13,"Shiyue Zhang, P. Xie, Dong Wang, E. Xing","Medical Diagnosis From Laboratory Tests by Combining Generative and Discriminative Learning",2017,"","","","",12,"2022-07-13 10:11:42","","","","",,,,,13,2.60,3,4,5,"A primary goal of computational phenotype research is to conduct medical diagnosis. In hospital, physicians rely on massive clinical data to make diagnosis decisions, among which laboratory tests are one of the most important resources. However, the longitudinal and incomplete nature of laboratory test data casts a significant challenge on its interpretation and usage, which may result in harmful decisions by both human physicians and automatic diagnosis systems. In this work, we take advantage of deep generative models to deal with the complex laboratory tests. Specifically, we propose an end-to-end architecture that involves a deep generative variational recurrent neural networks (VRNN) to learn robust and generalizable features, and a discriminative neural network (NN) model to learn diagnosis decision making, and the two models are trained jointly. Our experiments are conducted on a dataset involving 46,252 patients, and the 50 most frequent tests are used to predict the 50 most common diagnoses. The results show that our model, VRNN+NN, significantly (p<0.001) outperforms other baseline models. Moreover, we demonstrate that the representations learned by the joint training are more informative than those learned by pure generative models. Finally, we find that our model offers a surprisingly good imputation for missing values.","",""
6,"J. Barrilleaux","A biologically motivated algorithm for image interpretation based on multi-pass multi-resolution techniques",1990,"","","","",13,"2022-07-13 10:11:42","","10.1109/IJCNN.1990.137796","","",,,,,6,0.19,6,1,32,"Previous attempts at performing automatic photointerpretation have met with limited success. Many rely on one-shot methods which try to interpret an image in one pass through the data with no provisions for error correction, while effectively looking through a peephole, one small image segment at a time. They generally lack the ability to utilize contextual information (parking lots are associated with roads and buildings), internal details (roads contain shoulders, center stripes, and vehicles), as well as multiple competing interpretation hypotheses to build up the `big picture', as is likely done in the mind of a human photointerpreter. The objective of this work is to apply such concepts, which are motivated by biological systems directly, or through neural network paradigms indirectly, to achieve a more synergistic and robust algorithm for performing image interpretation. The following is a presentation of the second version of this algorithm, called Multi-Pass multi-Resolution (MPR) image interpretation","",""
0,"M. Morgenstern","Redefining Health Economics: How it Can Contribute to Better Healthcare Systems?",2016,"","","","",14,"2022-07-13 10:11:42","","","","",,,,,0,0.00,0,1,6,"Health Economics involves quantitative and qualitative methods and assimilates concepts and analytical structures deriving from different disciplines suggesting neural network synaptic connections, algorithms and learning rules. A transdisciplinary analytical perspective and a multidisciplinary execution should converge on transparency, systematizing use of evidence supporting the decision process.Activity in (HTA) and (HEOR) increased attention on theoretical instruments designed for academic debates, instead of evidence from molecular biology, applied research medicine and robust statistical instruments. The work proposes a constructive synthetic review of differences of Classical and Bayesian statistics applied to Clinical trials.Statistical methods are better than human intuitive rules of thumb; however misuse of statistics produces errors in design and interpretation of trials, health policies and medical practice. Some literature recommends paths for improvement however it remarks that better powered evidence will require a change in scientific mentality that might be difficult to achieve.","",""
1,"H. Freeman","Studies in Pattern Recognition: A Memorial to the Late Professor King-Sun Fu",1997,"","","","",15,"2022-07-13 10:11:42","","10.1142/3250","","",,,,,1,0.04,1,1,25,"Pattern category assignment by neural networks and nearest neighbours rule - a synopsis and a characterization, A. Mitiche and J.K. Aggarwal pattern recognition - an approach to turn machine translation concepts into creation and reality, J.T. Tou learning in navigation - goal finding in graphs, P. Cucka et al subset least squares method for robust speech and image processing, R.L. Kashyap and J.-N. Liaw shape recognition by human-like trial and error random processes, M. Nagao 3-D face modelling and its applications, T.S. Huang and L.-A. Tang dimension reduction, feature extraction and interpretation of data with network computing, Y.-H. Pao characteristic-view modelling of curved-surface solids, S. Chen and H. Freeman propagating covariance in computer vision, R.M. Haralick shape description by a syntactic pyramidal approach, S. Levialdi and L. Cinque genetic selection and neural modelling of piecewise linear-classifiers, J. Sklansky and M. Vriesenga.","",""
4,"A. Hamer, D. Simms, T. Waine","Replacing human interpretation of agricultural land in Afghanistan with a deep convolutional neural network",2021,"","","","",16,"2022-07-13 10:11:42","","10.1080/01431161.2020.1864059","","",,,,,4,4.00,1,3,1,"ABSTRACT Afghanistan’s annual opium survey relies upon time-consuming human interpretation of satellite images to map the area of potential poppy cultivation for statistical sample design. Deep Convolutional Neural Networks (CNNs) have shown ground-breaking performance for image classification tasks by encoding local contextual information, in some cases outperforming trained analysts. In this study, we investigate the development of a CNN to automate the classification of agriculture from medium-resolution satellite imagery as an alternative to manual interpretation. The residual network (ResNet50) CNN architecture was trained and validated for delineating the agricultural area using labelled multi-seasonal Disaster Monitoring Constellation (DMC) satellite imagery (32 m) of Helmand and Kandahar provinces. The effect of input image chip size, training sampling strategy, elevation data, and multi-seasonal imagery were investigated. The best-performing single-year classification used an input chip size of 33 × 33 pixels, a targeted sampling strategy and transfer learning, resulting in high overall accuracy (94%). The inclusion of elevation data marginally lowered performance (93%). Multi-seasonal classification achieved an overall accuracy of 89% using the previous two years’ data. Only 25% of the target year’s training samples were necessary to update the model to achieve >94% overall accuracy. A data-driven approach to automate agricultural mask production using CNNs is proposed to reduce the burden of human interpretation. The ability to continually update CNN models with new data has the potential to significantly improve automatic classification of vegetation across years.","",""
3,"Daniel Weber, C. Gühmann, T. Seel","RIANN—A Robust Neural Network Outperforms Attitude Estimation Filters",2021,"","","","",17,"2022-07-13 10:11:42","","10.3390/ai2030028","","",,,,,3,3.00,1,3,1,"Inertial-sensor-based attitude estimation is a crucial technology in various applications, from human motion tracking to autonomous aerial and ground vehicles. Application scenarios differ in characteristics of the performed motion, presence of disturbances, and environmental conditions. Since state-of-the-art attitude estimators do not generalize well over these characteristics, their parameters must be tuned for the individual motion characteristics and circumstances. We propose RIANN, a ready-to-use, neural network-based, parameter-free, real-time-capable inertial attitude estimator, which generalizes well across different motion dynamics, environments, and sampling rates, without the need for application-specific adaptations. We gather six publicly available datasets of which we exploit two datasets for the method development and the training, and we use four datasets for evaluation of the trained estimator in three different test scenarios with varying practical relevance. Results show that RIANN outperforms state-of-the-art attitude estimation filters in the sense that it generalizes much better across a variety of motions and conditions in different applications, with different sensor hardware and different sampling frequencies. This is true even if the filters are tuned on each individual test dataset, whereas RIANN was trained on completely separate data and has never seen any of these test datasets. RIANN can be applied directly without adaptations or training and is therefore expected to enable plug-and-play solutions in numerous applications, especially when accuracy is crucial but no ground-truth data is available for tuning or when motion and disturbance characteristics are uncertain. We made RIANN publicly available.","",""
32,"F. Noori, Benedikte Wallace, Md. Zia Uddin, J. Tørresen","A Robust Human Activity Recognition Approach Using OpenPose, Motion Features, and Deep Recurrent Neural Network",2019,"","","","",18,"2022-07-13 10:11:42","","10.1007/978-3-030-20205-7_25","","",,,,,32,10.67,8,4,3,"","",""
248,"Lukas Schott, Jonas Rauber, M. Bethge, Wieland Brendel","Towards the first adversarially robust neural network model on MNIST",2018,"","","","",19,"2022-07-13 10:11:42","","","","",,,,,248,62.00,62,4,4,"Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful defense by Madry et al. (1) overfits on the L-infinity metric (it's highly susceptible to L2 and L0 perturbations), (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-infinity perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.","",""
44,"M. Mahmood, A. Jalal, M. A. Sidduqi","Robust Spatio-Temporal Features for Human Interaction Recognition Via Artificial Neural Network",2018,"","","","",20,"2022-07-13 10:11:42","","10.1109/FIT.2018.00045","","",,,,,44,11.00,15,3,4,"Human Interaction Recognition plays a key role in identification of usual and unusual human behaviors and facilitates public dealings, violence detection, robots perception, and virtual entertainments. This paper presents a novel human interaction recognition (HIR) system to recognize human interactions in continuous image sequences. The proposed technology segments full body silhouettes and identifies key body points to extract robust spatio-temporal features having distinct characteristics for each interaction. Our descriptors focus on local descriptions, capture intensity variations, point-to-point distances and time based relations. The system is trained through artificial neural network to recognize six basic interactions taken from UT-Interaction dataset.","",""
0,"Ying Lin, Junji Ma, Bingjing Huang, Jinbo Zhang, Yining Zhang, Zhengjia Dai","Predicting Human Intrinsic Functional Connectivity From Structural Connectivity: An Artificial Neural Network Approach",2021,"","","","",21,"2022-07-13 10:11:42","","10.1109/tnse.2021.3102667","","",,,,,0,0.00,0,6,1,"How structural connectivity (SC) constrains and shapes functional connectivity (FC) in the human brain to support rich cognitive functions has long been a core issue in neuroscience. Although evidence accumulate to suggest that FC strength is correlated with multiple aspects of SC, few studies has analyzed the SC-to-FC relationship in a multivariate manner. This paper proposed a novel usage of the feedforward neural network to predict FC strength as a nonlinear combination of 115 features that described the geometric and topological aspects of SC. The resulting model outperformed four state-of-the-art models in both terms of predictive power and generalizability. Model interpretation analyses found that the geometric features were generally more predictive than the topological ones, providing novel evidence for the significant impact of geometric relationships on FC generation. Comparison of feature contributions to predicting FC with different structural properties further revealed the crucial role of indirect structural paths for inducing FC, particularly between disconnected and/or distanced regions. Together, our results suggested that the flexible FC is significantly but unevenly influenced by the combination of geometric and topological characteristics of the structural network. The proposed framework would also be used for link prediction on top of an underlying topology.","",""
3,"G. I. Parisi","Human Action Recognition and Assessment via Deep Neural Network Self-Organization",2020,"","","","",22,"2022-07-13 10:11:42","","10.1007/978-3-030-46732-6_10","","",,,,,3,1.50,3,1,2,"","",""
193,"Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, Dan Pei","Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network",2019,"","","","",23,"2022-07-13 10:11:42","","10.1145/3292500.3330672","","",,,,,193,64.33,32,6,3,"Industry devices (i.e., entities) such as server machines, spacecrafts, engines, etc., are typically monitored with multivariate time series, whose anomaly detection is critical for an entity's service quality management. However, due to the complex temporal dependence and stochasticity of multivariate time series, their anomaly detection remains a big challenge. This paper proposes OmniAnomaly, a stochastic recurrent neural network for multivariate time series anomaly detection that works well robustly for various devices. Its core idea is to capture the normal patterns of multivariate time series by learning their robust representations with key techniques such as stochastic variable connection and planar normalizing flow, reconstruct input data by the representations, and use the reconstruction probabilities to determine anomalies. Moreover, for a detected entity anomaly, OmniAnomaly can provide interpretations based on the reconstruction probabilities of its constituent univariate time series. The evaluation experiments are conducted on two public datasets from aerospace and a new server machine dataset (collected and released by us) from an Internet company. OmniAnomaly achieves an overall F1-Score of 0.86 in three real-world datasets, signicantly outperforming the best performing baseline method by 0.09. The interpretation accuracy for OmniAnomaly is up to 0.89.","",""
4,"Onur Sevli","A deep convolutional neural network-based pigmented skin lesion classification application and experts evaluation",2021,"","","","",24,"2022-07-13 10:11:42","","10.1007/S00521-021-05929-4","","",,,,,4,4.00,4,1,1,"","",""
356,"M. Mirman, Timon Gehr, Martin T. Vechev","Differentiable Abstract Interpretation for Provably Robust Neural Networks",2018,"","","","",25,"2022-07-13 10:11:42","","","","",,,,,356,89.00,119,3,4,"We introduce a scalable method for training robust neural networks based on abstract interpretation. We present several abstract transformers which balance efficiency with precision and show these can be used to train large neural networks that are certifiably robust to adversarial perturbations.","",""
1,"R. A. Gonzales, Qiang Zhang, B. Papież, K. Werys, E. Lukaschuk, Iulia A. Popescu, M. Burrage, M. Shanmuganathan, V. Ferreira, S. Piechnik","MOCOnet: Robust Motion Correction of Cardiovascular Magnetic Resonance T1 Mapping Using Convolutional Neural Networks",2021,"","","","",26,"2022-07-13 10:11:42","","10.3389/fcvm.2021.768245","","",,,,,1,1.00,0,10,1,"Background: Quantitative cardiovascular magnetic resonance (CMR) T1 mapping has shown promise for advanced tissue characterisation in routine clinical practise. However, T1 mapping is prone to motion artefacts, which affects its robustness and clinical interpretation. Current methods for motion correction on T1 mapping are model-driven with no guarantee on generalisability, limiting its widespread use. In contrast, emerging data-driven deep learning approaches have shown good performance in general image registration tasks. We propose MOCOnet, a convolutional neural network solution, for generalisable motion artefact correction in T1 maps. Methods: The network architecture employs U-Net for producing distance vector fields and utilises warping layers to apply deformation to the feature maps in a coarse-to-fine manner. Using the UK Biobank imaging dataset scanned at 1.5T, MOCOnet was trained on 1,536 mid-ventricular T1 maps (acquired using the ShMOLLI method) with motion artefacts, generated by a customised deformation procedure, and tested on a different set of 200 samples with a diverse range of motion. MOCOnet was compared to a well-validated baseline multi-modal image registration method. Motion reduction was visually assessed by 3 human experts, with motion scores ranging from 0% (strictly no motion) to 100% (very severe motion). Results: MOCOnet achieved fast image registration (<1 second per T1 map) and successfully suppressed a wide range of motion artefacts. MOCOnet significantly reduced motion scores from 37.1±21.5 to 13.3±10.5 (p < 0.001), whereas the baseline method reduced it to 15.8±15.6 (p < 0.001). MOCOnet was significantly better than the baseline method in suppressing motion artefacts and more consistently (p = 0.007). Conclusion: MOCOnet demonstrated significantly better motion correction performance compared to a traditional image registration approach. Salvaging data affected by motion with robustness and in a time-efficient manner may enable better image quality and reliable images for immediate clinical interpretation.","",""
54,"N. Ahmadi, G. Akbarizadeh","Hybrid robust iris recognition approach using iris image pre-processing, two-dimensional gabor features and multi-layer perceptron neural network/PSO",2017,"","","","",27,"2022-07-13 10:11:42","","10.1049/iet-bmt.2017.0041","","",,,,,54,10.80,27,2,5,"Computational intelligence is employed to solve factual and complicated global problems, though neural networks (NNs) and evolutionary computing have also affected these issues. Biometric traits are applicable for detecting crime in security systems because they offer attractive features such as stability and uniqueness. Although various methods have been proposed for this objective, feature shortcomings such as computational complexity, long run times, and high memory consumption remain. The current study proposes a novel human iris recognition approach based on a multi-layer perceptron NN and particle swarm optimisation (PSO) algorithms to train the network in order to increase generalisation performance. A combination of these algorithms was used as a classifier. A pre-processing step was performed on the iris images to improve the results and two-dimensional gabor kernel feature extraction was applied. The data was normalised, trained, and tested using the proposed method. A PSO algorithm was applied to train the NN for data classification. The experimental results show that the proposed method performs better than many other well-known techniques. The benchmark Chinese Academy of Science and Institute of Automation (CASIA)-iris V3 and Center for Machine Learning and Intelligent Systems at the University of California, Irvine (UCI) machine learning repository datasets were used for testing and comparison.","",""
2,"Yi Liu, Min Peng, M. Swash, Tong Chen, R. Qin, H. Meng","Holoscopic 3D Microgesture Recognition by Deep Neural Network Model Based on Viewpoint Images and Decision Fusion",2021,"","","","",28,"2022-07-13 10:11:42","","10.1109/THMS.2020.3047914","","",,,,,2,2.00,0,6,1,"Finger microgestures have been widely used in human computer interaction (HCI), particularly for interactive applications, such as virtual reality (VR) and augmented reality (AR) technologies, to provide immersive experience. However, traditional 2D image-based microgesture recognition suffers from low accuracy due to the limitations of 2D imaging sensors, which have no depth information. In this article, we proposed an innovative 3D microgesture recognition system based on a holoscopic 3D imaging sensor. Due to the lack of holoscopic 3D datasets, a comprehensive holoscopic 3D microgesture (HoMG) database is created and used to develop a robust 3D microgesture recognition method. Then, a fast algorithm is proposed to extract multiviewpoint images from one holoscopic image. Furthermore, we applied a CNN model with an attention-based residual block to each viewpoint image to improve the algorithm performance. Finally, bagging classification tree decision-level fusion is applied to combine the predictions. The experimental results demonstrate that the proposed method outperforms state-of-the-art methods and delivers a better accuracy than existing methods.","",""
43,"Yair Dgani, H. Greenspan, J. Goldberger","Training a neural network based on unreliable human annotation of medical images",2018,"","","","",29,"2022-07-13 10:11:42","","10.1109/ISBI.2018.8363518","","",,,,,43,10.75,14,3,4,"Building classification models from clinical data often requires human experts for example labeling. However, it is difficult to obtain a perfect set of labels due to the complexity of the medical data and the large variability between experts. In this study we present a neural-network training strategy that is more robust to unreliable labeling by explicitly modeling the label noise as part of the network architecture. Our method is demonstrated on breast microcalcifications classification into benign and malignant categories, given multi-view mammograms. We show that the proposed training procedure outperforms standard training methods that ignore the existence of label noise.","",""
15,"M. Gogate, K. Dashtipour, P. Bell, A. Hussain","Deep Neural Network Driven Binaural Audio Visual Speech Separation",2020,"","","","",30,"2022-07-13 10:11:42","","10.1109/IJCNN48605.2020.9207517","","",,,,,15,7.50,4,4,2,"The central auditory pathway exploits the auditory signals and visual information sent by both ears and eyes to segregate speech from multiple competing noise sources and help disambiguate phonological ambiguity. In this study, inspired from this unique human ability, we present a deep neural network (DNN) that ingest the binaural sounds received at the two ears as well as the visual frames to selectively suppress the competing noise sources individually at both ears. The model exploits the noisy binaural cues and noise robust visual cues to improve speech intelligibility. The comparative simulation results in terms of objective metrics such as PESQ, STOI, SI-SDR and DBSTOI demonstrate significant performance improvement of the proposed audio-visual (AV) DNN as compared to the audio-only (A-only) variant of the proposed model. Finally, subjective listening tests with the real noisy AV ASPIRE corpus shows the superiority of the proposed AV DNN as compared to state-of-the-art approaches.","",""
16,"Nian Chi Tay, T. Connie, T. Ong, K. Goh, Pin Shen Teh","A Robust Abnormal Behavior Detection Method Using Convolutional Neural Network",2018,"","","","",31,"2022-07-13 10:11:42","","10.1007/978-981-13-2622-6_4","","",,,,,16,4.00,3,5,4,"","",""
46,"Wen Qi, Hang Su, Chenguang Yang, G. Ferrigno, E. Momi, A. Aliverti","A Fast and Robust Deep Convolutional Neural Networks for Complex Human Activity Recognition Using Smartphone",2019,"","","","",32,"2022-07-13 10:11:42","","10.3390/s19173731","","",,,,,46,15.33,8,6,3,"As a significant role in healthcare and sports applications, human activity recognition (HAR) techniques are capable of monitoring humans’ daily behavior. It has spurred the demand for intelligent sensors and has been giving rise to the explosive growth of wearable and mobile devices. They provide the most availability of human activity data (big data). Powerful algorithms are required to analyze these heterogeneous and high-dimension streaming data efficiently. This paper proposes a novel fast and robust deep convolutional neural network structure (FR-DCNN) for human activity recognition (HAR) using a smartphone. It enhances the effectiveness and extends the information of the collected raw data from the inertial measurement unit (IMU) sensors by integrating a series of signal processing algorithms and a signal selection module. It enables a fast computational method for building the DCNN classifier by adding a data compression module. Experimental results on the sampled 12 complex activities dataset show that the proposed FR-DCNN model is the best method for fast computation and high accuracy recognition. The FR-DCNN model only needs 0.0029 s to predict activity in an online way with 95.27% accuracy. Meanwhile, it only takes 88 s (average) to establish the DCNN classifier on the compressed dataset with less precision loss 94.18%.","",""
4,"J. Mok, Byunggook Na, Hyeokjun Choe, Sungroh Yoon","AdvRush: Searching for Adversarially Robust Neural Architectures",2021,"","","","",33,"2022-07-13 10:11:42","","10.1109/iccv48922.2021.01210","","",,,,,4,4.00,1,4,1,"Deep neural networks continue to awe the world with their remarkable performance. Their predictions, however, are prone to be corrupted by adversarial examples that are imperceptible to humans. Current efforts to improve the robustness of neural networks against adversarial examples are focused on developing robust training methods, which update the weights of a neural network in a more robust direction. In this work, we take a step beyond training of the weight parameters and consider the problem of designing an adversarially robust neural architecture with high intrinsic robustness. We propose AdvRush, a novel adversarial robustness-aware neural architecture search algorithm, based upon a finding that independent of the training method, the intrinsic robustness of a neural network can be represented with the smoothness of its input loss landscape. Through a regularizer that favors a candidate architecture with a smoother input loss landscape, AdvRush successfully discovers an adversarially robust neural architecture. Along with a comprehensive theoretical motivation for AdvRush, we conduct an extensive amount of experiments to demonstrate the efficacy of AdvRush on various benchmark datasets. Notably, on CIFAR-10, AdvRush achieves 55.91% robust accuracy under FGSM attack after standard training and 50.04% robust accuracy under AutoAttack after 7-step PGD adversarial training.","",""
8,"Yangyang Xia, R. Stern","A Priori SNR Estimation Based on a Recurrent Neural Network for Robust Speech Enhancement",2018,"","","","",34,"2022-07-13 10:11:42","","10.21437/Interspeech.2018-2423","","",,,,,8,2.00,4,2,4,"Speech enhancement under highly non-stationary noise conditions remains a challenging problem. Classical methods typically attempt to identify a frequency-domain optimal gain function that suppresses noise in noisy speech. These algorithms typically produce artifacts such as “musical noise” that are detrimental to machine and human understanding, largely due to inaccurate estimation of noise power spectra. The optimal gain function is commonly referred to as the ideal ratio mask (IRM) in neural-network-based systems, and the goal becomes estimation of the IRM from the short-time Fourier transform amplitude of degraded speech. While these data-driven techniques are able to enhance speech quality with reduced artifacts, they are frequently not robust to types of noise that they had not been exposed to in the training process. In this paper, we propose a novel recurrent neural network (RNN) that bridges the gap between classical and neural-network-based methods. By reformulating the classical decision-directed approach, the a priori and a posteriori SNRs become latent variables in the RNN, from which the frequency-dependent estimated likelihood of speech presence is used to update recursively the latent variables. The proposed method provides substantial enhancement of speech quality and objective accuracy in machine interpretation of speech.","",""
7,"Muhammad Fayyaz, Mussarat Yasmin, Muhammad Sharif, M. Raza","J-LDFR: joint low-level and deep neural network feature representations for pedestrian gender classification",2020,"","","","",35,"2022-07-13 10:11:42","","10.1007/s00521-020-05015-1","","",,,,,7,3.50,2,4,2,"","",""
49,"Abeer N. Al-Nafjan, M. Hosny, A. Al-Wabil, Y. Al-Ohali","Classification of Human Emotions from Electroencephalogram (EEG) Signal using Deep Neural Network",2017,"","","","",36,"2022-07-13 10:11:42","","10.14569/IJACSA.2017.080955","","",,,,,49,9.80,12,4,5,"Estimation of human emotions from Electroencephalogram (EEG) signals plays a vital role in developing robust Brain-Computer Interface (BCI) systems. In our research, we used Deep Neural Network (DNN) to address EEG-based emotion recognition. This was motivated by the recent advances in accuracy and efficiency from applying deep learning techniques in pattern recognition and classification applications. We adapted DNN to identify human emotions of a given EEG signal (DEAP dataset) from power spectral density (PSD) and frontal asymmetry features. The proposed approach is compared to state-of-the-art emotion detection systems on the same dataset. Results show how EEG based emotion recognition can greatly benefit from using DNNs, especially when a large amount of training data is available.","",""
40,"Mohanad Babiker, Othman Omran Khalifa, K. K. Htike, Aisha Hassan, Muhamed Zaharadeen","Automated daily human activity recognition for video surveillance using neural network",2017,"","","","",37,"2022-07-13 10:11:42","","10.1109/ICSIMA.2017.8312024","","",,,,,40,8.00,8,5,5,"Surveillance video systems are gaining increasing attention in the field of computer vision due to its demands of users for the seek of security. It is promising to observe the human movement and predict such kind of sense of movements. The need arises to develop a surveillance system that capable to overcome the shortcoming of depending on the human resource to stay monitoring, observing the normal and suspect event all the time without any absent mind and to facilitate the control of huge surveillance system network. In this paper, an intelligent human activity system recognition is developed. Series of digital image processing techniques were used in each stage of the proposed system, such as background subtraction, binarization, and morphological operation. A robust neural network was built based on the human activities features database, which was extracted from the frame sequences. Multi-layer feed forward perceptron network used to classify the activities model in the dataset. The classification results show a high performance in all of the stages of training, testing and validation. Finally, these results lead to achieving a promising performance in the activity recognition rate.","",""
1,"Muhammad Abdullah Hanif, M. Shafique","Dependable Deep Learning: Towards Cost-Efficient Resilience of Deep Neural Network Accelerators against Soft Errors and Permanent Faults",2020,"","","","",38,"2022-07-13 10:11:42","","10.1109/IOLTS50870.2020.9159734","","",,,,,1,0.50,1,2,2,"Deep Learning has enabled machines to learn computational models (i.e., Deep Neural Networks – DNNs) that can perform certain complex tasks with claims to be close to human-level precision. This state-of-the-art performance offered by DNNs in many Artificial Intelligence (AI) applications has paved their way to being used in several safety-critical applications where even a single failure can lead to catastrophic results. Therefore, improving the robustness of these models to hardware-induced faults (such as soft errors, aging, and manufacturing defects) is of significant importance to avoid any disastrous event. Traditional redundancy-based fault mitigation techniques cannot be employed in a wide of applications due to their high overheads, which, when coupled with the compute-intensive nature of DNNs, lead to undesirable resource consumption. In this article, we present an overview of different low-cost fault-mitigation techniques that exploit the intrinsic characteristics of DNNs to limit their overheads. We discuss how each technique can contribute to the overall resilience of a DNN-based system, and how they can be integrated together to offer resilience against multiple diverse hardware-induced reliability threats. Towards the end, we highlight several key future directions that are envisioned to help in achieving highly dependable DL-based systems.","",""
0,"P. Kebria, D. Nahavandi, A. Khosravi, S. Nahavandi, F. Bello","Adaptive Neural Network-based Perception and Awareness of Teleoperation Systems in Human-Machine Interactions",2020,"","","","",39,"2022-07-13 10:11:42","","10.1109/ICHMS49158.2020.9209437","","",,,,,0,0.00,0,5,2,"This paper addresses the problem of perception and awareness of teleoperation systems in the presence of human collaboratives/objects in the workspace. Although the term teleoperation generally refers to operations being executed remotely, in many applications, like telemedicine, there exist human beings in the remote workspace. Hence, it is critically important that the teleoperator system to operate safely enough in the presence of human kinds in the workspace. In this paper, we propose a perception and awareness scheme for a teleoperation system that prevents the teleoperator from imposing extreme and unwanted forces/movements. To achieve this goal, we train a neural network to estimate and predict the motion and force commands from the human operator. Furthermore, we develop an adaptive algorithm for fine-tuning network parameters for robustness purposes. Theoretically proven, stability and performance of the proposed scheme is comparatively evaluated in comprehensive simulations.","",""
276,"Timothy Niven, Hung-Yu Kao","Probing Neural Network Comprehension of Natural Language Arguments",2019,"","","","",40,"2022-07-13 10:11:42","","10.18653/v1/P19-1459","","",,,,,276,92.00,138,2,3,"We are surprised to find that BERT’s peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.","",""
20,"Minjing Dong, Yanxi Li, Yunhe Wang, Chang Xu","Adversarially Robust Neural Architectures",2020,"","","","",41,"2022-07-13 10:11:42","","","","",,,,,20,10.00,5,4,2,"Deep Neural Network (DNN) are vulnerable to adversarial attack. Existing methods are devoted to developing various robust training strategies or regularizations to update the weights of the neural network. But beyond the weights, the overall structure and information flow in the network are explicitly determined by the neural architecture, which remains unexplored. This paper thus aims to improve the adversarial robustness of the network from the architecture perspective with NAS framework. We explore the relationship among adversarial robustness, Lipschitz constant, and architecture parameters and show that an appropriate constraint on architecture parameters could reduce the Lipschitz constant to further improve the robustness. For NAS framework, all the architecture parameters are equally treated when the discrete architecture is sampled from supernet. However, the importance of architecture parameters could vary from operation to operation or connection to connection, which is not explored and might reduce the confidence of robust architecture sampling. Thus, we propose to sample architecture parameters from trainable multivariate log-normal distributions, with which the Lipschitz constant of entire network can be approximated using a univariate log-normal distribution with mean and variance related to architecture parameters. Compared with adversarially trained neural architectures searched by various NAS algorithms as well as efficient human-designed models, our algorithm empirically achieves the best performance among all the models under various attacks on different datasets.","",""
0,"P. Kebria, A. Khosravi, S. Nahavandi","Neural Network Control of Teleoperation Systems with Delay and Uncertainties based on Multilayer Perceptron Estimations",2020,"","","","",42,"2022-07-13 10:11:42","","10.1109/IJCNN48605.2020.9207035","","",,,,,0,0.00,0,3,2,"This paper investigates a novel synchronisation strategy for controlling Internet-based teleoperation systems. These kinds of systems considerably suffer from network-induced latencies. Random time-varying delays resulted by the Internet deteriorate the stability and performance of teleoperation processes. Moreover, uncertain dynamic elements, including human operators and partially known remote environments introduce further difficulties to the control design of such systems. Utilising the learning capabilities of artificial neural networks, this paper develops an adaptive algorithm to deal with time-delays and uncertainties negatively affecting an Internet-based teleoperation process. The stable convergence of the proposed control algorithm is proved by Lyapunov-Krasovskii stability criteria. Moreover, the robust performance of the controller is also verified via experimental evaluations.","",""
16,"Xiao Qi, L. G. Brown, D. Foran, J. Nosher, I. Hacihaliloglu","Chest X-ray image phase features for improved diagnosis of COVID-19 using convolutional neural network",2020,"","","","",43,"2022-07-13 10:11:42","","10.1007/s11548-020-02305-w","","",,,,,16,8.00,3,5,2,"","",""
0,"Pei Ma, Feng Yu, Changlong Zhou, Minghua Jiang","An Integrated Smoke Detection Method based on Convolutional Neural Network and Image Processing",2020,"","","","",44,"2022-07-13 10:11:42","","10.1109/ICCSNT50940.2020.9304985","","",,,,,0,0.00,0,4,2,"Fire is one of the disasters against the Safety of human life and property. Generally, early smoke features are more obvious than fire in the surveillance environment. However, due to the variability of smoke characteristics (e.g., color, shape) and the interference of smoke-like objects (e.g., clouds, rivulet, and fog), the main challenge of smoke detection is false alarms in real-world. To tackle this problem, an integrated method is proposed which combines HSV color space, background subtraction with Faster R-CNN. This method can enhance smoke feature, meanwhile, it can reduce disturbance from smoke-like objects. Furthermore, a dataset is created which are collected from surveillance cameras that are installed in the wild. Our experiments show that the integrated method is more accurate and robust than previous work.","",""
37,"Zhidong Zhao, Yang Zhang, Zafer Comert, Yanjun Deng","Computer-Aided Diagnosis System of Fetal Hypoxia Incorporating Recurrence Plot With Convolutional Neural Network",2019,"","","","",45,"2022-07-13 10:11:42","","10.3389/fphys.2019.00255","","",,,,,37,12.33,9,4,3,"Background: Electronic fetal monitoring (EFM) is widely applied as a routine diagnostic tool by clinicians using fetal heart rate (FHR) signals to prevent fetal hypoxia. However, visual interpretation of the FHR usually leads to significant inter-observer and intra-observer variability, and false positives become the main cause of unnecessary cesarean sections. Goal: The main aim of this study was to ensure a novel, consistent, robust, and effective model for fetal hypoxia detection. Methods: In this work, we proposed a novel computer-aided diagnosis (CAD) system integrated with an advanced deep learning (DL) algorithm. For a 1-dimensional preprocessed FHR signal, the 2-dimensional image was transformed using recurrence plot (RP), which is considered to greatly capture the non-linear characteristics. The ultimate image dataset was enriched by changing several parameters of the RP and was then used to feed the convolutional neural network (CNN). Compared to conventional machine learning (ML) methods, a CNN can self-learn useful features from the input data and does not perform complex manual feature engineering (i.e., feature extraction and selection). Results: Finally, according to the optimization experiment, the CNN model obtained the average performance using optimal configuration across 10-fold: accuracy = 98.69%, sensitivity = 99.29%, specificity = 98.10%, and area under the curve = 98.70%. Conclusion: To the best of our knowledge, this approached achieved better classification performance in predicting fetal hypoxia using FHR signals compared to the other state-of-the-art works. Significance: In summary, the satisfied result proved the effectiveness of our proposed CAD system for assisting obstetricians making objective and accurate medical decisions based on RP and powerful CNN algorithm.","",""
268,"Xuanqing Liu, Minhao Cheng, Huan Zhang, Cho-Jui Hsieh","Towards Robust Neural Networks via Random Self-ensemble",2017,"","","","",46,"2022-07-13 10:11:42","","10.1007/978-3-030-01234-2_23","","",,,,,268,53.60,67,4,5,"","",""
117,"Cheng Zhang, Wu Liu, Huadong Ma, Huiyuan Fu","Siamese neural network based gait recognition for human identification",2016,"","","","",47,"2022-07-13 10:11:42","","10.1109/ICASSP.2016.7472194","","",,,,,117,19.50,29,4,6,"As the remarkable characteristics of remote accessed, robust and security, gait recognition has gained significant attention in the biometrics based human identification task. However, the existed methods mainly employ the handcrafted gait features, which cannot well handle the indistinctive inter-class differences and large intra-class variations of human gait in real-world situation. In this paper, we have developed a Siamese neural network based gait recognition framework to automatically extract robust and discriminative gait features for human identification. Different from conventional deep neural network, the Siamese network can employ distance metric learning to drive the similarity metric to be small for pairs of gait from the same person, and large for pairs from different persons. In particular, to further learn effective model with limited training data, we composite the gait energy images instead of raw sequence of gaits. Consequently, the experiments on the world's largest gait database show our framework impressively outperforms state-of-the-arts.","",""
62,"Brendon Lutnick, B. Ginley, D. Govind, S. McGarry, P. LaViolette, R. Yacoub, Sanjay Jain, J. Tomaszewski, K. Jen, P. Sarder","An integrated iterative annotation technique for easing neural network training in medical image analysis",2019,"","","","",48,"2022-07-13 10:11:42","","10.1038/S42256-019-0018-3","","",,,,,62,20.67,6,10,3,"","",""
37,"Hang Su, Wen Qi, Chenguang Yang, A. Aliverti, G. Ferrigno, E. De Momi","Deep Neural Network Approach in Human-Like Redundancy Optimization for Anthropomorphic Manipulators",2019,"","","","",49,"2022-07-13 10:11:42","","10.1109/ACCESS.2019.2937380","","",,,,,37,12.33,6,6,3,"Human-like behavior has emerged in the robotics area for improving the quality of Human-Robot Interaction (HRI). For the human-like behavior imitation, the kinematic mapping between a human arm and robot manipulator is one of the popular solutions. To fulfill this requirement, a reconstruction method called swivel motion was adopted to achieve human-like imitation. This approach aims at modeling the regression relationship between robot pose and swivel motion angle. Then it reaches the human-like swivel motion using its redundant degrees of the manipulator. This characteristic holds for most of the redundant anthropomorphic robots. Although artificial neural network (ANN) based approaches show moderate robustness, the predictive performance is limited. In this paper, we propose a novel deep convolutional neural network (DCNN) structure for reconstruction enhancement and reducing online prediction time. Finally, we utilized the trained DCNN model for managing redundancy control a 7 DoFs anthropomorphic robot arm (LWR4+, KUKA, Germany) for validation. A demonstration is presented to show the human-like behavior on the anthropomorphic manipulator. The proposed approach can also be applied to control other anthropomorphic robot manipulators in industry area or biomedical engineering.","",""
77,"Lixin Fan, Kam Woh Ng, Chee Seng Chan","Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks",2019,"","","","",50,"2022-07-13 10:11:42","","","","",,,,,77,25.67,26,3,3,"With substantial amount of time, resources and human (team) efforts invested to explore and develop successful deep neural networks (DNN), there emerges an urgent need to protect these inventions from being illegally copied, redistributed, or abused without respecting the intellectual properties of legitimate owners. Following recent progresses along this line, we investigate a number of watermark-based DNN ownership verification methods in the face of ambiguity attacks, which aim to cast doubts on the ownership verification by forging counterfeit watermarks. It is shown that ambiguity attacks pose serious threats to existing DNN watermarking methods. As remedies to the above-mentioned loophole, this paper proposes novel passport-based DNN ownership verification schemes which are both robust to network modifications and resilient to ambiguity attacks. The gist of embedding digital passports is to design and train DNN models in a way such that, the DNN inference performance of an original task will be significantly deteriorated due to forged passports. In other words, genuine passports are not only verified by looking for the predefined signatures, but also reasserted by the unyielding DNN model inference performances. Extensive experimental results justify the effectiveness of the proposed passport-based DNN ownership verification schemes. Code and models are available at https://github.com/kamwoh/DeepIPR","",""
3,"Shanlin Zhong, Junjie Zhou, Hong Qiao","Bioinspired Gain-Modulated Recurrent Neural Network for Controlling Musculoskeletal Robot.",2021,"","","","",51,"2022-07-13 10:11:42","","10.1109/TNNLS.2021.3071196","","",,,,,3,3.00,1,3,1,"The motor cortex can arouse abundant transient responses to generate complex movements with the regulation of neuromodulators, while its architecture remains unchanged. This characteristic endows humans with flexible and robust abilities in adapting to dynamic environments, which is exactly the bottleneck in the control of complex robots. In this article, inspired by the mechanisms of the motor cortex in encoding information and modulating motor commands, a biologically plausible gain-modulated recurrent neural network is proposed to control a highly redundant, coupled, and nonlinear musculoskeletal robot. As the characteristics observed in the motor cortex, this network is able to learn gain patterns for arousing transient responses to complete the desired movements, while the connections of synapses keep unchanged, and the dynamic stability of the network is maintained. A novel learning rule that mimics the mechanism of neuromodulators in regulating the learning process of the brain is put forward to learn gain patterns effectively. Meanwhile, inspired by error-based movement correction mechanism in the cerebellum, gain patterns learned from demonstration samples are leveraged as prior knowledge to improve calculation efficiency of the network in controlling novel movements. Experiments were conducted on an upper extremity musculoskeletal model with 11 muscles and a general articulated robot to perform goal-directed tasks. The results indicate that the gain-modulated neural network can effectively control a complex robot to complete various movements with high accuracy, and the proposed algorithms make it possible to realize fast generalization and incremental learning ability.","",""
0,"Nian Chi Tay, T. Ong","Tay, NC and Connie, T and Ong, TS and Goh, KOM and Teh, PS (2019) A robust abnormal behavior detection method using convolutional neural net-",2020,"","","","",52,"2022-07-13 10:11:42","","","","",,,,,0,0.00,0,2,2,"A behavior is considered abnormal when it is seen as unusual under certain contexts. The definition for abnormal behavior varies depending on situations. For example, people running in a field is considered normal but is deemed abnormal if it takes place in a mall. Similarly, loitering in the alleys, fighting or pushing each other in public areas are considered abnormal under specific circumstances. Abnormal behavior detection is crucial due to the increasing crime rate in the society. If an abnormal behavior can be detected earlier, tragedies can be avoided. In recent years, deep learning has been widely applied in the computer vision field and has acquired great success for human detection. In particular, Convolutional Neural Network (CNN) has shown to have achieved state-ofthe-art performance in human detection. In this paper, a CNN-based abnormal behavior detection method is presented. The proposed approach automatically learns the most discriminative characteristics pertaining to human behavior from a large pool of videos containing normal and abnormal behaviors. Since the interpretation for abnormal behavior varies across contexts, extensive experiments have been carried out to assess various conditions and scopes including crowd and single person behavior detection and recognition. The proposed method represents an end-to-end solution to deal with abnormal behavior under different conditions including variations in background, number of subjects (individual, two persons or crowd), and a range of diverse unusual human activities. Experiments on five benchmark datasets validate the performance of the proposed approach.","",""
26,"Tahmina Zebin, Patricia Jane Scully, Niels Peek, A. Casson, K. Ozanyan","Design and Implementation of a Convolutional Neural Network on an Edge Computing Smartphone for Human Activity Recognition",2019,"","","","",53,"2022-07-13 10:11:42","","10.1109/ACCESS.2019.2941836","","",,,,,26,8.67,5,5,3,"Edge computing aims to integrate computing into everyday settings, enabling the system to be context-aware and private to the user. With the increasing success and popularity of deep learning methods, there is an increased demand to leverage these techniques in mobile and wearable computing scenarios. In this paper, we present an assessment of a deep human activity recognition system’s memory and execution time requirements, when implemented on a mid-range smartphone class hardware and the memory implications for embedded hardware. This paper presents the design of a convolutional neural network (CNN) in the context of human activity recognition scenario. Here, layers of CNN automate the feature learning and the influence of various hyper-parameters such as the number of filters and filter size on the performance of CNN. The proposed CNN showed increased robustness with better capability of detecting activities with temporal dependence compared to models using statistical machine learning techniques. The model obtained an accuracy of 96.4% in a five-class static and dynamic activity recognition scenario. We calculated the proposed model memory consumption and execution time requirements needed for using it on a mid-range smartphone. Per-channel quantization of weights and per-layer quantization of activation to 8-bits of precision post-training produces classification accuracy within 2% of floating-point networks for dense, convolutional neural network architecture. Almost all the size and execution time reduction in the optimized model was achieved due to weight quantization. We achieved more than four times reduction in model size when optimized to 8-bit, which ensured a feasible model capable of fast on-device inference.","",""
22,"Bo Li, Cheng Han, Baoxing Bai","Hybrid approach for human posture recognition using anthropometry and BP neural network based on Kinect V2",2019,"","","","",54,"2022-07-13 10:11:42","","10.1186/S13640-018-0393-4","","",,,,,22,7.33,7,3,3,"","",""
367,"V. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, S. Gordon, C. Hung, Brent Lance","EEGNet: A Compact Convolutional Neural Network for EEG-based Brain-Computer Interfaces",2021,"","","","",55,"2022-07-13 10:11:42","","","","",,,,,367,367.00,61,6,1,"Objective: Brain computer interfaces (BCI) enable direct communication with a computer, using neural activity as the control signal. This neural signal is generally chosen from a variety of well-studied electroencephalogram (EEG) signals. For a given BCI paradigm, feature extractors and classifiers are tailored to the distinct characteristics of its expected EEG control signal, limiting its application to that specific signal. Convolutional Neural Networks (CNNs), which have been used in computer vision and speech recognition to perform automatic feature extraction and classification, have successfully been applied to EEG-based BCIs; however, they have mainly been applied to single BCI paradigms and thus it remains unclear how these architectures generalize to other paradigms. Here, we ask if we can design a single CNN architecture to accurately classify EEG signals from different BCI paradigms, while simultaneously being as compact as possible (defined as the number of parameters in the model). Approach: In this work we introduce EEGNet, a compact convolutional neural network for EEG-based BCIs. We introduce the use of depthwise and separable convolutions to construct an EEG-specific model which encapsulates well-known EEG feature extraction concepts for BCI. We compare EEGNet, both for within-subject and cross-subject classification, to current state-of-the-art approaches across four BCI paradigms: P300 visual-evoked potentials, error-related negativity responses (ERN), movement-related cortical potentials (MRCP), and sensory motor rhythms (SMR). Results: We show that EEGNet generalizes across paradigms better than, and achieves comparably high performance to, the reference algorithms when only limited training data is available. We also show that EEGNet effectively generalizes to both ERP and oscillatory-based BCIs. In addition, we demonstrate three different approaches to visualize the contents of a trained EEGNet model to enable interpretation of the learned features. Significance: Our results suggest that EEGNet is robust enough to learn a wide variety of interpretable features over a range of BCI tasks, suggesting that the observed performances were not due to artifact or noise sources in the data. Our models can be found at: https://github.com/vlawhern/arl-eegmodels. 1 ar X iv :1 61 1. 08 02 4v 4 [ cs .L G ] 1 6 M ay 2 01 8","",""
0,"mina hanna, M. Tayel","A Hybrid Encoded and Adapted-tuned Neural Network for Asset Medical Image Watermarking Technique",2022,"","","","",56,"2022-07-13 10:11:42","","10.21203/rs.3.rs-831690/v1","","",,,,,0,0.00,0,2,1,"  Digital image watermarking techniques are used to authenticate identity of owners and copyright protection of asset images. Asset medical images (AMI) specially require extreme care when embed a watermark message because additional information should affect the AMI quality and the changes in AMI gray levels may interfere with its interpretation. This paper introduces a hybrid encoded and adapted-tuned neural network (TNN) for AMI watermarking technique to cover almost essential watermarking requirements. To attain robustness, security and invisibility, uses human visual system (HVS) and TNN to tune the AMI and to find the maximum amount of adaptive watermark message before the watermark message becomes visible. To achieve transparency, enhance AMI using histogram equalization. Embedding is performed into the middle frequency coefficients of discrete cosine transform of the AMI, to avoid visual parts in the low frequency coefficient and the noise and attacks in high frequency to improve image robustness and increase capacity comparing to spatial domain.","",""
1,"Zhoufan Jiang, Yanming Wang, Chenwei Shi, Yueyang Wu, Rongjie Hu, Shishuo Chen, Sheng Hu, Xiaoxiao Wang, B. Qiu","Attention module improves both performance and interpretability of four‐dimensional functional magnetic resonance imaging decoding neural network",2022,"","","","",57,"2022-07-13 10:11:42","","10.1002/hbm.25813","","",,,,,1,1.00,0,9,1,"Decoding brain cognitive states from neuroimaging signals is an important topic in neuroscience. In recent years, deep neural networks (DNNs) have been recruited for multiple brain state decoding and achieved good performance. However, the open question of how to interpret the DNN black box remains unanswered. Capitalizing on advances in machine learning, we integrated attention modules into brain decoders to facilitate an in‐depth interpretation of DNN channels. A four‐dimensional (4D) convolution operation was also included to extract temporo‐spatial interaction within the fMRI signal. The experiments showed that the proposed model obtains a very high accuracy (97.4%) and outperforms previous researches on the seven different task benchmarks from the Human Connectome Project (HCP) dataset. The visualization analysis further illustrated the hierarchical emergence of task‐specific masks with depth. Finally, the model was retrained to regress individual traits within the HCP and to classify viewing images from the BOLD5000 dataset, respectively. Transfer learning also achieves good performance. Further visualization analysis shows that, after transfer learning, low‐level attention masks remained similar to the source domain, whereas high‐level attention masks changed adaptively. In conclusion, the proposed 4D model with attention module performed well and facilitated interpretation of DNNs, which is helpful for subsequent research.","",""
1028,"Awni Y. Hannun, Pranav Rajpurkar, M. Haghpanahi, G. Tison, Codie Bourn, M. Turakhia, A. Ng","Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network",2019,"","","","",58,"2022-07-13 10:11:42","","10.1038/s41591-018-0268-3","","",,,,,1028,342.67,147,7,3,"","",""
6,"Nikolaos Livathinos, Cesar Berrospi, M. Lysak, Viktor Kuropiatnyk, Ahmed Nassar, A. Carvalho, Michele Dolfi, Christoph Auer, K. Dinkla, P. Staar","Robust PDF Document Conversion Using Recurrent Neural Networks",2021,"","","","",59,"2022-07-13 10:11:42","","","","",,,,,6,6.00,1,10,1,"The number of published PDF documents in both the academic and commercial world has increased exponentially in recent decades. There is a growing need to make their rich content discoverable to information retrieval tools. Achieving high-quality semantic searches demands that a document's structural components such as title, section headers, paragraphs, (nested) lists, tables and figures (including their captions) are properly identified. Unfortunately, the PDF format is known to not conserve such structural information because it simply represents a document as a stream of low-level printing commands, in which one or more characters are placed in a bounding box with a particular styling. In this paper, we present a novel approach to document structure recovery in PDF using recurrent neural networks to process the low-level PDF data representation directly, instead of relying on a visual re-interpretation of the rendered PDF page, as has been proposed in previous literature. We demonstrate how a sequence of PDF printing commands can be used as input into a neural network and how the network can learn to classify each printing command according to its structural function in the page. This approach has three advantages: First, it can distinguish among more fine-grained labels (typically 10-20 labels as opposed to 1-5 with visual methods), which results in a more accurate and detailed document structure resolution. Second, it can take into account the text flow across pages more naturally compared to visual methods because it can concatenate the printing commands of sequential pages. Last, our proposed method needs less memory and it is computationally less expensive than visual methods. This allows us to deploy such models in production environments at a much lower cost. Through extensive architectural search in combination with advanced feature engineering, we were able to implement a model that yields a weighted average F-1 score of 97% across 17 distinct structural labels. The best model we achieved is currently served in production environments on our Corpus Conversion Service (CCS), which was presented at KDD18. This model enhances the capabilities of CCS significantly, as it eliminates the need for human annotated label ground-truth for every unseen document layout. This proved particularly useful when applied to a huge corpus of PDF articles related to COVID-19.","",""
97,"E. Rusak, Lukas Schott, Roland S. Zimmermann, Julian Bitterwolf, O. Bringmann, M. Bethge, Wieland Brendel","A Simple Way to Make Neural Networks Robust Against Diverse Image Corruptions",2020,"","","","",60,"2022-07-13 10:11:42","","10.1007/978-3-030-58580-8_4","","",,,,,97,48.50,14,7,2,"","",""
3,"Graham Roberts, Rajat Sainju, Brian Hutchinson, M. Toloczko, David Edwards, Yuanyuan Zhu","DefectNet – A Deep Convolutional Neural Network for Semantic Segmentation of Crystallographic Defects in Advanced Microscopy Images",2019,"","","","",61,"2022-07-13 10:11:42","","10.1017/S1431927619001557","","",,,,,3,1.00,1,6,3,"The current practice of identifying defects in microscopy images and deriving metrics such as dislocation density and precipitates/voids diameter remains largely in the purview of human analysis. The lack of automated defect analysis for statistically meaningful quantification of a variety types of crystallographic defects is causing an increasingly large bottleneck for rational alloy design. The first and most important step of automating defect analysis is perceptual defect identification. In terms of digital image processing, semantic segmentation best emulates human recognition of defect features – it tells what defects are in an image and where they are located. In this work, we developed a novel deep convolutional neural network (CNN) model, called DefectNet, for robust and automated semantic segmentation of three crystallographic defects including line dislocations, precipitates, and voids commonly observed in structural metals and alloys [1]. Defect semantic segmentation in TEM micrographs is a challenging deep learning task due to the nature of the image itself. Unlike everyday photographs, the interpretation of image contrast in TEM micrographs is usually not straightforward; multiple contrast mechanisms often contribute to the observation of defect features. Here, we aim at resolving this image-induced challenge by optimizing the image quality. In previous work, we established an experimental protocol for a diffraction contrast imaging scanning transmission electron microscopy (DCI STEM) technique and tailored it specifically for imaging defects in popular iron-based structural alloys [2]. Thus, the DefectNet was trained on a small set of high-quality DCI STEM defect images obtained from HT-9 martensitic steels. The performance of the resulting model for each defect was assessed quantitatively by standard semantic segmentation evaluation metrics, and the resulting defect density and size measurements compared to that from a group of human experts.","",""
6,"Xueyuan She, Yun Long, S. Mukhopadhyay","Improving Robustness of ReRAM-based Spiking Neural Network Accelerator with Stochastic Spike-timing-dependent-plasticity",2019,"","","","",62,"2022-07-13 10:11:42","","10.1109/IJCNN.2019.8851825","","",,,,,6,2.00,2,3,3,"Spike-timing-dependent-plasticity (STDP) is an unsupervised learning algorithm for spiking neural network (SNN), which promises to achieve deeper understanding of human brain and more powerful artificial intelligence. While conventional computing system fails to simulate SNN efficiently, process-inmemory (PIM) based on devices such as ReRAM can be used in designing fast and efficient STDP based SNN accelerators, as it operates in high resemblance with biological neural network. However, the real-life implementation of such design still suffers from impact of input noise and device variation. In this work, we present a novel stochastic STDP algorithm that uses spiking frequency information to dynamically adjust synaptic behavior. The algorithm is tested in pattern recognition task with noisy input and shows accuracy improvement over deterministic STDP. In addition, we show that the new algorithm can be used for designing a robust ReRAM based SNN accelerator that has strong resilience to device variation.","",""
3,"Artur Petrosyan, M. Sinkin, M. Lebedev, A. Ossadtchi","Decoding and interpreting cortical signals with a compact convolutional neural network",2021,"","","","",63,"2022-07-13 10:11:42","","10.1088/1741-2552/abe20e","","",,,,,3,3.00,1,4,1,"Objective. Brain–computer interfaces (BCIs) decode information from neural activity and send it to external devices. The use of Deep Learning approaches for decoding allows for automatic feature engineering within the specific decoding task. Physiologically plausible interpretation of the network parameters ensures the robustness of the learned decision rules and opens the exciting opportunity for automatic knowledge discovery. Approach. We describe a compact convolutional network-based architecture for adaptive decoding of electrocorticographic (ECoG) data into finger kinematics. We also propose a novel theoretically justified approach to interpreting the spatial and temporal weights in the architectures that combine adaptation in both space and time. The obtained spatial and frequency patterns characterizing the neuronal populations pivotal to the specific decoding task can then be interpreted by fitting appropriate spatial and dynamical models. Main results. We first tested our solution using realistic Monte-Carlo simulations. Then, when applied to the ECoG data from Berlin BCI competition IV dataset, our architecture performed comparably to the competition winners without requiring explicit feature engineering. Using the proposed approach to the network weights interpretation we could unravel the spatial and the spectral patterns of the neuronal processes underlying the successful decoding of finger kinematics from an ECoG dataset. Finally we have also applied the entire pipeline to the analysis of a 32-channel EEG motor-imagery dataset and observed physiologically plausible patterns specific to the task. Significance. We described a compact and interpretable CNN architecture derived from the basic principles and encompassing the knowledge in the field of neural electrophysiology. For the first time in the context of such multibranch architectures with factorized spatial and temporal processing we presented theoretically justified weights interpretation rules. We verified our recipes using simulations and real data and demonstrated that the proposed solution offers a good decoder and a tool for investigating motor control neural mechanisms.","",""
0,"Jiahua Xu, Zheng Wu, A. Nürnberger, B. Sabel","Decoding Resting-state EEG to Predict Visual Field Defect with Convolutional Neural Network in Stroke",2021,"","","","",64,"2022-07-13 10:11:42","","10.1109/NER49283.2021.9441458","","",,,,,0,0.00,0,4,1,"Stroke is one of the leading factors of human being's death and disability. One-third of stroke patients may suffer partial visual field loss in both eyes. The relationship between brain oscillation after a unilateral occipital stroke and visual field defect is worth investigating. Decoding resting-state Electroencephalography (EEG) to predict patients' visual field distribution could be an essential reference for a better understanding of the compensation of visual functions after a stroke. The result could be beneficial for clinical diagnostics and treatment. This paper proposed a frequency spectrum-based 2D convolutional neural network(CNN) and brain connectivity-based 3DCNN model to predict the visual field defect. The results show that the frequency spectrum-based 2DCNN achieved a higher accuracy on visual field location than the connectivity-based 3DCNN model. Simultaneously, the percentage seems to be not predictable for both domains, and therefore we also explored the patterns of electrophysiological data for feature visualization and interpretation.","",""
16,"Lisa Fan, Dong Yu, Lu Wang","Robust Neural Abstractive Summarization Systems and Evaluation against Adversarial Information",2018,"","","","",65,"2022-07-13 10:11:42","","","","",,,,,16,4.00,5,3,4,"Sequence-to-sequence (seq2seq) neural models have been actively investigated for abstractive summarization. Nevertheless, existing neural abstractive systems frequently generate factually incorrect summaries and are vulnerable to adversarial information, suggesting a crucial lack of semantic understanding. In this paper, we propose a novel semantic-aware neural abstractive summarization model that learns to generate high quality summaries through semantic interpretation over salient content. A novel evaluation scheme with adversarial samples is introduced to measure how well a model identifies off-topic information, where our model yields significantly better performance than the popular pointer-generator summarizer. Human evaluation also confirms that our system summaries are uniformly more informative and faithful as well as less redundant than the seq2seq model.","",""
78,"Hsien-I Lin, Ming-Hsiang Hsu, Wei-Kai Chen","Human hand gesture recognition using a convolution neural network",2014,"","","","",66,"2022-07-13 10:11:42","","10.1109/CoASE.2014.6899454","","",,,,,78,9.75,26,3,8,"Automatic human gesture recognition from camera images is an interesting topic for developing intelligent vision systems. In this paper, we propose a convolution neural network (CNN) method to recognize hand gestures of human task activities from a camera image. To achieve the robustness performance, the skin model and the calibration of hand position and orientation are applied to obtain the training and testing data for the CNN. Since the light condition seriously affects the skin color, we adopt a Gaussian Mixture model (GMM) to train the skin model which is used to robustly filter out non-skin colors of an image. The calibration of hand position and orientation aims at translating and rotating the hand image to a neutral pose. Then the calibrated images are used to train the CNN. In our experiment, we provided a validation of the proposed method on recognizing human gestures which shows robust results with various hand positions and orientations and light conditions. Our experimental evaluation of seven subjects performing seven hand gestures with average recognition accuracies around 95.96% shows the feasibility and reliability of the proposed method.","",""
50,"Kenneth P. Smith, A. D. Kang, J. Kirby","Automated Interpretation of Blood Culture Gram Stains by Use of a Deep Convolutional Neural Network",2017,"","","","",67,"2022-07-13 10:11:42","","10.1128/JCM.01521-17","","",,,,,50,10.00,17,3,5,"ABSTRACT Microscopic interpretation of stained smears is one of the most operator-dependent and time-intensive activities in the clinical microbiology laboratory. Here, we investigated application of an automated image acquisition and convolutional neural network (CNN)-based approach for automated Gram stain classification. Using an automated microscopy platform, uncoverslipped slides were scanned with a 40× dry objective, generating images of sufficient resolution for interpretation. We collected 25,488 images from positive blood culture Gram stains prepared during routine clinical workup. These images were used to generate 100,213 crops containing Gram-positive cocci in clusters, Gram-positive cocci in chains/pairs, Gram-negative rods, or background (no cells). These categories were targeted for proof-of-concept development as they are associated with the majority of bloodstream infections. Our CNN model achieved a classification accuracy of 94.9% on a test set of image crops. Receiver operating characteristic (ROC) curve analysis indicated a robust ability to differentiate between categories with an area under the curve of >0.98 for each. After training and validation, we applied the classification algorithm to new images collected from 189 whole slides without human intervention. Sensitivity and specificity were 98.4% and 75.0% for Gram-positive cocci in chains and pairs, 93.2% and 97.2% for Gram-positive cocci in clusters, and 96.3% and 98.1% for Gram-negative rods. Taken together, our data support a proof of concept for a fully automated classification methodology for blood-culture Gram stains. Importantly, the algorithm was highly adept at identifying image crops with organisms and could be used to present prescreened, classified crops to technologists to accelerate smear review. This concept could potentially be extended to all Gram stain interpretive activities in the clinical laboratory.","",""
1,"M. H. Mozaffari, Chanho Kim, Won-sook Lee","Ultrasound Tongue Contour Extraction using Dilated Convolutional Neural Network",2019,"","","","",68,"2022-07-13 10:11:42","","10.1109/BIBM47256.2019.8983002","","",,,,,1,0.33,0,3,3,"One application of medical ultrasound imaging is to visualize and characterize human tongue shape and motion to study healthy or impaired speech production. Due to the low-contrast characteristic and noisy nature of ultrasound images, it requires knowledge about the tongue structure and ultrasound data interpretation for users to recognize tongue gestures. Moreover, quantitative analysis of tongue motion needs the tongue contour to be extracted, tracked and visualized automatically. This paper presents two novel deep neural networks that benefit from the ability of global prediction of encoding-decoding fully convolutional networks and the capability of full-resolution extraction of dilated convolutions. Assessment studies over datasets from different ultrasound machines disclosed the outstanding performances of the proposed models in terms of accuracy and robustness.","",""
2,"Simone Dari, Nikolay Kadrileev, E. Hüllermeier","A Neural Network-Based Driver Gaze Classification System with Vehicle Signals",2020,"","","","",69,"2022-07-13 10:11:42","","10.1109/IJCNN48605.2020.9207709","","",,,,,2,1.00,1,3,2,"Driver monitoring can play an essential part in avoiding accidents by warning the driver and shifting the driver’s attention to the traffic scenery in time during critical situations. This may apply for the different levels of automated driving, for take-over requests as well as for driving in manual mode. A great proxy for this purpose has always been the driver’s gazing direction. The aim of this work is to introduce a robust gaze detection system. In this regard, we make several contributions that are novel in the area of gaze detection systems. In particular, we propose a deep learning approach to predict gaze regions, which is based on informative features such as eye landmarks and head pose angles of the driver. Moreover, we introduce different post-processing techniques that improve the accuracy by exploiting temporal information from videos and the availability of other vehicle signals. Last but not least, we confirm our method with a leave-one-driver-out cross-validation. Unlike previous studies, we do not use gazes to predict maneuver changes, but we consider the human-computer-interaction aspect and use vehicle signals to improve the performance of the estimation. The proposed system is able to achieve an accuracy of 92.3% outperforming earlier landmark-based gaze estimators.","",""
50,"A. Hramov, N. Frolov, V. Maksimenko, V. Makarov, A. Koronovskii, J. Garcia-Prieto, L. Antón-Toro, F. Maestú, A. Pisarchik","Artificial neural network detects human uncertainty.",2018,"","","","",70,"2022-07-13 10:11:42","","10.1063/1.5002892","","",,,,,50,12.50,6,9,4,"Artificial neural networks (ANNs) are known to be a powerful tool for data analysis. They are used in social science, robotics, and neurophysiology for solving tasks of classification, forecasting, pattern recognition, etc. In neuroscience, ANNs allow the recognition of specific forms of brain activity from multichannel EEG or MEG data. This makes the ANN an efficient computational core for brain-machine systems. However, despite significant achievements of artificial intelligence in recognition and classification of well-reproducible patterns of neural activity, the use of ANNs for recognition and classification of patterns in neural networks still requires additional attention, especially in ambiguous situations. According to this, in this research, we demonstrate the efficiency of application of the ANN for classification of human MEG trials corresponding to the perception of bistable visual stimuli with different degrees of ambiguity. We show that along with classification of brain states associated with multistable image interpretations, in the case of significant ambiguity, the ANN can detect an uncertain state when the observer doubts about the image interpretation. With the obtained results, we describe the possible application of ANNs for detection of bistable brain activity associated with difficulties in the decision-making process.","",""
81,"Kaipeng Zhang, Zhanpeng Zhang, Chia-Wen Cheng, Winston H. Hsu, Y. Qiao, W. Liu, T. Zhang","Super-Identity Convolutional Neural Network for Face Hallucination",2018,"","","","",71,"2022-07-13 10:11:42","","10.1007/978-3-030-01252-6_12","","",,,,,81,20.25,12,7,4,"","",""
10,"Drew Linsley, Junkyung Kim, D. Berson, Thomas Serre","Robust neural circuit reconstruction from serial electron microscopy with convolutional recurrent networks",2018,"","","","",72,"2022-07-13 10:11:42","","","","",,,,,10,2.50,3,4,4,"Recent successes in deep learning have started to impact neuroscience. Of particular significance are claims that current segmentation algorithms achieve ""super-human"" accuracy in an area known as connectomics. However, as we will show, these algorithms do not effectively generalize beyond the particular source and brain tissues used for training -- severely limiting their usability by the broader neuroscience community. To fill this gap, we describe a novel connectomics challenge for source- and tissue-agnostic reconstruction of neurons (STAR), which favors broad generalization over fitting specific datasets. We first demonstrate that current state-of-the-art approaches to neuron segmentation perform poorly on the challenge. We further describe a novel convolutional recurrent neural network module that combines short-range horizontal connections within a processing stage and long-range top-down connections between stages. The resulting architecture establishes the state of the art on the STAR challenge and represents a significant step towards widely usable and fully-automated connectomics analysis.","",""
30,"Yu Zeng, Kebei Jiang, Jie Chen","Automatic Seismic Salt Interpretation with Deep Convolutional Neural Networks",2018,"","","","",73,"2022-07-13 10:11:42","","10.1145/3325917.3325926","","",,,,,30,7.50,10,3,4,"One of the most crucial tasks in seismic reflection imaging is to identify the salt bodies with high precision. Traditionally, this is accomplished by visually picking the salt/sediment boundaries, which requires a great amount of manual work and may introduce systematic bias. With recent progress of deep learning algorithm and growing computational power, a great deal of efforts has been made to replace human effort with machine power in salt body interpretation. Currently, the method of Convolutional neural networks (CNN) is revolutionizing the computer vision field and has been a hot topic in the image analysis. In this paper, the benefits of CNN-based classification are demonstrated by using a state-of-art network structure U-Net, along with the residual learning framework ResNet, to delineate salt body with high precision. Network adjustments, including the Exponential Linear Units (ELU) activation function, the Lovasz-Softmax loss function, and stratified K-fold cross-validation, have been deployed to further improve the prediction accuracy. The preliminary result using SEG-SEAM data shows good agreement between the predicted salt body and manually interpreted salt body, especially in areas with weak reflections. This indicates the great potential of applying CNN for salt-related interpretations.","",""
1,"Huijun Wu, Chen Wang, R. Nock, Wei Wang, Jie Yin, Kai Lu, Liming Zhu","SMINT: Toward Interpretable and Robust Model Sharing for Deep Neural Networks",2020,"","","","",74,"2022-07-13 10:11:42","","10.1145/3381833","","",,,,,1,0.50,0,7,2,"Sharing a pre-trained machine learning model, particularly a deep neural network via prediction APIs, is becoming a common practice on machine learning as a service (MLaaS) platforms nowadays. Although deep neural networks (DNN) have shown remarkable successes in many tasks, they are also criticized for the lack of interpretability and transparency. Interpreting a shared DNN model faces two additional challenges compared with interpreting a general model. (1) Limited training data can be disclosed to users. (2) The internal structure of the models may not be available. These two challenges impede the application of most existing interpretability approaches, such as saliency maps or influence functions, for DNN models. Case-based reasoning methods have been used for interpreting decisions; however, how to select and organize the data points under the constraints of shared DNN models is not discussed. Moreover, simply providing cases as explanations may not be sufficient for supporting instance level interpretability. Meanwhile, existing interpretation methods for DNN models generally lack the means to evaluate the reliability of the interpretation. In this article, we propose a framework named Shared Model INTerpreter (SMINT) to address the above limitations. We propose a new data structure called a boundary graph to organize training points to mimic the predictions of DNN models. We integrate local features, such as saliency maps and interpretable input masks, into the data structure to help users to infer the model decision boundaries. We show that the boundary graph is able to address the reliability issues in many local interpretation methods. We further design an algorithm named hidden-layer aware p-test to measure the reliability of the interpretations. Our experiments show that SMINT is able to achieve above 99% fidelity to corresponding DNN models on both MNIST and ImageNet by sharing only a tiny fraction of training data to make these models interpretable. The human pilot study demonstrates that SMINT provides better interpretability compared with existing methods. Moreover, we demonstrate that SMINT is able to assist model tuning for better performance on different user data.","",""
0,"Jindong Gu, Rui Zhao, Volker Tresp","Semantics for Global and Local Interpretation of Deep Convolutional Neural Networks",2021,"","","","",75,"2022-07-13 10:11:42","","10.1109/IJCNN52387.2021.9533809","","",,,,,0,0.00,0,3,1,"A large number of saliency methods have been proposed to explain individual decisions of deep convolutional neural networks (DCNNs). They work by identifying the relevance of each input feature to the predicted output class. However, the feature representations of hidden layers are difficult to interpret semantically. In this work, human-interpretable semantic concepts are associated with vectors in feature space. The association process is mathematically formulated as an optimization problem. The semantic vectors obtained from the optimal solution are applied to interpret deep neural networks globally and locally. The global interpretations are useful to understand the knowledge learned by DCNNs. The interpretation of local behaviors can help to gain a better understanding of the individual decisions made by DCNNs. The empirical experiments demonstrate how to use identified semantics to interpret the existing DCNNs.","",""
180,"Junwen Chen, Zhigang Liu, Hongrui Wang, A. Núñez, Zhiwei Han","Automatic Defect Detection of Fasteners on the Catenary Support Device Using Deep Convolutional Neural Network",2018,"","","","",76,"2022-07-13 10:11:42","","10.1109/TIM.2017.2775345","","",,,,,180,45.00,36,5,4,"The excitation and vibration triggered by the long-term operation of railway vehicles inevitably result in defective states of catenary support devices. With the massive construction of high-speed electrified railways, automatic defect detection of diverse and plentiful fasteners on the catenary support device is of great significance for operation safety and cost reduction. Nowadays, the catenary support devices are periodically captured by the cameras mounted on the inspection vehicles during the night, but the inspection still mostly relies on human visual interpretation. To reduce the human involvement, this paper proposes a novel vision-based method that applies the deep convolutional neural networks (DCNNs) in the defect detection of the fasteners. Our system cascades three DCNN-based detection stages in a coarse-to-fine manner, including two detectors to sequentially localize the cantilever joints and their fasteners and a classifier to diagnose the fasteners’ defects. Extensive experiments and comparisons of the defect detection of catenary support devices along the Wuhan–Guangzhou high-speed railway line indicate that the system can achieve a high detection rate with good adaptation and robustness in complex environments.","",""
72,"N. Sharma, C. Gregory, Marcus Johnson, W. Dixon","Closed-Loop Neural Network-Based NMES Control for Human Limb Tracking",2012,"","","","",77,"2022-07-13 10:11:42","","10.1109/TCST.2011.2125792","","",,,,,72,7.20,18,4,10,"Closed-loop control of skeletal muscle is complicated by the nonlinear muscle force to length and velocity relationships and the inherent unstructured and time-varying uncertainties in available models. Some pure feedback methods have been developed with some success, but the most promising and popular control methods for neuromuscular electrical stimulation (NMES) are neural network (NN)-based methods. Efforts in this paper focus on the use of a NN feedforward controller that is augmented with a continuous robust feedback term to yield an asymptotic result (in lieu of typical uniformly ultimately bounded stability). Specifically, an NN-based controller and Lyapunov-based stability analysis are provided to enable semi-global asymptotic tracking of a desired limb time-varying trajectory (i.e., non-isometric contractions). The developed controller is applied as an amplitude modulated voltage to external electrodes attached to the distal-medial and proximal-lateral portion of the quadriceps femoris muscle group in non-impaired volunteers. The added value of incorporating a NN feedforward term is illustrated through experiments that compare the developed controller with and without the NN feedforward component.","",""
21,"C. Li, Shouqian Sun, Xin Min, Wenqian Lin, Binling Nie, Xianfu Zhang","End-to-end learning of deep convolutional neural network for 3D human action recognition",2017,"","","","",78,"2022-07-13 10:11:42","","10.1109/ICMEW.2017.8026281","","",,,,,21,4.20,4,6,5,"Recently, skeleton-based human action recognition has been receiving significant attention from various research communities due to its robustness, succinctness, and view-invariant representation. Most of the existing skeleton-based methods use either well-designed classifiers with hand-crafted features or current neural network (RNN) to recognize human actions. In this paper, inspired by the deep convolutional neural network's breakthroughs in the image domain, we transform a skeleton sequence into an image and perform end-to-end learning of deep convolutional neural network (CNN). The skeleton sequence based image contains spatial temporal information. Our proposed method is tested on the NTU RGB+D dataset which is so far the largest skeleton-based human action dataset, and achieves the state-of-the-art performance for both the cross-view and cross-subject evaluations.","",""
1,"D. Dellavale, Osvaldo Matías Velarde, G. Mato, E. Urdapilleta","Complex interplay between spectral harmonicity and different types of cross-frequency couplings in nonlinear oscillators and biologically plausible neural network models.",2020,"","","","",79,"2022-07-13 10:11:42","","10.1103/physreve.102.062401","","",,,,,1,0.50,0,4,2,"Cross-frequency coupling (CFC) refers to the nonlinear interaction between oscillations in different frequency bands, and it is a rather ubiquitous phenomenon that has been observed in a variety of physical and biophysical systems. In particular, the coupling between the phase of slow oscillations and the amplitude of fast oscillations, referred as phase-amplitude coupling (PAC), has been intensively explored in the brain activity recorded from animals and humans. However, the interpretation of these CFC patterns remains challenging since harmonic spectral correlations characterizing nonsinusoidal oscillatory dynamics can act as a confounding factor. Specialized signal processing techniques are proposed to address the complex interplay between spectral harmonicity and different types of CFC, not restricted only to PAC. For this, we provide an in-depth characterization of the time locked index (TLI) as a tool aimed to efficiently quantify the harmonic content of noisy time series. It is shown that the proposed TLI measure is more robust and outperforms traditional phase coherence metrics (e.g., phase locking value, pairwise phase consistency) in several aspects. We found that a nonlinear oscillator under the effect of additive noise can produce spurious CFC with low spectral harmonic content. On the other hand, two coupled oscillatory dynamics with independent fundamental frequencies can produce true CFC with high spectral harmonic content via a rectification mechanism or other post-interaction nonlinear processing mechanisms. These results reveal a complex interplay between CFC and harmonicity emerging in the dynamics of biologically plausible neural network models and more generic nonlinear and parametric oscillators. We show that, contrary to what is usually assumed in the literature, the high harmonic content observed in nonsinusoidal oscillatory dynamics is neither a sufficient nor necessary condition to interpret the associated CFC patterns as epiphenomenal. There is mounting evidence suggesting that the combination of multimodal recordings, specialized signal processing techniques, and theoretical modeling is becoming a required step to completely understand CFC patterns observed in oscillatory rich dynamics of physical and biophysical systems.","",""
430,"Amirata Ghorbani, Abubakar Abid, James Y. Zou","Interpretation of Neural Networks is Fragile",2017,"","","","",80,"2022-07-13 10:11:42","","10.1609/aaai.v33i01.33013681","","",,,,,430,86.00,143,3,5,"In order for machine learning to be trusted in many applications, it is critical to be able to reliably explain why the machine learning algorithm makes certain predictions. For this reason, a variety of methods have been developed recently to interpret neural network predictions by providing, for example, feature importance maps. For both scientific robustness and security reasons, it is important to know to what extent can the interpretations be altered by small systematic perturbations to the input data, which might be generated by adversaries or by measurement biases. In this paper, we demonstrate how to generate adversarial perturbations that produce perceptively indistinguishable inputs that are assigned the same predicted label, yet have very different interpretations. We systematically characterize the robustness of interpretations generated by several widely-used feature importance interpretation methods (feature importance maps, integrated gradients, and DeepLIFT) on ImageNet and CIFAR-10. In all cases, our experiments show that systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly susceptible to adversarial attack. Our analysis of the geometry of the Hessian matrix gives insight on why robustness is a general challenge to current interpretation approaches.","",""
0,"D. Dellavale, Osvaldo Matías Velarde, G. Mato, E. Urdapilleta","On the complex interplay between spectral harmonicity and different types of cross frequency couplings in non linear oscillators and biologically plausible neural network models",2020,"","","","",81,"2022-07-13 10:11:42","","10.1101/2020.10.15.341800","","",,,,,0,0.00,0,4,2,"Background Cross-frequency coupling (CFC) refers to the non linear interaction between oscillations in different frequency bands, and it is a rather ubiquitous phenomenon that has been observed in a variety of physical and biophysical systems. In particular, the coupling between the phase of slow oscillations and the amplitude of fast oscillations, referred as phase-amplitude coupling (PAC), has been intensively explored in the brain activity recorded from animals and humans. However, the interpretation of these CFC patterns remains challenging since harmonic spectral correlations characterizing non sinusoidal oscillatory dynamics can act as a confounding factor. Methods Specialized signal processing techniques are proposed to address the complex interplay between spectral harmonicity and different types of CFC, not restricted only to PAC. For this, we provide an in-depth characterization of the Time Locked Index (TLI) as a novel tool aimed to efficiently quantify the harmonic content of noisy time series. It is shown that the proposed TLI measure is more robust and outperform traditional phase coherence metrics (e.g. Phase Locking Value, Pairwise Phase Consistency) in several aspects. Results We found that a non linear oscillator under the effect of additive noise can produce spurious CFC with low spectral harmonic content. On the other hand, two coupled oscillatory dynamics with independent fundamental frequencies can produce true CFC with high spectral harmonic content via a rectification mechanism or other post-interaction nonlinear processing mechanisms. These results reveal a complex interplay between CFC and harmonicity emerging in the dynamics of biologically plausible neural network models and more generic non linear and parametric oscillators. Conclusions We show that, contrary to what is usually assumed in the literature, the high harmonic content observed in non sinusoidal oscillatory dynamics, is neither sufficient nor necessary condition to interpret the associated CFC patterns as epiphenomenal. There is mounting evidence suggesting that the combination of multimodal recordings, specialized signal processing techniques and theoretical modeling is becoming a required step to completely understand CFC patterns observed in oscillatory rich dynamics of physical and biophysical systems. Highlights Time locked index efficiently quantifies the harmonic content of noisy time series. A non linear oscillator under the effect of additive noise can produce spurious cross frequency couplings (CFC) with low spectral harmonic content. Two coupled oscillatory dynamics with independent fundamental frequencies can produce true CFC with high spectral harmonic content via rectification mechanisms or other post-interaction nonlinear processing mechanisms. A non sinusoidal oscillatory dynamics with high harmonic content is neither sufficient nor necessary condition for spurious CFC. A complex interplay between CFC and harmonicity emerges from the dynamics of nonlinear, parametric and biologically plausible oscillators.","",""
32,"Yuxuan Yang, Zhongke Gao, Xinmin Wang, Yanli Li, J. Han, N. Marwan, J. Kurths","A recurrence quantification analysis-based channel-frequency convolutional neural network for emotion recognition from EEG.",2018,"","","","",82,"2022-07-13 10:11:42","","10.1063/1.5023857","","",,,,,32,8.00,5,7,4,"Constructing a reliable and stable emotion recognition system is a critical but challenging issue for realizing an intelligent human-machine interaction. In this study, we contribute a novel channel-frequency convolutional neural network (CFCNN), combined with recurrence quantification analysis (RQA), for the robust recognition of electroencephalogram (EEG) signals collected from different emotion states. We employ movie clips as the stimuli to induce happiness, sadness, and fear emotions and simultaneously measure the corresponding EEG signals. Then the entropy measures, obtained from the RQA operation on EEG signals of different frequency bands, are fed into the novel CFCNN. The results indicate that our system can provide a high emotion recognition accuracy of 92.24% and a relatively excellent stability as well as a satisfactory Kappa value of 0.884, rendering our system particularly useful for the emotion recognition task. Meanwhile, we compare the performance of the entropy measures, extracted from each frequency band, in distinguishing the three emotion states. We mainly find that emotional features extracted from the gamma band present a considerably higher classification accuracy of 90.51% and a Kappa value of 0.858, proving the high relation between emotional process and gamma frequency band.","",""
73,"Chuan Qin, Hengshu Zhu, Tong Xu, Chen Zhu, Liang Jiang, Enhong Chen, Hui Xiong","Enhancing Person-Job Fit for Talent Recruitment: An Ability-aware Neural Network Approach",2018,"","","","",83,"2022-07-13 10:11:42","","10.1145/3209978.3210025","","",,,,,73,18.25,10,7,4,"The wide spread use of online recruitment services has led to information explosion in the job market. As a result, the recruiters have to seek the intelligent ways for Person-Job Fit, which is the bridge for adapting the right job seekers to the right positions. Existing studies on Person-Job Fit have a focus on measuring the matching degree between the talent qualification and the job requirements mainly based on the manual inspection of human resource experts despite of the subjective, incomplete, and inefficient nature of the human judgement. To this end, in this paper, we propose a novel end-to-end A bility-aware P erson-J ob F it N eural N etwork (APJFNN) model, which has a goal of reducing the dependence on manual labour and can provide better interpretation about the fitting results. The key idea is to exploit the rich information available at abundant historical job application data. Specifically, we propose a word-level semantic representation for both job requirements and job seekers' experiences based on Recurrent Neural Network (RNN). Along this line, four hierarchical ability-aware attention strategies are designed to measure the different importance of job requirements for semantic representation, as well as measuring the different contribution of each job experience to a specific ability requirement. Finally, extensive experiments on a large-scale real-world data set clearly validate the effectiveness and interpretability of the APJFNN framework compared with several baselines.","",""
83,"Juyeon Heo, Sunghwan Joo, Taesup Moon","Fooling Neural Network Interpretations via Adversarial Model Manipulation",2019,"","","","",84,"2022-07-13 10:11:42","","","","",,,,,83,27.67,28,3,3,"We ask whether the neural network interpretation methods can be fooled via adversarial model manipulation, which is defined as a model fine-tuning step that aims to radically alter the explanations without hurting the accuracy of the original models, e.g., VGG19, ResNet50, and DenseNet121. By incorporating the interpretation results directly in the penalty term of the objective function for fine-tuning, we show that the state-of-the-art saliency map based interpreters, e.g., LRP, Grad-CAM, and SimpleGrad, can be easily fooled with our model manipulation. We propose two types of fooling, Passive and Active, and demonstrate such foolings generalize well to the entire validation set as well as transfer to other interpretation methods. Our results are validated by both visually showing the fooled explanations and reporting quantitative metrics that measure the deviations from the original explanations. We claim that the stability of neural network interpretation method with respect to our adversarial model manipulation is an important criterion to check for developing robust and reliable neural network interpretation method.","",""
22,"Adam Kortylewski, Qing Liu, Angtian Wang, Yihong Sun, A. Yuille","Compositional Convolutional Neural Networks: A Robust and Interpretable Model for Object Recognition under Occlusion",2020,"","","","",85,"2022-07-13 10:11:42","","10.1007/s11263-020-01401-3","","",,,,,22,11.00,4,5,2,"","",""
67,"Keisuke Sakaguchi, Kevin Duh, Matt Post, Benjamin Van Durme","Robsut Wrod Reocginiton via Semi-Character Recurrent Neural Network",2016,"","","","",86,"2022-07-13 10:11:42","","10.1609/aaai.v31i1.10970","","",,,,,67,11.17,17,4,6,"    Language processing mechanism by humans is generally more robust than computers. The Cmabrigde Uinervtisy (Cambridge University) effect from the psycholinguistics literature has demonstrated such a robust word processing mechanism, where jumbled words (e.g. Cmabrigde / Cambridge) are recognized with little cost. On the other hand, computational models for word recognition (e.g. spelling checkers) perform poorly on data with such noise. Inspired by the findings from the Cmabrigde Uinervtisy effect, we propose a word recognition model based on a semi-character level recurrent neural network (scRNN). In our experiments, we demonstrate that scRNN has significantly more robust performance in word spelling correction (i.e. word recognition) compared to existing spelling checkers and character-based convolutional neural network. Furthermore, we demonstrate that the model is cognitively plausible by replicating a psycholinguistics experiment about human reading difficulty using our model.   ","",""
0,"D. Ebuna, J. Kluesner, K. Cunningham, J. H. Edwards","Statistical approach to neural network imaging of karst systems in 3D seismic reflection data",2018,"","","","",87,"2022-07-13 10:11:42","","10.1190/INT-2017-0197.1","","",,,,,0,0.00,0,4,4,"The current lack of a robust standardized technique for geophysical mapping of karst systems can be attributed to the complexity of the environment and prior technological limitations. Abrupt lateral variations in physical properties that are inherent to karst systems generate significant geophysical noise, challenging conventional seismic signal processing and interpretation. The application of neural networks (NNs) to multiattribute seismic interpretation can provide a semiautomated method for identifying and leveraging the nonlinear relationships exhibited among seismic attributes. The ambiguity generally associated with designing NNs for seismic object detection can be reduced via statistical analysis of the extracted attribute data. A data-driven approach to selecting the appropriate set of input seismic attributes, as well as the locations and suggested number of training examples, provides a more objective and computationally efficient method for identifying karst systems using reflection seismology. This statistically optimized NN technique is demonstrated using 3D seismic reflection data collected from the southeastern portion of the Florida carbonate platform. Several dimensionality reduction methods are applied, and the resulting karst probability models are evaluated relative to one another based on quantitative and qualitative criteria. Comparing the preferred model, using quadratic discriminant analysis, with previously available seismic object detection workflows demonstrates the karst-specific nature of the tool. Results suggest that the karst multiattribute workflow presented is capable of approximating the structural boundaries of karst systems with more accuracy and efficiency than a human counterpart or previously presented seismic interpretation schemes. This objective technique, using solely 3D seismic reflection data, is proposed as a practical approach to mapping karst systems for subsequent hydrogeologic modeling.","",""
86,"Lingxue Song, Dihong Gong, Zhifeng Li, Changsong Liu, Wei Liu","Occlusion Robust Face Recognition Based on Mask Learning With Pairwise Differential Siamese Network",2019,"","","","",88,"2022-07-13 10:11:42","","10.1109/ICCV.2019.00086","","",,,,,86,28.67,17,5,3,"Deep Convolutional Neural Networks (CNNs) have been pushing the frontier of face recognition over past years. However, existing CNN models are far less accurate when handling partially occluded faces. These general face models generalize poorly for occlusions on variable facial areas. Inspired by the fact that human visual system explicitly ignores the occlusion and only focuses on the non-occluded facial areas, we propose a mask learning strategy to find and discard corrupted feature elements from recognition. A mask dictionary is firstly established by exploiting the differences between the top conv features of occluded and occlusion-free face pairs using innovatively designed pairwise differential siamese network (PDSN). Each item of this dictionary captures the correspondence between occluded facial areas and corrupted feature elements, which is named Feature Discarding Mask (FDM). When dealing with a face image with random partial occlusions, we generate its FDM by combining relevant dictionary items and then multiply it with the original features to eliminate those corrupted feature elements from recognition. Comprehensive experiments on both synthesized and realistic occluded face datasets show that the proposed algorithm significantly outperforms the state-of-the-art systems.","",""
66,"Z. Zheng, Pengyu Hong","Robust Detection of Adversarial Attacks by Modeling the Intrinsic Properties of Deep Neural Networks",2018,"","","","",89,"2022-07-13 10:11:42","","","","",,,,,66,16.50,33,2,4,"It has been shown that deep neural network (DNN) based classifiers are vulnerable to human-imperceptive adversarial perturbations which can cause DNN classifiers to output wrong predictions with high confidence. We propose an unsupervised learning approach to detect adversarial inputs without any knowledge of attackers. Our approach tries to capture the intrinsic properties of a DNN classifier and uses them to detect adversarial inputs. The intrinsic properties used in this study are the output distributions of the hidden neurons in a DNN classifier presented with natural images. Our approach can be easily applied to any DNN classifiers or combined with other defense strategy to improve robustness. Experimental results show that our approach demonstrates state-of-the-art robustness in defending black-box and gray-box attacks.","",""
58,"Greg Anderson, Shankara Pailoor, Isil Dillig, Swarat Chaudhuri","Optimization and abstraction: a synergistic approach for analyzing neural network robustness",2019,"","","","",90,"2022-07-13 10:11:42","","10.1145/3314221.3314614","","",,,,,58,19.33,15,4,3,"In recent years, the notion of local robustness (or robustness for short) has emerged as a desirable property of deep neural networks. Intuitively, robustness means that small perturbations to an input do not cause the network to perform misclassifications. In this paper, we present a novel algorithm for verifying robustness properties of neural networks. Our method synergistically combines gradient-based optimization methods for counterexample search with abstraction-based proof search to obtain a sound and (δ -)complete decision procedure. Our method also employs a data-driven approach to learn a verification policy that guides abstract interpretation during proof search. We have implemented the proposed approach in a tool called Charon and experimentally evaluated it on hundreds of benchmarks. Our experiments show that the proposed approach significantly outperforms three state-of-the-art tools, namely AI^2, Reluplex, and Reluval.","",""
86,"J. Wenzel, H. Matter, K. Schmidt","Predictive Multitask Deep Neural Network Models for ADME-Tox Properties: Learning from Large Data Sets",2019,"","","","",91,"2022-07-13 10:11:42","","10.1021/acs.jcim.8b00785","","",,,,,86,28.67,29,3,3,"Successful drug discovery projects require control and optimization of compound properties related to pharmacokinetics, pharmacodynamics, and safety. While volume and chemotype coverage of public and corporate ADME-Tox (absorption, distribution, excretion, metabolism, and toxicity) databases are constantly growing, deep neural nets (DNN) emerged as transformative artificial intelligence technology to analyze those challenging data. Relevant features are automatically identified, while appropriate data can also be combined to multitask networks to evaluate hidden trends among multiple ADME-Tox parameters for implicitly correlated data sets. Here we describe a novel, fully industrialized approach to parametrize and optimize the setup, training, application, and visual interpretation of DNNs to model ADME-Tox data. Investigated properties include microsomal lability in different species, passive permeability in Caco-2/TC7 cells, and logD. Statistical models are developed using up to 50 000 compounds from public or corporate databases. Both the choice of DNN hyperparameters and the type and quantity of molecular descriptors were found to be important for successful DNN modeling. Alternate learning of multiple ADME-Tox properties, resulting in a multitask approach, performs statistically superior on most studied data sets in comparison to DNN single-task models and also provides a scalable method to predict ADME-Tox properties from heterogeneous data. For example, predictive quality using external validation sets was improved from R2 of 0.6 to 0.7 comparing single-task and multitask DNN networks from human metabolic lability data. Besides statistical evaluation, a new visualization approach is introduced to interpret DNN models termed ""response map"", which is useful to detect local property gradients based on structure fragmentation and derivatization. This method is successfully applied to visualize fragmental contributions to guide further design in drug discovery programs, as illustrated by CRCX3 antagonists and renin inhibitors, respectively.","",""
46,"Xiaoxiao Li, N. Dvornek, Yuan Zhou, Juntang Zhuang, P. Ventola, J. Duncan","Graph Neural Network for Interpreting Task-fMRI Biomarkers",2019,"","","","",92,"2022-07-13 10:11:42","","10.1007/978-3-030-32254-0_54","","",,,,,46,15.33,8,6,3,"","",""
16,"P. Feldens, A. Darr, Agata Feldens, F. Tauber","Detection of Boulders in Side Scan Sonar Mosaics by a Neural Network",2019,"","","","",93,"2022-07-13 10:11:42","","10.3390/GEOSCIENCES9040159","","",,,,,16,5.33,4,4,3,"Boulders provide ecologically important hard grounds in shelf seas, and form protected habitats under the European Habitats Directive. Boulders on the seafloor can usually be recognized in backscatter mosaics due to a characteristic pattern of high backscatter intensity followed by an acoustic shadow. The manual identification of boulders on mosaics is tedious and subjective, and thus could benefit from automation. In this study, we train an object detection framework, RetinaNet, based on a neural network backbone, ResNet, to detect boulders in backscatter mosaics derived from a sidescan-sonar operating at 384 kHz. A training dataset comprising 4617 boulders and 2005 negative examples similar to boulders was used to train RetinaNet. The trained model was applied to a test area located in the Kriegers Flak area (Baltic Sea), and the results compared to mosaic interpretation by expert analysis. Some misclassification of water column noise and boundaries of artificial plough marks occurs, but the results of the trained model are comparable to the human interpretation. While the trained model correctly identified a higher number of boulders, the human interpreter had an advantage at recognizing smaller objects comprising a bounding box of less than 7 × 7 pixels. Almost identical performance between the best model and expert analysis was found when classifying boulder density into three classes (0, 1–5, more than 5) over 10,000 m² areas, with the best performing model reaching an agreement with the human interpretation of 90%.","",""
0,"Tianyi Lan, ZongBin Shi, Ke-jun Wang, Chaoqun Yin","Gait Recognition Algorithm based on Spatial-temporal Graph Neural Network",2022,"","","","",94,"2022-07-13 10:11:42","","10.1109/BDICN55575.2022.00018","","",,,,,0,0.00,0,4,1,"Gait recognition is an emerging biometric recognition technology. Gait features have the advantages of non-contact, long collection distance and so on. It has received extensive attention from researchers in the field of biometric identification. We propose a novel model-based gait recognition method. Early methods were mainly based on appearance. Appearance-based features usually use gait contour maps as input. The gait contour map is easy to obtain and proved to be effective for recognition tasks. However, its appearance will be affected by the changes of clothing and carrying items. Contrast to the contour-based method is the model-based method. We use the human pose estimation algorithm to obtain 3D key points, use the key points coordinates as graph nodes feature to build a spatial-temporal graph, and use graph neural network to extract features for gait recognition tasks. This method is experimented on the large-scale dataset CSAIA-B dataset. The experimental results show that the proposed method can achieve advanced performance. It is also robust to covariate changes.","",""
90,"Chandan Singh, W. James Murdoch, Bin Yu","Hierarchical interpretations for neural network predictions",2018,"","","","",95,"2022-07-13 10:11:42","","","","",,,,,90,22.50,30,3,4,"Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.","",""
13,"Hao Wu, Bo Zhang","A deep convolutional encoder-decoder neural network in assisting seismic horizon tracking",2018,"","","","",96,"2022-07-13 10:11:42","","","","",,,,,13,3.25,7,2,4,"Seismic horizons are geologically significant surfaces that can be used for building geology structure and stratigraphy models. However, horizon tracking in 3D seismic data is a time-consuming and challenging problem. Relief human from the tedious seismic interpretation is one of the hot research topics. We proposed a novel automatically seismic horizon tracking method by using a deep convolutional neural network. We employ a state-of-art end-to-end semantic segmentation method to track the seismic horizons automatically. Experiment result shows that our proposed neural network can automatically track multiple horizons simultaneously. We validate the effectiveness and robustness of our proposed method by comparing automatically tracked horizons with manually picked horizons.","",""
64,"Andras Rozsa, Manuel Günther, T. Boult","Towards Robust Deep Neural Networks with BANG",2016,"","","","",97,"2022-07-13 10:11:42","","10.1109/WACV.2018.00093","","",,,,,64,10.67,21,3,6,"Machine learning models, including state-of-the-art deep neural networks, are vulnerable to small perturbations that cause unexpected classification errors. This unexpected lack of robustness raises fundamental questions about their generalization properties and poses a serious concern for practical deployments. As such perturbations can remain imperceptible – the formed adversarial examples demonstrate an inherent inconsistency between vulnerable machine learning models and human perception – some prior work casts this problem as a security issue. Despite the significance of the discovered instabilities and ensuing research, their cause is not well understood and no effective method has been developed to address the problem. In this paper, we present a novel theory to explain why this unpleasant phenomenon exists in deep neural networks. Based on that theory, we introduce a simple, efficient, and effective training approach, Batch Adjusted Network Gradients (BANG), which significantly improves the robustness of machine learning models. While the BANG technique does not rely on any form of data augmentation or the utilization of adversarial images for training, the resultant classifiers are more resistant to adversarial perturbations while maintaining or even enhancing the overall classification performance.","",""
129,"Wenzhi Zhao, S. Du, W. Emery","Object-Based Convolutional Neural Network for High-Resolution Imagery Classification",2017,"","","","",98,"2022-07-13 10:11:42","","10.1109/JSTARS.2017.2680324","","",,,,,129,25.80,43,3,5,"Timely and accurate classification and interpretation of high-resolution images are very important for urban planning and disaster rescue. However, as spatial resolution gets finer, it is increasingly difficult to recognize complex patterns in high-resolution remote sensing images. Deep learning offers an efficient strategy to fill the gap between complex image patterns and their semantic labels. However, due to the hierarchical abstract nature of deep learning methods, it is difficult to capture the precise outline of different objects at the pixel level. To further reduce this problem, we propose an object-based deep learning method to accurately classify the high-resolution imagery without intensive human involvement. In this study, high-resolution images were used to accurately classify three different urban scenes: Beijing (China), Pavia (Italy), and Vaihingen (Germany). The proposed method is built on a combination of a deep feature learning strategy and an object-based classification for the interpretation of high-resolution images. Specifically, high-level feature representations extracted through the convolutional neural networks framework have been systematically investigated over five different layer configurations. Furthermore, to improve the classification accuracy, an object-based classification method also has been integrated with the deep learning strategy for more efficient image classification. Experimental results indicate that with the combination of deep learning and object-based classification, it is possible to discriminate different building types in Beijing Scene, such as commercial buildings and residential buildings with classification accuracies above 90%.","",""
1,"J. Constantin, A. Bigand, I. Constantin","Pooling spike neural network for fast rendering in global illumination",2019,"","","","",99,"2022-07-13 10:11:42","","10.1007/s00521-018-3941-z","","",,,,,1,0.33,0,3,3,"","",""
0,"S. Wein, A. Schüller, Ana Maria Tom'e, W. M. Malloni, M. Greenlee, E. Lang","Forecasting Brain Activity Based on Models of Spatio-Temporal Brain Dynamics: A Comparison of Graph Neural Network Architectures",2021,"","","","",100,"2022-07-13 10:11:42","","10.1162/netn_a_00252","","",,,,,0,0.00,0,6,1,"  Comprehending the interplay between spatial and temporal characteristics of neural dynamics can contribute to our understanding of information processing in the human brain. Graph neural networks (GNNs) provide a new possibility to interpret graph structured signals like those observed in complex brain networks. In our study we compare different spatio-temporal GNN architectures and study their ability to model neural activity distributions obtained in functional MRI (fMRI) studies. We evaluate the performance of the GNN models on a variety of scenarios in MRI studies and also compare it to a VAR model, which is currently often used for directed functional connectivity analysis. We show that by learning localized functional interactions on the anatomical substrate, GNN based approaches are able to robustly scale to large network studies, even when available data are scarce. By including anatomical connectivity as the physical substrate for information propagation, such GNNs also provide a multi-modal perspective on directed connectivity analysis, offering a novel possibility to investigate the spatio-temporal dynamics in brain networks.","",""
40,"Patrick Esser, Robin Rombach, B. Ommer","A Disentangling Invertible Interpretation Network for Explaining Latent Representations",2020,"","","","",101,"2022-07-13 10:11:42","","10.1109/cvpr42600.2020.00924","","",,,,,40,20.00,13,3,2,"Neural networks have greatly boosted performance in computer vision by learning powerful representations of input data. The drawback of end-to-end training for maximal overall performance are black-box models whose hidden representations are lacking interpretability: Since distributed coding is optimal for latent layers to improve their robustness, attributing meaning to parts of a hidden feature vector or to individual neurons is hindered. We formulate interpretation as a translation of hidden representations onto semantic concepts that are comprehensible to the user. The mapping between both domains has to be bijective so that semantic modifications in the target domain correctly alter the original representation. The proposed invertible interpretation network can be transparently applied on top of existing architectures with no need to modify or retrain them. Consequently, we translate an original representation to an equivalent yet interpretable one and backwards without affecting the expressiveness and performance of the original. The invertible interpretation network disentangles the hidden representation into separate, semantically meaningful concepts. Moreover, we present an efficient approach to define semantic concepts by only sketching two images and also an unsupervised strategy. Experimental evaluation demonstrates the wide applicability to interpretation of existing classification and image generation networks as well as to semantically guided image manipulation.","",""
7,"Q. Song, W. Hu","Robust neural network controller for variable airflow volume system",2002,"","","","",102,"2022-07-13 10:11:42","","10.1109/ACC.2002.1024841","","",,,,,7,0.35,4,2,20,"The neural network technology, that is based on the operating mechanism of the human brain, is the best suited technology to enhanced the PID control function. However, it is important to address stability and disturbance properly, to obtain optimal performance of the control system. The paper discusses the design and application of a robust neural network algorithm, and how it compliments the fixed proportional control algorithm to provide the desired functionality and the adaptation of the VAV control system for a wide range of disturbances and parameter changes.","",""
24,"Omar Costilla-Reyes, R. Vera-Rodríguez, Patricia J. Scully, K. Ozanyan","Analysis of Spatio-Temporal Representations for Robust Footstep Recognition with Deep Residual Neural Networks",2019,"","","","",103,"2022-07-13 10:11:42","","10.1109/TPAMI.2018.2799847","","",,,,,24,8.00,6,4,3,"Human footsteps can provide a unique behavioural pattern for robust biometric systems. We propose spatio-temporal footstep representations from floor-only sensor data in advanced computational models for automatic biometric verification. Our models deliver an artificial intelligence capable of effectively differentiating the fine-grained variability of footsteps between legitimate users (clients) and impostor users of the biometric system. The methodology is validated in the largest to date footstep database, containing nearly 20,000 footstep signals from more than 120 users. The database is organized by considering a large cohort of impostors and a small set of clients to verify the reliability of biometric systems. We provide experimental results in 3 critical data-driven security scenarios, according to the amount of footstep data made available for model training: at airports security checkpoints (smallest training set), workspace environments (medium training set) and home environments (largest training set). We report state-of-the-art footstep recognition rates with an optimal equal false acceptance and false rejection rate (equal error rate) of 0.7 percent an improvement ratio of 371 percent compared to previous state-of-the-art. We perform a feature analysis of deep residual neural networks showing effective clustering of client's footstep data and to provide insights of the feature learning process.","",""
9,"T. Kusuma, B. Fish","Toward More Robust Neural-Network First Break And Horizon Pickers",1993,"","","","",104,"2022-07-13 10:11:42","","10.1190/1.1822449","","",,,,,9,0.31,5,2,29,"The Artificial Neural Networks discussed to this date in the geophysical literature have been single trace processing tools. We show that significant improvements can be realized by incorporating a variety of multi-trace prediction criteria into the neural network. The expansion of the network into the multi-trace realm enables it to “see” the ensemble or stack as a single entity, rather than as an isolated series of traces. The robustness of the tool is enhanced by the feature that allows the human processor to add control points, correct, or retrain the network during the application phase until the network performs satisfactorily. We use a cascade-correlation network whose inherent speed and incremental learning ability make it possible with most data sets to train and apply the network interactively. Examples of the effectiveness of the interactive multitrace technique are shown for first break picking and for post-stack horizon recognition.","",""
180,"Noah D. Brenowitz, C. Bretherton","Prognostic Validation of a Neural Network Unified Physics Parameterization",2018,"","","","",105,"2022-07-13 10:11:42","","10.1029/2018GL078510","","",,,,,180,45.00,90,2,4,"Weather and climate models approximate diabatic and sub‐grid‐scale processes in terms of grid‐scale variables using parameterizations. Current parameterizations are designed by humans based on physical understanding, observations, and process modeling. As a result, they are numerically efficient and interpretable, but potentially oversimplified. However, the advent of global high‐resolution simulations and observations enables a more robust approach based on machine learning. In this letter, a neural network‐based parameterization is trained using a near‐global aqua‐planet simulation with a 4‐km resolution (NG‐Aqua). The neural network predicts the apparent sources of heat and moisture averaged onto (160 km)2 grid boxes. A numerically stable scheme is obtained by minimizing the prediction error over multiple time steps rather than single one. In prognostic single‐column model tests, this scheme matches both the fluctuations and equilibrium of NG‐Aqua simulation better than the Community Atmosphere Model does.","",""
429,"Fu-Chen Chen, M. Jahanshahi","NB-CNN: Deep Learning-Based Crack Detection Using Convolutional Neural Network and Naïve Bayes Data Fusion",2018,"","","","",106,"2022-07-13 10:11:42","","10.1109/TIE.2017.2764844","","",,,,,429,107.25,215,2,4,"Regular inspection of nuclear power plant components is important to guarantee safe operations. However, current practice is time consuming, tedious, and subjective, which involves human technicians reviewing the inspection videos and identifying cracks on reactors. A few vision-based crack detection approaches have been developed for metallic surfaces, and they typically perform poorly when used for analyzing nuclear inspection videos. Detecting these cracks is a challenging task since they are tiny, and noisy patterns exist on the components’ surfaces. This study proposes a deep learning framework, based on a convolutional neural network (CNN) and a Naïve Bayes data fusion scheme, called NB-CNN, to analyze individual video frames for crack detection while a novel data fusion scheme is proposed to aggregate the information extracted from each video frame to enhance the overall performance and robustness of the system. To this end, a CNN is proposed to detect crack patches in each video frame, while the proposed data fusion scheme maintains the spatiotemporal coherence of cracks in videos, and the Naïve Bayes decision making discards false positives effectively. The proposed framework achieves a 98.3% hit rate against 0.1 false positives per frame that is significantly higher than state-of-the-art approaches as presented in this paper.","",""
29,"C. Corbane, V. Syrris, F. Sabo, P. Politis, M. Melchiorri, M. Pesaresi, P. Soille, T. Kemper","Convolutional Neural Networks for Global Human Settlements Mapping from Sentinel-2 Satellite Imagery",2020,"","","","",107,"2022-07-13 10:11:42","","10.1007/S00521-020-05449-7","","",,,,,29,14.50,4,8,2,"","",""
44,"Minh Nguyen Nhat To, Q. Vu, B. Turkbey, P. Choyke, J. T. Kwak","Deep dense multi-path neural network for prostate segmentation in magnetic resonance imaging",2018,"","","","",108,"2022-07-13 10:11:42","","10.1007/s11548-018-1841-4","","",,,,,44,11.00,9,5,4,"","",""
182,"Laksshman Sundaram, Hong Gao, Samskruthi Reddy Padigepati, J. McRae, Yanjun Li, J. Kosmicki, Nondas Fritzilas, J. Hakenberg, Anindita Dutta, J. Shon, Jinbo Xu, S. Batzoglou, Xiaolin Li, K. Farh","Predicting the clinical impact of human mutation with deep neural networks",2018,"","","","",109,"2022-07-13 10:11:42","","10.1038/s41588-018-0167-z","","",,,,,182,45.50,18,14,4,"","",""
152,"P. Khosravi, Ehsan Kazemi, Q. Zhan, J. Malmsten, M. Toschi, Pantelis Zisimopoulos, Alexandros Sigaras, S. Lavery, L. Cooper, C. Hickman, M. Meseguer, Z. Rosenwaks, O. Elemento, N. Zaninovic, I. Hajirasouliha","Deep learning enables robust assessment and selection of human blastocysts after in vitro fertilization",2019,"","","","",110,"2022-07-13 10:11:42","","10.1038/s41746-019-0096-y","","",,,,,152,50.67,15,15,3,"","",""
0,"Kenneth P. Smith, A. D. Kang, J. Kirby","Automated Interpretation of Blood Culture Gram Stains using a Deep Convolutional Neural 1 Network 2 3 Running Title : Gram stain interpretation with deep learning",2017,"","","","",111,"2022-07-13 10:11:42","","","","",,,,,0,0.00,0,3,5,"24 Microscopic interpretation of stained smears is one of the most operator-dependent and time 25 intensive activities in the clinical microbiology laboratory. Here, we investigated application of 26 an automated image acquisition and convolutional neural network (CNN)-based approach for 27 automated Gram stain classification. Using an automated microscopy platform, uncoverslipped 28 slides were scanned with a 40x dry objective, generating images of sufficient resolution for 29 interpretation. We collected 25,488 images from positive blood culture Gram stains prepared 30 during routine clinical workup. These images were used to generate 100,213 crops containing 31 Gram-positive cocci in clusters, Gram-positive cocci in chains/pairs, Gram-negative rods, or 32 background (no cells). These categories were targeted for proof-of-concept development as they 33 are associated with the majority of bloodstream infections. Our CNN model achieved 34 classification accuracy of 94.9% on a test set of image crops. Receiver operating characteristic 35 curve (ROC) analysis indicated a robust ability to differentiate between categories with area 36 under the curve >0.98 for each. After training and validation, we applied the classification 37 algorithm to new images collected from 189 whole slides without human intervention. 38 Sensitivity/specificity was 98.4/75.0% for Gram-positive cocci in chains/pairs; 93.2/97.2% for 39 Gram-positive cocci in clusters; and 96.3/98.1% for Gram-negative rods. Taken together, our 40 data support proof-of-concept for a fully automated classification methodology for blood-culture 41 Gram-stains. Importantly, the algorithm was highly adept at identifying image crops with 42 organisms and could be used to present prescreened, classified crops to technologists to 43 accelerate smear review. This concept could potentially be extended to all Gram stain 44 interpretive activities in the clinical laboratory. 45 46 on D ecem er 1, 2017 by gest ht://jcm .sm .rg/ D ow nladed fom Introduction 47 Bloodstream infections (BSI) are rapidly progressive infections with mortality rates up to 48 nearly 40% (1, 2). Each day delay in institution of active antimicrobial therapy is associated with 49 up to a ~10% increase in mortality (3, 4). Due to relatively low bacterial burden (<10 CFU mL 50 1 )(5), patient blood is pre-incubated in broth culture to detect presence of bacteria, typically by 51 semi-continuous measurement of CO2 production or pH with an automated blood culture 52 instrument. If organism growth is detected, an aliquot of broth (now containing >10 6 CFU mL -1 ) 53 is removed for Gram stain smear and subculture. The Gram stain provides the first critical piece 54 of information that allows a clinician to tailor appropriate therapy and optimize outcome (6). 55 Despite recent advances in automation in other stages of the BSI diagnosis process 56 (automated blood culture incubators and Gram staining systems) (7), Gram stain interpretation 57 remains labor and time intensive, and highly operator-dependent. With consolidation of hospital 58 systems, increasing workloads, and potential unavailability of highly trained microbiologists on 59 site (8), automated image collection paired with computational interpretation of Gram stains to 60 augment and complement manual testing would provide benefit. However, there has been a 61 dearth of scientific exploration in this area, and several technical difficulties need to be 62 overcome. 63 Practically, automated Gram stain interpretation requires both automated slide imaging 64 and automated image analysis. Although automated slide scanners and microscopes are being 65 used in anatomic pathology, for example, telepathology (9), their application in clinical 66 microbiology has been limited based on several technical challenges. First, Gram stained slides 67 are typically read using 100X objectives, greatly complicating image acquisition due to the need 68 for addition of oil during scanning. Second, microbiology smear material can adequately be 69 on D ecem er 1, 2017 by gest ht://jcm .sm .rg/ D ow nladed fom imaged only in a very narrow field of focus, a challenge for existing slide scanners. Third, Gram 70 stained slides exhibit ubiquitous and highly variable background staining. This background may 71 cause autofocus algorithms to target areas that are either devoid of bacteria or miss the 72 appropriate focal plane entirely. Image analysis to identify Gram stain characteristics presents 73 separate hurdles. Importantly, background and staining artifacts, both fairly ubiquitous, often 74 mimics the shape and color of bacterial cells. Therefore, algorithms relying on color intensity 75 thresholding and shape detection will provide suboptimal accuracy. 76 Here, we provide proof-of-concept for automated, deep learning-based Gram stain 77 analysis. The major conceptual and technical innovations were twofold. First, we developed an 78 imaging protocol using an automated slide imaging platform equipped with a 40X air objective 79 to collect highly resolved data from Gram-stained blood culture slides. Second, image data were 80 used to train a convolutional neural network (CNN)-based model to recognize morphologies 81 representing the most common causative agents of BSI: Gram-negative rods, Gram-positive 82 cocci in clusters, and Gram-positive cocci in pairs or chains (1). CNNs are modeled based on the 83 organization of neurons within the mammalian visual cortex, and were applied here based on 84 their ability to excel in image recognition tasks without requiring time-intensive selective feature 85 extraction by humans (10). Our trained model was subsequently evaluated for accuracy in 86 comparison to manual classification. 87 88 Results 89 Slide collection and manual classification. Blood culture Gram stain slides prepared 90 manually during the course of normal laboratory operation were used for analysis. Slides were 91 selected based on the presence of any of the three most common morphotypes observed in 92 on D ecem er 1, 2017 by gest ht://jcm .sm .rg/ D ow nladed fom bloodstream infection: Gram-positive cocci in clusters, Gram-positive cocci in pairs and chains, 93 and Gram-negative rods. Less common morphotypes (e.g. Gram-positive rods or yeast) and 94 polymicrobial infections were excluded. To capture real-world variability, slides were not pre95 screened for suitability for automated microscopy or deep learning, and had characteristic slide96 to-slide variability in staining intensity, staining artifacts, and sample distribution. We 97 anticipated that inherent variability would pose a real-world challenge to slide classification 98 models. 99 Automated image collection. CNN-based deep learning models require large datasets 100 for training, typically at least on the order of thousands of images (and ideally at least an order of 101 magnitude more). Therefore, an automated microscopy image acquisition strategy was used. We 102 performed image acquisition on the MetaFer Slide Scanning and Imaging Platform 103 (MetaSystems Group, Inc., Newton, MA) based on a robust Gram stain-compatible autofocus 104 system, ability to sample multiple distributed positions on a slide to account for variations in 105 specimen distribution, and automated slide loading capability to enable high throughput slide 106 scanning. 107 Clinically, Gram stains are read under oil immersion. However, semi-continuous addition 108 of oil during automated microscopy was undesirable. In preliminary experiments with 109 uncoverslipped slides (data not shown), we determined that the 40x dry objective provided 110 sufficient resolution for machine-learning applications based on our prior experience (11). 111 Therefore, we selected use of the 40x air objective for image acquisition, thus avoiding the 112 requirement for oil immersion and allowing us to capture a larger field of view in each image. 113 Deep convolutional neural network training. For CNN training, a total of 25,488 114 images were automatically collected from distributed locations on 180 slides. A representative 115 on D ecem er 1, 2017 by gest ht://jcm .sm .rg/ D ow nladed fom image is shown in Fig. 1. This image demonstrates features typical of blood culture Gram stain 116 smears including: (A) intense background staining; (B) stain crystallization artifact; (C) diffuse 117 background staining; (D) individually resolvable, high-contrast Gram-negative cells; and (E) 118 individually resolvable, low-contrast Gram-negative cells. Of note, ubiquitous background 119 material was often similar in color, intensity, and/or shape to bacterial cells. 120 Highly experienced medical technologists can readily differentiate bacteria from this 121 background. However, it is prohibitively difficult to manually define computational rules for 122 Gram-stain classification that would adequately distinguish signal from noise in highly variable 123 Gram-stain preparations. Therefore, we chose instead to use a deep learning approach, more 124 specifically, a CNN, for image analysis. CNNs do not interpret raw images directly. Rather, they 125 consist of a number of layers, each of which convolutes regions of the image to detect specific 126 features. During each step of the learning process, a subset of images is presented to the network, 127 allowing function parameters to be changed such that the CNN identifies features important for 128 classification based on optimization of output accuracy. The final model is defined by a set of 129 weights and biases that control the flow of information through the network such that the most 130 discriminatory features in the images are used for classification. 131 Each CNN model has a unique architecture that differs in organization, function and 132 number of convolutional layers (10). The model used in our analysis, Inception v3, has 133 previously been shown to perform robustly on complex image classification ","",""
108,"Joshua C. Peterson, R. Battleday, T. Griffiths, Olga Russakovsky","Human Uncertainty Makes Classification More Robust",2019,"","","","",112,"2022-07-13 10:11:42","","10.1109/ICCV.2019.00971","","",,,,,108,36.00,27,4,3,"The classification performance of deep neural networks has begun to asymptote at near-perfect levels. However, their ability to generalize outside the training set and their robustness to adversarial attacks have not. In this paper, we make progress on this problem by training with full label distributions that reflect human perceptual uncertainty. We first present a new benchmark dataset which we call CIFAR10H, containing a full distribution of human labels for each image of the CIFAR10 test set. We then show that, while contemporary classifiers fail to exhibit human-like uncertainty on their own, explicit training on our dataset closes this gap, supports improved generalization to increasingly out-of-training-distribution test datasets, and confers robustness to adversarial attacks.","",""
539,"Timon Gehr, M. Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, Martin T. Vechev","AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation",2018,"","","","",113,"2022-07-13 10:11:42","","10.1109/SP.2018.00058","","",,,,,539,134.75,90,6,4,"We present AI2, the first sound and scalable analyzer for deep neural networks. Based on overapproximation, AI2 can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks). The key insight behind AI2 is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers. We present a complete implementation of AI2 together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) AI2 is precise enough to prove useful specifications (e.g., robustness), (ii) AI2 can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) AI2 is significantly faster than existing analyzers based on symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) AI2 can handle deep convolutional networks, which are beyond the reach of existing methods.","",""
40,"B. Almaslukh, A. Artoli, J. Al-Muhtadi","A Robust Deep Learning Approach for Position-Independent Smartphone-Based Human Activity Recognition",2018,"","","","",114,"2022-07-13 10:11:42","","10.3390/s18113726","","",,,,,40,10.00,13,3,4,"Recently, modern smartphones equipped with a variety of embedded-sensors, such as accelerometers and gyroscopes, have been used as an alternative platform for human activity recognition (HAR), since they are cost-effective, unobtrusive and they facilitate real-time applications. However, the majority of the related works have proposed a position-dependent HAR, i.e., the target subject has to fix the smartphone in a pre-defined position. Few studies have tackled the problem of position-independent HAR. They have tackled the problem either using handcrafted features that are less influenced by the position of the smartphone or by building a position-aware HAR. The performance of these studies still needs more improvement to produce a reliable smartphone-based HAR. Thus, in this paper, we propose a deep convolution neural network model that provides a robust position-independent HAR system. We build and evaluate the performance of the proposed model using the RealWorld HAR public dataset. We find that our deep learning proposed model increases the overall performance compared to the state-of-the-art traditional machine learning method from 84% to 88% for position-independent HAR. In addition, the position detection performance of our model improves superiorly from 89% to 98%. Finally, the recognition time of the proposed model is evaluated in order to validate the applicability of the model for real-time applications.","",""
7,"Shengzhong Liu, Shuochao Yao, Yifei Huang, Dongxin Liu, Huajie Shao, Yiran Zhao, Jinyang Li, Tianshi Wang, Ruijie Wang, Chaoqi Yang, T. Abdelzaher","Handling Missing Sensors in Topology-Aware IoT Applications with Gated Graph Neural Network",2020,"","","","",115,"2022-07-13 10:11:42","","10.1145/3411818","","",,,,,7,3.50,1,11,2,"Reliable data collection, transmission, and delivery on Internet of Things (IoT) systems is crucial in order to provide high-quality intelligent services. However, sensor data delivery can be interrupted for various reasons, such as sensor malfunction, network failures, and external attacks. Thus, only data from a partial set of sensors may be available. We call it the missing sensor problem. This problem can lead to severe performance degradation at inference time by neural-network-based recognition models trained on the complete sensor set. This paper enhances the robustness of neural network models to the missing sensor problem by introducing a novel feature reconstruction module, named the graph recovery module, that handles missing sensors directly inside the network. Specifically, we consider topology-aware IoT applications, where sensors are placed on a physically interconnected network. We design a novel neural message passing mechanism that logically mimics physical network topology, based on recent advances in graph neural networks (GNNs). We rely on a spatial locality assumption, where only correlations between physically connected sensors are explicitly explored. When encountering missing sensors, information is passed from available sensors to missing sensors to be used to reconstruct their features. Moreover, at each message passing step, we utilize a gating mechanism inspired by Gated Recurrent Units (GRUs) to automatically control information flow between available sensors and missing sensors. We empirically evaluate the reconstruction performance of the graph recovery module with two representative IoT applications; human activity recognition (HAR) and electroencephalogram (EEG)-based motor-imagery classification, on three public datasets. Two different backbone networks are utilized for the tasks. Our design is shown to effectively maintain model performance, suffering only 7% to 18% accuracy loss when as much as 90% of sensors are removed, compared to a drop of 15% to 47% in the accuracy of competing state-of-the-art algorithms under the same conditions. The accuracy gap is largest when more sensors are missing.","",""
94,"David Sussillo, S. Stavisky, J. Kao, S. Ryu, K. Shenoy","Making brain–machine interfaces robust to future neural variability",2016,"","","","",116,"2022-07-13 10:11:42","","10.1038/ncomms13749","","",,,,,94,15.67,19,5,6,"","",""
1,"Kunling Geng, Dae C. Shin, D. Song, R. Hampson, S. Deadwyler, T. Berger, V. Marmarelis","Multi-Input, Multi-Output Neuronal Mode Network Approach to Modeling the Encoding Dynamics and Functional Connectivity of Neural Systems",2019,"","","","",117,"2022-07-13 10:11:42","","10.1162/neco_a_01204","","",,,,,1,0.33,0,7,3,"This letter proposes a novel method, multi-input, multi-output neuronal mode network (MIMO-NMN), for modeling encoding dynamics and functional connectivity in neural ensembles such as the hippocampus. Compared with conventional approaches such as the Volterra-Wiener model, linear-nonlinear-cascade (LNC) model, and generalized linear model (GLM), the NMN has several advantages in terms of estimation accuracy, model interpretation, and functional connectivity analysis. We point out the limitations of current neural spike modeling methods, especially the estimation biases caused by the imbalanced class problem when the number of zeros is significantly larger than ones in the spike data. We use synthetic data to test the performance of NMN with a comparison of the traditional methods, and the results indicate the NMN approach could reduce the imbalanced class problem and achieve better predictions. Subsequently, we apply the MIMO-NMN method to analyze data from the human hippocampus. The results indicate that the MIMO-NMN method is a promising approach to modeling neural dynamics and analyzing functional connectivity of multi-neuronal data.","",""
2,"J. Hind, A. Hussain, D. Al-Jumeily, B. Abdulaimma, C. C. Montañez, P. Lisboa","A robust method for the interpretation of genomic data",2017,"","","","",118,"2022-07-13 10:11:42","","10.1109/IJCNN.2017.7966281","","",,,,,2,0.40,0,6,5,"This paper presents a robust methodology to find biomarkers that are predictive of any given clinical outcome, by combining three critical steps: Adjustment for correlated biomarkers, through Linkage Disequilibrium pre-processing; False Detection Rate (FD) control with q-values; multivariate predictive modelling with neural networks. The results show that neural network modelling with pre-processing using p-values can be misleading. In particular, the interpretation of the neural network through calculation of the conditional probabilities P(x|c) where x represents covariates and c the classes, haw an important role in elucidating the predictive power (or lack of it) of the biomarkers. The methodology is generally applicable to p>n modelling where the initial pool of potential predictive parameters p, e.g. biomarkers, is greater than the sample size n.","",""
40,"Min Wang, Heba El-Fiqi, Jiankun Hu, H. Abbass","Convolutional Neural Networks Using Dynamic Functional Connectivity for EEG-Based Person Identification in Diverse Human States",2019,"","","","",119,"2022-07-13 10:11:42","","10.1109/TIFS.2019.2916403","","",,,,,40,13.33,10,4,3,"Highly secure access control requires Swiss-cheese-type multi-layer security protocols. The use of electroencephalogram (EEG) to provide cognitive indicators for human workload and fatigue has created environments where the EEG data are well-integrated into systems, making it readily available for more forms of innovative uses including biometrics. However, most of the existing studies on EEG biometrics rely on resting state signals or require specific and repetitive sensory stimulation, limiting their uses in naturalistic settings. Moreover, the limited discriminatory power of uni-variate measures denies an opportunity to use dependences information inherent in brain regions to design more robust biometric identifiers. In this paper, we proposed a novel model for ongoing EEG biometric identification using EEG collected during a diverse set of tasks. The novelty lies in representing EEG signals as a graph based on within-frequency and cross-frequency functional connectivity estimates, and the use of graph convolutional neural network (GCNN) to automatically capture deep intrinsic structural representations from the EEG graphs for person identification. An extensive investigation was carried out to assess the robustness of the method against diverse human states, including resting states under eye-open and eye-closed conditions and active states drawn during the performance of four different tasks. We compared our method with the state-of-the-art EEG features, classifiers, and models of EEG biometrics. Results show that the representation drawn from EEG functional connectivity graphs demonstrates more robust biometric traits than direct use of uni-variate features. Moreover, the GCNN can effectively and efficiently capture discriminative traits, thus generalizing better over diverse human states.","",""
207,"I. Mcloughlin, Haomin Zhang, Zhipeng Xie, Yan Song, Wei Xiao","Robust Sound Event Classification Using Deep Neural Networks",2015,"","","","",120,"2022-07-13 10:11:42","","10.1109/TASLP.2015.2389618","","",,,,,207,29.57,41,5,7,"The automatic recognition of sound events by computers is an important aspect of emerging applications such as automated surveillance, machine hearing and auditory scene understanding. Recent advances in machine learning, as well as in computational models of the human auditory system, have contributed to advances in this increasingly popular research field. Robust sound event classification, the ability to recognise sounds under real-world noisy conditions, is an especially challenging task. Classification methods translated from the speech recognition domain, using features such as mel-frequency cepstral coefficients, have been shown to perform reasonably well for the sound event classification task, although spectrogram-based or auditory image analysis techniques reportedly achieve superior performance in noise. This paper outlines a sound event classification framework that compares auditory image front end features with spectrogram image-based front end features, using support vector machine and deep neural network classifiers. Performance is evaluated on a standard robust classification task in different levels of corrupting noise, and with several system enhancements, and shown to compare very well with current state-of-the-art classification techniques.","",""
160,"Pengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jianru Xue, Nanning Zheng","View Adaptive Neural Networks for High Performance Skeleton-Based Human Action Recognition",2018,"","","","",121,"2022-07-13 10:11:42","","10.1109/TPAMI.2019.2896631","","",,,,,160,40.00,27,6,4,"Skeleton-based human action recognition has recently attracted increasing attention thanks to the accessibility and the popularity of 3D skeleton data. One of the key challenges in action recognition lies in the large variations of action representations when they are captured from different viewpoints. In order to alleviate the effects of view variations, this paper introduces a novel view adaptation scheme, which automatically determines the virtual observation viewpoints over the course of an action in a learning based data driven manner. Instead of re-positioning the skeletons using a fixed human-defined prior criterion, we design two view adaptive neural networks, i.e., VA-RNN and VA-CNN, which are respectively built based on the recurrent neural network (RNN) with the Long Short-term Memory (LSTM) and the convolutional neural network (CNN). For each network, a novel view adaptation module learns and determines the most suitable observation viewpoints, and transforms the skeletons to those viewpoints for the end-to-end recognition with a main classification network. Ablation studies find that the proposed view adaptive models are capable of transforming the skeletons of various views to much more consistent virtual viewpoints. Therefore, the models largely eliminate the influence of the viewpoints, enabling the networks to focus on the learning of action-specific features and thus resulting in superior performance. In addition, we design a two-stream scheme (referred to as VA-fusion) that fuses the scores of the two networks to provide the final prediction, obtaining enhanced performance. Moreover, random rotation of skeleton sequences is employed to improve the robustness of view adaptation models and alleviate overfitting during training. Extensive experimental evaluations on five challenging benchmarks demonstrate the effectiveness of the proposed view-adaptive networks and superior performance over state-of-the-art approaches.","",""
2,"Andreas Huemer, M. Gongora, D. Elizondo","A robust reinforcement based self constructing neural network",2010,"","","","",122,"2022-07-13 10:11:42","","10.1109/IJCNN.2010.5596762","","",,,,,2,0.17,1,3,12,"Usually, many high-skilled human resources are required to create sophisticated control systems. Automatic generation of control systems can overcome these requirements. Because of their versatility and flexibility neural networks gained an important role for this task. While evolutionary methods have been relatively successful in generating neural networks, they have some limitations, in addition to being computationally expensive, because they rely on adapting populations instead of individuals. Reinforcement methods on the other hand can improve and adapt the behaviour of an individual; the reinforcement methods that are presented in this paper can grow a neural network during operation. We show that neural networks can be created for various domains without changing any parameters. Additionally, our neural network can learn the action selection policy and the value function locally within the neurons. These features make our neural network highly flexible and distinguish it from other reinforcement based constructive neural networks.","",""
25,"A. Vessoni, R. Herai, Jerome V. Karpiak, Angelica M S Leal, C. Trujillo, A. Quinet, L. F. Agnez Lima, C. Menck, A. Muotri","Cockayne syndrome-derived neurons display reduced synapse density and altered neural network synchrony.",2016,"","","","",123,"2022-07-13 10:11:42","","10.1093/hmg/ddw008","","",,,,,25,4.17,3,9,6,"Cockayne syndrome (CS) is a rare genetic disorder in which 80% of cases are caused by mutations in the Excision Repair Cross-Complementation group 6 gene (ERCC6). The encoded ERCC6 protein is more commonly referred to as Cockayne Syndrome B protein (CSB). Classical symptoms of CS patients include failure to thrive and a severe neuropathology characterized by microcephaly, hypomyelination, calcification and neuronal loss. Modeling the neurological aspect of this disease has proven difficult since murine models fail to mirror classical neurological symptoms. Therefore, a robust human in vitro cellular model would advance our fundamental understanding of the disease and reveal potential therapeutic targets. Herein, we successfully derived functional CS neural networks from human CS induced pluripotent stem cells (iPSCs) providing a new tool to facilitate studying this devastating disease. We identified dysregulation of the Growth Hormone/Insulin-like Growth Factor-1 (GH/IGF-1) pathway as well as pathways related to synapse formation, maintenance and neuronal differentiation in CSB neurons using unbiased RNA-seq gene expression analyses. Moreover, when compared to unaffected controls, CSB-deficient neural networks displayed altered electrophysiological activity, including decreased synchrony, and reduced synapse density. Collectively, our work reveals that CSB is required for normal neuronal function and we have established an alternative to previously available models to further study neural-specific aspects of CS.","",""
37,"Daniel Zoran, Mike Chrzanowski, Po-Sen Huang, Sven Gowal, A. Mott, Pushmeet Kohli","Towards Robust Image Classification Using Sequential Attention Models",2019,"","","","",124,"2022-07-13 10:11:42","","10.1109/cvpr42600.2020.00950","","",,,,,37,12.33,6,6,3,"In this paper we propose to augment a modern neural-network architecture with an attention model inspired by human perception. Specifically, we adversarially train and analyze a neural model incorporating a human inspired, visual attention component that is guided by a recurrent top-down sequential process. Our experimental evaluation uncovers several notable findings about the robustness and behavior of this new model. First, introducing attention to the model significantly improves adversarial robustness resulting in state-of-the-art ImageNet accuracies under a wide range of random targeted attack strengths. Second, we show that by varying the number of attention steps (glances/fixations) for which the model is unrolled, we are able to make its defense capabilities stronger, even in light of stronger attacks --- resulting in a ``computational race'' between the attacker and the defender. Finally, we show that some of the adversarial examples generated by attacking our model are quite different from conventional adversarial examples --- they contain global, salient and \emph{spatially coherent} structures coming from the target class that would be recognizable even to a human, and work by distracting the attention of the model away from the main object in the original image.","",""
2,"Rafael Berral-Soler, F. J. Madrid-Cuevas, R. Muñoz-Salinas, Manuel J. Mar'in-Jim'enez","RealHePoNet: a robust single-stage ConvNet for head pose estimation in the wild",2020,"","","","",125,"2022-07-13 10:11:42","","10.1007/s00521-020-05511-4","","",,,,,2,1.00,1,4,2,"","",""
2,"Seok-Hwan Choi, Jinmyeong Shin, Peng Liu, Yoon-Ho Choi","EEJE: Two-Step Input Transformation for Robust DNN Against Adversarial Examples",2021,"","","","",126,"2022-07-13 10:11:42","","10.1109/tnse.2020.3008394","","",,,,,2,2.00,1,4,1,"Adversarial examples are human-imperceptible perturbations to inputs to machine learning models. While attacking machine learning models, adversarial examples cause the model to make a false positive or a false negative. So far, two representative defense architectures have shown a significant effect: (1) model retraining architecture; and (2) input transformation architecture. However, previous defense methods belonging to these two architectures do not produce good outputs for every input, i.e., adversarial examples and legitimate inputs. Specifically, model retraining methods generate false negatives for unknown adversarial examples, and input transformation methods generate false positives for legitimate inputs. To produce good-enough outputs for every input, we propose and evaluate a new input transformation architecture based on two-step input transformation. To solve the limitations of the previous two defense methods, we intend to answer the following question: How to maintain the performance of Deep Neural Network (DNN) models for legitimate inputs while providing good robustness against various adversarial examples? From the evaluation results under various conditions, we show that the proposed two-step input transformation architecture provides good robustness to DNN models against state-of-the-art adversarial perturbations, while maintaining the high accuracy even for legitimate inputs.","",""
78,"Bo Luo, Yannan Liu, Lingxiao Wei, Q. Xu","Towards Imperceptible and Robust Adversarial Example Attacks against Neural Networks",2018,"","","","",127,"2022-07-13 10:11:42","","10.1609/aaai.v32i1.11499","","",,,,,78,19.50,20,4,4,"    Machine learning systems based on deep neural networks, being able to produce state-of-the-art results on various perception tasks, have gained mainstream adoption in many applications. However, they are shown to be vulnerable to adversarial example attack, which generates malicious output by adding slight perturbations to the input. Previous adversarial example crafting methods, however, use simple metrics to evaluate the distances between the original examples and the adversarial ones, which could be easily detected by human eyes. In addition, these attacks are often not robust due to the inevitable noises and deviation in the physical world. In this work, we present a new adversarial example attack crafting method, which takes the human perceptual system into consideration and maximizes the noise tolerance of the crafted adversarial example. Experimental results demonstrate the efficacy of the proposed technique.   ","",""
79,"Sang-Ki Ko, Chang Jo Kim, Hyedong Jung, C. Cho","Neural Sign Language Translation based on Human Keypoint Estimation",2018,"","","","",128,"2022-07-13 10:11:42","","10.3390/APP9132683","","",,,,,79,19.75,20,4,4,"We propose a sign language translation system based on human keypoint estimation. It is well-known that many problems in the field of computer vision require a massive dataset to train deep neural network models. The situation is even worse when it comes to the sign language translation problem as it is far more difficult to collect high-quality training data. In this paper, we introduce the KETI (Korea Electronics Technology Institute) sign language dataset, which consists of 14,672 videos of high resolution and quality. Considering the fact that each country has a different and unique sign language, the KETI sign language dataset can be the starting point for further research on the Korean sign language translation. Using the KETI sign language dataset, we develop a neural network model for translating sign videos into natural language sentences by utilizing the human keypoints extracted from the face, hands, and body parts. The obtained human keypoint vector is normalized by the mean and standard deviation of the keypoints and used as input to our translation model based on the sequence-to-sequence architecture. As a result, we show that our approach is robust even when the size of the training data is not sufficient. Our translation model achieved 93.28% (55.28%, respectively) translation accuracy on the validation set (test set, respectively) for 105 sentences that can be used in emergency situations. We compared several types of our neural sign translation models based on different attention mechanisms in terms of classical metrics for measuring the translation performance.","",""
34,"Chaolong Li, Zhen Cui, Wenming Zheng, Chunyan Xu, R. Ji, Jian Yang","Action-Attending Graphic Neural Network",2017,"","","","",129,"2022-07-13 10:11:42","","10.1109/TIP.2018.2815744","","",,,,,34,6.80,6,6,5,"The motion analysis of human skeletons is crucial for human action recognition, which is one of the most active topics in computer vision. In this paper, we propose a fully end-to-end action-attending graphic neural network (A2GNN) for skeleton-based action recognition, in which each irregular skeleton is structured as an undirected attribute graph. To extract high-level semantic representation from skeletons, we perform the local spectral graph filtering on the constructed attribute graphs like the standard image convolution operation. Considering not all joints are informative for action analysis, we design an action-attending layer to detect those salient action units by adaptively weighting skeletal joints. Herein, the filtering responses are parameterized into a weighting function irrelevant to the order of input nodes. To further encode continuous motion variations, the deep features learnt from skeletal graphs are gathered along consecutive temporal slices and then fed into a recurrent gated network. Finally, the spectral graph filtering, action-attending, and recurrent temporal encoding are integrated together to jointly train for the sake of robust action recognition as well as the intelligibility of human actions. To evaluate our A2GNN, we conduct extensive experiments on four benchmark skeleton-based action datasets, including the large-scale challenging NTU RGB+D dataset. The experimental results demonstrate that our network achieves the state-of-the-art performances.","",""
32,"Fei Tao, C. Busso","Gating Neural Network for Large Vocabulary Audiovisual Speech Recognition",2018,"","","","",130,"2022-07-13 10:11:42","","10.1109/TASLP.2018.2815268","","",,,,,32,8.00,16,2,4,"Audio-based automatic speech recognition (A-ASR) systems are affected by noisy conditions in real-world applications. Adding visual cues to the ASR system is an appealing alternative to improve the robustness of the system, replicating the audiovisual perception process used during human interactions. A common problem observed when using audiovisual automatic speech recognition (AV-ASR) is the drop in performance when speech is clean. In this case, visual features may not provide complementary information, introducing variability that negatively affects the performance of the system. The experimental evaluation in this study clearly demonstrates this problem when we train an audiovisual state-of-the-art hybrid system with a deep neural network (DNN) and hidden Markov models (HMMs). This study proposes a framework that addresses this problem, improving, or at least, maintaining the performance when visual features are used. The proposed approach is a deep learning solution with a gating layer that diminishes the effect of noisy or uninformative visual features, keeping only useful information. The framework is implemented with a subset of the audiovisual CRSS-4ENGLISH-14 corpus which consists of 61 h of speech from 105 subjects simultaneously collected with multiple cameras and microphones. The proposed framework is compared with conventional HMMs with observation models implemented with either a Gaussian mixture model or DNNs. We also compare the system with a multi-stream HMM system. The experimental evaluation indicates that the proposed framework outperforms alternative methods under all configurations, showing the robustness of the gating-based framework for AV-ASR.","",""
22,"Zifan Wang, Yilin Yang, Ankit Shrivastava, Varun Rawal, Zihao Ding","Towards Frequency-Based Explanation for Robust CNN",2020,"","","","",131,"2022-07-13 10:11:42","","","","",,,,,22,11.00,4,5,2,"Current explanation techniques towards a transparent Convolutional Neural Network (CNN) mainly focuses on building connections between the human-understandable input features with models' prediction, overlooking an alternative representation of the input, the frequency components decomposition. In this work, we present an analysis of the connection between the distribution of frequency components in the input dataset and the reasoning process the model learns from the data. We further provide quantification analysis about the contribution of different frequency components toward the model's prediction. We show that the vulnerability of the model against tiny distortions is a result of the model is relying on the high-frequency features, the target features of the adversarial (black and white-box) attackers, to make the prediction. We further show that if the model develops stronger association between the low-frequency component with true labels, the model is more robust, which is the explanation of why adversarially trained models are more robust against tiny distortions.","",""
0,"K. Li, Jiang Wang, Shanshan Li, Bin Deng, Haitao Yu","Latent Characteristics and Neural Manifold of Brain Functional Network Under Acupuncture",2022,"","","","",132,"2022-07-13 10:11:42","","10.1109/TNSRE.2022.3157380","","",,,,,0,0.00,0,5,1,"Acupuncture can regulate the cognition of brain system, and different manipulations are the keys of realizing the curative effect of acupuncture on human body. Therefore, it is crucial to distinguish and monitor the different acupuncture manipulations automatically. In this brief, in order to enhance the robustness of electroencephalogram (EEG) detection against noise and interference, we propose an acupuncture manipulation detecting framework based on supervised ISOMAP and recurrent neural network (RNN). Primarily, the low-dimensional embedding neural manifold of brain dynamical functional network is extracted via the reconstructed geodetic distance. It is found that there exhibits stronger acupuncture-specific reconfiguration of brain network. Besides, we show that the distance travel along this manifold correlates strongly with changes of acupuncture manipulations. The low-dimensional brain topological structure of all subjects shows crescent-like feature when acupuncturing at Zusanli acupoints, and fixed-points are varying under diverse manipulation methods. Moreover, Takagi-Sugeno-Kang (TSK) classifier is adopted to identify acupuncture manipulations according to the nonlinear characteristics of neural manifolds. Compared with different classifier, TSK can further improve the accuracy of manipulation identification at 96.71%. The results demonstrate the effectiveness of our model in detecting the acupuncture manipulations, which may provide neural biomarkers for acupuncture physicians.","",""
47,"Changjian Li, Hao Pan, Yang Liu, Xin Tong, A. Sheffer, Wenping Wang","Robust flow-guided neural prediction for sketch-based freeform surface modeling",2018,"","","","",133,"2022-07-13 10:11:42","","10.1145/3272127.3275051","","",,,,,47,11.75,8,6,4,"Sketching provides an intuitive user interface for communicating free form shapes. While human observers can easily envision the shapes they intend to communicate, replicating this process algorithmically requires resolving numerous ambiguities. Existing sketch-based modeling methods resolve these ambiguities by either relying on expensive user annotations or by restricting the modeled shapes to specific narrow categories. We present an approach for modeling generic freeform 3D surfaces from sparse, expressive 2D sketches that overcomes both limitations by incorporating convolution neural networks (CNN) into the sketch processing workflow. Given a 2D sketch of a 3D surface, we use CNNs to infer the depth and normal maps representing the surface. To combat ambiguity we introduce an intermediate CNN layer that models the dense curvature direction, or flow, field of the surface, and produce an additional output confidence map along with depth and normal. The flow field guides our subsequent surface reconstruction for improved regularity; the confidence map trained unsupervised measures ambiguity and provides a robust estimator for data fitting. To reduce ambiguities in input sketches users can refine their input by providing optional depth values at sparse points and curvature hints for strokes. Our CNN is trained on a large dataset generated by rendering sketches of various 3D shapes using non-photo-realistic line rendering (NPR) method that mimics human sketching of free-form shapes. We use the CNN model to process both single- and multi-view sketches. Using our multi-view framework users progressively complete the shape by sketching in different views, generating complete closed shapes. For each new view, the modeling is assisted by partial sketches and depth cues provided by surfaces generated in earlier views. The partial surfaces are fused into a complete shape using predicted confidence levels as weights. We validate our approach, compare it with previous methods and alternative structures, and evaluate its performance with various modeling tasks. The results demonstrate our method is a new approach for efficiently modeling freeform shapes with succinct but expressive 2D sketches.","",""
74,"Hong Liu, Juanhui Tu, Mengyuan Liu","Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action Recognition",2017,"","","","",134,"2022-07-13 10:11:42","","","","",,,,,74,14.80,25,3,5,"It remains a challenge to efficiently extract spatialtemporal information from skeleton sequences for 3D human action recognition. Although most recent action recognition methods are based on Recurrent Neural Networks which present outstanding performance, one of the shortcomings of these methods is the tendency to overemphasize the temporal information. Since 3D convolutional neural network(3D CNN) is a powerful tool to simultaneously learn features from both spatial and temporal dimensions through capturing the correlations between three dimensional signals, this paper proposes a novel two-stream model using 3D CNN. To our best knowledge, this is the first application of 3D CNN in skeleton-based action recognition. Our method consists of three stages. First, skeleton joints are mapped into a 3D coordinate space and then encoding the spatial and temporal information, respectively. Second, 3D CNN models are seperately adopted to extract deep features from two streams. Third, to enhance the ability of deep features to capture global relationships, we extend every stream into multitemporal version. Extensive experiments on the SmartHome dataset and the large-scale NTU RGB-D dataset demonstrate that our method outperforms most of RNN-based methods, which verify the complementary property between spatial and temporal information and the robustness to noise.","",""
173,"Lipeng Ke, Ming-Ching Chang, H. Qi, Siwei Lyu","Multi-Scale Structure-Aware Network for Human Pose Estimation",2018,"","","","",135,"2022-07-13 10:11:42","","10.1007/978-3-030-01216-8_44","","",,,,,173,43.25,43,4,4,"","",""
61,"Da Sun, F. Naghdy, H. Du","Neural Network-Based Passivity Control of Teleoperation System Under Time-Varying Delays",2017,"","","","",136,"2022-07-13 10:11:42","","10.1109/TCYB.2016.2554630","","",,,,,61,12.20,20,3,5,"In this paper, a novel neural network (NN)-based four-channel wave-based time domain passivity approach (TDPA) is proposed for a teleoperation system with time-varying delays. The designed wave-based TDPA aims to robustly guarantee the channels passivity and provide higher transparency than the previous power-based TDPA. The applied NN is used to estimate and eliminate the system’s dynamic uncertainties. The system stability with linearity assumption on human and environment has been analyzed using Lyapunov method. The proposed algorithm is validated through experimental work based on a 3-DOF bilateral teleoperation platform in the presence of different time delays.","",""
30,"Chih-Ta Yen, Yi-Jie Huang","Frequency domain digital watermark recognition using image code sequences with a back-propagation neural network",2016,"","","","",137,"2022-07-13 10:11:42","","10.1007/s11042-015-2718-y","","",,,,,30,5.00,15,2,6,"","",""
29,"Boning Li, A. Sano","Extraction and Interpretation of Deep Autoencoder-based Temporal Features from Wearables for Forecasting Personalized Mood, Health, and Stress",2020,"","","","",138,"2022-07-13 10:11:42","","10.1145/3397318","","",,,,,29,14.50,15,2,2,"Continuous wearable sensor data in high resolution contain physiological and behavioral information that can be utilized to predict human health and wellbeing, establishing the foundation for developing early warning systems to eventually improve human health and wellbeing. We propose a deep neural network framework, the Locally Connected Long Short-Term Memory Denoising AutoEncoder (LC-LSTM-DAE), to automatically extract features from passively collected raw sensor data and perform personalized prediction of self-reported mood, health, and stress scores with high precision. We enabled personalized learning of features by finetuning the general representation model with participant-specific data. The framework was evaluated using wearable sensor data and wellbeing labels collected from college students (total 6391 days from N=239). Sensor data include skin temperature, skin conductance, and acceleration; wellbeing labels include self-reported mood, health and stress scored 0 - 100. Compared to the prediction performance based on hand-crafted features, the proposed framework achieved higher precision with a smaller number of features. We also provide statistical interpretation and visual explanation to the automatically learned features and the prediction models. Our results show the possibility of predicting self-reported mood, health, and stress accurately using an interpretable deep learning framework, ultimately for developing real-time health and wellbeing monitoring and intervention systems that can benefit various populations.","",""
327,"M. Fullana, B. Harrison, C. Soriano-Mas, B. Vervliet, N. Cardoner, A. Àvila-Parcet, J. Raduà","Neural signatures of human fear conditioning: an updated and extended meta-analysis of fMRI studies",2016,"","","","",139,"2022-07-13 10:11:42","","10.1038/mp.2015.88","","",,,,,327,54.50,47,7,6,"","",""
122,"Bowen Xu, Deheng Ye, Zhenchang Xing, Xin Xia, Guibin Chen, Shanping Li","Predicting semantically linkable knowledge in developer online forums via convolutional neural network",2016,"","","","",140,"2022-07-13 10:11:42","","10.1145/2970276.2970357","","",,,,,122,20.33,20,6,6,"Consider a question and its answers in Stack Overflow as a knowledge unit. Knowledge units often contain semantically relevant knowledge, and thus linkable for different purposes, such as duplicate questions, directly linkable for problem solving, indirectly linkable for related information. Recognising different classes of linkable knowledge would support more targeted information needs when users search or explore the knowledge base. Existing methods focus on binary relatedness (i.e., related or not), and are not robust to recognize different classes of semantic relatedness when linkable knowledge units share few words in common (i.e., have lexical gap). In this paper, we formulate the problem of predicting semantically linkable knowledge units as a multiclass classification problem, and solve the problem using deep learning techniques. To overcome the lexical gap issue, we adopt neural language model (word embeddings) and convolutional neural network (CNN) to capture word- and document-level semantics of knowledge units. Instead of using human-engineered classifier features which are hard to design for informal user-generated content, we exploit large amounts of different types of user-created knowledge-unit links to train the CNN to learn the most informative wordlevel and document-level features for the multiclass classification task. Our evaluation shows that our deep-learning based approach significantly and consistently outperforms traditional methods using traditional word representations and human-engineered classifier features.","",""
2,"Milla S. A. Ferro, Bruno José Torres Fernandes, C. J. A. B. Filho","Non-negative Structured Pyramidal Neural Network for Pattern Recognition",2018,"","","","",141,"2022-07-13 10:11:42","","10.1109/IJCNN.2018.8489216","","",,,,,2,0.50,1,3,4,"Deep learning is a machine learning paradigm that has been widely exploited in the last years due to its high performance in many different problems, most of them related to computer vision. The Structured Pyramidal Neural Network (SPNN) is an artificial neural network which implements some of the deep learning concepts, such as multiple processing layers and receptive fields, with no need to pre-define the features of the problem as inputs. This network architecture has presented equivalent or even better results than other deep network approaches applied to solve specific tasks, but with a much lower computational cost. However, one of the SPNN limitations is the difficulty in contributing to human interpretations, since it has opaque learning, like most of the neural networks. Thus, we propose a non-negative model of the SPNN, to obtain better interpretability of the network learning. We restrict the values of the weights and biases of the network to be non-negative. The proposed model is evaluated in a gender recognition problem using the Face Recognition Technology (FERET) database. The results show that SPNN including non-negative constraint returns comparable recognition rates, but providing gains in the interpretability and stability of the model.","",""
0,"J. Jopling, Brian C. Pridgen, S. Yeung","Deep Convolutional Neural Networks as a Diagnostic Aid-A Step Toward Minimizing Undetected Scaphoid Fractures on Initial Hand Radiographs.",2021,"","","","",142,"2022-07-13 10:11:42","","10.1001/jamanetworkopen.2021.6393","","",,,,,0,0.00,0,3,1,"Scaphoid fractures are the most common carpal bone fracture. If missed and untreated at initial evaluation, they can lead to a progressive pattern of debilitating wrist arthritis that may ultimately require salvage procedures, including wrist fusion. The concern over missing scaphoid fractures that areradiographicallyoccultoninitialplainradiographshasbeenaninspirationformanypriorstudies. 1 However, overtreating scaphoid fractures because of these concerns can lead to costly advanced imaging or unnecessary immobilization in a cast or splint. It is this challenge that Yoon et al 2 addressed in their study. Advances in machine learning and computer vision have allowed for an increasing number of investigations into the potential of computer vision to augment human interpretation of radiographs. The work by Yoon et al 2 adds to this growing body of knowledge. They trained a deep convolutional neural network model that can successfully identify scaphoid fractures in plain radiographs. To highlight the potential future clinical impact,theychosetofocustheiralgorithmnotonlyonradiographicallyapparentscaphoidfractures but specifically on detecting radiographically occult scaphoid fractures, ie, those missed by human interpretation. For radiographs run through their entire algorithm pipeline, 20 of 22 radiographically occult fractures were detected. These results indicate the potential of computer vision algorithms to eventually become a clinically meaningful tool for assessing possible scaphoid fractures in initial radiographs. In a fully realized form, they could facilitate prompt identification of these fractures, expeditious treatment, and decreased reliance on costly advanced imaging. By looking at the details of the study, we can see where additional algorithm development can make contributions toward furthering this goal. The underlying rate of occult scaphoid fractures was","",""
29,"N. Amoroso, R. Errico, S. Bruno, A. Chincarini, E. Garuccio, F. Sensi, S. Tangaro, A. Tateo, R. Bellotti","Hippocampal unified multi-atlas network (HUMAN): protocol and scale validation of a novel segmentation tool.",2015,"","","","",143,"2022-07-13 10:11:42","","10.1088/0031-9155/60/22/8851","","",,,,,29,4.14,3,9,7,"In this study we present a novel fully automated Hippocampal Unified Multi-Atlas-Networks (HUMAN) algorithm for the segmentation of the hippocampus in structural magnetic resonance imaging. In multi-atlas approaches atlas selection is of crucial importance for the accuracy of the segmentation. Here we present an optimized method based on the definition of a small peri-hippocampal region to target the atlas learning with linear and non-linear embedded manifolds. All atlases were co-registered to a data driven template resulting in a computationally efficient method that requires only one test registration. The optimal atlases identified were used to train dedicated artificial neural networks whose labels were then propagated and fused to obtain the final segmentation. To quantify data heterogeneity and protocol inherent effects, HUMAN was tested on two independent data sets provided by the Alzheimer's Disease Neuroimaging Initiative and the Open Access Series of Imaging Studies. HUMAN is accurate and achieves state-of-the-art performance (Dice[Formula: see text] and Dice[Formula: see text]). It is also a robust method that remains stable when applied to the whole hippocampus or to sub-regions (patches). HUMAN also compares favorably with a basic multi-atlas approach and a benchmark segmentation tool such as FreeSurfer.","",""
23,"Lei Zhao, Zengcai Wang, Guoxin Zhang, Yazhou Qi, Xiaojing Wang","Eye state recognition based on deep integrated neural network and transfer learning",2018,"","","","",144,"2022-07-13 10:11:42","","10.1007/s11042-017-5380-8","","",,,,,23,5.75,5,5,4,"","",""
6,"Zi Wang, Aws Albarghouthi, Gautam Prakriya, S. Jha","Interval universal approximation for neural networks",2020,"","","","",145,"2022-07-13 10:11:42","","10.1145/3498675","","",,,,,6,3.00,2,4,2,"To verify safety and robustness of neural networks, researchers have successfully applied abstract interpretation, primarily using the interval abstract domain. In this paper, we study the theoretical power and limits of the interval domain for neural-network verification. First, we introduce the interval universal approximation (IUA) theorem. IUA shows that neural networks not only can approximate any continuous function f (universal approximation) as we have known for decades, but we can find a neural network, using any well-behaved activation function, whose interval bounds are an arbitrarily close approximation of the set semantics of f (the result of applying f to a set of inputs). We call this notion of approximation interval approximation. Our theorem generalizes the recent result of Baader et al. from ReLUs to a rich class of activation functions that we call squashable functions. Additionally, the IUA theorem implies that we can always construct provably robust neural networks under ℓ∞-norm using almost any practical activation function. Second, we study the computational complexity of constructing neural networks that are amenable to precise interval analysis. This is a crucial question, as our constructive proof of IUA is exponential in the size of the approximation domain. We boil this question down to the problem of approximating the range of a neural network with squashable activation functions. We show that the range approximation problem (RA) is a Δ2-intermediate problem, which is strictly harder than NP-complete problems, assuming coNP⊄NP. As a result, IUA is an inherently hard problem: No matter what abstract domain or computational tools we consider to achieve interval approximation, there is no efficient construction of such a universal approximator. This implies that it is hard to construct a provably robust network, even if we have a robust network to start with.","",""
0,"A. Demidovskij","Encoding and Decoding of Recursive Structures in Neural-Symbolic Systems",2021,"","","","",146,"2022-07-13 10:11:42","","10.3103/S1060992X21010033","","",,,,,0,0.00,0,1,1,"","",""
42,"Bo Li, K. Sim","A Spectral Masking Approach to Noise-Robust Speech Recognition Using Deep Neural Networks",2014,"","","","",147,"2022-07-13 10:11:42","","10.1109/TASLP.2014.2329237","","",,,,,42,5.25,21,2,8,"Improving the noise robustness of automatic speech recognition systems has been a challenging task for many years. Recently, it was found that Deep Neural Networks (DNNs) yield large performance gains over conventional GMM-HMM systems, when used in both hybrid and tandem systems. However, they are still far from the level of human expectations especially under adverse environments. Motivated by the separation-prior-to-recognition process of the human auditory system, we propose a robust spectral masking system where power spectral domain masks are predicted using a DNN trained on the same filter-bank features used for acoustic modeling. To further improve performance, Linear Input Network (LIN) adaptation is applied to both the mask estimator and the acoustic model DNNs. Since the estimation of LINs for the mask estimator requires stereo data, which is not available during testing, we proposed using the LINs estimated for the acoustic model DNNs to adapt the mask estimators. Furthermore, we used the same set of weights obtained from pre-training for the input layers of both the mask estimator and the acoustic model DNNs to ensure a better consistency for sharing LINs. Experimental results on benchmark Aurora2 and Aurora4 tasks demonstrated the effectiveness of our system, which yielded Word Error Rates (WERs) of 4.6% and 11.8% respectively. Furthermore, the simple averaging of posteriors from systems with and without spectral masking can further reduce the WERs to 4.3% on Aurora2 and 11.4% on Aurora4.","",""
1,"Leon C. Hardy","An application of neurohydrodynamics to a Hopfield neural network",2015,"","","","",148,"2022-07-13 10:11:42","","10.1109/IJCNN.2015.7280359","","",,,,,1,0.14,1,1,7,"In this paper, we apply our approach of Neurohydrodynamics (NHD) to a Hopfield neural network by introducing a one-dimensional spacial diffusion term. This reaction-diffusion equation includes an auxiliary equation that “guides” the weights of the network using the divergence of neuron's activation amplitude, which we call the neuropotential. This guiding principle is similar to de Broglie's “pilot wave” interpretation for Quantum Mechanics or Turing's oracle for “human intuition” of a Turing machine. Finally, using a numerical derivation of the dynamical equations of one-dimensional Hopfield neural network, we include a simulation of the network so that we can discuss its behavior and future directions of NHD.","",""
52,"Aisha Khan, Stephen Gould, M. Salzmann","Deep Convolutional Neural Networks for Human Embryonic Cell Counting",2016,"","","","",149,"2022-07-13 10:11:42","","10.1007/978-3-319-46604-0_25","","",,,,,52,8.67,17,3,6,"","",""
27,"Zeke Xie, Fengxiang He, Shaopeng Fu, Issei Sato, D. Tao, M. Sugiyama","Artificial Neural Variability for Deep Learning: On Overfitting, Noise Memorization, and Catastrophic Forgetting",2020,"","","","",150,"2022-07-13 10:11:42","","10.1162/neco_a_01403","","",,,,,27,13.50,5,6,2,"Deep learning is often criticized by two serious issues that rarely exist in natural nervous systems: overfitting and catastrophic forgetting. It can even memorize randomly labeled data, which has little knowledge behind the instance-label pairs. When a deep network continually learns over time by accommodating new tasks, it usually quickly overwrites the knowledge learned from previous tasks. Referred to as the neural variability, it is well known in neuroscience that human brain reactions exhibit substantial variability even in response to the same stimulus. This mechanism balances accuracy and plasticity/flexibility in the motor learning of natural nervous systems. Thus, it motivates us to design a similar mechanism, named artificial neural variability (ANV), that helps artificial neural networks learn some advantages from “natural” neural networks. We rigorously prove that ANV plays as an implicit regularizer of the mutual information between the training data and the learned model. This result theoretically guarantees ANV a strictly improved generalizability, robustness to label noise, and robustness to catastrophic forgetting. We then devise a neural variable risk minimization (NVRM) framework and neural variable optimizers to achieve ANV for conventional network architectures in practice. The empirical studies demonstrate that NVRM can effectively relieve overfitting, label noise memorization, and catastrophic forgetting at negligible costs.","",""
55,"Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu, Mohamed A. Elgharib, P. Fua, H. Seidel, Helge Rhodin, Gerard Pons-Moll, C. Theobalt","XNect: Real-time Multi-person 3D Human Pose Estimation with a Single RGB Camera",2019,"","","","",151,"2022-07-13 10:11:42","","10.1145/3386569.3392410","","",,,,,55,18.33,6,10,3,"We present a real-time approach for multi-person 3D motion capture at over 30 fps using a single RGB camera. It operates in generic scenes and is robust to difficult occlusions both by other people and objects. Our method operates in subsequent stages. The first stage is a convolutional neural network (CNN) that estimates 2D and 3D pose features along with identity assignments for all visible joints of all individuals. We contribute a new architecture for this CNN, called SelecSLS Net, that uses novel selective long and short range skip connections to improve the information flow allowing for a drastically faster network without compromising accuracy. In the second stage, a fully-connected neural network turns the possibly partial (on account of occlusion) 2D pose and 3D pose features for each subject into a complete 3D pose estimate per individual. The third stage applies space-time skeletal model fitting to the predicted 2D and 3D pose per subject to further reconcile the 2D and 3D pose, and enforce temporal coherence. Our method returns the full skeletal pose in joint angles for each subject. This is a further key distinction from previous work that neither extracted global body positions nor joint angle results of a coherent skeleton in real time for multi-person scenes. The proposed system runs on consumer hardware at a previously unseen speed of more than 30 fps given 512x320 images as input while achieving state-of-the-art accuracy, which we will demonstrate on a range of challenging real-world scenes.","",""
525,"David R Ha, D. Eck","A Neural Representation of Sketch Drawings",2017,"","","","",152,"2022-07-13 10:11:42","","","","",,,,,525,105.00,263,2,5,"We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on thousands of crude human-drawn images representing hundreds of classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.","",""
6,"R. Hamad, Masashi Kimura, Longzhi Yang, W. L. Woo, Bo Wei","Dilated causal convolution with multi-head self attention for sensor human activity recognition",2021,"","","","",153,"2022-07-13 10:11:42","","10.1007/S00521-021-06007-5","","",,,,,6,6.00,1,5,1,"","",""
12454,"Kaiming He, X. Zhang, Shaoqing Ren, Jian Sun","Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",2015,"","","","",154,"2022-07-13 10:11:42","","10.1109/ICCV.2015.123","","",,,,,12454,1779.14,3114,4,7,"Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.","",""
0,"O. Koyejo","APPROXIMATING CUTNORM: A ROBUST METHOD TO COMPUTE DISTANCE BETWEEN DENSE GRAPHS FOR PREDICTION AND INTERPRETATION BY PING-KO CHIU THESIS",2018,"","","","",155,"2022-07-13 10:11:42","","","","",,,,,0,0.00,0,1,4,"This thesis presents techniques of modeling large and dense networks and methods of computing distances between them. Large and dense networks arise in many disciplines. Through recent advancements in dense graph theory and graph convergence, we have a new perspective on how large graphs should be considered and how the similarity of graphs should be computed. The thesis discusses the steps to approximate the distance between graphs and the integration of a new search algorithm to accelerate computation. A software package is produced to estimate distances between graphs and made available as the Cutnorm package on PyPI. The algorithm and software shows great performance on theoretical models and is faster than existing implementations. The thesis also explores practical applications of the graph convergence theory and Cut-Distances. It presents the theory and techniques to analyze human brain connectivity graphs from the ADHD200 dataset of the 1000 Connectome Project. It also presents a new insight to monitoring Artificial Neural Network convergence during the training process.","",""
154,"V. Devabhaktuni, M. Yagoub, Qi-jun Zhang","A robust algorithm for automatic development of neural network models for microwave applications",2001,"","","","",156,"2022-07-13 10:11:42","","10.1109/22.971611","","",,,,,154,7.33,51,3,21,"In this paper, we propose a robust algorithm for automating the neural network based RF/Microwave model development process. The algorithm can build a neural model starting with zero amount of training/test data, and then proceeding with neural network training in a stage-wise manner. In each stage, the algorithm utilizes neural network error criteria to determine additional training/test samples required and their location in model input space. The algorithm dynamically generates these new data samples during training, by automatic driving of simulation tools, e.g., OSA90, Ansoft-HFSS. Initially, fewer hidden neurons are used, and the algorithm adjusts the neural network size whenever it detects under-learning. Our technique integrates all the sub-tasks involved in neural modeling, thereby facilitating a more efficient and automated model building process. It significantly reduces the intensive human effort demanded by the conventional step-by-step neural modeling approach. The algorithm is demonstrated through MESFET and Embedded Capacitor examples.","",""
69,"Khan Suhail Ahmad, Anil S. Thosar, J. Nirmal, V. S. Pande","A unique approach in text independent speaker recognition using MFCC feature sets and probabilistic neural network",2015,"","","","",157,"2022-07-13 10:11:42","","10.1109/ICAPR.2015.7050669","","",,,,,69,9.86,17,4,7,"This paper motivates the use of combination of mel frequency cepstral coefficients (MFCC) and its delta derivatives (DMFCC and DDMFCC) calculated using mel spaced Gaussian filter banks for text independent speaker recognition. MFCC modeled on the human auditory system shows robustness against noise and session changes and hence has become synonymous with speaker recognition. Our main aim is to test the accuracy of our proposed feature set for different values of frame overlap and MFCC feature vector sizes to identify the system having highest accuracy. Principal component analysis (PCA) is applied before the training and testing stages for feature dimensionality reduction thereby increasing computing speed and puts low constraint on the memory required for processing. The use of probabilistic neural network (PNN) in the modeling domain provided the advantages of achieving lower operational times during the training stages. The experiments examined the percentage identification accuracy (PIA) of MFCC, combination of MFCC and DMFCC as well as combination of all three feature sets MFCC, DMFCC and DDMFCC. The proposed feature set attains an identification accuracy of 94% for frame overlap of 90% and MFCC feature size of 18 coefficients. It outperforms the identification rates of the other two feature sets. These speaker recognition experiments were tested using the Voxforge database.","",""
0,"Xia Du, Jie Yu, Shasha Li, Zibo Yi, Hai Liu, Jun Ma","Combating Word-level Adversarial Text with Robust Adversarial Training",2021,"","","","",158,"2022-07-13 10:11:42","","10.1109/IJCNN52387.2021.9533725","","",,,,,0,0.00,0,6,1,"NLP models perform well on many tasks, but they are also easy to be fooled by adversarial examples. A small perturbation can change the output of the deep neural network model. This kind of perturbation is hard to be perceived by humans, especially adversarial examples generated by word-level adversarial attack. Character-level adversarial attack can be defended by grammar detection and word recognition. The existing word-level textual adversarial attacks are based on synonym replacement, so adversarial texts usually have correct grammar and semantics. The defense of word-level adversarial attack is more challenging. In this paper, we propose a framework which is called Robust Adversarial Training (RAT) to defend against word-level adversarial attacks. RAT enhances the model by combining adversarial training and data perturbation during training. Our experiments on two datasets show that the model based on our framework can effectively defend against word-level adversarial attacks. Compared with the existing defense methods, the model trained under RAT has a higher defense success rate on 1000 adversarial examples. In addition, the accuracy of our model on the standard testing set is also better than the existing defense methods, and the accuracy is very close to or even higher than that of the standard model.","",""
33,"V. Nguyen, J. Starzyk, Wooi-Boon Goh, Daniel Jachyra","Neural Network Structure for Spatio-Temporal Long-Term Memory",2012,"","","","",159,"2022-07-13 10:11:42","","10.1109/TNNLS.2012.2191419","","",,,,,33,3.30,8,4,10,"This paper proposes a neural network structure for spatio-temporal learning and recognition inspired by the long-term memory (LTM) model of the human cortex. Our structure is able to process real-valued and multidimensional sequences. This capability is attained by addressing three critical problems in sequential learning, namely the error tolerance, the significance of sequence elements and memory forgetting. We demonstrate the potential of the framework with a series of synthetic simulations and the Australian sign language (ASL) dataset. Results show that our LTM model is robust to different types of distortions. Second, our LTM model outperforms other sequential processing models in a classification task for the ASL dataset.","",""
58,"N. Lepora, Alex Church, Conrad de Kerckhove, R. Hadsell, J. Lloyd","From Pixels to Percepts: Highly Robust Edge Perception and Contour Following Using Deep Learning and an Optical Biomimetic Tactile Sensor",2018,"","","","",160,"2022-07-13 10:11:42","","10.1109/LRA.2019.2899192","","",,,,,58,14.50,12,5,4,"Deep learning has the potential to have same the impact on robot touch as it has had on robot vision. Optical tactile sensors act as a bridge between the subjects by allowing techniques from vision to be applied to touch. In this letter, we apply deep learning to an optical biomimetic tactile sensor, the TacTip, which images an array of papillae (pins) inside its sensing surface analogous to structures within human skin. Our main result is that the application of a deep convolutional neural network can give reliable edge perception, and, thus a robust policy for planning contact points to move around object contours. Robustness is demonstrated over several irregular and compliant objects with both tapping and continuous sliding, using a model trained only by tapping onto a disk. These results relied on using techniques to encourage generalization to tasks beyond which the model was trained. We expect this is a generic problem in practical applications of tactile sensing that deep learning will solve.","",""
50,"Mhaned Oubounyt, Z. Louadi, Hilal Tayara, K. Chong","DeePromoter: Robust Promoter Predictor Using Deep Learning",2019,"","","","",161,"2022-07-13 10:11:42","","10.3389/fgene.2019.00286","","",,,,,50,16.67,13,4,3,"The promoter region is located near the transcription start sites and regulates transcription initiation of the gene by controlling the binding of RNA polymerase. Thus, promoter region recognition is an important area of interest in the field of bioinformatics. Numerous tools for promoter prediction were proposed. However, the reliability of these tools still needs to be improved. In this work, we propose a robust deep learning model, called DeePromoter, to analyze the characteristics of the short eukaryotic promoter sequences, and accurately recognize the human and mouse promoter sequences. DeePromoter combines a convolutional neural network (CNN) and a long short-term memory (LSTM). Additionally, instead of using non-promoter regions of the genome as a negative set, we derive a more challenging negative set from the promoter sequences. The proposed negative set reconstruction method improves the discrimination ability and significantly reduces the number of false positive predictions. Consequently, DeePromoter outperforms the previously proposed promoter prediction tools. In addition, a web-server for promoter prediction is developed based on the proposed methods and made available at https://home.jbnu.ac.kr/NSCL/deepromoter.htm.","",""
33,"S. Ashwin, S. A. Kumar, J. Ramesh, K. Gunavathi","Efficient and reliable lung nodule detection using a neural network based computer aided diagnosis system",2012,"","","","",162,"2022-07-13 10:11:42","","10.1109/ICETEEEM.2012.6494454","","",,,,,33,3.30,8,4,10,"The manual examination of histological images like computed tomography (CT) images by physicians is prone to subjectivity and limited intra and inter-surgeon reproducibility, due to its heavy reliance on human interpretation. As result of which, diagnosis of cancer especially in lungs becomes less accurate and unreliable. So, a computer-aided diagnosis (CAD) system, based on artificial intelligence that efficiently detects nodules of any shape and size, is used for diagnosis without human intervention. In this work, we have developed a two stage CAD system in which the first stage involves pre-processing applied for a better quality image to enable higher success rate on detection following which the cancerous nodule region is segmented. The second stage involves artificial neural network (ANN) architecture which is trained using a modified BFGS algorithm. The proposed system was trained, tested, and evaluated specifically on the problem of detecting lung cancer nodules found on CT images to give a positive detection. A significant comparative analysis was done between the proposed method and several existing CAD systems used for lung nodule diagnosis and the proposed method using training-based neural networks prove to provide accuracy of 96.7% and also better specificity; thus, the overall performance of the CAD scheme was improved substantially.","",""
0,"N. Fjodorova, M. Novič, S. Zuperl, K. Venko","Counter-Propagation Artificial Neural Network Models for Prediction of Carcinogenicity of Non-congeneric Chemicals for Regulatory Uses",2017,"","","","",163,"2022-07-13 10:11:42","","10.1007/978-3-319-56850-8_14","","",,,,,0,0.00,0,4,5,"","",""
11,"M. Trompf","Neural network development for noise reduction in robust speech recognition",1992,"","","","",164,"2022-07-13 10:11:42","","10.1109/IJCNN.1992.227233","","",,,,,11,0.37,11,1,30,"Speech recognition systems with small and medium vocabulary are used as natural human interfaces in a variety of applications. To make such a system more robust, the development of a neural network based noise reduction module is described. Using standard feedforward networks, several topologies have been tested to learn about the properties of neural noise reduction. For the development of a sufficiently robust nonadaptive system, information about the characteristics of the noise and speech components of the input signal including context information was taken into account. The focus is on the stepwise experiment-oriented improvement of a basic linear neural noise reduction network. The isolated word recognition system and the database used for the experiments are described. Results from different noise reduction networks are given. To test their robustness, simulations with varying input signal characteristics were made and are discussed.<<ETX>>","",""
0,"J. Lope, M. Graña","A Hybrid Time-Distributed Deep Neural Architecture for Speech Emotion Recognition",2022,"","","","",165,"2022-07-13 10:11:42","","10.1142/S0129065722500241","","",,,,,0,0.00,0,2,1,"In recent years, speech emotion recognition (SER) has emerged as one of the most active human-machine interaction research areas. Innovative electronic devices, services and applications are increasingly aiming to check the user emotional state either to issue alerts under some predefined conditions or to adapt the system responses to the user emotions. Voice expression is a very rich and noninvasive source of information for emotion assessment. This paper presents a novel SER approach based on that is a hybrid of a time-distributed convolutional neural network (TD-CNN) and a long short-term memory (LSTM) network. Mel-frequency log-power spectrograms (MFLPSs) extracted from audio recordings are parsed by a sliding window that selects the input for the TD-CNN. The TD-CNN transforms the input image data into a sequence of high-level features that are feed to the LSTM, which carries out the overall signal interpretation. In order to reduce overfitting, the MFLPS representation allows innovative image data augmentation techniques that have no immediate equivalent on the original audio signal. Validation of the proposed hybrid architecture achieves an average recognition accuracy of 73.98% on the most widely and hardest publicly distributed database for SER benchmarking. A permutation test confirms that this result is significantly different from random classification ([Formula: see text]). The proposed architecture outperforms state-of-the-art deep learning models as well as conventional machine learning techniques evaluated on the same database trying to identify the same number of emotions.","",""
12,"S. Sadek, A. Al-Hamadi, B. Michaelis, Usama Sayed","A Robust Neural System for Objectionable Image Recognition",2009,"","","","",166,"2022-07-13 10:11:42","","10.1109/ICMV.2009.30","","",,,,,12,0.92,3,4,13,"A reliable model for human skin is a significant need for a wide range of computer vision applications ranging from face detection, gesture analysis, content-based image retrieval systems, searching and filtering image content on the web, and to various human computer interaction domains. In this paper, a robust neural model for human skin recognition is first presented. Then, a fully automated neural network based system for recognizing naked people in color images is proposed. The proposed system makes use of a fast and precise neural model, called Multi-level Sigmoidal Neural Network (MSNN). Furthermore, the system exploits four different color models in all their possible representations to precisely extract color features from skin regions. Receiver Operating Characteristics (ROC) curve illustrates that the proposed system outperforms other stat-of-the-art schemes of objectionable image recognition in the context of detection rate and false positive rate. Abundance of experimental results are presented including test images and the ROC curve calculated over a test set, which show stimulating performance of the proposed system.","",""
120,"Nianyin Zeng, Zidong Wang, B. Zineddin, Yurong Li, Min Du, Liang Xiao, Xiaohui Liu, T. Young","Image-Based Quantitative Analysis of Gold Immunochromatographic Strip via Cellular Neural Network Approach",2014,"","","","",167,"2022-07-13 10:11:42","","10.1109/TMI.2014.2305394","","",,,,,120,15.00,15,8,8,"Gold immunochromatographic strip assay provides a rapid, simple, single-copy and on-site way to detect the presence or absence of the target analyte. This paper aims to develop a method for accurately segmenting the test line and control line of the gold immunochromatographic strip (GICS) image for quantitatively determining the trace concentrations in the specimen, which can lead to more functional information than the traditional qualitative or semi-quantitative strip assay. The canny operator as well as the mathematical morphology method is used to detect and extract the GICS reading-window. Then, the test line and control line of the GICS reading-window are segmented by the cellular neural network (CNN) algorithm, where the template parameters of the CNN are designed by the switching particle swarm optimization (SPSO) algorithm for improving the performance of the CNN. It is shown that the SPSO-based CNN offers a robust method for accurately segmenting the test and control lines, and therefore serves as a novel image methodology for the interpretation of GICS. Furthermore, quantitative comparison is carried out among four algorithms in terms of the peak signal-to-noise ratio. It is concluded that the proposed CNN algorithm gives higher accuracy and the CNN is capable of parallelism and analog very-large-scale integration implementation within a remarkably efficient time.","",""
31,"Jie Song, Xu Chen, Otmar Hilliges","Human Body Model Fitting by Learned Gradient Descent",2020,"","","","",168,"2022-07-13 10:11:42","","10.1007/978-3-030-58565-5_44","","",,,,,31,15.50,10,3,2,"","",""
220,"J. Barron","A General and Adaptive Robust Loss Function",2017,"","","","",169,"2022-07-13 10:11:42","","10.1109/CVPR.2019.00446","","",,,,,220,44.00,220,1,5,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure, Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2 loss functions. By introducing robustness as a continuous parameter, our loss function allows algorithms built around robust loss minimization to be generalized, which improves performance on basic vision tasks such as registration and clustering. Interpreting our loss as the negative log of a univariate density yields a general probability distribution that includes normal and Cauchy distributions as special cases. This probabilistic interpretation enables the training of neural networks in which the robustness of the loss automatically adapts itself during training, which improves performance on learning-based tasks such as generative image synthesis and unsupervised monocular depth estimation, without requiring any manual parameter tuning.","",""
31,"Amirata Ghorbani, David Ouyang, Abubakar Abid, B. He, Jonathan H. Chen, R. Harrington, D. Liang, E. Ashley, James Y. Zou","Deep learning interpretation of echocardiograms",2020,"","","","",170,"2022-07-13 10:11:42","","10.1038/s41746-019-0216-8","","",,,,,31,15.50,3,9,2,"","",""
32,"Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, Jingdong Wang","Informative Dropout for Robust Representation Learning: A Shape-bias Perspective",2020,"","","","",171,"2022-07-13 10:11:42","","","","",,,,,32,16.00,5,6,2,"Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN's texture-bias and its robustness against distribution shift, adversarial perturbation, random corruption, etc. In this work, we attempt at improving various kinds of robustness universally by alleviating CNN's texture bias. With inspiration from the human visual system, we propose a light-weight model-agnostic method, namely Informative Dropout (InfoDrop), to improve interpretability and reduce texture bias. Specifically, we discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. Through extensive experiments, we observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation). To the best of our knowledge, this work is one of the earliest attempts to improve different kinds of robustness in a unified model, shedding new light on the relationship between shape-bias and robustness, also on new approaches to trustworthy machine learning algorithms. Code is available at this https URL.","",""
70,"Weijia Li, Runmin Dong, H. Fu, Le Yu","Large-Scale Oil Palm Tree Detection from High-Resolution Satellite Images Using Two-Stage Convolutional Neural Networks",2018,"","","","",172,"2022-07-13 10:11:42","","10.3390/rs11010011","","",,,,,70,17.50,18,4,4,"Being an important economic crop that contributes 35% of the total consumption of vegetable oil, remote sensing-based quantitative detection of oil palm trees has long been a key research direction for both agriculture and environmental purposes. While existing methods already demonstrate satisfactory effectiveness for small regions, performing the detection for a large region with satisfactory accuracy is still challenging. In this study, we proposed a two-stage convolutional neural network (TS-CNN)-based oil palm detection method using high-resolution satellite images (i.e. Quickbird) in a large-scale study area of Malaysia. The TS-CNN consists of one CNN for land cover classification and one CNN for object classification. The two CNNs were trained and optimized independently based on 20,000 samples collected through human interpretation. For the large-scale oil palm detection for an area of 55 km2, we proposed an effective workflow that consists of an overlapping partitioning method for large-scale image division, a multi-scale sliding window method for oil palm coordinate prediction, and a minimum distance filter method for post-processing. Our proposed approach achieves a much higher average F1-score of 94.99% in our study area compared with existing oil palm detection methods (87.95%, 81.80%, 80.61%, and 78.35% for single-stage CNN, Support Vector Machine (SVM), Random Forest (RF), and Artificial Neural Network (ANN), respectively), and much fewer confusions with other vegetation and buildings in the whole image detection results.","",""
0,"Hoseok Choi, Seokbeen Lim, Kyeongran Min, Kyoung-ha Ahn, K. Lee, D. Jang","Non–human primate epidural ECoG analysis using explainable deep learning technology",2021,"","","","",173,"2022-07-13 10:11:42","","10.1088/1741-2552/ac3314","","",,,,,0,0.00,0,6,1,"Objective. With the development in the field of neural networks, explainable AI (XAI), is being studied to ensure that artificial intelligence models can be explained. There are some attempts to apply neural networks to neuroscientific studies to explain neurophysiological information with high machine learning performances. However, most of those studies have simply visualized features extracted from XAI and seem to lack an active neuroscientific interpretation of those features. In this study, we have tried to actively explain the high-dimensional learning features contained in the neurophysiological information extracted from XAI, compared with the previously reported neuroscientific results. Approach. We designed a deep neural network classifier using 3D information (3D DNN) and a 3D class activation map (3D CAM) to visualize high-dimensional classification features. We used those tools to classify monkey electrocorticogram (ECoG) data obtained from the unimanual and bimanual movement experiment. Main results. The 3D DNN showed better classification accuracy than other machine learning techniques, such as 2D DNN. Unexpectedly, the activation weight in the 3D CAM analysis was high in the ipsilateral motor and somatosensory cortex regions, whereas the gamma-band power was activated in the contralateral areas during unimanual movement, which suggests that the brain signal acquired from the motor cortex contains information about both contralateral movement and ipsilateral movement. Moreover, the hand-movement classification system used critical temporal information at movement onset and offset when classifying bimanual movements. Significance. As far as we know, this is the first study to use high-dimensional neurophysiological information (spatial, spectral, and temporal) with the deep learning method, reconstruct those features, and explain how the neural network works. We expect that our methods can be widely applied and used in neuroscience and electrophysiology research from the point of view of the explainability of XAI as well as its performance.","",""
22,"Julian Bitterwolf, Alexander Meinke, Matthias Hein","Certifiably Adversarially Robust Detection of Out-of-Distribution Data",2020,"","","","",174,"2022-07-13 10:11:42","","","","",,,,,22,11.00,7,3,2,"Deep neural networks are known to be overconfident when applied to out-of-distribution (OOD) inputs which clearly do not belong to any class. This is a problem in safety-critical applications since a reliable assessment of the uncertainty of a classifier is a key property, allowing the system to trigger human intervention or to transfer into a safe state. In this paper, we aim for certifiable worst case guarantees for OOD detection by enforcing not only low confidence at the OOD point but also in an $l_\infty$-ball around it. For this purpose, we use interval bound propagation (IBP) to upper bound the maximal confidence in the $l_\infty$-ball and minimize this upper bound during training time. We show that non-trivial bounds on the confidence for OOD data generalizing beyond the OOD dataset seen at training time are possible. Moreover, in contrast to certified adversarial robustness which typically comes with significant loss in prediction performance, certified guarantees for worst case OOD detection are possible without much loss in accuracy.","",""
196,"B. Mišić, R. Betzel, M. D. de Reus, M. P. van den Heuvel, M. Berman, A. McIntosh, O. Sporns","Network-Level Structure-Function Relationships in Human Neocortex",2016,"","","","",175,"2022-07-13 10:11:42","","10.1093/cercor/bhw089","","",,,,,196,32.67,28,7,6,"The dynamics of spontaneous fluctuations in neural activity are shaped by underlying patterns of anatomical connectivity. While numerous studies have demonstrated edge-wise correspondence between structural and functional connections, much less is known about how large-scale coherent functional network patterns emerge from the topology of structural networks. In the present study, we deploy a multivariate statistical technique, partial least squares, to investigate the association between spatially extended structural networks and functional networks. We find multiple statistically robust patterns, reflecting reliable combinations of structural and functional subnetworks that are optimally associated with one another. Importantly, these patterns generally do not show a one-to-one correspondence between structural and functional edges, but are instead distributed and heterogeneous, with many functional relationships arising from nonoverlapping sets of anatomical connections. We also find that structural connections between high-degree hubs are disproportionately represented, suggesting that these connections are particularly important in establishing coherent functional networks. Altogether, these results demonstrate that the network organization of the cerebral cortex supports the emergence of diverse functional network configurations that often diverge from the underlying anatomical substrate.","",""
0,"Meng Yang, Haiping Huang, Lichao Huang, Nan Zhang, Jihong Wu, Huanming Yang, F. Mu","LOGO, a contextualized pre-trained language model of human genome flexibly adapts to various downstream tasks by fine-tuning",2021,"","","","",176,"2022-07-13 10:11:42","","10.21203/rs.3.rs-448927/v1","","",,,,,0,0.00,0,7,1,"  Interpretation of non-coding genome remains an unsolved challenge in human genetics due to impracticality of exhaustively annotate biochemically active elements in all conditions. Deep learning based computational approaches emerge recently to help interpretating non-coding regions. Here we present LOGO (Language of Genome), a self-attention based contextualized pre-trained language model that applies self-supervision techniques to learn bidirectional representations of unlabeled human reference genome and extend to a series of downstream tasks via fine-tuning. We also explore a novel knowledge embedded version of LOGO to incorporate prior human annotations. Experiments show that LOGO achieves 15% absolute improvement for promoter identification and up to 4.5% absolute improvement for enhancer-promoter interaction prediction. LOGO exhibits state-of-the-art predictive power on chromatin features with only 3% parameterization against fully supervised convolutional neural network, DeepSEA. Fine-tuned LOGO also shows outstanding performance in prioritizing non-coding variants associated with human diseases. In addition, we apply LOGO to interpret type 2 diabetes (T2D) GWAS signals and infer underlying regulatory mechanisms. We make a conceptual analogy between natural language and human genome and demonstrate LOGO is an accurate, fast, scalable, and robust framework with powerful adaptability to various tasks without substantial task-specific architecture modifications.","",""
0,"Rui Li, Le-Dian Liu, Baa-Liang Lu","Measuring Human Decision Confidence from EEG Signals in an Object Detection Task",2021,"","","","",177,"2022-07-13 10:11:42","","10.1109/NER49283.2021.9441157","","",,,,,0,0.00,0,3,1,"In this paper, we investigate human decision confidence during image interpretation in an object detection task using electroencephalography (EEG) signals. We develop an EEG dataset acquired from 14 subjects. Five popular EEG features, differential entropy (DE), power spectral density (PSD), differential asymmetry (DASM), rational asymmetry (RASM) and asymmetry (ASM), and two classifiers, a support vector machine (SVM) and a deep neural network with shortcut connections (DNNS), are adopted to measure decision confidence in the object detection task. The classification results indicate that the DE feature with the DNNS model achieves the best accuracy of 47.36% and F1-score of 43.5% for five decision confidence levels. For the extreme confidence levels, the recognition accuracy reaches 83.98%, with an average Fl-score of 80.93%. We also found that the delta band performs better than the other four bands and that the prefrontal area and parietal area might be sensitive brain regions that represent decision confidence in object detection tasks.","",""
23,"A. A. Alani, G. Cosma, Aboozar Taherkhani, T. McGinnity","Hand gesture recognition using an adapted convolutional neural network with data augmentation",2018,"","","","",178,"2022-07-13 10:11:42","","10.1109/INFOMAN.2018.8392660","","",,,,,23,5.75,6,4,4,"Hand gestures provide a natural way for humans to interact with computers to perform a variety of different applications. However, factors such as the complexity of hand gesture structures, differences in hand size, hand posture, and environmental illumination can influence the performance of hand gesture recognition algorithms. Recent advances in Deep Learning have significantly advanced the performance of image recognition systems. In particular, the Deep Convolutional Neural Network has demonstrated superior performance in image representation and classification, compared to conventional machine learning approaches. This paper proposes an Adapted Deep Convolutional Neural Network (ADCNN) suitable for hand gesture recognition tasks. Data augmentation is initially applied which shifts images both horizontally and vertically to an extent of 20% of the original dimensions randomly, in order to numerically increase the size of the dataset and to add the robustness needed for a deep learning approach. These images are input into the proposed ADCNN model which is empowered by the presence of network initialization (ReLU and Softmax) and L2 Regularization to eliminate the problem of data overfitting. With these modifications, the experimental results using the ADCNN model demonstrate that it is an effective method of increasing the performance of CNN for hand gesture recognition. The model was trained and tested using 3750 static hand gesture images, which incorporate variations in features such as scale, rotation, translation, illumination and noise. The proposed ADCNN was compared to a baseline Convolutional Neural Network and the results show that the proposed ADCNN achieved a classification recognition accuracy of 99.73%, and a 4% improvement over the baseline Convolutional Neural Network model (95.73%).","",""
53,"Hoang-Dung Tran, Stanley Bak, Weiming Xiang, Taylor T. Johnson","Verification of Deep Convolutional Neural Networks Using ImageStars",2020,"","","","",179,"2022-07-13 10:11:42","","10.1007/978-3-030-53288-8_2","","",,,,,53,26.50,13,4,2,"","",""
25,"H. Baumgartl, Ricardo Buettner","Development of a Highly Precise Place Recognition Module for Effective Human-robot Interactions in Changing Lighting and Viewpoint Conditions",2020,"","","","",180,"2022-07-13 10:11:42","","10.24251/hicss.2020.069","","",,,,,25,12.50,13,2,2,"We present a highly precise and robust module for indoor place recognition, extending the work by Lemaignan et al. and Robert Jr. by giving the robot the ability to recognize its environment context. We developed a full end-to-end convolutional neural network architecture, using a pre-trained deep convolutional neural network and the explicit inductive bias transfer learning strategy. Experimental results based on the York University and Rzeszów University dataset show excellent performance values (over 94.75 and 97.95 percent accuracy) and a high level of robustness over changes in camera viewpoint and lighting conditions, outperforming current benchmarks. Furthermore, our architecture is 82.46 percent smaller than the current benchmark, making our module suitable for embedding into mobile robots and easily adoptable to other datasets without the need for heavy adjustments.","",""
20,"K. Yesu, H. J. Chakravorty, P. Bhuyan, R. Hussain, K. Bhattacharyya","Hybrid features based face recognition method using Artificial Neural Network",2012,"","","","",181,"2022-07-13 10:11:42","","10.1109/NCCISP.2012.6189705","","",,,,,20,2.00,4,5,10,"Face recognition is a biometric tool for authentication and verification having both research and practical relevance. A facial recognition based verification system can further be deemed a computer application for automatically identifying or verifying a person in a digital image. Varied and innovative face recognition systems have been developed thus far with widely accepted algorithms. The two common approaches employed for face recognition are analytic (local features based) and holistic (global features based) approaches with acceptable success rates. In this paper, we present an intelligent hybrid features based face recognition method which combines the local and global approaches to produce a complete a robust and high success rate face recognition system. The global features are computed using principal component analysis while the local features are ascertained configuring the central moment and Eigen vectors and the standard deviation of the eyes, nose and mouth segments of the human face as the decision support entities of the Generalized Feed Forward Artificial Neural Network (GFFANN). The proposed method's correct recognition rate is over 97%.","",""
31,"B. Jagadeesh, P. R. Kumar, P. C. Reddy","Robust digital image watermarking based on fuzzy inference system and back propagation neural networks using DCT",2016,"","","","",182,"2022-07-13 10:11:42","","10.1007/s00500-015-1729-y","","",,,,,31,5.17,10,3,6,"","",""
23,"S. Jaiswal, G. Nandi","Robust real-time emotion detection system using CNN architecture",2019,"","","","",183,"2022-07-13 10:11:42","","10.1007/s00521-019-04564-4","","",,,,,23,7.67,12,2,3,"","",""
21,"Jianjie Lu, K. Tong","Robust Single Accelerometer-Based Activity Recognition Using Modified Recurrence Plot",2019,"","","","",184,"2022-07-13 10:11:42","","10.1109/JSEN.2019.2911204","","",,,,,21,7.00,11,2,3,"Using a single 3-axis accelerometer for human activity recognition is challenging but attractive in daily healthcare and monitoring with wearable sensors and devices. In this paper, an effective and efficient framework is proposed to address the recognition problem without any heavy preprocessing. We encode 3-axis signals as 3-channel images using a modified recurrence plot (RP) and train a tiny residual neural network to do image classification. The modified RP is first proposed in our paper to overcome its tendency confusion problem, which has improved our system performance significantly. We evaluate our algorithm on a new database and a public dataset. Results show that our recognition framework achieves highly competitive accuracies and good efficiencies with other state-of-the-art methods on both datasets. Moreover, our method shows stronger robustness to noise and low decimation rate through comparison experiments. Finally, we provide detailed discussion and analysis of our approach from two perspectives: the pattern analysis of encoding algorithm and the interpretation of classification model.","",""
0,"Tarek Khorshed, Mohamed N. Moustafa, A. Rafea","Learning & Visualizing Genomic Signatures of Cancer Tumors using Deep Neural Networks",2020,"","","","",185,"2022-07-13 10:11:42","","10.1109/IJCNN48605.2020.9207368","","",,,,,0,0.00,0,3,2,"Deep learning for medical diagnosis using genomics is extremely challenging given the high dimensionality of the data and lack of sufficient patient samples. Another challenge is that deep models are conceived as black boxes without much interpretation on how these complex models make predictions. We propose a deep transfer learning framework for cancer diagnosis with the capability of learning the sequence of DNA and RNA in cancer cells and identifying genetic changes that alter cell behavior and cause uncontrollable growth and malignancy. We design a new Convolutional Neural Network architecture with capabilities of learning the genomic signatures of whole-transcriptome gene expressions collected from multiple tumor types covering multiple organ sites. We demonstrate how our trained model can function as a comprehensive multi-tissue cancer classifier by using transfer learning to build classifiers for tumors lacking sufficient human samples to be trained independently. We introduce visualization procedures to provide more biological insight on how our model is learning genomic signatures and accurately making predictions across multiple cancer tissue types.","",""
72,"Jinchi Huang, Lie Qu, Rongfei Jia, Binqiang Zhao","O2U-Net: A Simple Noisy Label Detection Approach for Deep Neural Networks",2019,"","","","",186,"2022-07-13 10:11:42","","10.1109/ICCV.2019.00342","","",,,,,72,24.00,18,4,3,"This paper proposes a novel noisy label detection approach, named O2U-net, for deep neural networks without human annotations. Different from prior work which requires specifically designed noise-robust loss functions or networks, O2U-net is easy to implement but effective. It only requires adjusting the hyper-parameters of the deep network to make its status transfer from overfitting to underfitting (O2U) cyclically. The losses of each sample are recorded during iterations. The higher the normalized average loss of a sample, the higher the probability of being noisy labels. O2U-net is naturally compatible with active learning and other human annotation approaches. This introduces extra flexibility for learning with noisy labels. We conduct sufficient experiments on multiple datasets in various settings. The experimental results prove the state-of-the-art of O2S-net.","",""
54,"Alexander Grushin, Derek Monner, J. Reggia, A. Mishra","Robust human action recognition via long short-term memory",2013,"","","","",187,"2022-07-13 10:11:42","","10.1109/IJCNN.2013.6706797","","",,,,,54,6.00,14,4,9,"The long short-term memory (LSTM) neural network utilizes specialized modulation mechanisms to store information for extended periods of time. It is thus potentially well-suited for complex visual processing, where the current video frame must be considered in the context of past frames. Recent studies have indeed shown that LSTM can effectively recognize and classify human actions (e.g., running, hand waving) in video data; however, these results were achieved under somewhat restricted settings. In this effort, we seek to demonstrate that LSTM's performance remains robust even as experimental conditions deteriorate. Specifically, we show that classification accuracy exhibits graceful degradation when the LSTM network is faced with (a) lower quantities of available training data, (b) tighter deadlines for decision making (i.e., shorter available input data sequences) and (c) poorer video quality (resulting from noise, dropped frames or reduced resolution). We also clearly demonstrate the benefits of memory for video processing, particularly, under high noise or frame drop rates. Our study is thus an initial step towards demonstrating LSTM's potential for robust action recognition in real-world scenarios.","",""
24,"C. Torres-Huitzil, M. Nuño-Maganda","Robust smartphone-based human activity recognition using a tri-axial accelerometer",2015,"","","","",188,"2022-07-13 10:11:42","","10.1109/LASCAS.2015.7250435","","",,,,,24,3.43,12,2,7,"Mobile artifacts such as smartphones have made possible the development of wearable systems for user activity monitoring and recognition due to the synergy of communication, computation and sensing capabilities in battery-powered systems-on-chip. Due to user acceptability, smartphones are able to measure nonintrusively proprioceptive motion outside of a controlled environment for rather long periods of time using embedded inertial sensors. Though work has been done for accelerometer-based activity recognition, the portability of the smartphone to a single fixed tight position has been a major constraint to easy the interpretation of the collected data. In this paper, a human activity hierarchical recognition system based on time-domain features and neural networks without the need of the smartphone to be constrained to a single fixed body position is presented. Experimental results on Android-capable smartphones on four on-body locations show that the recognition system achieves high classification rates, above 92%, for five activities including static, walking, running, and up-down stairs walking, running continuously in near real-time with reduced power consumption.","",""
0,"Peter Kok-Yiu Wong, Han Luo, Mingzhu Wang, Jack C. P. Cheng","Enriched and Discriminative Human Features for Person Re-Identification Based on Explainable Behaviors of Convolutional Neural Networks",2020,"","","","",189,"2022-07-13 10:11:42","","10.1007/978-3-030-51295-8_5","","",,,,,0,0.00,0,4,2,"","",""
20,"Egor Lakomkin, M. Zamani, C. Weber, S. Magg, S. Wermter","On the Robustness of Speech Emotion Recognition for Human-Robot Interaction with Deep Neural Networks",2018,"","","","",190,"2022-07-13 10:11:42","","10.1109/IROS.2018.8593571","","",,,,,20,5.00,4,5,4,"Speech emotion recognition (SER) is an important aspect of effective human-robot collaboration and received a lot of attention from the research community. For example, many neural network-based architectures were proposed recently and pushed the performance to a new level. However, the applicability of such neural SER models trained only on in-domain data to noisy conditions is currently under-researched. In this work, we evaluate the robustness of state-of-the-art neural acoustic emotion recognition models in human-robot interaction scenarios. We hypothesize that a robot's ego noise, room conditions, and various acoustic events that can occur in a home environment can significantly affect the performance of a model. We conduct several experiments on the iCub robot platform and propose several novel ways to reduce the gap between the model's performance during training and testing in real-world conditions. Furthermore, we observe large improvements in the model performance on the robot and demonstrate the necessity of introducing several data augmentation techniques like overlaying background noise and loudness variations to improve the robustness of the neural approaches.","",""
5,"Sandareka Wickramanayake, W. Hsu, M. Lee","Comprehensible Convolutional Neural Networks via Guided Concept Learning",2021,"","","","",191,"2022-07-13 10:11:42","","10.1109/IJCNN52387.2021.9534269","","",,,,,5,5.00,2,3,1,"Learning concepts that are consistent with human perception is important for Deep Neural Networks to win end-user trust. Post-hoc interpretation methods lack transparency in the feature representations learned by the models. This work proposes a guided learning approach with an additional concept layer in a CNN-based architecture to learn the associations between visual features and word phrases. We design an objective function that optimizes both prediction accuracy and semantics of the learned feature representations. Experiment results demonstrate that the proposed model can learn concepts that are consistent with human perception and their corresponding contributions to the model decision without compromising accuracy. Further, these learned concepts are transferable to new classes of objects that have similar concepts.","",""
4,"H. Kondo, S. Rahman","Human-face recognition using neural network with mosaic pattern",1999,"","","","",192,"2022-07-13 10:11:42","","10.1109/ICSMC.1999.816659","","",,,,,4,0.17,2,2,23,"A flexible robust human-face recognition method using neural networks with mosaic pattern is presented. In human-face recognition differences in facial expressions makes it difficult. However an appropriate mosaic face keeps the fundamental feature of the face. In the paper typical faces with different expressions are used as teacher patterns. As a result of employing such teacher patterns the property of the robustness of the neural network human-face recognition becomes very large and a neural network which is not so sensitive to human expression can be constructed. First it is shown that the presented neural network works well for differences of facial expressions such as smiling, crying, and irritated faces. Furthermore facing, right rotated, and left rotated faces are examined. A high recognition rate is attained for forty persons. The utilized network is a backpropagation neural one with three layers.","",""
319,"Panagiotis Tzirakis, George Trigeorgis, Mihalis A. Nicolaou, Björn Schuller, S. Zafeiriou","End-to-End Multimodal Emotion Recognition Using Deep Neural Networks",2017,"","","","",193,"2022-07-13 10:11:42","","10.1109/JSTSP.2017.2764438","","",,,,,319,63.80,64,5,5,"Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human–computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states. Inspired by this success, we propose an emotion recognition system using auditory and visual modalities. To capture the emotional content for various styles of speaking, robust features need to be extracted. To this purpose, we utilize a convolutional neural network (CNN) to extract features from the speech, while for the visual modality a deep residual network of 50 layers is used. In addition to the importance of feature extraction, a machine learning algorithm needs also to be insensitive to outliers while being able to model the context. To tackle this problem, long short-term memory networks are utilized. The system is then trained in an end-to-end fashion where—by also taking advantage of the correlations of each of the streams—we manage to significantly outperform, in terms of concordance correlation coefficient, traditional approaches based on auditory and visual handcrafted features for the prediction of spontaneous and natural emotions on the RECOLA database of the AVEC 2016 research challenge on emotion recognition.","",""
23,"Andrew J. R. Simpson","Over-Sampling in a Deep Neural Network",2015,"","","","",194,"2022-07-13 10:11:42","","","","",,,,,23,3.29,23,1,7,"Deep neural networks (DNN) are the state of the art on many engineering problems such as computer vision and audition. A key factor in the success of the DNN is scalability - bigger networks work better. However, the reason for this scalability is not yet well understood. Here, we interpret the DNN as a discrete system, of linear filters followed by nonlinear activations, that is subject to the laws of sampling theory. In this context, we demonstrate that over-sampled networks are more selective, learn faster and learn more robustly. Our findings may ultimately generalize to the human brain.","",""
114,"E. Gelenbe, Yutao Feng, K. Krishnan","Neural network methods for volumetric magnetic resonance imaging of the human brain",1996,"","","","",195,"2022-07-13 10:11:42","","10.1109/5.537113","","",,,,,114,4.38,38,3,26,"Brain magnetic resonance (MR) images contain massive information requiring lengthy and complex interpretation (as in the identification of significant portions of the image), quantitative evaluation (as in the determination of the size of certain significant regions), and sophisticated interpretation (as in determining any image portions which indicate signs of lesions or of disease). In this paper we first survey the clinical and research needs for brain imaging. We present the state-of-the-art in relevant image analysis techniques. We then discuss our recent work on the use of novel artificial neural networks which have a recurrent structure to extract precise morphometric information from MRI scans of the human brain. Finally, experimental data using our novel approach is presented and suggestions are made for future research.","",""
100,"Maoqing Tian, Shuai Yi, Hongsheng Li, Shihua Li, Xuesen Zhang, Jianping Shi, Junjie Yan, Xiaogang Wang","Eliminating Background-bias for Robust Person Re-identification",2018,"","","","",196,"2022-07-13 10:11:42","","10.1109/CVPR.2018.00607","","",,,,,100,25.00,13,8,4,"Person re-identification is an important topic in intelligent surveillance and computer vision. It aims to accurately measure visual similarities between person images for determining whether two images correspond to the same person. State-of-the-art methods mainly utilize deep learning based approaches for learning visual features for describing person appearances. However, we observe that existing deep learning models are biased to capture too much relevance between background appearances of person images. We design a series of experiments with newly created datasets to validate the influence of background information. To solve the background bias problem, we propose a person-region guided pooling deep neural network based on human parsing maps to learn more discriminative person-part features, and propose to augment training data with person images with random background. Extensive experiments demonstrate the robustness and effectiveness of our proposed method.","",""
286,"Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu","Skeleton-Based Action Recognition With Directed Graph Neural Networks",2019,"","","","",197,"2022-07-13 10:11:42","","10.1109/CVPR.2019.00810","","",,,,,286,95.33,72,4,3,"The skeleton data have been widely used for the action recognition tasks since they can robustly accommodate dynamic circumstances and complex backgrounds. In existing methods, both the joint and bone information in skeleton data have been proved to be of great help for action recognition tasks. However, how to incorporate these two types of data to best take advantage of the relationship between joints and bones remains a problem to be solved. In this work, we represent the skeleton data as a directed acyclic graph based on the kinematic dependency between the joints and bones in the natural human body. A novel directed graph neural network is designed specially to extract the information of joints, bones and their relations and make prediction based on the extracted features. In addition, to better fit the action recognition task, the topological structure of the graph is made adaptive based on the training process, which brings notable improvement. Moreover, the motion information of the skeleton sequence is exploited and combined with the spatial information to further enhance the performance in a two-stream framework. Our final model is tested on two large-scale datasets, NTU-RGBD and Skeleton-Kinetics, and exceeds state-of-the-art performance on both of them.","",""
65,"I. Rizvi, B. Mohan","Object-Based Image Analysis of High-Resolution Satellite Images Using Modified Cloud Basis Function Neural Network and Probabilistic Relaxation Labeling Process",2011,"","","","",198,"2022-07-13 10:11:42","","10.1109/TGRS.2011.2171695","","",,,,,65,5.91,33,2,11,"Object-based image analysis is quickly gaining acceptance among remote sensing community, and object-based image classification methods are increasingly being used for classification of land use/cover units from high-resolution satellite images with results closer to human interpretation compared to per-pixel classifiers. The problem of nonlinear separability of classes in a feature space consisting of spectral/spatial/textural features is addressed by kernel-based nonlinear mapping of the feature vectors. This facilitates use of linear discriminant functions for classification as used in artificial neural networks (ANNs). In this paper, performance of a recently introduced kernel called cloud basis function (CBF) is investigated with some modification for classification. The CBF has demonstrated superior performance to the tune of about 4% higher classification accuracy compared to conventional radial basis function used in ANN. The results are further improved by using probabilistic relaxation labeling as a postprocessing step. This paper has potential applications in urban planning and urban studies.","",""
318,"Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits","Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment",2019,"","","","",199,"2022-07-13 10:11:42","","10.1609/AAAI.V34I05.6311","","",,,,,318,106.00,80,4,3,"Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective—it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving—it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient—it generates adversarial text with computational complexity linear to the text length.1","",""
0,"David M. Schwartz, Travis W. Sawyer, Noah Thurston, J. Barton, G. Ditzler","Ovarian cancer detection using optical coherence tomography and convolutional neural networks",2022,"","","","",200,"2022-07-13 10:11:42","","10.1007/s00521-022-06920-3","","",,,,,0,0.00,0,5,1,"","",""
