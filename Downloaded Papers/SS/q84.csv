Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
19,"P. Svenmarck, L. Luotsinen, Mattias Nilsson, J. Schubert","Possibilities and Challenges for Artificial Intelligence in Military Applications",2018,"","","","",1,"2022-07-13 09:32:36","","","","",,,,,19,4.75,5,4,4,"Recent developments in artificial intelligence (AI) have resulted in a breakthrough for many classical AIapplications, such as computer vision, natural language processing, robotics, and data mining. Therefore, there are many efforts to exploit these developments for military applications, such as surveillance, reconnaissance, threat evaluation, underwater mine warfare, cyber security, intelligence analysis, command and control, and education and training. However, despite the possibilities for AI in military applications, there are many challenges to consider. For instance, 1) high risks means that military AI-systems need to be transparent to gain decision maker trust and to facilitate risk analysis; this is a challenge since many AItechniques are black boxes that lack sufficient transparency, 2) military AI-systems need to be robust and reliable; this is a challenge since it has been shown that AI-techniques may be vulnerable to imperceptible manipulations of input data even without any knowledge about the AI-technique that is used, and 3) many AItechniques are based on machine learning that requires large amounts of training data; this is challenge since there is often a lack of sufficient data in military applications. This paper present results from ongoing projects to identity possibilities for AI in military applications, as well as how to address these challenges.","",""
0,"Shiddalingeshwar Mallayya Javali, Raghavender Surya Upadhyayula, Tanusree De","Comparative study of xAI layer-wise algorithms with a Robust Recommendation framework of Inductive Clustering for Polyp Segmentation and Classification",2022,"","","","",2,"2022-07-13 09:32:36","","10.1109/ISMODE53584.2022.9743003","","",,,,,0,0.00,0,3,1,"Explainable Artificial Intelligence (xAI) is aimed at unboxing the AI model choices by providing interpretable explanations mitigating the black-box nature of algorithms and improving the transparency. Availability of many xAI techniques for medical imaging studies, necessitates a need to understand and pick a suitable approach for different stakeholders (clinician/patient). Interpretation of model predictions could be benefitted by comparing different xAI approaches. We explored different xAI techniques on Kvasir-SEG polyp segmentation dataset. A comparative analysis of attributions from convolutional layers of Feature Pyramid Network (FPN) model (Dice coefficient = 0.942 and meanIOU = 0.916) is provided. We observed that Layered GradCAM provides a very robust visual trajectory of explanations flowing through encoder-decoder-segmentation blocks. We investigated an explanation-by-example approach wherein we compare and match the predicted mask with key-point features from the training corpus. In addition, we have developed a recommendation approach using Inductive Clustering for classifying predicted polyps based on their size and area into three classes – small, medium and large. The recommendation could assist in prioritizing the patient check-up and subsequent timely intervention by a clinician.","",""
1,"N. Prakash, K. Mathewson","Conceptualization and Framework of Hybrid Intelligence Systems",2020,"","","","",3,"2022-07-13 09:32:36","","","","",,,,,1,0.50,1,2,2,"As artificial intelligence (AI) systems are getting ubiquitous within our society, issues related to its fairness, accountability, and transparency are increasing rapidly. As a result, researchers are integrating humans with AI systems to build robust and reliable hybrid intelligence systems. However, a proper conceptualization of these systems does not underpin this rapid growth. This article provides a precise definition of hybrid intelligence systems as well as explains its relation with other similar concepts through our proposed framework and examples from contemporary literature. The framework breakdowns the relationship between a human and a machine in terms of the degree of coupling and the directive authority of each party. Finally, we argue that all AI systems are hybrid intelligence systems, so human factors need to be examined at every stage of such systems' lifecycle.","",""
27,"Michael Grüning","Artificial Intelligence Measurement of Disclosure (AIMD)",2011,"","","","",4,"2022-07-13 09:32:36","","10.1080/09638180.2011.585792","","",,,,,27,2.45,27,1,11,"Empirical research on voluntary disclosure lacks an appropriate measurement technique for quantifying the intensity of a firm's disclosure. In this paper, I introduce artificial intelligence measurement of disclosure (AIMD), a computerised technique for measuring disclosure using artificial intelligence, which derives disclosure proxies from English-language annual reports for 10 different information dimensions without human involvement. Criterion validity tests indicate that, controlling for a robust set of covariates and multiple statistical techniques, AIMD is negatively associated with information asymmetry as proxied by spreads and PIN. Furthermore, AIMD has construct validity when compared to the AIMR disclosure rating, Standard & Poor's Transparency and Disclosure Rating, several proprietary manual disclosure scorings and companies’ own assessment of their level of disclosure as indicated by a survey. I also demonstrate the applicability of AIMD as a cost-effective technique for measuring disclosure using a sample of 127,895 firm-year observations of companies regulated by the SEC.","",""
1,"K. Opoku-Agyemang","Using Blockchain for Scientific Transparency",2017,"","","","",5,"2022-07-13 09:32:36","","10.2139/SSRN.3053647","","",,,,,1,0.20,1,1,5,"Preregistration is widely recognized as a critical component of making experimental research transparent. It is less clear how to credibly use a preanalysis plan with observational data, which is more prevalent in the social sciences and becoming even more important for science in general due to the emergence of big data and artificial intelligence methodologies. Such data have not yet been proven reconcilable with preregistration: it is difficult to conclusively comfirm when data analysis occurred after and not before being preregistered in a preanalysis plan. Scientific replications are similarly difficult to credibly preregister, which can ultimately render replications nearly as uncertain as the analyses they seek to evaluate. Trusted timestamping, an approach based on blockchain technologies, offers a way to credibly use observational data with preregistered preanalysis plans. It also makes preregistered experiments more robust. I examine how automated trusted timestamping of statistical analyses makes research, replications and meta-analyses more transparent without sacrificing the rigor of science.","",""
7,"Charles Lu, Andr'eanne Lemay, Ken Chang, K. Hoebel, Jayashree Kalpathy-Cramer","Fair Conformal Predictors for Applications in Medical Imaging",2021,"","","","",6,"2022-07-13 09:32:36","","10.1609/aaai.v36i11.21459","","",,,,,7,7.00,1,5,1,"Deep learning has the potential to automate many clinically useful tasks in medical imaging. However translation of deep learning into clinical practice has been hindered by issues such as lack of the transparency and interpretability in these ``black box'' algorithms compared to traditional statistical methods. Specifically, many clinical deep learning models lack rigorous and robust techniques for conveying certainty (or lack thereof) in their predictions -- ultimately limiting their appeal for extensive use in medical decision-making. Furthermore, numerous demonstrations of algorithmic bias have increased hesitancy towards deployment of deep learning for clinical applications. To this end, we explore how conformal predictions can complement existing deep learning approaches by providing an intuitive way of expressing uncertainty while facilitating greater transparency to clinical users. In this paper, we conduct field interviews with radiologists to assess possible use-cases for conformal predictors. Using insights gathered from these interviews, we devise two clinical use-cases and empirically evaluate several methods of conformal predictions on a dermatology photography dataset for skin lesion classification. We show how to modify conformal predictions to be more adaptive to subgroup differences in patient skin tones through equalized coverage. Finally, we compare conformal prediction against measures of epistemic uncertainty.","",""
48,"Zhixing Zhang, Ling Wang, Huitao Yu, Fei Zhang, Lin Tang, Yiyu Feng, W. Feng","Highly Transparent, Self-Healable, and Adhesive Organogels for Bio-Inspired Intelligent Ionic Skins.",2020,"","","","",7,"2022-07-13 09:32:36","","10.1021/acsami.9b22707","","",,,,,48,24.00,7,7,2,"Development of intelligent adaptable materials with unprecedented sensitivity that can mimic the tactile sensing functions of natural skin is a major driving force in the realization of artificial intelligence. Herein, we judiciously designed and synthesized a series of lauryl acrylate-based polymeric organogels with high transparency, mechanical adaptability, self-healing property, and adhesive capability. Moreover, a robust capacitive sensor with high sensitivity (0.293 kPa-1) was developed by sandwiching the prepared soft, adaptable organogels between two tough conductive hydrogels, and was then used to monitor various human motions such as finger stretching, wrist bending, and throat movement during chewing. Interestingly, the resulting capacitive sensor could also function as prosthetic skin on a pneumatic soft artificial hand, enabling intelligent haptic perception. The research disclosed herein is expected to provide insights into the rational design of artificial human-like skins with unprecedented functionalities.","",""
4,"A. Das, Basudeb Bera, Debasis Giri","AI and Blockchain-Based Cloud-Assisted Secure Vaccine Distribution and Tracking in IoMT-Enabled COVID-19 Environment",2021,"","","","",8,"2022-07-13 09:32:36","","10.1109/IOTM.0001.2100016","","",,,,,4,4.00,1,3,1,"Coronavirus 2019, called COVID-19, is a transmissible disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It earlier impacted the citizens of China alone. However, it has rapidly spread all over the world. The COVID-19 supply chain system aims to facilitate access to several critical items, such as personal protective equipment (PPE), biomedical equipment, diagnostics supplies, and vaccines. In this article, we discuss a robust security framework for vaccine distribution and tracking in an Internet of Medical Things (IoMT)-based cloud-assisted COVID-19 environment by considering both intra-country and inter-country scenarios. Various transactions related to vaccine requests, orders, distribution, and tracking are put into the blockchain in the form of blocks. Since blockchain technology offers immutability, transparency, and decentralization, the security of the proposed framework has been improved significantly. The proposed framework also supports artificial intelligence(AI)-based big data analytics on the information stored into the blocks in the blockchain. Furthermore, a practical demonstration of the proposed framework has been done through a blockchain simulation study.","",""
4,"Markus Borg, Joshua Bronson, Linus Christensson, Fredrik Olsson, Olof Lennartsson, Elias Sonnsjö, Hamid Ebabi, Martin Karsberg","Exploring the Assessment List for Trustworthy AI in the Context of Advanced Driver-Assistance Systems",2021,"","","","",9,"2022-07-13 09:32:36","","10.1109/SEthics52569.2021.00009","","",,,,,4,4.00,1,8,1,"Artificial Intelligence (AI) is increasingly used in critical applications. Thus, the need for dependable AI systems is rapidly growing. In 2018, the European Commission appointed experts to a High-Level Expert Group on AI (AI-HLEG). AI- HLEG defined Trustworthy AI as 1) lawful, 2) ethical, and 3) robust and specified seven corresponding key requirements. To help development organizations, AI-HLEG recently published the Assessment List for Trustworthy AI (ALTAI). We present an illustrative case study from applying ALTAI to an ongoing development project of an Advanced Driver-Assistance System (ADAS) that relies on Machine Learning (ML). Our experience shows that ALTAI is largely applicable to ADAS development, but specific parts related to human agency and transparency can be disregarded. Moreover, bigger questions related to societal and environmental impact cannot be tackled by an ADAS supplier in isolation. We present how we plan to develop the ADAS to ensure ALTAI-compliance. Finally, we provide three recommendations for the next revision of ALTAI, i.e., life-cycle variants, domainspecific adaptations, and removed redundancy.","",""
3,"F. Biessmann, D. Refiano","Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated",2021,"","","","",10,"2022-07-13 09:32:36","","","","",,,,,3,3.00,2,2,1,"The field explainable artificial intelligence (XAI) has brought about an arsenal of methods to render Machine Learning (ML) predictions more interpretable. But how useful explanations provided by transparent ML methods are for humans remains difficult to assess. Here we investigate the quality of interpretable computer vision algorithms using techniques from psychophysics. In crowdsourced annotation tasks we study the impact of different interpretability approaches on annotation accuracy and task time. We compare these quality metrics with classical XAI, automated quality metrics. Our results demonstrate that psychophysical experiments allow for robust quality assessment of transparency in machine learning. Interestingly the quality metrics computed without humans in the loop did not provide a consistent ranking of interpretability methods nor were they representative for how useful an explanation was for humans. These findings highlight the potential of methods from classical psychophysics for modern machine learning applications. We hope that our results provide convincing arguments for evaluating interpretability in its natural habitat, human-ML interaction, if the goal is to obtain an authentic assessment of interpretability.","",""
1,"Geoffrey Rockwell, Emily Black, Evan Selinger, Antonio Davola, Elana Seide, K. Gulson","From Shortcut to Sleight of Hand: Why the Checklist Approach in the EU Guidelines Does Not Work",2019,"","","","",11,"2022-07-13 09:32:36","","","","",,,,,1,0.33,0,6,3,"Author(s): Rockwell, Geoffrey; Black, Emily; Selinger, Evan; Davola, Antonio; Seide, Elana; Gulson, Kalervo | Abstract: In April 2019, the High-Level Expert Group on Artificial Intelligence (AI) nominated by the EU Commission presented “Ethics Guidelines for Trustworthy Artificial Intelligence,” followed in June 2019 by a second “Policy and investment recommendations” Document.The Guidelines establish three characteristics (lawful, ethical, and robust) and seven key requirements (Human agency and oversight; Technical Robustness and safety; Privacy and data governance; Transparency; Diversity, non-discrimination and fairness; Societal and environmental well-being; and Accountability) that the development of AI should follow.The Guidelines are of utmost significance for the international debate over the regulation of AI. Firstly, they aspire to set a universal standard of care for the development of AI in the future. Secondly, they have been developed within a group of experts nominated by a regulatory body, and therefore will shape the normative approach in the EU regulation of AI and in its interaction with foreign countries. As the GDPR has shown, the effect of this normative activity goes way past the European Union territory.One of the most debated aspects of the Guidelines was the need to find an objective methodology to evaluate conformity with the key requirements. For this purpose, the Expert Group drafted an “assessment checklist” in the last part of the document: the list is supposed to be incorporated into existing practices, as a way for technology developers to consider relevant ethical issues and create more “trustworthy” AI. Our group undertook a critical assessment of the proposed tool from a multidisciplinary perspective, to assess its implications and limitations for global AI development.","",""
3,"Hanbai Lyu, Xinyu Ping, Ruichao Gao, Liangliang Xu, Lijia Pan","Transparent Electronic Skin Device Based on Microstructured Silver Nanowire Electrode",2017,"","","","",12,"2022-07-13 09:32:36","","10.1063/1674-0068/30/CJCP1706126","","",,,,,3,0.60,1,5,5,"Transparent, flexible electronic skin holds a wide range of applications in robotics, humanmachine interfaces, artificial intelligence, prosthetics, and health monitoring. Silver nanowire are mechanically flexible and robust, which exhibit great potential in transparent and electricconducting thin film. Herein, we report on a silver-nanowire spray-coating and electrodemicrostructure replicating strategy to construct a transparent, flexible, and sensitive electronic skin device. The electronic skin device shows highly sensitive piezo-capacitance response to pressure. It is found that micropatterning the surface of dielectric layer polyurethane elastomer by replicating from microstructures of natural-existing surfaces such as lotus leaf, silk, and frosted glass can greatly enhance the piezo-capacitance performance of the device. The microstructured pressure sensors based on silver nanowire exhibit good transparency, excellent flexibility, wide pressure detection range (0-150 kPa), and high sensitivity (1.28 kPa-1).","",""
28,"Karl Fine Licht, Jenny Fine Licht","Artificial intelligence, transparency, and public decision-making",2020,"","","","",13,"2022-07-13 09:32:36","","10.1007/s00146-020-00960-w","","",,,,,28,14.00,14,2,2,"","",""
24,"Karl de Fine Licht, Jenny de Fine Licht","Artificial intelligence, transparency, and public decision-making",2020,"","","","",14,"2022-07-13 09:32:36","","10.1007/s00146-020-00960-w","","",,,,,24,12.00,12,2,2,"","",""
3,"","Artificial intelligence, transparency, and public decision-making: Why explanations are key when trying to produce perceived legitimacy",2020,"","","","",15,"2022-07-13 09:32:36","","","","",,,,,3,1.50,0,0,2,"The increasing use of Artificial Intelligence (AI) for making decisions in public affairs has sparked a lively debate on the benefits and potential harms of self-learning technologies, ranging from the hopes of fully informed and objectively taken decisions to fear for the destruction of mankind. To prevent the negative outcomes and to achieve accountable systems, many have argued that we need to open up the “black box” of AI decision-making and make it more transparent. Whereas this debate has primarily focused on how transparency can secure high-quality, fair, and reliable decisions, far less attention has been devoted to the role of transparency when it comes to how the general public come to perceive AI decision-making as legitimate and worthy of acceptance. Since relying on coercion is not only normatively problematic but also costly and highly inefficient, perceived legitimacy is fundamental to the democratic system. This paper discusses how transparency in and about AI decision-making can affect the public’s perception of the legitimacy of decisions and decision-makers and produce a framework for analyzing these questions. We argue that a limited form of transparency that focuses on providing justifications for decisions has the potential to provide sufficient ground for perceived legitimacy without producing the harms full transparency would bring.","",""
13,"Karl de Fine Licht, Jenny de Fine Licht","Artificial intelligence, transparency, and public decision-making",2020,"","","","",16,"2022-07-13 09:32:36","","10.1007/S00146-020-00960-W","","",,,,,13,6.50,7,2,2,"","",""
14,"M. Najafzadeh, G. Oliveto","More reliable predictions of clear-water scour depth at pile groups by robust artificial intelligence techniques while preserving physical consistency",2021,"","","","",17,"2022-07-13 09:32:36","","10.1007/s00500-020-05567-3","","",,,,,14,14.00,7,2,1,"","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",18,"2022-07-13 09:32:36","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
23,"M. Alomar, M. Hameed, M. Alsaadi","Multi hours ahead prediction of surface ozone gas concentration: Robust artificial intelligence approach",2020,"","","","",19,"2022-07-13 09:32:36","","10.1016/j.apr.2020.06.024","","",,,,,23,11.50,8,3,2,"","",""
13,"Joel Walmsley","Artificial intelligence and the value of transparency",2020,"","","","",20,"2022-07-13 09:32:36","","10.1007/s00146-020-01066-z","","",,,,,13,6.50,13,1,2,"","",""
13,"R. Daneshjou, Mary P Smith, Mary D Sun, V. Rotemberg, James Zou","Lack of Transparency and Potential Bias in Artificial Intelligence Data Sets and Algorithms: A Scoping Review.",2021,"","","","",21,"2022-07-13 09:32:36","","10.1001/jamadermatol.2021.3129","","",,,,,13,13.00,3,5,1,"Importance Clinical artificial intelligence (AI) algorithms have the potential to improve clinical care, but fair, generalizable algorithms depend on the clinical data on which they are trained and tested.   Objective To assess whether data sets used for training diagnostic AI algorithms addressing skin disease are adequately described and to identify potential sources of bias in these data sets.   Data Sources In this scoping review, PubMed was used to search for peer-reviewed research articles published between January 1, 2015, and November 1, 2020, with the following paired search terms: deep learning and dermatology, artificial intelligence and dermatology, deep learning and dermatologist, and artificial intelligence and dermatologist.   Study Selection Studies that developed or tested an existing deep learning algorithm for triage, diagnosis, or monitoring using clinical or dermoscopic images of skin disease were selected, and the articles were independently reviewed by 2 investigators to verify that they met selection criteria.   Consensus Process Data set audit criteria were determined by consensus of all authors after reviewing existing literature to highlight data set transparency and sources of bias.   Results A total of 70 unique studies were included. Among these studies, 1 065 291 images were used to develop or test AI algorithms, of which only 257 372 (24.2%) were publicly available. Only 14 studies (20.0%) included descriptions of patient ethnicity or race in at least 1 data set used. Only 7 studies (10.0%) included any information about skin tone in at least 1 data set used. Thirty-six of the 56 studies developing new AI algorithms for cutaneous malignant neoplasms (64.3%) met the gold standard criteria for disease labeling. Public data sets were cited more often than private data sets, suggesting that public data sets contribute more to new development and benchmarks.   Conclusions and Relevance This scoping review identified 3 issues in data sets that are used to develop and test clinical AI algorithms for skin disease that should be addressed before clinical translation: (1) sparsity of data set characterization and lack of transparency, (2) nonstandard and unverified disease labels, and (3) inability to fully assess patient diversity used for algorithm development and testing.","",""
10,"M. Alomar, M. Hameed, N. Al‐Ansari, M. Alsaadi","Data-Driven Model for the Prediction of Total Dissolved Gas: Robust Artificial Intelligence Approach",2020,"","","","",22,"2022-07-13 09:32:36","","10.1155/2020/6618842","","",,,,,10,5.00,3,4,2,"Saturated total dissolved gas (TDG) is recently considered as a serious issue in the environmental engineering field since it stands behind the reasons for increasing the mortality rates of fish and aquatic organisms. The accurate and more reliable prediction of TDG has a very significant role in preserving the diversity of aquatic organisms and reducing the phenomenon of fish deaths. Herein, two machine learning approaches called support vector regression (SVR) and extreme learning machine (ELM) have been applied to predict the saturated TDG% at USGS 14150000 and USGS 14181500 stations which are located in the USA. For the USGS 14150000 station, the recorded samples from 13 October 2016 to 14 March 2019 (75%) were used for training set, and the rest from 15 March 2019 to 13 October 2019 (25%) were used for testing requirements. Similarly, for USGS 14181500 station, the hourly data samples which covered the period from 9 June 2017 till 11 March 2019 were used for calibrating the models and from 12 March 2019 until 9 October 2019 were used for testing the predictive models. Eight input combinations based on different parameters have been established as well as nine statistical performance measures have been used for evaluating the accuracy of adopted models, for instance, not limited, correlation of determination (        R      2        ), mean absolute relative error (MAE), and uncertainty at 95% (        U      95        ). The obtained results of the study for both stations revealed that the ELM managed efficiently to estimate the TDG in comparison to SVR technique. For USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.986 (0.986), MAE of 0.316 (0.441), and         U      95        of 3.592 (3.869). Lastly, for USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.991 (0.991), MAE of 0.338 (0.396), and         U      95        of 0.832 (0.837). In addition, ELM’s training process computational time is stated to be much shorter than that of SVM. The results also showed that the temperature parameter was the most significant variable that influenced TDG relative to the other parameters. Overall, the proposed model (ELM) proved to be an appropriate and efficient computer-assisted technology for saturated TDG modeling that will contribute to the basic knowledge of environmental considerations.","",""
3,"Dercilio Junior Verly Lopes, G. S. Bobadilha, Karl Michael Grebner","A fast and robust artificial intelligence technique for wood knot detection",2020,"","","","",23,"2022-07-13 09:32:36","","10.15376/BIORES.15.4.9351-9361","","",,,,,3,1.50,1,3,2,"This study reports the feasibility of using deep convolutional neural networks (CNN), for automatically detecting knots on the surface of wood with high speed and accuracy. A limited dataset of 921 images were photographed in different contexts and divided into 80:20 ratio for training and validation, respectively. The “You only look once” (YoloV3) CNN-based architecture was adopted for training the neural network. The Adam gradient descent optimizer algorithm was used to iteratively minimize the generalized intersection-over-union loss function. Knots on the surface of wood were manually annotated. Images and annotations were analyzed by a stack of convolutional and fully connected layers with skipped connections. After training, model checkpoint was created and inferences on the validation set were made. The quality of results was assessed by several metrics: precision, recall, F1-score, average precision, and precision x recall curve. Results indicated that YoloV3 provided knot detection time of approximately 0.0102 s per knot with a relatively low false positive and false negative ratios. Precision, recall, f1-score metrics reached 0.77, 0.79, and 0.78, respectively. The average precision was 80%. With an adequate number of images, it is possible to improve this tool for use within sawmills in the forms of both workstation and mobile device applications.","",""
120,"Hoang Nguyen, X. Bui","Predicting Blast-Induced Air Overpressure: A Robust Artificial Intelligence System Based on Artificial Neural Networks and Random Forest",2018,"","","","",24,"2022-07-13 09:32:36","","10.1007/s11053-018-9424-1","","",,,,,120,30.00,60,2,4,"","",""
0,"Bushra Rasheed, M. Usama, Asmara Safdar","Robust Artificial Intelligence Approach to Stabilize and Control Propeller Driven Hybrid UGV",2022,"","","","",25,"2022-07-13 09:32:36","","10.1109/ICAI55435.2022.9773375","","",,,,,0,0.00,0,3,1,"Hybrid Unmanned Ground Vehicle (HUGV) can drive on any terrain including walls and fly as well, using the multi directional thrust force of propellers. In the era of industrial revolution, hybrid UGVs need to be autonomous with intelligent decision making capabilities. During wall climbing of hybrid UGVs, stability is essential and depends on real time feedback from multiple sensors. To increase stability and control, it is proposed that PID control loops should be replaced by AI based algorithms that reduce the decision time and mathematical complexity. For autonomous movement in any terrain using the proposed model, intelligent UGVs can map and localize simultaneously.They can make intelligent decisions about mode of movement i.e. driving on ground or wall, steering on ground or wall, flying and maneuvering by using real time sensor readings. Integration of the proposed AI models with HUGV can be applied to many areas which are hard for humans to access, for instance; inspection of large structures, bio & nuclear hazard environments, planetary exploration & magnetic fields detection.","",""
129,"S. Vollmer, B. Mateen, G. Bohner, F. Király, R. Ghani, P. Jónsson, Sarah Cumbers, Adrian Jonas, K. McAllister, P. Myles, David Grainger, M. Birse, Richard Branson, K. Moons, G. Collins, J. Ioannidis, C. Holmes, H. Hemingway","Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness",2020,"","","","",26,"2022-07-13 09:32:36","","10.1136/bmj.l6927","","",,,,,129,64.50,13,18,2,"Machine learning, artificial intelligence, and other modern statistical methods are providing new opportunities to operationalise previously untapped and rapidly growing sources of data for patient benefit. Despite much promising research currently being undertaken, particularly in imaging, the literature as a whole lacks transparency, clear reporting to facilitate replicability, exploration for potential ethical concerns, and clear demonstrations of effectiveness. Among the many reasons why these problems exist, one of the most important (for which we provide a preliminary solution here) is the current lack of best practice guidance specific to machine learning and artificial intelligence. However, we believe that interdisciplinary groups pursuing research and impact projects involving machine learning and artificial intelligence for health would benefit from explicitly addressing a series of questions concerning transparency, reproducibility, ethics, and effectiveness (TREE). The 20 critical questions proposed here provide a framework for research groups to inform the design, conduct, and reporting; for editors and peer reviewers to evaluate contributions to the literature; and for patients, clinicians and policy makers to critically appraise where new findings may deliver patient benefit.","",""
35,"S. Larsson, F. Heintz","Transparency in artificial intelligence",2020,"","","","",27,"2022-07-13 09:32:36","","10.14763/2020.2.1469","","",,,,,35,17.50,18,2,2,"This conceptual paper addresses the issues of transparency as linked to artificial intelligence (AI) from socio-legal and computer scientific perspectives. Firstly, we discuss the conceptual distinction between transparency in AI and algorithmic transparency, and argue for the wider concept ‘in AI’, as a partly contested albeit useful notion in relation to transparency. Secondly, we show that transparency as a general concept is multifaceted, and of widespread theoretical use in multiple disciplines over time, particularly since the 1990s. Still, it has had a resurgence in contemporary notions of AI governance, such as in the multitude of recently published ethics guidelines on AI. Thirdly, we discuss and show the relevance of the fact that transparency expresses a conceptual metaphor of more general significance, linked to knowing, bringing positive connotations that may have normative effects to regulatory debates. Finally, we draw a possible categorisation of aspects related to transparency in AI, or what we interchangeably call AI transparency, and argue for the need of developing a multidisciplinary understanding, in order to contribute to the governance of AI as applied on markets and in society. (Less)","",""
40,"Philipp Schmidt, F. Biessmann, Timm Teubner","Transparency and trust in artificial intelligence systems",2020,"","","","",28,"2022-07-13 09:32:36","","10.1080/12460125.2020.1819094","","",,,,,40,20.00,13,3,2,"ABSTRACT Assistive technology featuring artificial intelligence (AI) to support human decision-making has become ubiquitous. Assistive AI achieves accuracy comparable to or even surpassing that of human experts. However, often the adoption of assistive AI systems is limited by a lack of trust of humans into an AI’s prediction. This is why the AI research community has been focusing on rendering AI decisions more transparent by providing explanations of an AIs decision. To what extent these explanations really help to foster trust into an AI system remains an open question. In this paper, we report the results of a behavioural experiment in which subjects were able to draw on the support of an ML-based decision support tool for text classification. We experimentally varied the information subjects received and show that transparency can actually have a negative impact on trust. We discuss implications for decision makers employing assistive AI technology.","",""
4,"I. Barclay, Harrison Taylor, A. Preece, Ian Taylor, Ian Taylor, D. Verma, Geeth de Mel","A framework for fostering transparency in shared artificial intelligence models by increasing visibility of contributions",2020,"","","","",29,"2022-07-13 09:32:36","","10.1002/cpe.6129","","",,,,,4,2.00,1,7,2,"Increased adoption of artificial intelligence (AI) systems into scientific workflows will result in an increasing technical debt as the distance between the data scientists and engineers who develop AI system components and scientists, researchers and other users grows. This could quickly become problematic, particularly where guidance or regulations change and once‐acceptable best practice becomes outdated, or where data sources are later discredited as biased or inaccurate. This paper presents a novel method for deriving a quantifiable metric capable of ranking the overall transparency of the process pipelines used to generate AI systems, such that users, auditors and other stakeholders can gain confidence that they will be able to validate and trust the data sources and contributors in the AI systems that they rely on. The methodology for calculating the metric, and the type of criteria that could be used to make judgements on the visibility of contributions to systems are evaluated through models published at ModelHub and PyTorch Hub, popular archives for sharing science resources, and is found to be helpful in driving consideration of the contributions made to generating AI systems and approaches toward effective documentation and improving transparency in machine learning assets shared within scientific communities.","",""
3,"Simone Stumpf, Lorenzo Strappelli, Subeida Ahmed, Yuri Nakao, A. Naseer, Giulia Del Gamba, D. Regoli","Design Methods for Artificial Intelligence Fairness and Transparency",2021,"","","","",30,"2022-07-13 09:32:36","","","","",,,,,3,3.00,0,7,1,"Fairness and transparency in artificial intelligence (AI) continue to become more prevalent as topics for research, design and development. General principles and guidelines for designing ethical and responsible AI systems have been proposed, yet there is a lack of design methods for these kinds of systems. In this paper, we present CoFAIR, a novel method to design user interfaces for exploring fairness, consisting of series of co-design workshops, and wider evaluation. This method can be readily applied in practice by researchers, designers and developers to create responsible and ethical AI systems.","",""
1,"Ana Lucic, Maurits Bleeker, Sami Jullien, Samarth Bhargav, M. de Rijke","Teaching Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence through the Lens of Reproducibility",2021,"","","","",31,"2022-07-13 09:32:36","","","","",,,,,1,1.00,0,5,1,"In this work we explain the setup for a technical, graduatelevel course on Fairness, Accountability, Confidentiality and Transparency in Artificial Intelligence (FACT-AI) at the University of Amsterdam, which teaches FACT-AI concepts through the lens of reproducibility. The focal point of the course is a group project based on reproducing existing FACT-AI algorithms from top AI conferences, and writing a report about their experiences. In the first iteration of the course, we created an open source repository with the code implementations from the group projects. In the second iteration, we encouraged students to submit their group projects to the Machine Learning Reproducibility Challenge, which resulted in 9 reports from our course being accepted to the challenge. We reflect on our experience teaching the course over two academic years, where one year coincided with a global pandemic, and propose guidelines for teaching FACTAI through reproducibility in graduate-level AI programs. We hope this can be a useful resource for instructors to set up similar courses at their universities in the future.","",""
81,"Thomas G. Dietterich","Steps Toward Robust Artificial Intelligence",2017,"","","","",32,"2022-07-13 09:32:36","","10.1609/aimag.v38i3.2756","","",,,,,81,16.20,81,1,5,"Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system’s models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world","",""
29,"H. Felzmann, E. Fosch-Villaronga, C. Lutz, A. Tamó-Larrieux","Towards Transparency by Design for Artificial Intelligence",2020,"","","","",33,"2022-07-13 09:32:36","","10.1007/s11948-020-00276-4","","",,,,,29,14.50,7,4,2,"","",""
109,"B. Haibe-Kains, George Adam, A. Hosny, F. Khodakarami, L. Waldron, Bo Wang, C. McIntosh, A. Goldenberg, A. Kundaje, C. Greene, Tamara Broderick, M. M. Hoffman, J. Leek, K. Korthauer, W. Huber, A. Brazma, Joelle Pineau, R. Tibshirani, T. Hastie, J. Ioannidis, John Quackenbush, H. Aerts","Transparency and reproducibility in artificial intelligence.",2020,"","","","",34,"2022-07-13 09:32:36","","10.1038/s41586-020-2766-y","","",,,,,109,54.50,11,22,2,"","",""
19,"Thomas G. Dietterich","Robust artificial intelligence and robust human organizations",2018,"","","","",35,"2022-07-13 09:32:36","","10.1007/s11704-018-8900-4","","",,,,,19,4.75,19,1,4,"","",""
18,"M. Alshamsi, S. Salloum, M. Alshurideh, S. Abdallah","Artificial Intelligence and Blockchain for Transparency in Governance",2020,"","","","",36,"2022-07-13 09:32:36","","10.1007/978-3-030-51920-9_11","","",,,,,18,9.00,5,4,2,"","",""
18,"B. Haibe-Kains, George Adam, A. Hosny, F. Khodakarami, Maqc Society Board, L. Waldron, Bo Wang, C. McIntosh, A. Kundaje, C. Greene, M. M. Hoffman, J. Leek, W. Huber, A. Brazma, Joelle Pineau, R. Tibshirani, T. Hastie, J. Ioannidis, John Quackenbush, H. Aerts","The importance of transparency and reproducibility in artificial intelligence research",2020,"","","","",37,"2022-07-13 09:32:36","","","","",,,,,18,9.00,2,20,2,"In their study, McKinney et al. showed the high potential of artificial intelligence for breast cancer screening. However, the lack of detailed methods and computer code undermines its scientific value. We identify obstacles hindering transparent and reproducible AI research as faced by McKinney et al and provide solutions with implications for the broader field.","",""
13,"B. Koçak, O. Kaya, Çağrı Erdim, Ece Ates Kus, O. Kilickesmez","Artificial Intelligence in Renal Mass Characterization: A Systematic Review of Methodologic Items Related to Modeling, Performance Evaluation, Clinical Utility, and Transparency.",2020,"","","","",38,"2022-07-13 09:32:36","","10.2214/AJR.20.22847","","",,,,,13,6.50,3,5,2,"OBJECTIVE. The objective of our study was to systematically review the literature about the application of artificial intelligence (AI) to renal mass characterization with a focus on the methodologic quality items. MATERIALS AND METHODS. A systematic literature search was conducted using PubMed to identify original research studies about the application of AI to renal mass characterization. Besides baseline study characteristics, a total of 15 methodologic quality items were extracted and evaluated on the basis of the following four main categories: modeling, performance evaluation, clinical utility, and transparency items. The qualitative synthesis was presented using descriptive statistics with an accompanying narrative. RESULTS. Thirty studies were included in this systematic review. Overall, the methodologic quality items were mostly favorable for modeling (63%) and performance evaluation (63%). Even so, the studies (57%) more frequently constructed their work on nonrobust features. Furthermore, only a few studies (10%) had a generalizability assessment with independent or external validation. The studies were mostly unsuccessful in terms of clinical utility evaluation (89%) and transparency (97%) items. For clinical utility, the interesting findings were lack of comparisons with both radiologists' evaluation (87%) and traditional models (70%) in most of the studies. For transparency, most studies (97%) did not share their data with the public. CONCLUSION. To bring AI-based renal mass characterization from research to practice, future studies need to improve modeling and performance evaluation strategies and pay attention to clinical utility and transparency issues.","",""
25,"Stephen Cory Robinson","Trust, transparency, and openness: How inclusion of cultural values shapes Nordic national public policy strategies for artificial intelligence (AI)",2020,"","","","",39,"2022-07-13 09:32:36","","10.1016/j.techsoc.2020.101421","","",,,,,25,12.50,25,1,2,"","",""
0,"Małgorzata Duda-Śmiałek","Transparency of artificial intelligence systems as an expression of the right to be informed",2021,"","","","",40,"2022-07-13 09:32:36","","10.2307/j.ctv282jgff.18","","",,,,,0,0.00,0,1,1,"","",""
7,"Tom van Nuenen, Xavier Ferrer, J. Such, M. Coté","Transparency for Whom? Assessing Discriminatory Artificial Intelligence",2020,"","","","",41,"2022-07-13 09:32:36","","10.1109/MC.2020.3002181","","",,,,,7,3.50,2,4,2,"Artificial intelligence decision making can cause discriminatory harm to many vulnerable groups. Redress is often suggested through increased transparency of these systems. But for what group are we implementing it? This article seeks to identify what transparency means for technical, legislative, and public realities and stakeholders.","",""
5,"Sara B. Jordan, Samantha L. Fenn, Benjamin B. Shannon","Transparency as Threat at the Intersection of Artificial Intelligence and Cyberbiosecurity",2020,"","","","",42,"2022-07-13 09:32:36","","10.1109/MC.2020.2995578","","",,,,,5,2.50,2,3,2,"Have separate actions designed to meet ethical norms of transparency inadvertently led to a situation of greater possible harm from biological warfare attacks? Efforts to make artificial intelligence and data from biological sciences more publicly available have raised novel concerns about national security.","",""
353,"Erico Tjoa, Cuntai Guan","A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI",2019,"","","","",43,"2022-07-13 09:32:36","","10.1109/TNNLS.2020.3027314","","",,,,,353,117.67,177,2,3,"Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.","",""
10,"Z. Xu-Monette, Hongwei H Zhang, Feng Zhu, A. Tzankov, G. Bhagat, C. Visco, K. Dybkaer, A. Chiu, W. Tam, Y. Zu, E. Hsi, Hua You, J. Huh, M. Ponzoni, A. Ferreri, M. Møller, B. Parsons, J. V. van Krieken, M. Piris, J. Winter, F. Hagemeister, B. Shahbaba, I. De Dios, Hong Zhang, Yong Li, Bing Xu, M. Albitar, K. Young","A refined cell-of-origin classifier with targeted NGS and artificial intelligence shows robust predictive value in DLBCL.",2020,"","","","",44,"2022-07-13 09:32:36","","10.1182/bloodadvances.2020001949","","",,,,,10,5.00,1,28,2,"Diffuse large B-cell lymphoma (DLBCL) is a heterogeneous entity of B-cell lymphoma. Cell-of-origin (COO) classification of DLBCL is required in routine practice by the World Health Organization classification for biological and therapeutic insights. Genetic subtypes uncovered recently are based on distinct genetic alterations in DLBCL, which are different from the COO subtypes defined by gene expression signatures of normal B cells retained in DLBCL. We hypothesize that classifiers incorporating both genome-wide gene-expression and pathogenetic variables can improve the therapeutic significance of DLBCL classification. To develop such refined classifiers, we performed targeted RNA sequencing (RNA-Seq) with a commercially available next-generation sequencing (NGS) platform in a large cohort of 418 DLBCLs. Genetic and transcriptional data obtained by RNA-Seq in a single run were explored by state-of-the-art artificial intelligence (AI) to develop a NGS-COO classifier for COO assignment and NGS survival models for clinical outcome prediction. The NGS-COO model built through applying AI in the training set was robust, showing high concordance with COO classification by either Affymetrix GeneChip microarray or the NanoString Lymph2Cx assay in 2 validation sets. Although the NGS-COO model was not trained for clinical outcome, the activated B-cell-like compared with the germinal-center B-cell-like subtype had significantly poorer survival. The NGS survival models stratified 30% high-risk patients in the validation set with poor survival as in the training set. These results demonstrate that targeted RNA-Seq coupled with AI deep learning techniques provides reproducible, efficient, and affordable assays for clinical application. The clinical grade assays and NGS models integrating both genetic and transcriptional factors developed in this study may eventually support precision medicine in DLBCL.","",""
16,"","Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness",2020,"","","","",45,"2022-07-13 09:32:36","","10.1136/bmj.m1312","","",,,,,16,8.00,0,0,2,"","",""
1,"Hayley Carlotto, S. Saperstein, F. Sanchez, Sherriff Balogun, Sears A. Merritt","Improving the Accuracy and Transparency of Underwriting with Artificial Intelligence to Transform the Life-Insurance Industry",2020,"","","","",46,"2022-07-13 09:32:36","","","","",,,,,1,0.50,0,5,2,"Copyright © 2020, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 78 AI MAGAZINE Life insurance is a critical financial tool for millions of households, providing security to families by reducing the financial impact of an untimely death. In the United States alone, life-insurance companies collectively manage trillions of dollars of protection while annually disbursing billions of dollars to beneficiaries; according to the American Council of Life Insurers, at the end of 2018, there was nearly $12.1 trillion of active coverage for individuals and $57 billion in payments to their beneficiaries.1 To support this large-scale financial ecosystem while simultaneously offering affordable prices, insurers must estimate the mortality risk of individual life-insurance applicants through an underwriting process. The accuracy of this underwriting ultimately drives the long-term stability of the life-insurance industry because the collective sum of incoming premiums, which are fixed post-underwriting, must be sufficient to offset future payouts from guaranteed death benefits. Unlike most types of insurance that are renewed and reassessed annually (such as property and health), nearly all life-insurance policies are one-time, long-duration contractual agreements. Thus, the veracity and completeness of health and behavioral data used for mortality-risk assessment is paramount. For the past few decades, life underwriting has been guided by manual review and point-based systems that predominately consider factors independently. Consequently, traditional underwriting limits the degree to which insurers can accurately estimate risk from data and achieve optimal price efficiency of products.  Life insurance provides trillions of dollars of financial security for hundreds of millions of individuals and fami lies worldwide. To simultaneously offer affordable products while managing this financial ecosystem, life-insurance companies use an underwriting process to assess the mortality risk posed by individual applicants. Traditional underwriting is largely based on examining an applicant’s health and behavioral profile. This manual process is incompatible with expectations of a rapid customer experience through digital capabilities. Fortunately, the availability of large historical data sets and the emergence of new data sources provide an unprecedented opportunity for artificial intelligence to transform underwriting in the life-insurance industry with standard measures of mortality risk. We combined one of the largest application data sets in the industry with a responsible artificial intelligence framework to develop a mortality model and life score. We describe how the life score serves as the primary risk-driving engine of deployed algorithmic underwriting systems and demonstrate its high level of accuracy, yielding a nine-percent reduction in claims within the healthiest pool of applicants. Additionally, we argue that, by embracing transparency, the industry can build consumer trust and respond to a dynamic regulatory environment focused on algorithmic decision-making. We present a consumer-facing tool that uses a state-of-the-art method for interpretable machine learning to offer transparency into the life score. Improving the Accuracy and Transparency of Underwriting with Artificial Intelligence to Transform the Life-Insurance Industry","",""
9,"S. M. McKinney, A. Karthikesalingam, Daniel Tse, Christopher J. Kelly, Yun Liu, G. Corrado, S. Shetty","Reply to: Transparency and reproducibility in artificial intelligence.",2020,"","","","",47,"2022-07-13 09:32:36","","10.1038/s41586-020-2767-x","","",,,,,9,4.50,1,7,2,"","",""
86,"H. Felzmann, E. F. Villaronga, C. Lutz, A. Tamó-Larrieux","Transparency you can trust: Transparency requirements for artificial intelligence between legal norms and contextual concerns",2019,"","","","",48,"2022-07-13 09:32:36","","10.1177/2053951719860542","","",,,,,86,28.67,22,4,3,"Transparency is now a fundamental principle for data processing under the General Data Protection Regulation. We explore what this requirement entails for artificial intelligence and automated decision-making systems. We address the topic of transparency in artificial intelligence by integrating legal, social, and ethical aspects. We first investigate the ratio legis of the transparency requirement in the General Data Protection Regulation and its ethical underpinnings, showing its focus on the provision of information and explanation. We then discuss the pitfalls with respect to this requirement by focusing on the significance of contextual and performative factors in the implementation of transparency. We show that human–computer interaction and human-robot interaction literature do not provide clear results with respect to the benefits of transparency for users of artificial intelligence technologies due to the impact of a wide range of contextual factors, including performative aspects. We conclude by integrating the information- and explanation-based approach to transparency with the critical contextual approach, proposing that transparency as required by the General Data Protection Regulation in itself may be insufficient to achieve the positive goals associated with transparency. Instead, we propose to understand transparency relationally, where information provision is conceptualized as communication between technology providers and users, and where assessments of trustworthiness based on contextual factors mediate the value of transparency communications. This relational concept of transparency points to future research directions for the study of transparency in artificial intelligence systems and should be taken into account in policymaking.","",""
51,"Shubham Sharma, Jette Henderson, Joydeep Ghosh","CERTIFAI: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models",2019,"","","","",49,"2022-07-13 09:32:36","","10.1145/3375627.3375812","","",,,,,51,17.00,17,3,3,"As artificial intelligence plays an increasingly important role in our society, there are ethical and moral obligations for both businesses and researchers to ensure that their machine learning models are designed, deployed, and maintained responsibly. These models need to be rigorously audited for fairness, robustness, transparency, and interpretability. A variety of methods have been developed that focus on these issues in isolation, however, managing these methods in conjunction with model development can be cumbersome and timeconsuming. In this paper, we introduce a unified and model-agnostic approach to address these issues: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models (CERTIFAI). Unlike previous methods in this domain, CERTIFAI is a general tool that can be applied to any black-box model and any type of input data. Given a model and an input instance, CERTIFAI uses a custom genetic algorithm to generate counterfactuals: instances close to the input that change the prediction of the model. We demonstrate how these counterfactuals can be used to examine issues of robustness, interpretability, transparency, and fairness. Additionally, we introduce CERScore, the first black-box model robustness score that performs comparably to methods that have access to model internals.","",""
111,"Zhihan Lv, Yang Han, A. Singh, Gunasekaran Manogaran, Haibin Lv","Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",2021,"","","","",50,"2022-07-13 09:32:36","","10.1109/TII.2020.2994747","","",,,,,111,111.00,22,5,1,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","",""
44,"A. Goli, H. Zare, R. Tavakkoli-Moghaddam, A. Sadeghieh","Hybrid artificial intelligence and robust optimization for a multi-objective product portfolio problem Case study: The dairy products industry",2019,"","","","",51,"2022-07-13 09:32:36","","10.1016/j.cie.2019.106090","","",,,,,44,14.67,11,4,3,"","",""
0,"Yanyong Du","On the Transparency of Artificial Intelligence System",2022,"","","","",52,"2022-07-13 09:32:36","","10.32629/jai.v5i1.486","","",,,,,0,0.00,0,1,1,"In order to improve the effectiveness of the management of artificial intelligence system, there is a growing demand for improving the transparency of artificial intelligence system from all parts of society. Improving the transparency of artificial intelligence system is conducive to relevant personnel better assuming their responsibilities and protecting the public’s right to know. Therefore, the principle of transparency appears most frequently in all kinds of ethical principles and ethical guidelines of artificial intelligence, but there are some differences in the definition of its connotation by different subjects. The transparency of artificial intelligence system is reflected in many aspects like algorithm interpretation, data transparency and function transparency. We need to fully understand the limit of artificial intelligence transparency from the perspective of the characteristics of intelligence, the current situation of artificial intelligence technology and the feasibility of technical governance. For the construction path of artificial intelligence system transparency, there are many ways, such as technical approach, ethical and legal regulation and cultural approach.","",""
4,"Kacper Sokol","Fairness, Accountability and Transparency in Artificial Intelligence: A Case Study of Logical Predictive Models",2019,"","","","",53,"2022-07-13 09:32:36","","10.1145/3306618.3314316","","",,,,,4,1.33,4,1,3,"Machine learning -- the part of artificial intelligence aimed at eliciting knowledge from data and automated decision making without explicit instructions -- is making great strides, with new algorithms being invented every day. These algorithms find myriads of applications, but their ubiquity often comes at the expense of limited interpretability, hidden biases and unexpected vulnerabilities. Whenever one of these factors is a priority, the learning algorithm of choice is often a method considered to be inherently interpretable, e.g. logical models such as decision trees. In my research I challenge this assumption and highlight (quite common) cases when the assumed interpretability fails to deliver. To restore interpretability of logical machine learning models (decision trees and their ensembles in particular) I propose to explain them with class-contrastive counterfactual statements, which are a very common type of explanation in human interactions, well-grounded in social science research. To evaluate transparency of such models I collate explainability desiderata that can be used to systematically assess and compare such methods as an addition to user studies. Given contrastive explanations, I investigate their influence on the model's security, in particular gaming and stealing the model. Finally, I evaluate model fairness, where I am interested in choosing the most fair model among all the models with equal performance.","",""
7,"Shlomit Yanisky-Ravid Sean K. Hallisey","“Equality and Privacy by Design”: A New Model of Artificial Intelligence Data Transparency via Auditing, Certification, and Safe Harbor Regimes",2019,"","","","",54,"2022-07-13 09:32:36","","","","",,,,,7,2.33,7,1,3,"Artificial Intelligence and Machine Learning (AI) are often described as technological breakthroughs that will completely transform our society and economy. AI systems have been implemented everywhere, from medicine, transportation, finance, art, to legal and social spheres, and even in weapons development. In many sectors, AI systems have already started making decisions previously made by humans. Promising as AI systems may be, they also pose urgent challenges to our everyday life. While much attention has concerned AI’s legal implications, the literature suffers from a lack of solutions that account for both legal and engineering practices and constraints. This leaves technology firms without * Professor Shlomit Yanisky-Ravid, Ph.D., Fordham Law School, Visiting Professor; Fordham Law Center on Law and Information Policy (CLIP), Head of AI-IP and Blockchain Project; Yale Law School, Information Society Project (ISP), Fellow; Ono Law School, Israel, Senior Faculty, the Shalom Comparative Legal Research Institute, OAC, Founder and Academic Director. Sean K. Hallisey, Fordham Law, CLIP, AI-IP Project, Fellow. We gratefully dedicate this Article to Joel Reidenberg, the founder and the head of Fordham Law Center of Law and Information Policy (CLIP), for his initiative, support, and encouragement, all of which tremendously contributed to the writing of this Article and the development of its ideas. We would also like to thank all the Fellows at the Fordham CLIP IP-AI and Blockchain Project, Yale Law, ISP, as well as to the students of the course “Intellectual Property and the Challenges of Advanced Technology: AI and Blockchain,” for their wonderful discussions, insights and comments. Finally, we thank Dean Matthew Diller, Fordham Law School, for promoting and stressing the challenges of advanced technology, data privacy, and intellectual property, and Linda Sugin, Associate Dean for Academic Affairs at Fordham Law, for her support. 2019] FORDHAM URB. L.J. 429 guidelines and increases the risk of societal harm. It also means that policymakers and judges operate without a regulatory regime to turn to when addressing these novel and unpredictable outcomes. This Article tries to fill the void by focusing on data rather than on the software and programmers. It suggests a new model that stems from a recognition of the significant role that the data plays in the development and functioning of AI systems. Data is the most important aspect of teaching AI systems to operate. AI algorithms begin with a massive preexisting dataset, which data providers use to train the system. But the data that AI systems “swallow” can be illegal, discriminatory, altered, unreliable, or simply incomplete. Thus, the more data fed to the AI systems, the higher the likelihood that they could produce biased, discriminatory decisions and violate privacy rights. The Article discusses how discrimination can arise, even inadvertently, from the operation of “trusted” and “objective” AI systems. To address this problem, this Article proposes a new AI Data Transparency Model that focuses on disclosure of data rather than, as some scholars argue, focusing on the initial software program and programmers. The Model includes an auditing regime and a certification program, run either by a governmental body or, in the absence of such entity, by private institutions. This Model will encourage the industry to take proactive steps to ensure and publicize that datasets are trustworthy. The suggested Model includes a safe harbor, which incentivizes firms to implement transparency recommendations even without massive regulatory oversight. From an engineering point of view, the Model recognizes data providers and big data as the most important components in the process of creating, training and operating AI systems. Even more importantly, the Model is technologically feasible because data can be easily absorbed and kept by a technological tool. Further, this Model is also practically feasible because it follows already existing legal frameworks of data transparency, such as the ones being implemented by the FDA and the SEC. Improving transparency in data systems would result in less harmful AI systems, better protect societal rights and norms, and produce improved outcomes in this emerging field, especially for minority communities that often lack resources or representation to challenge AI systems. Increased transparency of the data used while developing, training or operating AI systems would mitigate and reduce these harms. Additionally, to better identify the risks of faulty data, industry players must conduct critical evaluations and audits of the data used to train AI systems; one way to incentivize this is a 430 FORDHAM URB. L.J. [Vol. XLVI certification system to publicize good-faith efforts to reduce the possibility of discriminatory outcomes and privacy violations in AI systems. This Article strives to incentivize the creation of new standards, which the industry could implement from the genesis of AI systems to mitigate the possibility of harm, rather than post-hoc assignments of liability.","",""
1,"Jeanna Neefe Matthews","Patterns and Antipatterns, Principles, and Pitfalls: Accountability and Transparency in Artificial Intelligence",2019,"","","","",55,"2022-07-13 09:32:36","","","","",,,,,1,0.33,1,1,3,"Copyright © 2020, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 82 AI MAGAZINE Increasingly, decisions that significantly impact the lives of individuals (such as decisions about hiring, housing, insurance, loans, criminal justice, or medical treatment) are being made in a partnership between human decisionmakers and artificial intelligence (AI) systems. As builders of AI systems, we know how easy it is for errors to occur. We also know how difficult it can be to push the boundaries and adapt a system developed in one context into another. As developers of AI, we know how our systems learn from people and from the past, assimilating latent biases. Understanding all of this, who better than us to insist that the systems we build support investigation and iterative improvement, so that others are empowered to help counter the limitations of AI while benefiting from its strengths?  This article discusses a set of principles for accountability and transparency in AI as well as a set of antipatterns or harmful trends too often seen in deployed systems. It provides concrete suggestions for what can be done to shift the balance away from these antipatterns and toward more positive ones. Patterns and Antipatterns, Principles, and Pitfalls: Accountability and Transparency in Artificial Intelligence","",""
11,"T. Wischmeyer","Artificial Intelligence and Transparency: Opening the Black Box",2019,"","","","",56,"2022-07-13 09:32:36","","10.1007/978-3-030-32361-5_4","","",,,,,11,3.67,11,1,3,"","",""
16,"Vikram Puri, A. Kataria, Vishal Sharma","Artificial intelligence‐powered decentralized framework for Internet of Things in Healthcare 4.0",2021,"","","","",57,"2022-07-13 09:32:36","","10.1002/ETT.4245","","",,,,,16,16.00,5,3,1,"Correspondence to: Vishal Sharma, School of Electronics, Electrical Engineering and Computer Science (EEECS), Queen’s University Belfast, Belfast, UK. Email:vishal_sharma2012@hotmail.com Abstract Remote patient monitoring and data management have gained much popularity in recent years because of their enhanced access to low-cost healthcare services. A cloud-based healthcare system provides numerous solutions for collecting patient data and offers on-demand well-managed reports to patients and healthcare providers. However, it equally suffers from single-point failure, security, privacy, and non-transparency issues with the data, impacting the continuity of the system. To resolve such concerns, this article proposes an artificial intelligence (AI)-enabled decentralized healthcare framework that accesses and authenticates Internet of Things (IoT) devices and create trust and transparency in patient healthcare records (PHR). The mechanism is based on the AI-enabled smart contracts and the conceptualization of the public blockchain network. Alongside this, the framework identifies the malicious IoT nodes in the system. The experimental analyses are performed on the real-time test environment, and significant improvements are suggested in terms of device energy consumption, data request time, throughput, average latency, and transaction fee.","",""
19,"V. Sounderajah, H. Ashrafian, R. Golub, S. Shetty, Jeffrey De Fauw, L. Hooft, K. Moons, G. Collins, D. Moher, P. Bossuyt, A. Darzi, A. Karthikesalingam, A. Denniston, B. Mateen, D. Ting, D. Treanor, Dominic King, F. Greaves, Jonathan Godwin, J. Pearson-Stuttard, L. Harling, M. McInnes, Nader Rifai, Nenad Tomašev, P. Normahani, P. Whiting, R. Aggarwal, S. Vollmer, S. Markar, T. Panch, Xiaoxuan Liu","Developing a reporting guideline for artificial intelligence-centred diagnostic test accuracy studies: the STARD-AI protocol",2021,"","","","",58,"2022-07-13 09:32:36","","10.1136/bmjopen-2020-047709","","",,,,,19,19.00,2,31,1,"Introduction Standards for Reporting of Diagnostic Accuracy Study (STARD) was developed to improve the completeness and transparency of reporting in studies investigating diagnostic test accuracy. However, its current form, STARD 2015 does not address the issues and challenges raised by artificial intelligence (AI)-centred interventions. As such, we propose an AI-specific version of the STARD checklist (STARD-AI), which focuses on the reporting of AI diagnostic test accuracy studies. This paper describes the methods that will be used to develop STARD-AI. Methods and analysis The development of the STARD-AI checklist can be distilled into six stages. (1) A project organisation phase has been undertaken, during which a Project Team and a Steering Committee were established; (2) An item generation process has been completed following a literature review, a patient and public involvement and engagement exercise and an online scoping survey of international experts; (3) A three-round modified Delphi consensus methodology is underway, which will culminate in a teleconference consensus meeting of experts; (4) Thereafter, the Project Team will draft the initial STARD-AI checklist and the accompanying documents; (5) A piloting phase among expert users will be undertaken to identify items which are either unclear or missing. This process, consisting of surveys and semistructured interviews, will contribute towards the explanation and elaboration document and (6) On finalisation of the manuscripts, the group’s efforts turn towards an organised dissemination and implementation strategy to maximise end-user adoption. Ethics and dissemination Ethical approval has been granted by the Joint Research Compliance Office at Imperial College London (reference number: 19IC5679). A dissemination strategy will be aimed towards five groups of stakeholders: (1) academia, (2) policy, (3) guidelines and regulation, (4) industry and (5) public and non-specific stakeholders. We anticipate that dissemination will take place in Q3 of 2021.","",""
1,"Ana Lucic, Maurits Bleeker, Sami Jullien, Samarth Bhargav, M. de Rijke","Reproducibility as a Mechanism for Teaching Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence",2021,"","","","",59,"2022-07-13 09:32:36","","10.1609/aaai.v36i11.21558","","",,,,,1,1.00,0,5,1,"In this work, we explain the setup for a technical, graduate-level course on Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence (FACT-AI) at the University of Amsterdam, which teaches FACT-AI concepts through the lens of reproducibility.  The focal point of the course is a group project based on reproducing existing FACT-AI algorithms from top AI conferences and writing a corresponding report.  In the first iteration of the course, we created an open source repository with the code implementations from the group projects.  In the second iteration, we encouraged students to submit their group projects to the Machine Learning Reproducibility Challenge, resulting in 9 reports from our course being accepted for publication in the ReScience journal.  We reflect on our experience teaching the course over two years, where one year coincided with a global pandemic, and propose guidelines for teaching FACT-AI through reproducibility in graduate-level AI study programs.  We hope this can be a useful resource for instructors who want to set up similar courses in the future.","",""
16,"A. Amritphale, Ranojoy Chatterjee, Suvo Chatterjee, N. Amritphale, Ali Rahnavard, G. Awan, B. Omar, G. Fonarow","Predictors of 30-Day Unplanned Readmission After Carotid Artery Stenting Using Artificial Intelligence",2021,"","","","",60,"2022-07-13 09:32:36","","10.1007/s12325-021-01709-7","","",,,,,16,16.00,2,8,1,"","",""
0,"Y. Hayashi","Toward the transparency of deep learning in radiological imaging: beyond quantitative to qualitative artificial intelligence",2019,"","","","",61,"2022-07-13 09:32:36","","10.21037/jmai.2019.09.0","","",,,,,0,0.00,0,1,3,"In the near future, nearly every type of clinician, from paramedics to certificated medical specialists, will be expected to utilize artificial intelligence (AI) technology, and deep learning (DL) in particular (1). In terms of exceeding human ability, DL has been the backbone of computer science. DL mostly involves automated feature extraction using deep neural networks (DNNs), which can aid in the classification and discrimination of medical images, including mammograms, skin lesions, pathological slides, radiological images, and retinal fundus photographs.","",""
17,"Yi-Shan Lin, Wen-Chuan Lee, Z. B. Celik","What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors",2020,"","","","",62,"2022-07-13 09:32:36","","10.1145/3447548.3467213","","",,,,,17,8.50,6,3,2,"EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the input parts deemed important to arrive at a decision for a specific target. However, it remains challenging to quantify the correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for the systematic evaluation of explanations that an XAI method generates. We evaluate seven state-of-the-art model-free and model-specific post-hoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We found six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region. We made our code available at https://github.com/yslin013/evalxai.","",""
1793,"E. Topol","High-performance medicine: the convergence of human and artificial intelligence",2019,"","","","",63,"2022-07-13 09:32:36","","10.1038/s41591-018-0300-7","","",,,,,1793,597.67,1793,1,3,"","",""
14,"A. Rosenfeld","Better Metrics for Evaluating Explainable Artificial Intelligence",2021,"","","","",64,"2022-07-13 09:32:36","","10.5555/3463952.3463962","","",,,,,14,14.00,14,1,1,"This paper presents objective metrics for how explainable artificial intelligence (XAI) can be quantified. Through an overview of current trends, we show that many explanations are generated post-hoc and independent of the agent’s logical process, which in turn creates explanations with limited meaning as they lack transparency and fidelity. While user studies are a known basis for evaluating XAI, studies that do not consider objective metrics for evaluating XAI may have limited meaning and may suffer from confirmation bias, particularly if they use low fidelity explanations unnecessarily. To avoid this issue, this paper suggests a paradigm shift in evaluating XAI that focuses on metrics that quantify the explanation itself and its appropriateness given the XAI goal. We suggest four such metrics based on performance differences, D, between the explanation’s logic and the agent’s actual performance, the number of rules, R, outputted by the explanation, the number of features, F , used to generate that explanation, and the stability, S, of the explanation. We believe that user studies that focus on these metrics in their evaluations are inherently more valid and should be integrated in future XAI research.","",""
14,"S. Modgil, R. Singh, C. Hannibal","Artificial intelligence for supply chain resilience: learning from Covid-19",2021,"","","","",65,"2022-07-13 09:32:36","","10.1108/ijlm-02-2021-0094","","",,,,,14,14.00,5,3,1,"PurposeMany supply chains have faced disruption during Covid-19. Artificial intelligence (AI) is one mechanism that can be used to improve supply chain resilience by developing business continuity capabilities. This study examines how firms employ AI and consider the opportunities for AI to enhance supply chain resilience by developing visibility, risk, sourcing and distribution capabilities.Design/methodology/approachThe authors have gathered rich data by conducting semistructured interviews with 35 experts from the e-commerce supply chain. The authors have adopted a systematic approach of coding using open, axial and selective methods to map and identify the themes that represent the critical elements of AI-enabled supply chain resilience.FindingsThe results of the study highlight the emergence of five critical areas where AI can contribute to enhanced supply chain resilience; (1) transparency, (2) ensuring last-mile delivery, (3) offering personalized solutions to both upstream and downstream supply chain stakeholders, (4) minimizing the impact of disruption and (5) facilitating an agile procurement strategy.Research limitations/implicationsThe study offers interesting implications for bridging the theory–practice gap by drawing on contemporary empirical data to demonstrate how enhancing dynamic capabilities via AI technologies further strengthens supply chain resilience. The study also offers suggestions for utilizing the findings and proposes a framework to strengthen supply chain resilience through AI.Originality/valueThe study presents the dynamic capabilities for supply chain resilience through the employment of AI. AI can contribute to readying supply chains to reduce their risk of disruption through enhanced resilience.","",""
411,"R. Vinuesa, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, S. Domisch, Anna Felländer, S. Langhans, Max Tegmark, F. F. Nerini","The role of artificial intelligence in achieving the Sustainable Development Goals",2019,"","","","",66,"2022-07-13 09:32:36","","10.1038/s41467-019-14108-y","","",,,,,411,137.00,41,10,3,"","",""
195,"Jessica Fjeld, Nele Achten, Hannah Hilligoss, Ádám Nagy, Madhulika Srikumar","Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI",2020,"","","","",67,"2022-07-13 09:32:36","","10.2139/ssrn.3518482","","",,,,,195,97.50,39,5,2,"The rapid spread of artificial intelligence (AI) systems has precipitated a rise in ethical and human rights-based frameworks intended to guide the development and use of these technologies. Despite the proliferation of these ""AI principles,"" there has been little scholarly focus on understanding these efforts either individually or as contextualized within an expanding universe of principles with discernible trends.    To that end, this white paper and its associated data visualization compare the contents of thirty-six prominent AI principles documents side-by-side. This effort uncovered a growing consensus around eight key thematic trends: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. Underlying this “normative core,” our analysis examined the forty-seven individual principles that make up the themes, detailing notable similarities and differences in interpretation found across the documents. In sharing these observations, it is our hope that policymakers, advocates, scholars, and others working to maximize the benefits and minimize the harms of AI will be better positioned to build on existing efforts and to push the fractured, global conversation on the future of AI toward consensus.","",""
159,"Xiaoxuan Liu, S. C. Rivera, D. Moher, M. Calvert, A. Denniston, H. Ashrafian, A. Beam, A. Chan, G. Collins, A. Darzi, J. Deeks, M. Elzarrad, Cyrus Espinoza, Andre Esteva, L. Faes, L. Ferrante di Ruffano, J. Fletcher, R. Golub, H. Harvey, C. Haug, Christopher Holmes, Adrian Jonas, P. Keane, Christopher J. Kelly, Aaron Y. Lee, Cecilia S Lee, Elaine Manna, J. Matcham, Melissa D. McCradden, Joao Monteiro, C. Mulrow, L. Oakden-Rayner, D. Paltoo, M. Panico, G. Price, Samuel d. Rowley, Richard Savage, Rupa Sarkar, S. Vollmer, C. Yau","Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI Extension",2020,"","","","",68,"2022-07-13 09:32:36","","10.1136/bmj.m3164","","",,,,,159,79.50,16,40,2,"Abstract The CONSORT 2010 (Consolidated Standards of Reporting Trials) statement provides minimum guidelines for reporting randomised trials. Its widespread use has been instrumental in ensuring transparency when evaluating new interventions. More recently, there has been a growing recognition that interventions involving artificial intelligence (AI) need to undergo rigorous, prospective evaluation to demonstrate impact on health outcomes. The CONSORT-AI extension is a new reporting guideline for clinical trials evaluating interventions with an AI component. It was developed in parallel with its companion statement for clinical trial protocols: SPIRIT-AI. Both guidelines were developed through a staged consensus process, involving a literature review and expert consultation to generate 29 candidate items, which were assessed by an international multi-stakeholder group in a two-stage Delphi survey (103 stakeholders), agreed on in a two-day consensus meeting (31 stakeholders) and refined through a checklist pilot (34 participants). The CONSORT-AI extension includes 14 new items, which were considered sufficiently important for AI interventions, that they should be routinely reported in addition to the core CONSORT 2010 items. CONSORT-AI recommends that investigators provide clear descriptions of the AI intervention, including instructions and skills required for use, the setting in which the AI intervention is integrated, the handling of inputs and outputs of the AI intervention, the human-AI interaction and providing analysis of error cases. CONSORT-AI will help promote transparency and completeness in reporting clinical trials for AI interventions. It will assist editors and peer-reviewers, as well as the general readership, to understand, interpret and critically appraise the quality of clinical trial design and risk of bias in the reported outcomes.","",""
12,"H. Ibrahim, Xiaoxuan Liu, S. C. Rivera, D. Moher, A. Chan, M. Sydes, M. Calvert, A. Denniston","Reporting guidelines for clinical trials of artificial intelligence interventions: the SPIRIT-AI and CONSORT-AI guidelines",2021,"","","","",69,"2022-07-13 09:32:36","","10.1186/s13063-020-04951-6","","",,,,,12,12.00,2,8,1,"","",""
10,"R. Berk","Artificial Intelligence, Predictive Policing, and Risk Assessment for Law Enforcement",2021,"","","","",70,"2022-07-13 09:32:36","","10.1146/annurev-criminol-051520-012342","","",,,,,10,10.00,10,1,1,"There are widespread concerns about the use of artificial intelligence in law enforcement. Predictive policing and risk assessment are salient examples. Worries include the accuracy of forecasts that guide both activities, the prospect of bias, and an apparent lack of operational transparency. Nearly breathless media coverage of artificial intelligence helps shape the narrative. In this review, we address these issues by first unpacking depictions of artificial intelligence. Its use in predictive policing to forecast crimes in time and space is largely an exercise in spatial statistics that in principle can make policing more effective and more surgical. Its use in criminal justice risk assessment to forecast who will commit crimes is largely an exercise in adaptive, nonparametric regression. It can in principle allow law enforcement agencies to better provide for public safety with the least restrictive means necessary, which can mean far less use of incarceration. None of this is mysterious. Nevertheless, concerns about accuracy, fairness, and transparency are real, and there are tradeoffs between them for which there can be no technical fix. You can't have it all. Solutions will be found through political and legislative processes achieving an acceptable balance between competing priorities.","",""
10,"T. Penzkofer, A. Padhani, B. Turkbey, M. Haider, H. Huisman, J. Walz, G. Salomon, I. Schoots, J. Richenberg, G. Villeirs, V. Panebianco, O. Rouvière, V. Løgager, J. Barentsz","ESUR/ESUI position paper: developing artificial intelligence for precision diagnosis of prostate cancer using magnetic resonance imaging",2021,"","","","",71,"2022-07-13 09:32:36","","10.1007/s00330-021-08021-6","","",,,,,10,10.00,1,14,1,"","",""
7,"S. Yanisky-Ravid, Sean Hallisey","‘Equality and Privacy by Design’: Ensuring Artificial Intelligence (AI) Is Properly Trained & Fed: A New Model of AI Data Transparency & Certification As Safe Harbor Procedures",2018,"","","","",72,"2022-07-13 09:32:36","","10.2139/SSRN.3278490","","",,,,,7,1.75,4,2,4,"Artificial Intelligence systems (“AI”) are often described as a technological breakthrough that will completely transform our society and economy. AI systems have been implemented in all facets of the economy, from medicine to transportation, finance, art, legal, social, and weapons; making decisions previously determined by humans. While this article recognizes that AI systems promise benefits, it also identifies urgent challenges to our everyday life. Just as the technology has become prolific, so has the literature concerning its legal implications. However, the literature suffers from a lack of solutions that address the legal and engineering perspectives. This leaves technology firms without guidelines and increases the risk of societal harm. Policymakers, including judges, operate without a regulatory regime to turn to when addressing these novel and unpredictable outcomes. This article tries to fill the void by focusing on the use of data by these systems, rather than on the software and software programmers. It suggests a new Model that stems from a recognition of the significant role that the data plays in the development and functioning of AI systems.    One of the most important phases of teaching AI systems to operate starts with a preexisting massive dataset that the data providers use to train the system. The data providers are programmers, trainers; the stakeholders who enable access to data or the systems’ users. In this article, we analyze and discuss the threats the use of data by AI systems pose in terms of producing discriminatory outcomes as well as violations of privacy.    The data can be illegal, discriminatory, manufacture, unreliable, or simply incomplete. The more data that AI systems “swallow,” the likelihood increases that AI systems could produce biased, discriminatory decisions and/or violate privacy. The article discusses how discrimination can arise, even inadvertently, from the operation of “trusted” and ""objective"" AI systems. The article addresses, on the one hand, the hurdles and challenges behind the use of big data by AI systems, and on the other, suggests a possible, new solution.    We propose a new AI data transparency Model that focuses on disclosure of the data being used by AI systems, when necessary. To perfect the Model we recommend an auditing regime and a certification program, either by governmental body or, in the absence of such entity, by private institutes. This Model will encourage the industry to take steps, proactively, to ensure that the dataset is trustworthy and then, to publicly exhibit the quality of the data (that their AI systems rely on). By receiving and publicizing a quality “stamp” the firms will fairly build their competitive reputation and will strengthen the public control of the systems.    We envision that the implementation of this Model will help firms and individuals become educated about the potential issues concerning AI, discrimination and the continued weakening of societal expectations of privacy. In this sense, the AI data transparency Model operates as a safe harbor mechanism that incentivizes the industry, from the first steps of developing and training AI systems, to the actual operation of the AI systems, to implement effective standards, that we coin Equality and Privacy by Design.    The suggested AI Transparency Model functions as a safe harbor, even without massive regulatory steps. From an engineering point of view, not only does the model recognize the data providers and the big data as the most important components in the process of creating, training and operating AI systems, but the AI Data Transparency Model is also technologically feasible as data can be easily absorbed and kept by a technological tool. This Model is feasible from a practical perspective, as it follows already existing legal frameworks of data transparency, such as the ones being implemented by the FDA and SEC.    We argue that improving transparency in data systems should result in less harmful AI systems, better protect societal rights and norms, and produce improved outcomes in this emerging field, specifically for minority communities, who often lack resources or representation to combat the use of AI systems. We assert that improvements in transparency regarding the data used while developing, training or operating AI systems could mitigate and reduce these harms. We recommend critical evaluations and audits of data used to train AI systems to identify such risks, and propose a certification system whereby AI systems can publicize good faith efforts to reduce the possibility of discriminatory outcomes and privacy violations. We do not purport to solve the riddle of every possible negative outcome created by AI systems; instead, we are trying to incentivize the creation of new standards that the industry could implement, from day one of developing AI systems that addresses the possibility of harm, rather than post-hoc assignments of liability.","",""
2,"B. Nair, Yakov Diskin, V. Asari","Multi-modal low cost mobile indoor surveillance system on the Robust Artificial Intelligence-based Defense Electro Robot (RAIDER)",2012,"","","","",73,"2022-07-13 09:32:36","","10.1117/12.930353","","",,,,,2,0.20,1,3,10,"We present an autonomous system capable of performing security check routines. The surveillance machine, the Clearpath Husky robotic platform, is equipped with three IP cameras with different orientations for the surveillance tasks of face recognition, human activity recognition, autonomous navigation and 3D reconstruction of its environment. Combining the computer vision algorithms onto a robotic machine has given birth to the Robust Artificial Intelligencebased Defense Electro-Robot (RAIDER). The end purpose of the RAIDER is to conduct a patrolling routine on a single floor of a building several times a day. As the RAIDER travels down the corridors off-line algorithms use two of the RAIDER's side mounted cameras to perform a 3D reconstruction from monocular vision technique that updates a 3D model to the most current state of the indoor environment. Using frames from the front mounted camera, positioned at the human eye level, the system performs face recognition with real time training of unknown subjects. Human activity recognition algorithm will also be implemented in which each detected person is assigned to a set of action classes picked to classify ordinary and harmful student activities in a hallway setting.The system is designed to detect changes and irregularities within an environment as well as familiarize with regular faces and actions to distinguish potentially dangerous behavior. In this paper, we present the various algorithms and their modifications which when implemented on the RAIDER serves the purpose of indoor surveillance.","",""
462,"Stuart J. Russell, Dan Dewey, Max Tegmark","Research Priorities for Robust and Beneficial Artificial Intelligence",2015,"","","","",74,"2022-07-13 09:32:36","","10.1609/aimag.v36i4.2577","","",,,,,462,66.00,154,3,7,"Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.","",""
9,"B. N. Manjunatha Reddy, S. K. Pramada, T. Roshni","Monthly surface runoff prediction using artificial intelligence: A study from a tropical climate river basin",2021,"","","","",75,"2022-07-13 09:32:36","","10.1007/s12040-020-01508-8","","",,,,,9,9.00,3,3,1,"","",""
9,"Nathalie A. Smuha, Emma Ahmed-Rengers, Adam Harkens, Wenlong Li, J. Maclaren, Riccardo Piselli, K. Yeung","How the EU Can Achieve Legally Trustworthy AI: A Response to the European Commission’s Proposal for an Artificial Intelligence Act",2021,"","","","",76,"2022-07-13 09:32:36","","10.2139/ssrn.3899991","","",,,,,9,9.00,1,7,1,"This document contains the response to the European Commission’s Proposal for an Artificial Intelligence Act from members of the Legal, Ethical & Accountable Digital Society (LEADS) Lab at the University of Birmingham. The Proposal seeks to give expression to the concept of ‘Lawful AI.’ This concept was mentioned, but not developed in the Commission’s High-Level Expert Group on AI’s Ethics Guidelines for Trustworthy AI (2019), which instead confined its discussion to the concepts of ‘Ethical’ and ‘Robust’ AI. After a brief introduction (Chapter 1), we set out the many aspects of the Proposal which we welcome, and stress our wholehearted support for its aim to protect fundamental rights (Chapter 2). Subsequently, we develop the concept of ‘Legally Trustworthy AI,’ arguing that it should be grounded in respect for three pillars on which contemporary liberal democratic societies are founded, namely: fundamental rights, the rule of law, and democracy (Chapter 3). Drawing on this conceptual framework, we first argue that the Proposal fails to reflect fundamental rights as claims with enhanced moral and legal status, which subjects any rights interventions to a demanding regime of scrutiny and must satisfy tests of necessity and proportionality. Moreover, the Proposal does not always accurately recognise the wrongs and harms associated with different kinds of AI systems and appropriately allocates responsibility for them. Second, the Proposal does not provide an effective framework for the enforcement of legal rights and duties, and does not ensure legal certainty and consistency, which are essential for the rule of law. Third, the Proposal neglects to ensure meaningful transparency, accountability, and rights of public participation, thereby failing to reflect adequate protection for democracy (Chapter 4). Based on these shortcomings in respecting and promoting the three pillars of Legally Trustworthy AI, we provide detailed recommendations for the Proposal’s revision (Chapter 5).","",""
63,"M. VerMilyea, J. Hall, S. Diakiw, A. Johnston, T. Nguyen, D. Perugini, A. Miller, A. Picou, A. P. Murphy, M. Perugini","Development of an artificial intelligence-based assessment model for prediction of embryo viability using static images captured by optical light microscopy during IVF",2020,"","","","",77,"2022-07-13 09:32:36","","10.1093/humrep/deaa013","","",,,,,63,31.50,6,10,2,"Abstract STUDY QUESTION Can an artificial intelligence (AI)-based model predict human embryo viability using images captured by optical light microscopy? SUMMARY ANSWER We have combined computer vision image processing methods and deep learning techniques to create the non-invasive Life Whisperer AI model for robust prediction of embryo viability, as measured by clinical pregnancy outcome, using single static images of Day 5 blastocysts obtained from standard optical light microscope systems. WHAT IS KNOWN ALREADY Embryo selection following IVF is a critical factor in determining the success of ensuing pregnancy. Traditional morphokinetic grading by trained embryologists can be subjective and variable, and other complementary techniques, such as time-lapse imaging, require costly equipment and have not reliably demonstrated predictive ability for the endpoint of clinical pregnancy. AI methods are being investigated as a promising means for improving embryo selection and predicting implantation and pregnancy outcomes. STUDY DESIGN, SIZE, DURATION These studies involved analysis of retrospectively collected data including standard optical light microscope images and clinical outcomes of 8886 embryos from 11 different IVF clinics, across three different countries, between 2011 and 2018. PARTICIPANTS/MATERIALS, SETTING, METHODS The AI-based model was trained using static two-dimensional optical light microscope images with known clinical pregnancy outcome as measured by fetal heartbeat to provide a confidence score for prediction of pregnancy. Predictive accuracy was determined by evaluating sensitivity, specificity and overall weighted accuracy, and was visualized using histograms of the distributions of predictions. Comparison to embryologists’ predictive accuracy was performed using a binary classification approach and a 5-band ranking comparison. MAIN RESULTS AND THE ROLE OF CHANCE The Life Whisperer AI model showed a sensitivity of 70.1% for viable embryos while maintaining a specificity of 60.5% for non-viable embryos across three independent blind test sets from different clinics. The weighted overall accuracy in each blind test set was >63%, with a combined accuracy of 64.3% across both viable and non-viable embryos, demonstrating model robustness and generalizability beyond the result expected from chance. Distributions of predictions showed clear separation of correctly and incorrectly classified embryos. Binary comparison of viable/non-viable embryo classification demonstrated an improvement of 24.7% over embryologists’ accuracy (P = 0.047, n = 2, Student’s t test), and 5-band ranking comparison demonstrated an improvement of 42.0% over embryologists (P = 0.028, n = 2, Student’s t test). LIMITATIONS, REASONS FOR CAUTION The AI model developed here is limited to analysis of Day 5 embryos; therefore, further evaluation or modification of the model is needed to incorporate information from different time points. The endpoint described is clinical pregnancy as measured by fetal heartbeat, and this does not indicate the probability of live birth. The current investigation was performed with retrospectively collected data, and hence it will be of importance to collect data prospectively to assess real-world use of the AI model. WIDER IMPLICATIONS OF THE FINDINGS These studies demonstrated an improved predictive ability for evaluation of embryo viability when compared with embryologists’ traditional morphokinetic grading methods. The superior accuracy of the Life Whisperer AI model could lead to improved pregnancy success rates in IVF when used in a clinical setting. It could also potentially assist in standardization of embryo selection methods across multiple clinical environments, while eliminating the need for complex time-lapse imaging equipment. Finally, the cloud-based software application used to apply the Life Whisperer AI model in clinical practice makes it broadly applicable and globally scalable to IVF clinics worldwide. STUDY FUNDING/COMPETING INTEREST(S) Life Whisperer Diagnostics, Pty Ltd is a wholly owned subsidiary of the parent company, Presagen Pty Ltd. Funding for the study was provided by Presagen with grant funding received from the South Australian Government: Research, Commercialisation and Startup Fund (RCSF). ‘In kind’ support and embryology expertise to guide algorithm development were provided by Ovation Fertility. J.M.M.H., D.P. and M.P. are co-owners of Life Whisperer and Presagen. Presagen has filed a provisional patent for the technology described in this manuscript (52985P pending). A.P.M. owns stock in Life Whisperer, and S.M.D., A.J., T.N. and A.P.M. are employees of Life Whisperer.","",""
10,"A. C. Horta, A. Silva, C. Sargo, V. M. Gonçalves, T. C. Zangirolami, Roberto Campos Giordano","Robust artificial intelligence tool for automatic start-up of the supplementary medium feeding in recombinant E. coli cultivations",2011,"","","","",78,"2022-07-13 09:32:36","","10.1007/s00449-011-0540-0","","",,,,,10,0.91,2,6,11,"","",""
61,"S. Gerke, T. Minssen, Glenn Cohen","Ethical and legal challenges of artificial intelligence-driven healthcare",2020,"","","","",79,"2022-07-13 09:32:36","","10.2139/ssrn.3570129","","",,,,,61,30.50,20,3,2,"  Abstract    This chapter will map the ethical and legal challenges posed by artificial intelligence (AI) in healthcare and suggest directions for resolving them. Section 1 will briefly clarify what AI is and Section 2 will give an idea of the trends and strategies in the United States (US) and Europe, thereby tailoring the discussion to the ethical and legal debate of AI-driven healthcare. This will be followed in Section 3 by a discussion of four primary ethical challenges, namely, (1) informed consent to use, (2) safety and transparency, (3) algorithmic fairness and biases, and (4) data privacy. Section 4 will then analyze five legal challenges in the US and Europe: (1) safety and effectiveness, (2) liability, (3) data protection and privacy, (4) cybersecurity, and (5) intellectual property law. Finally, Section 5 will summarize the major conclusions and especially emphasize the importance of building an AI-driven healthcare system that is successful and promotes trust and the motto Health AIs for All of Us.   ","",""
132,"Xiaoxuan Liu, Samantha Cruz Rivera, D. Moher, M. Calvert, A. Denniston","Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI extension",2020,"","","","",80,"2022-07-13 09:32:36","","10.1038/s41591-020-1034-x","","",,,,,132,66.00,26,5,2,"","",""
8,"S. Atakishiyev, H. Babiker, Nawshad Farruque, R. Goebel1, M-Y. Kima, M. H. Motallebi, J. Rabelo, T. Syed, Osmar R Zaiane","A multi-component framework for the analysis and design of explainable artificial intelligence",2020,"","","","",81,"2022-07-13 09:32:36","","10.3390/make3040045","","",,,,,8,4.00,1,9,2,"The rapid growth of research in explainable artificial intelligence (XAI) follows on two substantial developments. First, the enormous application success of modern machine learning methods, especially deep and reinforcement learning, which have created high expectations for industrial, commercial and social value. Second, the emergence of concern for creating trusted AI systems, including the creation of regulatory principles to ensure transparency and trust of AI systems.These two threads have created a kind of ""perfect storm"" of research activity, all eager to create and deliver it any set of tools and techniques to address the XAI demand. As some surveys of current XAI suggest, there is yet to appear a principled framework that respects the literature of explainability in the history of science, and which provides a basis for the development of a framework for transparent XAI. Here we intend to provide a strategic inventory of XAI requirements, demonstrate their connection to a history of XAI ideas, and synthesize those ideas into a simple framework to calibrate five successive levels of XAI.","",""
8,"Akshat Pandey, Aylin Caliskan","Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy's Price Discrimination Algorithms",2020,"","","","",82,"2022-07-13 09:32:36","","10.1145/3461702.3462561","","",,,,,8,4.00,4,2,2,"Ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trip's fare pricing with automated algorithms that rely on artificial intelligence (AI). This type of AI algorithm, namely a price discrimination algorithm, is widely used in the industry's black box systems for dynamic individualized pricing. Lacking transparency, studying such AI systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms. Recently, in an effort to enhance transparency in city planning, the city of Chicago regulation mandated that transportation providers publish anonymized data on ridehailing. As a result, we present the first large-scale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications. The application of random effects models from the meta-analysis literature combines the city-level effects of AI bias on fare pricing from census tract attributes, aggregated from the American Community Survey. An analysis of 100 million ridehailing samples from the city of Chicago indicates a significant disparate impact in fare pricing of neighborhoods due to AI bias learned from ridehailing utilization patterns associated with demographic attributes. Neighborhoods with larger non-white populations, higher poverty levels, younger residents, and high education levels are significantly associated with higher fare prices, with combined effect sizes, measured in Cohen's d, of -0.32, -0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods hold promise for identifying and addressing the sources of disparate impact in AI algorithms learning from datasets that contain U.S. geolocations.","",""
8,"M. Dora, Ashwani Kumar, S. Mangla, Abhay Pant, M. Kamal","Critical success factors influencing artificial intelligence adoption in food supply chains",2021,"","","","",83,"2022-07-13 09:32:36","","10.1080/00207543.2021.1959665","","",,,,,8,8.00,2,5,1,"The adoption of Artificial Intelligence (AI) in the food supply chains (FSC) can address unique challenges of food safety, quality and wastage by improving transparency and traceability. However, t...","",""
8,"Linbo Liu, Mingcheng Bi, Yunhua Wang, Junfeng Liu, Xiwen Jiang, Zhongbin Xu, Xingcai Zhang","Artificial intelligence-powered microfluidics for nanomedicine and materials synthesis.",2021,"","","","",84,"2022-07-13 09:32:36","","10.1039/d1nr06195j","","",,,,,8,8.00,1,7,1,"Artificial intelligence (AI) is an emerging technology with great potential, and its robust calculation and analysis capabilities are unmatched by traditional calculation tools. With the promotion of deep learning and open-source platforms, the threshold of AI has also become lower. Combining artificial intelligence with traditional fields to create new fields of high research and application value has become a trend. AI has been involved in many disciplines, such as medicine, materials, energy, and economics. The development of AI requires the support of many kinds of data, and microfluidic systems can often mine object data on a large scale to support AI. Due to the excellent synergy between the two technologies, excellent research results have emerged in many fields. In this review, we briefly review AI and microfluidics and introduce some applications of their combination, mainly in nanomedicine and material synthesis. Finally, we discuss the development trend of the combination of the two technologies.","",""
46,"I. Poel","Embedding Values in Artificial Intelligence (AI) Systems",2020,"","","","",85,"2022-07-13 09:32:36","","10.1007/s11023-020-09537-4","","",,,,,46,23.00,46,1,2,"","",""
81,"M. Coeckelbergh","Artificial Intelligence, Responsibility Attribution, and a Relational Justification of Explainability",2019,"","","","",86,"2022-07-13 09:32:36","","10.1007/s11948-019-00146-8","","",,,,,81,27.00,81,1,3,"","",""
4,"A. Gaggioli","Bringing More Transparency to Artificial Intelligence",2017,"","","","",87,"2022-07-13 09:32:36","","10.1089/cyber.2016.29060.csi","","",,,,,4,0.80,4,1,5,"The development of artificial intelligence (AI) has taken giant steps during the last decade to the point that for many experts, including the world-renowned atrophysics Stephen Hawking and hi-tech entrepreneur Elon Musk, AI could even destroy civilization by overtaking humans. However, on the other hand, AI may bring about huge benefits for humankind, some of which may be still beyond our imagination today. Thus, the scientific community is faced with the challenge of how we can develop powerful AI systems that support civilization while at the same time preventing the potential side effects of an uncontrolled AI evolution. To address these challenges, in late September 2016, tech giants Google, Facebook, Microsoft, Amazon, and IBM launched a ‘‘Partnership on Artificial Intelligence To Benefit People and Society.’’ The new alliance has been established ‘‘to study and formulate best practices on AI technologies, to advance the public’s understanding of AI, and to serve as an open platform for discussion and engagement about AI and its influences on people and society’’ (https://www .partnershiponai.org/). As claimed in the mission statement, a specific goal of the initiative is to help improving public awareness of what is happening in the AI field, where a number of players are shaping the future of intelligent services. Also, the Partnership aims to create more inclusive discussion by extending participation from AI specialists to activists and experts in other disciplines, such as psychology, philosophy, economics, finance, sociology, public policy, and law, to discuss and provide guidance on emerging issues related to the impact of AI on society. The Partnership has the potential to create a greater multidisciplinary understanding of the opportunities and challenges associated with potential breakthroughs in this field. Yet, some key players, such as Apple and Elon Musk’s OpenAI—a nonprofit AI research project (https://www .openai.com/blog/)—have not yet joined the club. While the goals of the Partnership have been set, the strategy that the alliance intends to put in place to attain these objectives is still unclear. Thus, it is too early to understand how the association will concretely address the challenges that need to be addressed with the public, such as how AI can be used safely to support military activities, or how to deal with the legal responsibilities for any damage caused by AI to humans.","",""
5,"F. Hussain, R. Hussain, E. Hossain","Explainable Artificial Intelligence (XAI): An Engineering Perspective",2021,"","","","",88,"2022-07-13 09:32:36","","","","",,,,,5,5.00,2,3,1,"The remarkable advancements in Deep Learning (DL) algorithms have fueled enthusiasm for using Artificial Intelligence (AI) technologies in almost every domain; however, the opaqueness of these algorithms put a question mark on their applications in safety-critical systems. In this regard, the ‘explainability’ dimension is not only essential to both explain the inner workings of black-box algorithms, but it also adds accountability and transparency dimensions that are of prime importance for regulators, consumers, and service providers. eXplainable Artificial Intelligence (XAI) is the set of techniques and methods to convert the so-called black-box AI algorithms to white-box algorithms, where the results achieved by these algorithms and the variables, parameters, and steps taken by the algorithm to reach the obtained results, are transparent and explainable. To complement the existing literature on XAI, in this paper, we take an ‘engineering’ approach to illustrate the concepts of XAI. We discuss the stakeholders in XAI and describe the mathematical contours of XAI from engineering perspective. Then we take the autonomous car as a use-case and discuss the applications of XAI for its different components such as object detection, perception, control, action decision, and so on. This work is an exploratory study to identify new avenues of research in the field of XAI.","",""
4,"Irene-Angelica Chounta, Emanuele Bardone, Aet Raudsep, M. Pedaste","Exploring Teachers’ Perceptions of Artificial Intelligence as a Tool to Support their Practice in Estonian K-12 Education",2021,"","","","",89,"2022-07-13 09:32:36","","10.1007/S40593-021-00243-5","","",,,,,4,4.00,1,4,1,"","",""
61,"N. Mirchi, Vincent Bissonnette, R. Yilmaz, N. Ledwos, A. Winkler-Schwartz, R. Del Maestro","The Virtual Operative Assistant: An explainable artificial intelligence tool for simulation-based training in surgery and medicine",2020,"","","","",90,"2022-07-13 09:32:36","","10.1371/journal.pone.0229596","","",,,,,61,30.50,10,6,2,"Simulation-based training is increasingly being used for assessment and training of psychomotor skills involved in medicine. The application of artificial intelligence and machine learning technologies has provided new methodologies to utilize large amounts of data for educational purposes. A significant criticism of the use of artificial intelligence in education has been a lack of transparency in the algorithms’ decision-making processes. This study aims to 1) introduce a new framework using explainable artificial intelligence for simulation-based training in surgery, and 2) validate the framework by creating the Virtual Operative Assistant, an automated educational feedback platform. Twenty-eight skilled participants (14 staff neurosurgeons, 4 fellows, 10 PGY 4–6 residents) and 22 novice participants (10 PGY 1–3 residents, 12 medical students) took part in this study. Participants performed a virtual reality subpial brain tumor resection task on the NeuroVR simulator using a simulated ultrasonic aspirator and bipolar. Metrics of performance were developed, and leave-one-out cross validation was employed to train and validate a support vector machine in Matlab. The classifier was combined with a unique educational system to build the Virtual Operative Assistant which provides users with automated feedback on their metric performance with regards to expert proficiency performance benchmarks. The Virtual Operative Assistant successfully classified skilled and novice participants using 4 metrics with an accuracy, specificity and sensitivity of 92, 82 and 100%, respectively. A 2-step feedback system was developed to provide participants with an immediate visual representation of their standing related to expert proficiency performance benchmarks. The educational system outlined establishes a basis for the potential role of integrating artificial intelligence and virtual reality simulation into surgical educational teaching. The potential of linking expertise classification, objective feedback based on proficiency benchmarks, and instructor input creates a novel educational tool by integrating these three components into a formative educational paradigm.","",""
50,"Emilio Calvano, G. Calzolari, V. Denicoló, S. Pastorello","Artificial Intelligence, Algorithmic Pricing, and Collusion",2020,"","","","",91,"2022-07-13 09:32:36","","10.1257/AER.20190623","","",,,,,50,25.00,13,4,2,"Increasingly, pricing algorithms are supplanting human decision making in real marketplaces. To inform the competition policy debate on the possible consequences of this development, we experiment with pricing algorithms powered by Artificial Intelligence (AI) in controlled environments (computer simulations), studying the interaction among a number of Q-learning algorithms in a workhorse oligopoly model of price competition with Logit demand and constant marginal costs. In this setting the algorithms consistently learn to charge supra-competitive prices, without communicating with one another. The high prices are sustained by classical collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand and to changes in the number of players.","",""
35,"S. Lo Piano","Ethical principles in machine learning and artificial intelligence: cases from the field and possible ways forward",2020,"","","","",92,"2022-07-13 09:32:36","","10.1057/s41599-020-0501-9","","",,,,,35,17.50,35,1,2,"","",""
96,"Eduardo H. B. Maia, L. Assis, Tiago Alves de Oliveira, Alisson Marques da Silva, A. Taranto","Structure-Based Virtual Screening: From Classical to Artificial Intelligence",2020,"","","","",93,"2022-07-13 09:32:36","","10.3389/fchem.2020.00343","","",,,,,96,48.00,19,5,2,"The drug development process is a major challenge in the pharmaceutical industry since it takes a substantial amount of time and money to move through all the phases of developing of a new drug. One extensively used method to minimize the cost and time for the drug development process is computer-aided drug design (CADD). CADD allows better focusing on experiments, which can reduce the time and cost involved in researching new drugs. In this context, structure-based virtual screening (SBVS) is robust and useful and is one of the most promising in silico techniques for drug design. SBVS attempts to predict the best interaction mode between two molecules to form a stable complex, and it uses scoring functions to estimate the force of non-covalent interactions between a ligand and molecular target. Thus, scoring functions are the main reason for the success or failure of SBVS software. Many software programs are used to perform SBVS, and since they use different algorithms, it is possible to obtain different results from different software using the same input. In the last decade, a new technique of SBVS called consensus virtual screening (CVS) has been used in some studies to increase the accuracy of SBVS and to reduce the false positives obtained in these experiments. An indispensable condition to be able to utilize SBVS is the availability of a 3D structure of the target protein. Some virtual databases, such as the Protein Data Bank, have been created to store the 3D structures of molecules. However, sometimes it is not possible to experimentally obtain the 3D structure. In this situation, the homology modeling methodology allows the prediction of the 3D structure of a protein from its amino acid sequence. This review presents an overview of the challenges involved in the use of CADD to perform SBVS, the areas where CADD tools support SBVS, a comparison between the most commonly used tools, and the techniques currently used in an attempt to reduce the time and cost in the drug development process. Finally, the final considerations demonstrate the importance of using SBVS in the drug development process.","",""
6,"M. Loi, M. Spielkamp","Towards Accountability in the Use of Artificial Intelligence for Public Administrations",2021,"","","","",94,"2022-07-13 09:32:36","","10.1145/3461702.3462631","","",,,,,6,6.00,3,2,1,"We argue that the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation when tasks are delegated to computationally-driven systems. Imperfect delegation challenges human accountability. We hold that both direct public accountability via public transparency and indirect public accountability via transparency to auditors in public organizations can be both instrumentally ethically valuable and required as a matter of deontology from the principle of democratic self-government. We analyze the regulatory content of 16 guideline documents about the use of AI in the public sector, by mapping their requirements to those of our philosophical account of accountability, and conclude that while some guidelines refer processes that amount to auditing, it seems that the debate would benefit from more clarity about the nature of the entitlement of auditors and the goals of auditing, also in order to develop ethically meaningful standards with respect to which different forms of auditing can be evaluated and compared.","",""
3,"Irene A. Niet, R. Est, F. Veraart","Governing AI in Electricity Systems: Reflections on the EU Artificial Intelligence Bill",2021,"","","","",95,"2022-07-13 09:32:36","","10.3389/frai.2021.690237","","",,,,,3,3.00,1,3,1,"The Proposal for an Artificial Intelligence Act, published by the European Commission in April 2021, marks a major step in the governance of artificial intelligence (AI). This paper examines the significance of this Act for the electricity sector, specifically investigating to what extent the current European Union Bill addresses the societal and governance challenges posed by the use of AI that affects the tasks of system operators. For this we identify various options for the use of AI by system operators, as well as associated risks. AI has the potential to facilitate grid management, flexibility asset management and electricity market activities. Associated risks include lack of transparency, decline of human autonomy, cybersecurity, market dominance, and price manipulation on the electricity market. We determine to what extent the current bill pays attention to these identified risks and how the European Union intends to govern these risks. The proposed AI Act addresses well the issue of transparency and clarifying responsibilities, but pays too little attention to risks related to human autonomy, cybersecurity, market dominance and price manipulation. We make some governance suggestions to address those gaps.","",""
3,"Amy Papadopoulos, J. Salinas, Cindy Crump","Computational modeling approaches to characterize risk and achieve safe, effective, and trusted designs in the development of artificial intelligence and autonomous closed-loop medical systems",2021,"","","","",96,"2022-07-13 09:32:36","","10.1117/12.2586101","","",,,,,3,3.00,1,3,1,"While software using artificial intelligence and machine learning (AI/ML) is pervasive in many areas of society today, the use of these technologies to diagnose and treat medical conditions is limited due to a number of challenges associated with the trustworthiness of the results. This may include the inability to fully explain how an algorithm works inherent to the black-box nature of the system. Additionally, AI/ML may create a potential for bias and artifacts that cannot be validated due to the same limitations. In a medical application, the lack of transparency in how the system operates may lead to a loss of trust by users. Bayesian approaches that use computational modeling to quantify the level of uncertainty in a given result may provide a path towards improved confidence and use. In this paper, evidence from studies in a range of medical applications is presented and discussed, showing how Bayesian approaches can help to foster trust. A retrospective study using a publicly available dataset explored the feasibility of creating predictive models for early intervention in a Type 1 diabetes population. Creating the perfect model was not the goal of the exercise, rather the study aimed to demonstrate how Bayesian methods could be used to identify areas of uncertainty during model development. Feature selection was based on analytical assessment of various patterns found in the data. Models were trained, validated, and tested, generating uncertainty estimates. A two-feature Gaussian Naïve Bayes (GNB) model, using the previous five minutes and ten minutes of blood glucose values, showed similar results for predictive accuracy as a threefeature model that included average change over the preceding 30 minutes. The two-feature model was selected because it allowed for a more easily understood visualization of uncertainty. The 2-feature GNB achieved an AUC = .94. The model showed good sensitivity for exceeding the < 180 mg/dl limit, obtaining threshold prediction = 89.8% and normal range prediction = 90.8%. The sensitivity was lower for the < 70 mg/dl limit, attaining a sensitivity = 77.5%. Posterior probabilities showed differing levels of uncertainty in the prediction of high and low out-of-range conditions. The model demonstrated the feasibility of providing robust parameter estimates. Bayesian machine learning approaches to model uncertainty may improve the transparency, explainability, and applicability of AI/ML in medical treatment, realizing the promise to improve patient safety and outcomes.","",""
4,"Shivam Mehta, Y. Suhail, J. Nelson, M. Upadhyay","Artificial Intelligence for radiographic image analysis",2021,"","","","",97,"2022-07-13 09:32:36","","10.1053/J.SODO.2021.05.007","","",,,,,4,4.00,1,4,1,"Abstract Automated identification of landmarks on lateral cephalogram and cone-beam computed tomography (CBCT) scans can save time for the clinicians and act as a second set of eyes for analysis of radiographic images in diagnosis and treatment planning. Several machine-learning techniques have been utilized for this purpose with varying accuracies. However, high degree of variability in the clinical presentation of orthodontic patients, limitations of the algorithms, lack of labelled data, high compute power, etc. are some drawbacks that have limited robust clinical application of such techniques. In recent years, artificial neural networks like deep learning and more specifically deep neural networks are making significant inroads in the true adoption of this technology. YOLOv3 and Single Shot Multibox Detector are some of the deep learning algorithms that have shown promising results. This paper is a theoretical review of the evolution of these technologies and the current state of the art in orthodontic image analysis.","",""
1,"Ke Zhang, Peidong Xu, Tianlu Gao, Jun Zhang","A Trustworthy Framework of Artificial Intelligence for Power Grid Dispatching Systems",2021,"","","","",98,"2022-07-13 09:32:36","","10.1109/DTPI52967.2021.9540198","","",,,,,1,1.00,0,4,1,"With the widespread application of artificial intelligence (AI) technologies in power systems, the properties of lack of reliability and transparency for AI technologies have revealed gradually. Here, how to build a trustworthy-AI framework based on the power system is the focus. Due to the multidimensional and heterogeneous information of power grid data, the heterogeneous graph attention network (HGAT) model of power grid dispatching is established, and the corresponding explainer (HGAT-Explainer) for the model of power equipment faults is proposed to provide more favorable support for the trustworthy-AI systems.","",""
0,"Caijin Ling, Ting Zeng, Yang Su","Research on Intelligent Supervision and Application System of Food Traceability Based on Blockchain and Artificial intelligence",2021,"","","","",99,"2022-07-13 09:32:36","","10.1109/ICIBA52610.2021.9688295","","",,,,,0,0.00,0,3,1,"The lack of transparency in the production and circulation of commodities and the lack of corresponding supervision have led to endless problems such as food safety, counterfeit and shoddy products, loss and damage of commodities, and damage to the rights and interests of consumers. The traditional centralized database traceability monitoring system has serious problems of data trust, data fragmentation, difficulty in accountability, and low enthusiasm of merchants. In order to solve the traditional system problems, it is proposed to build an intelligent supervision system model for food traceability based on blockchain and artificial intelligence. Blockchain technology can effectively make up and improve the shortcomings of the existing commodity traceability technology, and achieve full process control and real-time storage. Forensic forensics, increase transparency, prevent counterfeiting, and increase consumer trust; AI uses industry-sharing data to perform big data analysis to guide companies in business decisions; at the same time, in order to increase user stickiness and increase the ecological environment of the platform, the article proposes to increase Blockchain and artificial intelligence and application ecology form a more practical and complete integrated system model of traceability, supervision and application. Finally, using FISCO BCOS as a blockchain platform development platform, the validity of the model is verified, and it can provide a certain reference for food traceability companies, software R&D companies, and government regulatory agencies.","",""
0,"Hui Li, Zhenjing Pang, Yanjun Li","Research on the Realistic Dilemma and Optimized Path of Education Governance Modernization from the Perspective of Artificial Intelligence",2021,"","","","",100,"2022-07-13 09:32:36","","10.1109/ICAIE53562.2021.00009","","",,,,,0,0.00,0,3,1,"Supported by Artificial Intelligence centered on heuristic search, machine learning, and expert systems, modern education governance presents the typical characteristics of governance process transparency, subject diversification and system intelligence. However, from the perspective of Artificial Intelligence, education governance modernization is not only stuck in the “structural” absence of multiple governance subjects, but also trapped in the “initiative” shortage of vertical one-way organizational structure and the “data” block of modern education governance paradigm. Thus, the application of Artificial Intelligence in intelligently mining data and information, autonomously adapting to complex educational situations, and systematically optimizing governance decisionmaking behavior is limited. This paper explores the optimized path of education governance from three dimensions of concept cultivation, subject training and technology innovation, rationally discusses the “interactive” application of Artificial Intelligence and education governance modernization, so as to promote the deep integration of artificial intelligence and education governance, and improve education governance modernization as well as talent cultivation in the new era.","",""
0,"Feng Xiaohua, Conrad Marc, E. Elias, Hussein Khalid","Artificial Intelligence and Blockchain for Future Cyber Security Application",2021,"","","","",101,"2022-07-13 09:32:36","","10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00133","","",,,,,0,0.00,0,4,1,"AI (Artificial intelligence) application on Big Data had been developed fast. AI cyber security defense for the facing threats were required. Blockchain technology was invented in 2008 with BTC (Bit coin. This technology could be benefited alongside the custom of Blockchain, AI, Big Data and so on. There were a rapid progress in the advancement of Blockchain. This subject had recently become a discussion topic in the ICT (Information and Communications Technology) world. In this paper, AI security is discussed from the initial stage. Suggestion: In this paper, we discussed the impact of AI security from the initial stage and its impact and benefits to IT engineers, ICT students and CS (Computer Sciences) academic researchers, using a case study of medical records with personal recognizable identification privacy information that needs strict access control security. We considered its need for trustworthy cyber security, anti-fake, anti-alteration and transaction accounting transparency reputation to be applied to the NHS (National Health Service). Lastly, the paper provided some necessarily analysis. Blockchain technology had trustworthy cyber security, anti-fake, anti-alteration and transaction accounting transparency reputation to be considered to be applied to NHS (National Health Service). This short paper provided some analysis necessarily.","",""
0,"M. Aggarwal, Christian Gingras, R. Deber","Artificial Intelligence in Healthcare from a Policy Perspective",2021,"","","","",102,"2022-07-13 09:32:36","","10.1007/978-3-030-67303-1_5","","",,,,,0,0.00,0,3,1,"","",""
30,"S. L. Piano","Ethical principles in machine learning and artificial intelligence: cases from the field and possible ways forward",2020,"","","","",103,"2022-07-13 09:32:36","","10.1057/S41599-020-0501-9","","",,,,,30,15.00,30,1,2,"","",""
0,"Xiaohong W. Gao, B. Braden","Artificial intelligence in endoscopy: The challenges and future directions",2021,"","","","",104,"2022-07-13 09:32:36","","10.37126/aige.v2.i4.117","","",,,,,0,0.00,0,2,1,"Artificial intelligence based approaches, in particular deep learning, have achieved state-of-the-art performance in medical fields with increasing number of software systems being approved by both Europe and United States. This paper reviews their applications to early detection of oesophageal cancers with a focus on their advantages and pitfalls. The paper concludes with future recommendations towards the development of a real-time, clinical implementable, interpretable and robust diagnosis support systems.","",""
29,"Grayson W. Armstrong, A. Lorch","A(eye): A Review of Current Applications of Artificial Intelligence and Machine Learning in Ophthalmology",2019,"","","","",105,"2022-07-13 09:32:36","","10.1097/IIO.0000000000000298","","",,,,,29,9.67,15,2,3,"Artificial intelligence (AI) is a branch of computer science that aims to enable computers to perform human-like tasks. Although AI is a broad discipline, machine learning is a specific branch of AI that uses computer algorithms capable of “learning” through the simulation of human intelligence. Machine learning algorithms have been applied to the medical field since the 1970s,1 and since that time have proven useful in computerassisted diagnosis, screening, and prognostication of disease.2–7 Ophthalmology is uniquely capable of capitalizing on the promise of AI. Ophthalmologists, during routine clinical encounters, generate robust data sources capable of supporting machine learning algorithms including multimodal ophthalmic images and quantifiable metrics such as visual acuity (VA), intraocular pressure, and cup to disk ratio. To date, AI techniques have been applied to ophthalmology to screen for and diagnose diseases, such as diabetic retinopathy (DR), age-related macular degeneration (AMD), macular edema (ME), glaucoma, keratoconus, postlaserassisted in situ keratomileusis corneal ectasia, retinopathy of prematurity (ROP), and cataracts, as well as predict the prognosis of various ophthalmic diseases. Advances in ophthalmology-specific AI stand to increase patient access to clinical screening and diagnosis as well as decrease health care costs, especially when applied to high-risk populations, low-resource communities, or when combined with telemedicine initiatives. This review provides an introduction to AI andmachine learning, as well as an overview of current applications in the field of ophthalmology.","",""
20,"L. McCoy, Sujay Nagaraj, F. Morgado, V. Harish, Sunit Das, L. Celi","What do medical students actually need to know about artificial intelligence?",2020,"","","","",106,"2022-07-13 09:32:36","","10.1038/s41746-020-0294-7","","",,,,,20,10.00,3,6,2,"","",""
24,"Paul Henman","Improving public services using artificial intelligence: possibilities, pitfalls, governance",2020,"","","","",107,"2022-07-13 09:32:36","","10.1080/23276665.2020.1816188","","",,,,,24,12.00,24,1,2,"Artificial intelligence arising from the use of machine learning is rapidly being developed and deployed by governments to enhance operations, public services, and compliance and security activities. This article reviews how artificial intelligence is being used in public sector for automated decision making, for chatbots to provide information and advice, and for public safety and security. It then outlines four public administration challenges to deploying artificial intelligence in public administration: accuracy, bias and discrimination; legality, due process and administrative justice; responsibility, accountability, transparency and explainability; and power, compliance and control. The article outlines technological and governance innovations that are being developed to address these challenges.","",""
25,"C. Goldstein, R. Berry, D. Kent, D. Kristo, A. Seixas, S. Redline, M. Westover","Artificial intelligence in sleep medicine: Background and implications for clinicians.",2020,"","","","",108,"2022-07-13 09:32:36","","10.5664/jcsm.8388","","",,,,,25,12.50,4,7,2,"None Polysomnography (PSG) remains the cornerstone of objective testing in sleep medicine and results in massive amounts of electrophysiological data, which is well-suited for analysis with artificial intelligence (AI)-based tools. Combined with other sources of health data, AI is expected to provide new insights to inform the clinical care of sleep disorders and advance our understanding of the integral role sleep plays in human health. Additionally, AI has the potential to streamline day-to-day operations and therefore optimize direct patient care by the sleep disorders team. However, clinicians, scientists, and other stakeholders must develop best practices to integrate this rapidly evolving technology into our daily work while maintaining the highest degree of quality and transparency in health care and research. Ultimately, when harnessed appropriately in conjunction with human expertise, AI will improve the practice of sleep medicine and further sleep science for the health and well-being of our patients.","",""
28,"H. Alami, L. Rivard, P. Lehoux, S. Hoffman, Stephanie B. M. Cadeddu, Mathilde Savoldelli, M. A. Samri, M. A. Ag Ahmed, R. Fleet, J. Fortin","Artificial intelligence in health care: laying the Foundation for Responsible, sustainable, and inclusive innovation in low- and middle-income countries",2020,"","","","",109,"2022-07-13 09:32:36","","10.1186/s12992-020-00584-1","","",,,,,28,14.00,3,10,2,"","",""
44,"Chiara Longoni, Luca Cian","Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect",2020,"","","","",110,"2022-07-13 09:32:36","","10.1177/0022242920957347","","",,,,,44,22.00,22,2,2,"Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).","",""
23,"M. Rohaim, E. Clayton, I. Sahin, J. Vilela, M. Khalifa, M. Al-Natour, M. Bayoumi, A. Poirier, M. Branavan, M. Tharmakulasingam, N. S. Chaudhry, R. Sodi, A. Brown, P. Burkhart, W. Hacking, J. Botham, J. Boyce, H. Wilkinson, Craig Williams, Jayde Whittingham-Dowd, E. Shaw, Matt D. Hodges, L. Butler, M. Bates, R. L. La Ragione, W. Balachandran, A. Fernando, M. Munir","Artificial Intelligence-Assisted Loop Mediated Isothermal Amplification (AI-LAMP) for Rapid Detection of SARS-CoV-2",2020,"","","","",111,"2022-07-13 09:32:36","","10.3390/v12090972","","",,,,,23,11.50,2,28,2,"Until vaccines and effective therapeutics become available, the practical solution to transit safely out of the current coronavirus disease 19 (CoVID-19) lockdown may include the implementation of an effective testing, tracing and tracking system. However, this requires a reliable and clinically validated diagnostic platform for the sensitive and specific identification of SARS-CoV-2. Here, we report on the development of a de novo, high-resolution and comparative genomics guided reverse-transcribed loop-mediated isothermal amplification (LAMP) assay. To further enhance the assay performance and to remove any subjectivity associated with operator interpretation of results, we engineered a novel hand-held smart diagnostic device. The robust diagnostic device was further furnished with automated image acquisition and processing algorithms and the collated data was processed through artificial intelligence (AI) pipelines to further reduce the assay run time and the subjectivity of the colorimetric LAMP detection. This advanced AI algorithm-implemented LAMP (ai-LAMP) assay, targeting the RNA-dependent RNA polymerase gene, showed high analytical sensitivity and specificity for SARS-CoV-2. A total of ~200 coronavirus disease (CoVID-19)-suspected NHS patient samples were tested using the platform and it was shown to be reliable, highly specific and significantly more sensitive than the current gold standard qRT-PCR. Therefore, this system could provide an efficient and cost-effective platform to detect SARS-CoV-2 in resource-limited laboratories.","",""
25,"D. Schiff","Out of the laboratory and into the classroom: the future of artificial intelligence in education",2020,"","","","",112,"2022-07-13 09:32:36","","10.1007/s00146-020-01033-8","","",,,,,25,12.50,25,1,2,"","",""
37,"H.J. Yu, S. Cho, M. Kim, Won Hwa Kim, J.W. Kim, J. Choi","Automated Skeletal Classification with Lateral Cephalometry Based on Artificial Intelligence",2020,"","","","",113,"2022-07-13 09:32:36","","10.1177/0022034520901715","","",,,,,37,18.50,6,6,2,"Lateral cephalometry has been widely used for skeletal classification in orthodontic diagnosis and treatment planning. However, this conventional system, requiring manual tracing of individual landmarks, contains possible errors of inter- and intravariability and is highly time-consuming. This study aims to provide an accurate and robust skeletal diagnostic system by incorporating a convolutional neural network (CNN) into a 1-step, end-to-end diagnostic system with lateral cephalograms. A multimodal CNN model was constructed on the basis of 5,890 lateral cephalograms and demographic data as an input. The model was optimized with transfer learning and data augmentation techniques. Diagnostic performance was evaluated with statistical analysis. The proposed system exhibited >90% sensitivity, specificity, and accuracy for vertical and sagittal skeletal diagnosis. Clinical performance of the vertical classification showed the highest accuracy at 96.40 (95% CI, 93.06 to 98.39; model III). The receiver operating characteristic curve and the area under the curve both demonstrated the excellent performance of the system, with a mean area under the curve >95%. The heat maps of cephalograms were also provided for deeper understanding of the quality of the learned model by visually representing the region of the cephalogram that is most informative in distinguishing skeletal classes. In addition, we present broad applicability of this system through subtasks. The proposed CNN-incorporated system showed potential for skeletal orthodontic diagnosis without the need for intermediary steps requiring complicated diagnostic procedures.","",""
5,"Vanessa Laurim, Selin Arpaci, Barbara Prommegger, H. Krcmar","Computer, Whom Should I Hire? - Acceptance Criteria for Artificial Intelligence in the Recruitment Process",2021,"","","","",114,"2022-07-13 09:32:36","","10.24251/HICSS.2021.668","","",,,,,5,5.00,1,4,1,"In the war for talents, the need for appropriate tools to fill open positions with the right talents is becoming increasingly important for employers. AI-based technologies simplify recruiters’ daily work and increase the efficiency of the recruitment process by replacing time-consuming approaches. However, little is known about the reactions of stakeholders to AI-based recruiting. Thus, this paper aims to identify personal and contextual factors that influence the acceptance of AI-based technologies in the recruitment process. Based on the interviews with recruiters, managers, and applicants involved in the recruitment process, we present that transparency, complementary features of the AI tools, and a sense of control play key roles in the acceptance of AI-based technology when used for recruiting. The findings contribute to research on the adoption of AI in the recruitment process and provide recommendations on the use of AI technologies when hiring talents.","",""
1,"K. Vogel, Gwendolynne Reid, Christopher Kampe, Paul Jones","The impact of AI on intelligence analysis: tackling issues of collaboration, algorithmic transparency, accountability, and management",2021,"","","","",115,"2022-07-13 09:32:36","","10.1080/02684527.2021.1946952","","",,,,,1,1.00,0,4,1,"In January 2019, the U.S. Office of the Director of National Intelligence (ODNI) released a new strategy on the use of artificial intelligence (AI) technologies in U.S. intelligence. The report called for incorporating AI and automation technologies into intelligence work ‘to amplify the effectiveness of our workforce . . . advance mission capability and enhance the IC’s [Intelligence Community’s] ability to provide data interpretation to decision makers’. The ODNI noted it was evaluating and monitoring how these technologies might also have ‘vulnerabilities in development and adoption’. The report stated it was critical to address issues of ‘AI assurance, transparency, and reliability . . . to . . . understand how AI algorithms may fail’, and noted the importance of developing AI systems that ‘can demonstrate the underlying rationale behind decisions and responses to both users and overseers’. Finally, it emphasized the importance of monitoring ‘implementation and user feedback’ in a future AI-enabled workforce. This imagined future is not only to come; it is being realized now. Within the past few years, probably one of the most visibly controversial IC projects involving AI and intelligence analysis was Project Maven, a 2017 Department of Defense–driven intelligence project that used advances in big data, machine learning, and deep learning to extract objects of interest from drone-derived imagery, saving intelligence analysts hours of tedious imagery processing time. Project Maven partnered with Google to use some of the tech giant’s AI technology, but the project was ultimately cancelled in 2018 because of Google employee protests against the use of company algorithms for military targeting. Beyond Project Maven, a number of less controversial R&D programs ARE underway that aim to augment intelligence analysts’ capabilities. For example, the Defense Advanced Research Project Agency (DARPA)’s Explainable (XAI) Program is creating a set of new machine-learning techniques to ‘enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners’. The National Security Agency’s Laboratory for Analytic Sciences is developing big-data and AI-enabled technological platforms to bring more AI-enabled systems to analysts’ desktops. To date, these are individual proof-ofconcept technologies that may be applied and integrated into a variety of potential intelligence tools, such as: Journaling, a productivity device for intelligence analysts that enables them to understand their own individual work flows; OpenKE, which automates the collection and analysis of open-source information; BEAST, a platform of natural language processing and other extraction services that can scrape and process data from various sources for anticipatory intelligence analysis; and CyberMonkey, which allows a large number of analytic tools to be executed proactively in-browser instead of having to utilize tools sequentially or separately. All of these technologies aim ‘to assist the analyst without the analyst explicitly telling the machine everything it needs to do’. The Intelligence Advanced Research Project Activity (IARPA)’s Multimodal","",""
30,"L. Longo, R. Goebel, F. Lécué, Peter Kieseberg, Andreas Holzinger","Explainable Artificial Intelligence: Concepts, Applications, Research Challenges and Visions",2020,"","","","",116,"2022-07-13 09:32:36","","10.1007/978-3-030-57321-8_1","","",,,,,30,15.00,6,5,2,"","",""
1,"Michael Tsang, James Enouen, Yan Liu","Interpretable Artificial Intelligence through the Lens of Feature Interaction",2021,"","","","",117,"2022-07-13 09:32:36","","","","",,,,,1,1.00,0,3,1,"Deep learning, alongside other modern machine learning techniques, has become the state of the art solution for a diverse range of real-world tasks. These include a variety of sensitive applications such as healthcare, finance, autonomous driving, criminal justice, and others which all pose significant concerns for fairness, robustness, safety, and trustworthiness. Despite these applications to critical tasks, deep networks are infamously referred to as black-box models because of their total lack of transparency in decision-making. If we are able to gain insight into how a model is coming to its conclusions, we are able to more clearly assess the trustworthiness and validity of its decisions. Consequently, an abundance of ongoing research is attempting to address model interpretability as the key problem to resolving these issues. There are many methods which are currently used to provide explanations of complex model predictions. LIME (Ribeiro et al., 2016) fits a local linear model around a data point, showing which features positively and negatively influence the prediction results. Despite the overall model being nonlinear, the local model gets an interpretable picture of how the model looks at small scales around the data point. Extensions of this method use other interpretable models like small decision tress. Shapley Values and SHAP follows a similar idea to assign a score to each feature, using a gametheoretic formulation which treats each feature as a player causing the final prediction (Lundberg and Lee, 2017). Its more rigorous formulation yields guarantees of its explanations summing up to the prediction score, but practically it usually must be estimated because of its high computational cost. Shuffle-based feature importance permutes the data of each feature to ascertain its importance in the final prediction in comparison to its normal prediction (Fisher et al., 2018). IG uses the fundamental theorem of calculus to provide additive explanations of a prediction (Sundararajan et al., 2017). This method is very popular in computer vision where its computational efficiency and saliency are prized, even though other work has exposed some of its shortcomings in providing an interpretation (Adebayo et al., 2018). Other methods are specifically designed for computer vision like TCAV (Kim et al., 2018) which finds a ‘concept direction’ corresponding to a large sample of concept images from the user. Surprisingly, all of these most popular interpretability methods share the same one limitation. None of these methods consider the shared importance of groups of two or more features; they only look at the effects had by each of the features individually. A feature interaction between two variables broadly describes a situation where both of the features/ variables are simultaneously important for a model’s prediction. In text applications for sentiment, ”not good” is a very simple example of two words strongly interacting with one another to create a negative sentiment. In modern-day applications, neural networks are usually hailed as amazing function approximators exactly because of their incredible ability to automatically uncover these kinds of complex relationships between the variables of the dataset. In many ways, however,","",""
22,"G. Francolini, I. Desideri, G. Stocchi, V. Salvestrini, L. Ciccone, P. Garlatti, M. Loi, L. Livi","Artificial Intelligence in radiotherapy: state of the art and future directions",2020,"","","","",118,"2022-07-13 09:32:36","","10.1007/s12032-020-01374-w","","",,,,,22,11.00,3,8,2,"","",""
1549,"Amina Adadi, M. Berrada","Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)",2018,"","","","",119,"2022-07-13 09:32:36","","10.1109/ACCESS.2018.2870052","","",,,,,1549,387.25,775,2,4,"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.","",""
2,"Thomas Garvin, Scott Kimbleton","Artificial intelligence as ally in hazard analysis",2021,"","","","",120,"2022-07-13 09:32:36","","10.1002/prs.12243","","",,,,,2,2.00,1,2,1,"Hazard analysis techniques have been around for many years, and have proven effective in the prevention of incidents and no doubt the saving of lives. Process hazard analysis (PHA) is now fairly robust and regulated, focused on overarching risks associated with the safe handling of hazardous materials and approaches to engineer‐out such risks. Occupational hazard analysis (OHA) is keenly focused on human activity, and personal protection in hazardous working conditions. Both approaches are critical ‐ but are often carried out separately, by different parts of an organization, which could result in an incomplete picture of the full set of operational risks in the field. Developing a holistic picture of both past and present dangers calls for a deep exploration of evidence. HAZOPs, PHA's, incident records and investigations provide expert analysis of hazards and mitigating strategies. Near‐miss reports and safety observations add a large amount of information as well; the reporting frequency of these “leading indicators” can be both a blessing and a curse, as time and available resources constrain the ability to analyze and detect hazard signals within. As important as analyzing the historical record is for lessons learned, the more recent observations could indicate new hazards or highlight concerning trends. These could feed valuable “real time” information back to operations and maintenance teams to improve risk assessments and task planning. Enter artificial intelligence (AI) as a means to analyze the large amount of written hazard analyses, reports and observations to quickly extract insights around hazardous conditions, activities, incident causes and risk mitigation measures. Trained to understand concepts and contexts in both process and personal safety, AI can provide a natural‐language information exploration environment for scanning thousands of documents in seconds and present common themes and related records. Not unlike us humans, AI learns from the past, informs the present and can help reduce risks in the future.","",""
27,"Omar Alshorman, Muhammad Irfan, N. Saad, D. Zhen, Noman Haider, A. Głowacz, Ahmad M. Alshorman","A Review of Artificial Intelligence Methods for Condition Monitoring and Fault Diagnosis of Rolling Element Bearings for Induction Motor",2020,"","","","",121,"2022-07-13 09:32:36","","10.1155/2020/8843759","","",,,,,27,13.50,4,7,2,"The fault detection and diagnosis (FDD) along with condition monitoring (CM) and of rotating machinery (RM) have critical importance for early diagnosis to prevent severe damage of infrastructure in industrial environments. Importantly, valuable industrial equipment needs continuous monitoring to enhance the safety, reliability, and availability and to decrease the cost of maintenance of modern industrial systems and applications. However, induction motor (IM) has been extensively used in several industrial processes because it is cheap, reliable, and robust. Rolling bearings are considered to be the main component of IM. Undoubtedly, any failure of this basic component can lead to a serious breakdown of IM and for whole industrial system. Thus, many current methods based on different techniques are employed as a fault prognosis and diagnosis of rolling elements bearing of IM. Moreover, these techniques include signal/image processing, intelligent diagnostics, data fusion, data mining, and expert systems for time and frequency as well as time-frequency domains. Artificial intelligence (AI) techniques have proven their significance in every field of digital technology. Industrial machines, automation, and processes are the net frontiers of AI adaptation. There are quite developed literatures that have been approaching the issues using signals and data processing techniques. However, the key contribution of this work is to present an extensive review of CM and FDD of the IM, especially for rolling elements bearings, based on artificial intelligent (AI) methods. This study highlights the advantages and performance limitations of each method. Finally, challenges and future trends are also highlighted.","",""
0,"Lucas Mendes Lima, Victor Calebe Cavalcante, Mariana Guimarães de Sousa, Cláudio Afonso Fleury, D. Oliveira, Eduardo Noronha de Andrade Freitas","Artificial Intelligence in Support of Welfare Monitoring of Dairy Cattle: A Systematic Literature Review",2021,"","","","",122,"2022-07-13 09:32:36","","10.1109/CSCI54926.2021.00324","","",,,,,0,0.00,0,6,1,"Context: Although agribusiness corresponded to more than 20% of Brazil’s Gross Domestic Product (GDP), most livestock is under manual control and manual monitoring. Additionally, alternative technologies are either uncomfortable and stressful, or expensive. Now, despite the great scientific advances in the area, there is still a pressing need for an automated robust, inexpensive and (sub)optimal technology to monitor animal behavior in a cost-effective, contact-less and stress-free fashion. Overall, this niche can leverage the benefits of Deep Learning schemes.Objective: This review aims to provide a systematic overview of most current projects in the area of comfort monitoring dairy cattle, as well as their corresponding image recognition-based techniques and technologies.Methods: First, a systematic review planning was carried out, and objectives, research questions, search strings, among others, were defined. Subsequently,a broad survey was conducted to extract, analyze and compile the data, to generate a easy-to-read visual source of information (tables and graphics).Results: Information was extracted from the reviewed papers. Among this data collected from the papers are techniques utilized, target behaviors, cow bodyparts identified in visual computational, besides their paper source font, the publication date, and localization. For example, the papers present are mostly recent. China has had a larger number of relevant papers in the area. The back was the body region most analyzed by the papers and the behaviors most analyzed were body condition score, lameness, cow’s body position and feeding/drinking behavior. Among the methods used is RCNN Inception V3 with the best accuracy for cow’s back region.Conclusion: The aim of this work is to present some of the papers that are being carried out in the area of dairy cow behavior monitoring, using techniques of Artifical Intelligence. It is expected that the information collected and presented in the present systematic review paper contribute to the future researches and projects of the area and the application of new techniques.","",""
0,"Keeley A. Crockett, Edwin Colyer, A. Latham","The Ethical Landscape of Data and Artificial Intelligence: Citizen Perspectives",2021,"","","","",123,"2022-07-13 09:32:36","","10.1109/SSCI50451.2021.9660153","","",,,,,0,0.00,0,3,1,"Globally, there is growing acknowledgement that those involved in the development and deployment of AI products and services should act responsibly and conduct their work within robust ethical frameworks. Many of the ethical guidelines now published highlight a requirement for citizens to have greater voice and involvement in this process and to hold actors to account regarding compliance and the impacts of their AI innovations. For citizens to participate in co-creation activities they need to be representative of the diverse communities of society and have an appropriate level of understanding of basic AI concepts. This paper presents the preliminary results of a longitudinal survey designed to capture citizen perspectives of the ethical landscape of data and AI. Forty participants were asked to participate in a survey and results were analyzed based on gender, age range and educational attainment. Results have shown that participant perception of AI, trust, bias and fairness is different but related to specific AI applications, and the context in which is applied. Citizens also are also very receptive to undertaking free courses/workshops on a wide range of AI concepts, ranging from family workshops to work-based training.","",""
19,"M. Kuzlu, Umit Cali, Vinayak Sharma, Özgür Güler","Gaining Insight Into Solar Photovoltaic Power Generation Forecasting Utilizing Explainable Artificial Intelligence Tools",2020,"","","","",124,"2022-07-13 09:32:36","","10.1109/ACCESS.2020.3031477","","",,,,,19,9.50,5,4,2,"Over the last two decades, Artificial Intelligence (AI) approaches have been applied to various applications of the smart grid, such as demand response, predictive maintenance, and load forecasting. However, AI is still considered to be a “black-box” due to its lack of explainability and transparency, especially for something like solar photovoltaic (PV) forecasts that involves many parameters. Explainable Artificial Intelligence (XAI) has become an emerging research field in the smart grid domain since it addresses this gap and helps understand why the AI system made a forecast decision. This article presents several use cases of solar PV energy forecasting using XAI tools, such as LIME, SHAP, and ELI5, which can contribute to adopting XAI tools for smart grid applications. Understanding the inner workings of a prediction model based on AI can give insights into the application field. Such insight can provide improvements to the solar PV forecasting models and point out relevant parameters.","",""
43,"Stuart J. Russell, Thomas G. Dietterich, Eric Horvitz, B. Selman, F. Rossi, D. Hassabis, S. Legg, Mustafa Suleyman, D. George, D. Phoenix","Letter to the Editor: Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter",2015,"","","","",125,"2022-07-13 09:32:36","","10.1609/aimag.v36i4.2621","","",,,,,43,6.14,4,10,7,"Artificial intelligence (AI) research has explored a variety of problems and approaches since its inception, but for the last 20 years or so has been focused on the problems surrounding the construction of intelligent agents — systems that perceive and act in some environment. In this context, ""intelligence"" is related to statistical and economic notions of rationality — colloquially, the ability to make good decisions, plans, or inferences. The adoption of probabilistic and decision-theoretic representations and statistical learning methods has led to a large degree of integration and cross-fertilization among AI, machine learning, statistics, control theory, neuroscience, and other fields. The establishment of shared theoretical frameworks, combined with the availability of data and processing power, has yielded remarkable successes in various component tasks such as speech recognition, image classification, autonomous vehicles, machine translation, legged locomotion, and question-answering systems. As capabilities in these areas and others cross the threshold from laboratory research to economically valuable technologies, a virtuous cycle takes hold whereby even small improvements in performance are worth large sums of money, prompting greater investments in research. There is now a broad consensus that AI research is progressing steadily, and that its impact on society is likely to increase. The potential benefits are huge, since everything that civilization has to offer is a product of human intelligence; we cannot predict what we might achieve when this intelligence is magnified by the tools AI may provide, but the eradication of disease and poverty are not unfathomable. Because of the great potential of AI, it is important to research how to reap its benefits while avoiding potential pitfalls. The progress in AI research makes it timely to focus research not only on making AI more capable, but also on maximizing the societal benefit of AI. Such considerations motivated the AAAI 2008–09 Presidential Panel on Long-Term AI Futures and other projects on AI impacts, and constitute a significant expansion of the field of AI itself, which up to now has focused largely on techniques that are neutral with respect to purpose. We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial: our AI systems must do what we want them to do. The attached research priorities document [see page X] gives many examples of such research directions that can help maximize the societal benefit of AI. This research is by necessity interdisciplinary, because it involves both society and AI. It ranges from economics, law and philosophy to computer security, formal methods and, of course, various branches of AI itself. In summary, we believe that research on how to make AI systems robust and beneficial is both important and timely, and that there are concrete research directions that can be pursued today.","",""
18,"Ahmed Gowida, Salaheldin Elkatatny, Saad F. K. Al-Afnan, A. Abdulraheem","New Computational Artificial Intelligence Models for Generating Synthetic Formation Bulk Density Logs While Drilling",2020,"","","","",126,"2022-07-13 09:32:36","","10.3390/su12020686","","",,,,,18,9.00,5,4,2,"Synthetic well log generation using artificial intelligence tools is a robust solution for situations in which logging data are not available or are partially lost. Formation bulk density (RHOB) logging data greatly assist in identifying downhole formations. These data are measured in the field while drilling by using a density log tool in the form of either a logging while drilling (LWD) technique or (more often) by wireline logging after the formations are drilled. This is due to operational limitations during the drilling process. Therefore, the objective of this study was to develop a predictive tool for estimating RHOB while drilling using an adaptive network-based fuzzy interference system (ANFIS), functional network (FN), and support vector machine (SVM). The proposed model uses the mechanical drilling constraints as feeding input parameters, and the conventional RHOB log data as an output parameter. These mechanical drilling parameters are usually measured while drilling, and their responses vary with different formations. A dataset of 2400 actual datapoints, obtained from a horizontal well in the Middle East, were used to build the proposed models. The obtained dataset was divided into a 70/30 ratio for model training and testing, respectively. The optimized ANFIS-based model outperformed the FN- and SVM-based models with a correlation coefficient (R) of 0.93, and average absolute percentage error (AAPE) of 0.81% between the predicted and measured RHOB values. These results demonstrate the reliability of the developed ANFIS model for predicting RHOB while drilling, based on the mechanical drilling parameters. Subsequently, the ANFIS-based model was validated using unseen data from another well within the same field. The validation process yielded an AAPE of 0.97% between the predicted and actual RHOB values, which confirmed the robustness of the developed model as an effective predictive tool for RHOB.","",""
23,"M. Mitchell","Abstraction and analogy‐making in artificial intelligence",2021,"","","","",127,"2022-07-13 09:32:36","","10.1111/nyas.14619","","",,,,,23,23.00,23,1,1,"Conceptual abstraction and analogy‐making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.","",""
14,"I. Hazarika","Artificial intelligence: opportunities and implications for the health workforce",2020,"","","","",128,"2022-07-13 09:32:36","","10.1093/inthealth/ihaa007","","",,,,,14,7.00,14,1,2,"Abstract Healthcare involves cyclic data processing to derive meaningful, actionable decisions. Rapid increases in clinical data have added to the occupational stress of healthcare workers, affecting their ability to provide quality and effective services. Health systems have to radically rethink strategies to ensure that staff are satisfied and actively supported in their jobs. Artificial intelligence (AI) has the potential to augment provider performance. This article reviews the available literature to identify AI opportunities that can potentially transform the role of healthcare providers. To leverage AI’s full potential, policymakers, industry, healthcare providers and patients have to address a new set of challenges. Optimizing the benefits of AI will require a balanced approach that enhances accountability and transparency while facilitating innovation.","",""
14,"Claudia Gonzalez Viejo, S. Fuentes","Beer Aroma and Quality Traits Assessment Using Artificial Intelligence",2020,"","","","",129,"2022-07-13 09:32:36","","10.3390/fermentation6020056","","",,,,,14,7.00,7,2,2,"Increasing beer quality demands from consumers have put pressure on brewers to target specific steps within the beer-making process to modify beer styles and quality traits. However, this demands more robust methodologies to assess the final aroma profiles and physicochemical characteristics of beers. This research shows the construction of artificial intelligence (AI) models based on aroma profiles, chemometrics, and chemical fingerprinting using near-infrared spectroscopy (NIR) obtained from 20 commercial beers used as targets. Results showed that machine learning models obtained using NIR from beers as inputs were accurate and robust in the prediction of six important aromas for beer (Model 1; R = 0.91; b = 0.87) and chemometrics (Model 2; R = 0.93; b = 0.90). Additionally, two more accurate models were obtained from robotics (RoboBEER) to obtain the same aroma profiles (Model 3; R = 0.99; b = 1.00) and chemometrics (Model 4; R = 0.98; b = 1.00). Low-cost robotics and sensors coupled with computer vision and machine learning modeling could help brewers in the decision-making process to target specific consumer preferences and to secure higher consumer demands.","",""
12,"A. Lal, Yuliya Pinevich, O. Gajic, V. Herasevich, B. Pickering","Artificial intelligence and computer simulation models in critical illness",2020,"","","","",130,"2022-07-13 09:32:36","","10.5492/wjccm.v9.i2.13","","",,,,,12,6.00,2,5,2,"Widespread implementation of electronic health records has led to the increased use of artificial intelligence (AI) and computer modeling in clinical medicine. The early recognition and treatment of critical illness are central to good outcomes but are made difficult by, among other things, the complexity of the environment and the often non-specific nature of the clinical presentation. Increasingly, AI applications are being proposed as decision supports for busy or distracted clinicians, to address this challenge. Data driven “associative” AI models are built from retrospective data registries with missing data and imprecise timing. Associative AI models lack transparency, often ignore causal mechanisms, and, while potentially useful in improved prognostication, have thus far had limited clinical applicability. To be clinically useful, AI tools need to provide bedside clinicians with actionable knowledge. Explicitly addressing causal mechanisms not only increases validity and replicability of the model, but also adds transparency and helps gain trust from the bedside clinicians for real world use of AI models in teaching and patient care.","",""
14,"G. Coskuner, Majeed S Jassim, M. Zontul, Seda Karateke","Application of artificial intelligence neural network modeling to predict the generation of domestic, commercial and construction wastes",2020,"","","","",131,"2022-07-13 09:32:36","","10.1177/0734242X20935181","","",,,,,14,7.00,4,4,2,"Reliable prediction of municipal solid waste (MSW) generation rates is a significant element of planning and implementation of sustainable solid waste management strategies. In this study, the multi-layer perceptron artificial neural network (MLP-ANN) is applied to verify the prediction of annual generation rates of domestic, commercial and construction and demolition (C&D) wastes from the year 1997 to 2016 in Askar Landfill site in the Kingdom of Bahrain. The proposed robust predictive models incorporated selected explanatory variables to reflect the influence of social, demographical, economic, geographical and touristic factors upon waste generation rates (WGRs). The Mean Squared Error (MSE) and coefficient of determination (R2) are used as performance indicators to evaluate effectiveness of the developed models. MLP-ANN models exhibited strong accuracy in predictions with high R2 and low MSE values. The R2 values for domestic, commercial and C&D wastes are 0.95, 0.99 and 0.91, respectively. Our results show that the developed MLP-ANN models are effective for the prediction of WGRs from different sources and could be considered as a cost-effective approach for planning integrated MSW management systems.","",""
10,"J. E. Taylor, Graham W. Taylor","Artificial cognition: How experimental psychology can help generate explainable artificial intelligence.",2020,"","","","",132,"2022-07-13 09:32:36","","10.3758/s13423-020-01825-5","","",,,,,10,5.00,5,2,2,"","",""
10,"Virginia Dignum","Responsibility and Artificial Intelligence",2020,"","","","",133,"2022-07-13 09:32:36","","10.1093/oxfordhb/9780190067397.013.12","","",,,,,10,5.00,10,1,2,"This chapter explores the concept of responsibility in artificial intelligence (AI). Being fundamentally tools, AI systems are fully under the control and responsibility of their owners or users. However, their potential autonomy and capability to learn require that design considers accountability, responsibility, and transparency principles in an explicit and systematic manner. The main concern of Responsible AI is thus the identification of the relative responsibility of all actors involved in the design, development, deployment, and use of AI systems. Firstly, society must be prepared to take responsibility for AI impact. Secondly, Responsible AI implies the need for mechanisms that enable AI systems to act according to ethics and human values. Lastly, Responsible AI is about participation. It is necessary to understand how different people work with and live with AI technologies across cultures in order to develop frameworks for responsible AI.","",""
10,"Xiaohang Wu, Lixue Liu, Lanqin Zhao, Chong Guo, Ruiyang Li, Ting Wang, Xiaonan Yang, Peichen Xie, Yizhi Liu, Haotian Lin","Application of artificial intelligence in anterior segment ophthalmic diseases: diversity and standardization.",2020,"","","","",134,"2022-07-13 09:32:36","","10.21037/ATM-20-976","","",,,,,10,5.00,1,10,2,"Artificial intelligence (AI) based on machine learning (ML) and deep learning (DL) techniques has gained tremendous global interest in this era. Recent studies have demonstrated the potential of AI systems to provide improved capability in various tasks, especially in image recognition field. As an image-centric subspecialty, ophthalmology has become one of the frontiers of AI research. Trained on optical coherence tomography, slit-lamp images and even ordinary eye images, AI can achieve robust performance in the detection of glaucoma, corneal arcus and cataracts. Moreover, AI models based on other forms of data also performed satisfactorily. Nevertheless, several challenges with AI application in ophthalmology have also arisen, including standardization of data sets, validation and applicability of AI models, and ethical issues. In this review, we provided a summary of the state-of-the-art AI application in anterior segment ophthalmic diseases, potential challenges in clinical implementation and our prospects.","",""
11,"K. Mudgal, Neelanjan Das","The ethical adoption of artificial intelligence in radiology",2019,"","","","",135,"2022-07-13 09:32:36","","10.1259/bjro.20190020","","",,,,,11,3.67,6,2,3,"Artificial intelligence (AI) is rapidly transforming healthcare—with radiology at the pioneering forefront. To be trustfully adopted, AI needs to be lawful, ethical and robust. This article covers the different aspects of a safe and sustainable deployment of AI in radiology during: training, integration and regulation. For training, data must be appropriately valued, and deals with AI companies must be centralized. Companies must clearly define anonymization and consent, and patients must be well-informed about their data usage. Data fed into algorithms must be made AI-ready by refining, purification, digitization and centralization. Finally, data must represent various demographics. AI needs to be safely integrated with radiologists-in-the-loop: guiding forming concepts of AI solutions and supervising training and feedback. To be well-regulated, AI systems must be approved by a health authority and agreements must be made upon liability for errors, roles of supervised and unsupervised AI and fair workforce distribution (between AI and radiologists), with a renewal of policy at regular intervals. Any errors made must have a root-cause analysis, with outcomes fedback to companies to close the loop—thus enabling a dynamic best prediction system. In the distant future, AI may act autonomously with little human supervision. Ethical training and integration can ensure a ""transparent"" technology that will allow insight: helping us reflect on our current understanding of imaging interpretation and fill knowledge gaps, eventually moulding radiological practice. This article proposes recommendations for ethical practise that can guide a nationalized framework to build a sustainable and transparent system.","",""
0,"Yusen Xie, Ting Sun, Xinglong Cui, Shuixin Deng, Lei Deng, Baohua Chen","Fast-robust book information extraction system for automated intelligence library",2021,"","","","",136,"2022-07-13 09:32:36","","10.1109/AIID51893.2021.9456499","","",,,,,0,0.00,0,6,1,"At present, in the large-scale book management scene, book sorting, daily maintenance and book retrieval are very common, but the book information is complicated and the efficiency of relying on manual management is extremely poor. Although there have been many self-service book systems based on optics or vision, they are mostly based on traditional computer vision algorithms such as boundary extraction. Due to the fact that there are more artificial experience thresholds, some shortcomings such as low detection accuracy, poor robustness, and inability to systematically deploy on a large scale, which lack of insufficient intelligence. Therefore, we proposed a book information extraction algorithm based on object detection and optical character recognition (OCR) that is suitable for multiple book information recognition, multiple book image angles and multiple book postures. It can be applied to scenes such as book sorting, bookshelf management and book retrieval. The system we designed includes the classification of book covers and back covers, the classification of books upright and inverted, the detection of book pages side and spine side, the recognition of book pricing. In terms of accuracy, the classification accuracy of the front cover and the back cover is 99.9%, the upright classification accuracy of book front covers is 98.8%, the back cover reaches 99.9%, the accuracy of book price recognition get 94.5%, and the book spine/page side detection mAP reaches 99.6%; in terms of detection speed, Yolov5 detection model was improved and the statistical-based pre-pruning strategy was adopted, support by our algorithm the system reaches 2.09 FPS in book price recognition, which improves the detection speed to meet actual needs.","",""
8,"Baloko Makala, Tonci Bakovic","Artificial Intelligence in the Power Sector",2020,"","","","",137,"2022-07-13 09:32:36","","10.1596/34303","","",,,,,8,4.00,4,2,2,"The energy sector worldwide faces growing challenges related to rising demand, efficiency, changing supply and demand patterns, and a lack of analytics needed for optimal management. These challenges are more acute in emerging market nations. Efficiency issues are particularly problematic, as the prevalence of informal connections to the power grid means a large amount of power is neither measured nor billed, resulting in losses as well as greater CO2 emissions, as consumers have little incentive to rationally use energy they don’t pay for. The power sector in developed nations has already begun to use artificial intelligence and related technologies that allow for communication between smart grids, smart meters, and Internet of Things devices. These technologies can help improve power management, efficiency, and transparency, and increase the use of renewable energy sources.","",""
7,"R. Kusters, D. Misevic, H. Berry, Antoine Cully, Y. Le Cunff, Loic Dandoy, N. Díaz-Rodríguez, Marion Ficher, Jonathan Grizou, Alice Othmani, Themis Palpanas, M. Komorowski, P. Loiseau, Clément Moulin Frier, Santino Nanini, D. Quercia, M. Sebag, Françoise Soulié Fogelman, Sofiane Taleb, Liubov Tupikina, Vaibhav Sahu, J. Vie, Fatima Wehbi","Interdisciplinary Research in Artificial Intelligence: Challenges and Opportunities",2020,"","","","",138,"2022-07-13 09:32:36","","10.3389/fdata.2020.577974","","",,,,,7,3.50,1,23,2,"The use of artificial intelligence (AI) in a variety of research fields is speeding up multiple digital revolutions, from shifting paradigms in healthcare, precision medicine and wearable sensing, to public services and education offered to the masses around the world, to future cities made optimally efficient by autonomous driving. When a revolution happens, the consequences are not obvious straight away, and to date, there is no uniformly adapted framework to guide AI research to ensure a sustainable societal transition. To answer this need, here we analyze three key challenges to interdisciplinary AI research, and deliver three broad conclusions: 1) future development of AI should not only impact other scientific domains but should also take inspiration and benefit from other fields of science, 2) AI research must be accompanied by decision explainability, dataset bias transparency as well as development of evaluation methodologies and creation of regulatory agencies to ensure responsibility, and 3) AI education should receive more attention, efforts and innovation from the educational and scientific communities. Our analysis is of interest not only to AI practitioners but also to other researchers and the general public as it offers ways to guide the emerging collaborations and interactions toward the most fruitful outcomes.","",""
7,"Nariman Ammar, Arash Shaban-Nejad","Explainable Artificial Intelligence Recommendation System by Leveraging the Semantics of Adverse Childhood Experiences: Proof-of-Concept Prototype Development",2020,"","","","",139,"2022-07-13 09:32:36","","10.2196/18752","","",,,,,7,3.50,4,2,2,"Background The study of adverse childhood experiences and their consequences has emerged over the past 20 years. Although the conclusions from these studies are available, the same is not true of the data. Accordingly, it is a complex problem to build a training set and develop machine-learning models from these studies. Classic machine learning and artificial intelligence techniques cannot provide a full scientific understanding of the inner workings of the underlying models. This raises credibility issues due to the lack of transparency and generalizability. Explainable artificial intelligence is an emerging approach for promoting credibility, accountability, and trust in mission-critical areas such as medicine by combining machine-learning approaches with explanatory techniques that explicitly show what the decision criteria are and why (or how) they have been established. Hence, thinking about how machine learning could benefit from knowledge graphs that combine “common sense” knowledge as well as semantic reasoning and causality models is a potential solution to this problem. Objective In this study, we aimed to leverage explainable artificial intelligence, and propose a proof-of-concept prototype for a knowledge-driven evidence-based recommendation system to improve mental health surveillance. Methods We used concepts from an ontology that we have developed to build and train a question-answering agent using the Google DialogFlow engine. In addition to the question-answering agent, the initial prototype includes knowledge graph generation and recommendation components that leverage third-party graph technology. Results To showcase the framework functionalities, we here present a prototype design and demonstrate the main features through four use case scenarios motivated by an initiative currently implemented at a children’s hospital in Memphis, Tennessee. Ongoing development of the prototype requires implementing an optimization algorithm of the recommendations, incorporating a privacy layer through a personal health library, and conducting a clinical trial to assess both usability and usefulness of the implementation. Conclusions This semantic-driven explainable artificial intelligence prototype can enhance health care practitioners’ ability to provide explanations for the decisions they make.","",""
8,"I. Wiafe, F. N. Koranteng, Emmanuel Nyarko Obeng, Nana Assyne, Abigail Wiafe, S. Gulliver","Artificial Intelligence for Cybersecurity: A Systematic Mapping of Literature",2020,"","","","",140,"2022-07-13 09:32:36","","10.1109/ACCESS.2020.3013145","","",,,,,8,4.00,1,6,2,"Due to the ever-increasing complexities in cybercrimes, there is the need for cybersecurity methods to be more robust and intelligent. This will make defense mechanisms to be capable of making real-time decisions that can effectively respond to sophisticated attacks. To support this, both researchers and practitioners need to be familiar with current methods of ensuring cybersecurity (CyberSec). In particular, the use of artificial intelligence for combating cybercrimes. However, there is lack of summaries on artificial intelligent methods for combating cybercrimes. To address this knowledge gap, this study sampled 131 articles from two main scholarly databases (ACM digital library and IEEE Xplore). Using a systematic mapping, the articles were analyzed using quantitative and qualitative methods. It was observed that artificial intelligent methods have made remarkable contributions to combating cybercrimes with significant improvement in intrusion detection systems. It was also observed that there is a reduction in computational complexity, model training times and false alarms. However, there is a significant skewness within the domain. Most studies have focused on intrusion detection and prevention systems, and the most dominant technique used was support vector machines. The findings also revealed that majority of the studies were published in two journal outlets. It is therefore suggested that to enhance research in artificial intelligence for CyberSec, researchers need to adopt newer techniques and also publish in other related outlets.","",""
7,"Ashley Kras, L. Celi, John B. Miller","Accelerating ophthalmic artificial intelligence research: the role of an open access data repository.",2020,"","","","",141,"2022-07-13 09:32:36","","10.1097/ICU.0000000000000678","","",,,,,7,3.50,2,3,2,"PURPOSE OF REVIEW Artificial intelligence has already provided multiple clinically relevant applications in ophthalmology. Yet, the explosion of nonstandardized reporting of high-performing algorithms are rendered useless without robust and streamlined implementation guidelines. The development of protocols and checklists will accelerate the translation of research publications to impact on patient care.   RECENT FINDINGS Beyond technological scepticism, we lack uniformity in analysing algorithmic performance generalizability, and benchmarking impacts across clinical settings. No regulatory guardrails have been set to minimize bias or optimize interpretability; no consensus clinical acceptability thresholds or systematized postdeployment monitoring has been set. Moreover, stakeholders with misaligned incentives deepen the landscape complexity especially when it comes to the requisite data integration and harmonization to advance the field. Therefore, despite increasing algorithmic accuracy and commoditization, the infamous 'implementation gap' persists. Open clinical data repositories have been shown to rapidly accelerate research, minimize redundancies and disseminate the expertise and knowledge required to overcome existing barriers. Drawing upon the longstanding success of existing governance frameworks and robust data use and sharing agreements, the ophthalmic community has tremendous opportunity in ushering artificial intelligence into medicine. By collaboratively building a powerful resource of open, anonymized multimodal ophthalmic data, the next generation of clinicians can advance data-driven eye care in unprecedented ways.   SUMMARY This piece demonstrates that with readily accessible data, immense progress can be achieved clinically and methodologically to realize artificial intelligence's impact on clinical care. Exponentially progressive network effects can be seen by consolidating, curating and distributing data amongst both clinicians and data scientists.","",""
8,"Jun Zhu, Hang Su, Bo Zhang","Toward the third generation of artificial intelligence",2020,"","","","",142,"2022-07-13 09:32:36","","10.1360/ssi-2020-0204","","",,,,,8,4.00,3,3,2,"There have been two competing paradigms of artificial intelligence (AI) development since 1956, i.e., symbolism and connectionism (or subsymbolism). Both started at the same time, but symbolism had dominated AI development until the end of the 1980s. Connectionism began to develop in the 1990s and reached its climax at the beginning of this century, and it is likely to displace symbolism. Today, it seems that the two paradigms only simulate the human mind (or brain) in different ways and have their own advantages. True human intelligence cannot be achieved by relying on only one paradigm. Both are necessary to establish a new, explainable, and robust AI theory and method and develop safe, trustworthy, reliable, and extensible AI technology. To this end, it is imperative to combine the two paradigms, and the present article will illustrate this idea. For the sake of description, symbolism, connectionism, and the newly developed paradigm are termed as first-, second-, and third-generation AIs.","",""
6,"N. Elkin-Koren","Contesting algorithms: Restoring the public interest in content filtering by artificial intelligence",2020,"","","","",143,"2022-07-13 09:32:36","","10.1177/2053951720932296","","",,,,,6,3.00,6,1,2,"In recent years, artificial intelligence has been deployed by online platforms to prevent the upload of allegedly illegal content or to remove unwarranted expressions. These systems are trained to spot objectionable content and to remove it, block it, or filter it out before it is even uploaded. Artificial intelligence filters offer a robust approach to content moderation which is shaping the public sphere. This dramatic shift in norm setting and law enforcement is potentially game-changing for democracy. Artificial intelligence filters carry censorial power, which could bypass traditional checks and balances secured by law. Their opaque and dynamic nature creates barriers to oversight, and conceals critical value choices and tradeoffs. Currently, we lack adequate tools to hold them accountable. This paper seeks to address this gap by introducing an adversarial procedure— – Contesting Algorithms. It proposes to deliberately introduce friction into the dominant removal systems governed by artificial intelligence. Algorithmic content moderation often seeks to optimize a single goal, such as removing copyright-infringing materials or blocking hate speech, while other values in the public interest, such as fair use or free speech, are often neglected. Contesting algorithms introduce an adversarial design which reflects conflicting values, and thereby may offer a check on dominant removal systems. Facilitating an adversarial intervention may promote democratic principles by keeping society in the loop. An adversarial public artificial intelligence system could enhance dynamic transparency, facilitate an alternative public articulation of social values using machine learning systems, and restore societal power to deliberate and determine social tradeoffs.","",""
585,"J. He, Sally L. Baxter, Jie Xu, Jiming Xu, Xingtao Zhou, Kang Zhang","The practical implementation of artificial intelligence technologies in medicine",2019,"","","","",144,"2022-07-13 09:32:36","","10.1038/s41591-018-0307-0","","",,,,,585,195.00,98,6,3,"","",""
423,"Andreas Holzinger, G. Langs, H. Denk, K. Zatloukal, Heimo Müller","Causability and explainability of artificial intelligence in medicine",2019,"","","","",145,"2022-07-13 09:32:36","","10.1002/widm.1312","","",,,,,423,141.00,85,5,3,"Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black‐box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use‐case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system","",""
2,"Weiheng Li, Xiangmin Fan, He Zhu, Jingzheng Wu, Dongxing Teng","Research on the Influencing Factors of User Trust Based on Artificial Intelligence Self Diagnosis System",2020,"","","","",146,"2022-07-13 09:32:36","","10.1145/3393527.3393561","","",,,,,2,1.00,0,5,2,"Artificial intelligence technology has gradually become an important auxiliary means for people to obtain medical consultation. However, many researches reflect users' doubts about the feedback results of AI system. Therefore, this paper focuses on the design of the user-friendly intelligent self diagnosis system, using interdisciplinary research methods. Firstly, based on the psychological questionnaire and interview, this paper studies the public's cognition of the intelligent self diagnosis system, and discusses how to effectively improve the user's recognition and trust of the intelligent self diagnosis system. It is found that the credibility of the intelligent self diagnosis system can be improved by further improving the transparency of the system. For example, users can be provided with four different types of information, including system reasoning, system reliability, information source and personalized information. On the basis of interviews, 48 volunteers were invited to carry out a comparative experiment in groups to evaluate the impact of medical interpretation of intelligent self diagnosis system on patients' perception and trust under different transparency and accuracy models. We found that the improvement of system reliability information (i.e. accuracy model and confidence score) composed of accuracy model and confidence score strengthens patients' cognition of AI system and improves the user trust of the system; while the detailed display of system reasoning and logic can improve the comprehensibility of medical advice output of the system, but does not substantially improve the user trust. At the end of this paper, some suggestions are put forward to design an interpretable and reliable intelligent self diagnosis system.","",""
5,"David Abele, Sara D’Onofrio","Artificial Intelligence – The Big Picture",2020,"","","","",147,"2022-07-13 09:32:36","","10.1007/978-3-658-27941-7_2","","",,,,,5,2.50,3,2,2,"","",""
5,"Christian Meske, Enrico Bunde","Using Explainable Artificial Intelligence to Increase Trust in Computer Vision",2020,"","","","",148,"2022-07-13 09:32:36","","","","",,,,,5,2.50,3,2,2,"Computer Vision, and hence Artificial Intelligence-based extraction of information from images, has increasingly received attention over the last years, for instance in medical diagnostics. While the algorithms' complexity is a reason for their increased performance, it also leads to the ""black box"" problem, consequently decreasing trust towards AI. In this regard, ""Explainable Artificial Intelligence"" (XAI) allows to open that black box and to improve the degree of AI transparency. In this paper, we first discuss the theoretical impact of explainability on trust towards AI, followed by showcasing how the usage of XAI in a health-related setting can look like. More specifically, we show how XAI can be applied to understand why Computer Vision, based on deep learning, did or did not detect a disease (malaria) on image data (thin blood smear slide images). Furthermore, we investigate, how XAI can be used to compare the detection strategy of two different deep learning models often used for Computer Vision: Convolutional Neural Network and Multi-Layer Perceptron. Our empirical results show that i) the AI sometimes used questionable or irrelevant data features of an image to detect malaria (even if correctly predicted), and ii) that there may be significant discrepancies in how different deep learning models explain the same prediction. Our theoretical discussion highlights that XAI can support trust in Computer Vision systems, and AI systems in general, especially through an increased understandability and predictability.","",""
2,"Carlos E. Jimenez-Gomez, Jesús Cano Carrillo, F. Falcone","Artificial Intelligence in Government",2020,"","","","",149,"2022-07-13 09:32:36","","10.1109/MC.2020.3010043","","",,,,,2,1.00,1,3,2,"The articles in this special section focus on government applications that use artificial intelligence (AI). The repercussions of artificial intelligence (AI) in government are broad and significant. The characteristics of these technologies will have an impact on almost everything in public organizations, from governance or the multidimensional perspective of interoperability, to the organizational or social implications linked to concepts like public value, transparency, or accountability. This special issue seeks to shed light on foundations and key elements to be taken into account for AI adoption by public organizations. Governments are the primary enablers of technology and market stimulators and regulators of general activities in our society. Governments have always sought the common good and, therefore, the advancement of public and collective interests. This is key to understanding, as a first step, why the principles of public-sector organizations do not always match those of the private sector. Public and private perspectives are very different, whether they be management, strategy, or policy.","",""
2,"Richard, M. Surya, Avenia Clarissa Wibowo","Converging Artificial Intelligence and Blockchain Technology using Oracle Contract in Ethereum Blockchain Platform",2020,"","","","",150,"2022-07-13 09:32:36","","10.1109/ICIC50835.2020.9288611","","",,,,,2,1.00,1,3,2,"Artificial Intelligence (AI) and blockchain are two emerging concepts that explored exponentially in recent years. In software development, artificial Intelligence offers a sophisticated enhancement on the programming logic, while blockchain technology provides transparency and relief the obligation to trust in a central party. Both are two different technologies but considered a disruptive technology in recent years. Artificial Intelligence faces several issues related to the validity of data, algorithms, and policies, while blockchain is called so “friendly” with transparency, trust, and immutability. Both technologies are still in the early stages of their development age. Always, the opportunity for both technologies to converge in some ways would be an exciting area to be explored. This research elaborates on the technical possibilities of application development on the blockchain platform that communicate with AI to do prediction task. Oracle contract allows the data inside Ethereum blockchain to interact with the external data source.","",""
2,"M. Bashayreh, F. Sibai, Amer Tabbara","Artificial intelligence and legal liability: towards an international approach of proportional liability based on risk sharing",2021,"","","","",151,"2022-07-13 09:32:36","","10.1080/13600834.2020.1856025","","",,,,,2,2.00,1,3,1,"ABSTRACT This paper critically examines the allocation of liability when autonomous artificial intelligence (AI) systems cause accidents. Problems of applying existing principles of legal liability in AI environment are addressed. This paper argues that the sharing of risk as a basis for proportionate liability should be a basis for a new liability regime to govern future autonomous machines. It is argued that this approach favors the reality of parties’ consent to taking the risk of unpredictable AI behavior over the technicality of existing principles of legal liability. The suggested approach also encourages transparency and responsible decisions of developers and owners of AI systems. A flowchart to clarify possible outcomes of applying the suggested approach is provided. The paper also discusses the need for harmonization of national laws and international cooperation regarding AI incidents crossing national borders to ensure predictability of legal rules governing the liability ensuing from AI applications.","",""
5,"Cathy O'Neil, H. Gunn","Near-Term Artificial Intelligence and the Ethical Matrix",2020,"","","","",152,"2022-07-13 09:32:36","","10.1093/oso/9780190905033.003.0009","","",,,,,5,2.50,3,2,2,"This chapter takes up the issue of near-term artificial intelligence, or the algorithms that are already in place in a variety of public and private sectors, guiding decisions from advertising and to credit ratings to sentencing in the justice system. There is a pressing need to recognize and evaluate the ways that structural racism, sexism, classism, and ableism may be embedded in and amplified by these systems. The chapter proposes a framework for ethical analysis that can be used to facilitate more robust ethical reflection in AI development and implementation. It presents an ethical matrix that incorporates the language of data science as a tool that data scientists can build themselves in order to integrate ethical analysis into the design process, addressing the need for immediate analysis and accountability over the design and deployment of near-term AI.","",""
2,"Dr. Uma Devi, Maria Tresita, V. Paul","Artificial Intelligence: Pertinence in Supply Chain and Logistics Management",2020,"","","","",153,"2022-07-13 09:32:36","","","","",,,,,2,1.00,1,3,2,"-Artificial Intelligence (AI) is the revolutionary invention of human intelligence. Artificial Intelligence is nothing but the duplication of human in which machines are programmed to rationally think and behave like humans developed for very many purposes including business decision making, problem-solving, business data analysis and interpretation and information management. The application of AI in business endeavours decides the competitive advantage, market leadership, robust operating efficiency of corporates and other business houses. Exploiting the application of AI in the manufacturing and distribution process enables the organisations to reach the pinnacle in their business graph. Businesses are operating in the international market which is highly multifaceted and challenging to serve the world as a sole market for their products, services and their products and without the integration of technology into their business processes, they cannot assure the sustainable growth. The management of the process of transforming the raw materials into the final product is called Supply Chain Management (SCM) and the effective movement and storage of goods, services and information are called Logistics Management (LM). This article analyses the applications of Artificial Intelligence in Supply Chain and Logistics Management (SC&LM) Keywords--Artificial Intelligence, Supply Chain Management, Logistics Management, Supply Chain Profitability","",""
54,"G. Collins, P. Dhiman, Constanza L. Andaur Navarro, Jie Ma, L. Hooft, J. Reitsma, P. Logullo, Andrew Beam, Lily Peng, B. van Calster, M. van Smeden, R. Riley, K. Moons","Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence",2021,"","","","",154,"2022-07-13 09:32:36","","10.1136/bmjopen-2020-048008","","",,,,,54,54.00,5,13,1,"Introduction The Transparent Reporting of a multivariable prediction model of Individual Prognosis Or Diagnosis (TRIPOD) statement and the Prediction model Risk Of Bias ASsessment Tool (PROBAST) were both published to improve the reporting and critical appraisal of prediction model studies for diagnosis and prognosis. This paper describes the processes and methods that will be used to develop an extension to the TRIPOD statement (TRIPOD-artificial intelligence, AI) and the PROBAST (PROBAST-AI) tool for prediction model studies that applied machine learning techniques. Methods and analysis TRIPOD-AI and PROBAST-AI will be developed following published guidance from the EQUATOR Network, and will comprise five stages. Stage 1 will comprise two systematic reviews (across all medical fields and specifically in oncology) to examine the quality of reporting in published machine-learning-based prediction model studies. In stage 2, we will consult a diverse group of key stakeholders using a Delphi process to identify items to be considered for inclusion in TRIPOD-AI and PROBAST-AI. Stage 3 will be virtual consensus meetings to consolidate and prioritise key items to be included in TRIPOD-AI and PROBAST-AI. Stage 4 will involve developing the TRIPOD-AI checklist and the PROBAST-AI tool, and writing the accompanying explanation and elaboration papers. In the final stage, stage 5, we will disseminate TRIPOD-AI and PROBAST-AI via journals, conferences, blogs, websites (including TRIPOD, PROBAST and EQUATOR Network) and social media. TRIPOD-AI will provide researchers working on prediction model studies based on machine learning with a reporting guideline that can help them report key details that readers need to evaluate the study quality and interpret its findings, potentially reducing research waste. We anticipate PROBAST-AI will help researchers, clinicians, systematic reviewers and policymakers critically appraise the design, conduct and analysis of machine learning based prediction model studies, with a robust standardised tool for bias evaluation. Ethics and dissemination Ethical approval has been granted by the Central University Research Ethics Committee, University of Oxford on 10-December-2020 (R73034/RE001). Findings from this study will be disseminated through peer-review publications. PROSPERO registration number CRD42019140361 and CRD42019161764.","",""
496,"Christopher J. Kelly, A. Karthikesalingam, Mustafa Suleyman, Greg Corrado, Dominic King","Key challenges for delivering clinical impact with artificial intelligence",2019,"","","","",155,"2022-07-13 09:32:36","","10.1186/s12916-019-1426-2","","",,,,,496,165.33,99,5,3,"","",""
4,"Bahman Zohuri","From Business Intelligence to Artificial Intelligence",2020,"","","","",156,"2022-07-13 09:32:36","","10.32474/MAMS.2020.02.000137","","",,,,,4,2.00,4,1,2,"With today’s growing information and overloading of its volume based on tremendous size of data growing to the level of big data, Business Intelligence (BI) is not enough to handle any day-to-day business operation of any enterprises. It is becoming tremendously difficult to analyze the huge amounts of data that contain the information and makes it very strenuous and inconvenient to introduce an appropriate methodology of decision-making fast enough to the point that it can be, considered as real time, a methodology that we used to call it BI. The demand for real time processing information and related data both structured and unstructured is on the rise and consequently makes it harder and harder to implement correct decision making at enterprise level that was driven by BI, in order to keep the organization robust and resilient against either man made threats or natural disasters. With smart malware in modern computation world and necessity for Internet-of-Things (IoT), we are in need of a better intelligence system that today we know it as Artificial Intelligence (AI). AI with its two other subset that are called Machine Learning (ML) and Deep Learning (DL), we have a better chance against any cyber-attack and makes our day-to-day operation within our organization a more robust one as well makes our decision making as stakeholder more trust worthy one as well.","",""
5,"Lindong Zhao, Xuguang Zhang, Jianxin Chen, Liang Zhou","Physical Layer Security in the Age of Artificial Intelligence and Edge Computing",2020,"","","","",157,"2022-07-13 09:32:36","","10.1109/MWC.001.2000044","","",,,,,5,2.50,1,4,2,"Physical layer security (PLS) is emerging as an attractive security paradigm to complement or even replace complex cryptography. Although information-theoretical transmission optimization and physical-layer key generation have been thoroughly researched, there still exist many critical issues to be tackled before PLS is extensively applied. In this article, we investigate the prospect for exploiting artificial intelligent (AI) and edge computing (EC) to facilitate the practical application of PLS. First, two outstanding challenges facing PLS designers are identified by analyzing the fundamental assumptions regarding eavesdroppers and wireless channels. Accordingly, two enhancement schemes are designed by reaping the benefits offered by AI and EC. Specifically, a novel secure resource management framework is developed to enhance the adaptability of an optimization-based PLS paradigm, and a robust physical-layer key generation method is designed to cope with reciprocity failure. Finally, we discuss a coordinated defense architecture with multi-layer, multi-domain, and multi-dimension, which is expected to exploit the compatibility and complementarity of the existing PLS methods.","",""
0,"A. Paic","Policies for Artificial Intelligence in Science and Innovation",2020,"","","","",158,"2022-07-13 09:32:36","","10.22323/1.372.0045","","",,,,,0,0.00,0,1,2,"This contribution synthesizes the discussions of the special session on policies for Artificial Intelligence in Science and Innovation, organized by the OECD’s Directorate for Science, Technology and Innovation. The session was opened by Dr Judith Arrieta, Minister of the Foreign Service at the Chief of Staff’s Office of the Secretary of Foreign Affairs of Mexico, and the two panels included speakers from governments, industry and civil society from European countries, USA,Canada China and Australia. Participants discussed the disruptive nature of AI and the formidable challenges it poses. Most of the discussion focused under the umbrella title of ethics, but they span very different issues of human-centered values, fairness, transparency, explainability, and many more. Other challenges include employment, education, SME policy, enabling environment, access to data and computing technology. Responses by governments were also discussed with a particular focus on national strategies, whose main pillars are oriented toward knowledge creation through AI research, knowledge diffusion through linkages to the private sector, development of human capital which will underpin the development of the sector, and a strong values, ethical and regulatory framework to create the conditions for the development of trustworthy AI.  In a world of finite resources, discussants concluded that one cannot apply very stringent requirements to all AI decisions, and there is clearly a need to require more transparency, explainability and robustness from systems which have the greatest impact on human lives. Therefore an approach based on algorithmic impact assessment seems reasonable. Such an approach needs to be further developed and standardized.","",""
0,"Katanosh Morovat, B. Panda","A Survey of Artificial Intelligence in Cybersecurity",2020,"","","","",159,"2022-07-13 09:32:36","","10.1109/CSCI51800.2020.00026","","",,,,,0,0.00,0,2,2,"During the last decades, not only the number of cyberattacks have increased significantly, they have also become more sophisticated. Hence designing a cyber-resilient approach is of paramount importance. Traditional security methods are not adequate to prevent data breaches in case of cyberattacks. Cybercriminals have learned how to use new techniques and robust tools to hack, attack, and breach data. Fortunately, Artificial Intelligence (AI) technologies have been introduced into cyberspace to construct smart models for defending systems from attacks. Since AI technologies can rapidly evolve to address complex situations, they can be used as fundamental tools in the field of cybersecurity. Al-based techniques can provide efficient and powerful cyber defense tools to recognize malware attacks, network intrusions, phishing and spam emails, and data breaches, to name a few, and to alert security incidents when they occur. In this paper, we review the impact of AI in cybersecurity and summarize existing research in terms of benefits of AI in cybersecurity.","",""
17,"Shengjie Xu, Y. Qian, R. Hu","Data-Driven Edge Intelligence for Robust Network Anomaly Detection",2020,"","","","",160,"2022-07-13 09:32:36","","10.1109/TNSE.2019.2936466","","",,,,,17,8.50,6,3,2,"The advancement of networking platforms for assured online services requires robust and effective network intelligence systems against anomalous events and malicious threats. With the rapid development of modern communication technologies, artificial intelligence, and the revolution of computing devices, cloud computing empowered network intelligence will inevitably become a core platform for various smart applications. While cloud computing provides strong and powerful computation, storage, and networking services to detect and defend cyber threats, edge computing on the other hand will deliver more benefits in specific yet potential critical areas. In this paper, we present a study on the data-driven edge intelligence for robust network anomaly detection. We first highlight the main motivations for edge intelligence, and then propose an intelligence system empowered by edge computing for network anomaly detection. We further propose a scheme on the data-driven robust network anomaly detection. In the proposed scheme, four phases are designed to incorporate with data-driven approaches to train a learning model which is able to detect and identify a network anomaly in a robust way. In the performance evaluations with data experiments, we demonstrate that the proposed scheme achieves the robustness of trained model and the efficiency on the detection of specific anomalies.","",""
286,"Anna Jobin, M. Ienca, E. Vayena","Artificial Intelligence: the global landscape of ethics guidelines",2019,"","","","",161,"2022-07-13 09:32:36","","10.1038/s42256-019-0088-2","","",,,,,286,95.33,95,3,3,"","",""
11,"Aditya Kuppa, N. Le-Khac","Black Box Attacks on Explainable Artificial Intelligence(XAI) methods in Cyber Security",2020,"","","","",162,"2022-07-13 09:32:36","","10.1109/IJCNN48605.2020.9206780","","",,,,,11,5.50,6,2,2,"Cybersecurity community is slowly leveraging Machine Learning (ML) to combat ever evolving threats. One of the biggest drivers for successful adoption of these models is how well domain experts and users are able to understand and trust their functionality. As these black-box models are being employed to make important predictions, the demand for transparency and explainability is increasing from the stakeholders.Explanations supporting the output of ML models are crucial in cyber security, where experts require far more information from the model than a simple binary output for their analysis. Recent approaches in the literature have focused on three different areas: (a) creating and improving explainability methods which help users better understand the internal workings of ML models and their outputs; (b) attacks on interpreters in white box setting; (c) defining the exact properties and metrics of the explanations generated by models. However, they have not covered, the security properties and threat models relevant to cybersecurity domain, and attacks on explainable models in black box settings.In this paper, we bridge this gap by proposing a taxonomy for Explainable Artificial Intelligence (XAI) methods, covering various security properties and threat models relevant to cyber security domain. We design a novel black box attack for analyzing the consistency, correctness and confidence security properties of gradient based XAI methods. We validate our proposed system on 3 security-relevant data-sets and models, and demonstrate that the method achieves attacker’s goal of misleading both the classifier and explanation report and, only explainability method without affecting the classifier output. Our evaluation of the proposed approach shows promising results and can help in designing secure and robust XAI methods.","",""
21,"Adrien Bécue, Isabel Praça, J. Gama","Artificial intelligence, cyber-threats and Industry 4.0: challenges and opportunities",2021,"","","","",163,"2022-07-13 09:32:36","","10.1007/S10462-020-09942-2","","",,,,,21,21.00,7,3,1,"","",""
26,"P. Radanliev, D. D. Roure, M. V. Kleek, Omar Santos, U. Ani","Artificial intelligence in cyber physical systems",2019,"","","","",164,"2022-07-13 09:32:36","","10.1007/s00146-020-01049-0","","",,,,,26,8.67,5,5,3,"","",""
0,"","Patients set to benefit from new guidelines on artificial intelligence health solutions",2020,"","","","",165,"2022-07-13 09:32:36","","","","",,,,,0,0.00,0,0,2,"The use of these international guidelines will enable patients, health care professionals and policy-makers to be more confident on whether an AI intervention is safe and effective. This is a key step towards trustworthy AI in health. Development of new reporting guidelines which expand on the current SPIRIT 2013 and CONSORT 2010 reporting frameworks will boost transparency and robustness for clinical trials evaluating AI health solutions.","",""
15,"S. Ebrahimian, Fatemeh Homayounieh, M. Rockenbach, Preetham Putha, T. Raj, I. Dayan, B. Bizzo, Varun Buch, Dufan Wu, Kyungsang Kim, Quanzheng Li, S. Digumarthy, M. Kalra","Artificial intelligence matches subjective severity assessment of pneumonia for prediction of patient outcome and need for mechanical ventilation: a cohort study",2021,"","","","",166,"2022-07-13 09:32:36","","10.1038/s41598-020-79470-0","","",,,,,15,15.00,2,13,1,"","",""
19,"Ruhhee Tabbussum, A. Q. Dar","Performance evaluation of artificial intelligence paradigms—artificial neural networks, fuzzy logic, and adaptive neuro-fuzzy inference system for flood prediction",2021,"","","","",167,"2022-07-13 09:32:36","","10.1007/s11356-021-12410-1","","",,,,,19,19.00,10,2,1,"","",""
21,"Radhia Garraoui, M. Hamed, L. Sbita","Comparison of MPPT algorithms for DC-DC boost converters based PV systems using robust control technique and artificial intelligence algorithm",2015,"","","","",168,"2022-07-13 09:32:36","","10.1109/SSD.2015.7348163","","",,,,,21,3.00,7,3,7,"This paper proposes two methods of maximum power point tracking algorithm for photovoltaic systems, based on the first hand on fuzzy logic control and on the other hand on the first order sliding mode control. According to the nonlinear characteristic of photovoltaic array, it's necessary to find a solution to track the maximum power of the PV system in order to improve its efficiency. The fuzzy logic controller was presented in many works. It provides fast response and good performance against the climatic and load change and uses directly the DC/DC converter duty cycle as a control parameter. Moreover, the sliding mode control approach is recognized as one of the efficient tools to design robust controllers it has been receiving much more attention within the last two decades and many research are dealing with this type of robust controllers. A detailed comparison between the fuzzy logic and slinging mode controllers was presented in this work. Simulation results show that the proposed algorithms can effectively improve the efficiency of a photovoltaic array output.","",""
4,"L. Floridi","Artificial Intelligence as a Public Service: Learning from Amsterdam and Helsinki",2020,"","","","",169,"2022-07-13 09:32:36","","10.1007/s13347-020-00434-3","","",,,,,4,2.00,4,1,2,"","",""
0,"N. Köbis, Luca Mossink","Creative Artificial Intelligence - Algorithms vs. humans in an incentivized writing competition",2020,"","","","",170,"2022-07-13 09:32:36","","","","",,,,,0,0.00,0,2,2,"The release of openly available, robust text generation algorithms has spurred much public attention and debate, due to algorithm's purported ability to generate human-like text across various domains. Yet, empirical evidence using incentivized tasks to assess human behavioral reactions to such algorithms is lacking. We conducted two experiments assessing behavioral reactions to the state-of-the-art Natural Language Generation algorithm GPT-2 (Ntotal = 830). Using the identical starting lines of human poems, GPT-2 produced samples of multiple algorithmically-generated poems. From these samples, either a random poem was chosen (Human-out-of-the-loop) or the best one was selected (Human-in-the-loop) and in turn matched with a human written poem. Taking part in a new incentivized version of the Turing Test, participants failed to reliably detect the algorithmically-generated poems in the human-in-the-loop treatment, yet succeeded in the Human-out-of-the-loop treatment. Further, the results reveal a general aversion towards algorithmic poetry, independent on whether participants were informed about the algorithmic origin of the poem (Transparency) or not (Opacity). We discuss what these results convey about the performance of NLG algorithms to produce human-like text and propose methodologies to study such learning algorithms in experimental settings.","",""
196,"W. Samek, K. Müller","Towards Explainable Artificial Intelligence",2019,"","","","",171,"2022-07-13 09:32:36","","10.1007/978-3-030-28954-6_1","","",,,,,196,65.33,98,2,3,"","",""
755,"Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Q. Song, K. Cao, Daliang Liu, Guisheng Wang, Qizhong Xu, Xisheng Fang, Shiqin Zhang, J. Xia, Jun Xia","Artificial Intelligence Distinguishes COVID-19 from Community Acquired Pneumonia on Chest CT",2020,"","","","",172,"2022-07-13 09:32:36","","10.1148/radiol.2020200905","","",,,,,755,377.50,76,18,2,"Background Coronavirus disease has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performances. Materials and Methods In this retrospective and multi-center study, a deep learning model, COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT exams for the detection of COVID-19. Community acquired pneumonia (CAP) and other non-pneumonia CT exams were included to test the robustness of the model. The datasets were collected from 6 hospitals between August 2016 and February 2020. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity and specificity. Results The collected dataset consisted of 4356 chest CT exams from 3,322 patients. The average age is 49±15 years and there were slightly more male patients than female (1838 vs 1484; p-value=0.29). The per-exam sensitivity and specificity for detecting COVID-19 in the independent test set was 114 of 127 (90% [95% CI: 83%, 94%]) and 294 of 307 (96% [95% CI: 93%, 98%]), respectively, with an AUC of 0.96 (p-value<0.001). The per-exam sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175) and 92% (239 of 259), respectively, with an AUC of 0.95 (95% CI: 0.93, 0.97). Conclusions A deep learning model can accurately detect COVID-19 and differentiate it from community acquired pneumonia and other lung diseases.","",""
15,"J. Janet, Chenru Duan, A. Nandy, Fang Liu, H. Kulik","Navigating Transition-Metal Chemical Space: Artificial Intelligence for First-Principles Design.",2021,"","","","",173,"2022-07-13 09:32:36","","10.1021/acs.accounts.0c00686","","",,,,,15,15.00,3,5,1,"ConspectusThe variability of chemical bonding in open-shell transition-metal complexes not only motivates their study as functional materials and catalysts but also challenges conventional computational modeling tools. Here, tailoring ligand chemistry can alter preferred spin or oxidation states as well as electronic structure properties and reactivity, creating vast regions of chemical space to explore when designing new materials atom by atom. Although first-principles density functional theory (DFT) remains the workhorse of computational chemistry in mechanism deduction and property prediction, it is of limited use here. DFT is both far too computationally costly for widespread exploration of transition-metal chemical space and also prone to inaccuracies that limit its predictive performance for localized d electrons in transition-metal complexes. These challenges starkly contrast with the well-trodden regions of small-organic-molecule chemical space, where the analytical forms of molecular mechanics force fields and semiempirical theories have for decades accelerated the discovery of new molecules, accurate DFT functional performance has been demonstrated, and gold-standard methods from correlated wavefunction theory can predict experimental results to chemical accuracy.The combined promise of transition-metal chemical space exploration and lack of established tools has mandated a distinct approach. In this Account, we outline the path we charted in exploration of transition-metal chemical space starting from the first machine learning (ML) models (i.e., artificial neural network and kernel ridge regression) and representations for the prediction of open-shell transition-metal complex properties. The distinct importance of the immediate coordination environment of the metal center as well as the lack of low-level methods to accurately predict structural properties in this coordination environment first motivated and then benefited from these ML models and representations. Once developed, the recipe for prediction of geometric, spin state, and redox potential properties was straightforwardly extended to a diverse range of other properties, including in catalysis, computational ""feasibility"", and the gas separation properties of periodic metal-organic frameworks. Interpretation of selected features most important for model prediction revealed new ways to encapsulate design rules and confirmed that models were robustly mapping essential structure-property relationships. Encountering the special challenge of ensuring that good model performance could generalize to new discovery targets motivated investigation of how to best carry out model uncertainty quantification. Distance-based approaches, whether in model latent space or in carefully engineered feature space, provided intuitive measures of the domain of applicability. With all of these pieces together, ML can be harnessed as an engine to tackle the large-scale exploration of transition-metal chemical space needed to satisfy multiple objectives using efficient global optimization methods. In practical terms, bringing these artificial intelligence tools to bear on the problems of transition-metal chemical space exploration has resulted in ML-model assessments of large, multimillion compound spaces in minutes and validated new design leads in weeks instead of decades.","",""
93,"Mark O. Riedl","Human-Centered Artificial Intelligence and Machine Learning",2019,"","","","",174,"2022-07-13 09:32:36","","10.1002/HBE2.117","","",,,,,93,31.00,93,1,3,"Humans are increasingly coming into contact with artificial intelligence and machine learning systems. Human-centered artificial intelligence is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans. We lay forth an argument that human-centered artificial intelligence can be broken down into two aspects: (1) AI systems that understand humans from a sociocultural perspective, and (2) AI systems that help humans understand them. We further argue that issues of social responsibility such as fairness, accountability, interpretability, and transparency.","",""
99,"R. Colling, Helen Pitman, K. Oien, N. Rajpoot, P. Macklin, D. Snead, Tony Sackville, C. Verrill","Artificial intelligence in digital pathology: a roadmap to routine use in clinical practice",2019,"","","","",175,"2022-07-13 09:32:36","","10.1002/path.5310","","",,,,,99,33.00,12,8,3,"The use of artificial intelligence will transform clinical practice over the next decade and the early impact of this will likely be the integration of image analysis and machine learning into routine histopathology. In the UK and around the world, a digital revolution is transforming the reporting practice of diagnostic histopathology and this has sparked a proliferation of image analysis software tools. While this is an exciting development that could discover novel predictive clinical information and potentially address international pathology workforce shortages, there is a clear need for a robust and evidence‐based framework in which to develop these new tools in a collaborative manner that meets regulatory approval. With these issues in mind, the NCRI Cellular Molecular Pathology (CM‐Path) initiative and the British In Vitro Diagnostics Association (BIVDA) have set out a roadmap to help academia, industry, and clinicians develop new software tools to the point of approved clinical use. © 2019 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.","",""
85,"A. Grzybowski, Piotr Brona, Gilbert Lim, P. Ruamviboonsuk, G. Tan, M. Abràmoff, D. Ting","Artificial intelligence for diabetic retinopathy screening: a review",2019,"","","","",176,"2022-07-13 09:32:36","","10.1038/s41433-019-0566-0","","",,,,,85,28.33,12,7,3,"","",""
77,"K. Paranjape, M. Schinkel, R. N. Nannan Panday, J. Car, P. Nanayakkara","Introducing Artificial Intelligence Training in Medical Education",2019,"","","","",177,"2022-07-13 09:32:36","","10.2196/16048","","",,,,,77,25.67,15,5,3,"Health care is evolving and with it the need to reform medical education. As the practice of medicine enters the age of artificial intelligence (AI), the use of data to improve clinical decision making will grow, pushing the need for skillful medicine-machine interaction. As the rate of medical knowledge grows, technologies such as AI are needed to enable health care professionals to effectively use this knowledge to practice medicine. Medical professionals need to be adequately trained in this new technology, its advantages to improve cost, quality, and access to health care, and its shortfalls such as transparency and liability. AI needs to be seamlessly integrated across different aspects of the curriculum. In this paper, we have addressed the state of medical education at present and have recommended a framework on how to evolve the medical education curriculum to include AI.","",""
73,"Valentina Bellemo, Gilbert Lim, T. Rim, G. Tan, C. Cheung, S. Sadda, M. He, A. Tufail, M. Lee, W. Hsu, D. Ting","Artificial Intelligence Screening for Diabetic Retinopathy: the Real-World Emerging Application",2019,"","","","",178,"2022-07-13 09:32:36","","10.1007/s11892-019-1189-3","","",,,,,73,24.33,7,11,3,"","",""
67,"Xiaoxuan Liu, S. C. Rivera, L. Faes, L. F. D. Ruffano, C. Yau, P. Keane, H. Ashrafian, A. Darzi, S. Vollmer, J. Deeks, L. Bachmann, Christopher Holmes, A. Chan, D. Moher, M. Calvert, A. Denniston","Reporting guidelines for clinical trials evaluating artificial intelligence interventions are needed",2019,"","","","",179,"2022-07-13 09:32:36","","10.1038/s41591-019-0603-3","","",,,,,67,22.33,7,16,3,"","",""
51,"Miriam C. Buiten","Towards Intelligent Regulation of Artificial Intelligence",2019,"","","","",180,"2022-07-13 09:32:36","","10.1017/err.2019.8","","",,,,,51,17.00,51,1,3,"Artificial intelligence (AI) is becoming a part of our daily lives at a fast pace, offering myriad benefits for society. At the same time, there is concern about the unpredictability and uncontrollability of AI. In response, legislators and scholars call for more transparency and explainability of AI. This article considers what it would mean to require transparency of AI. It advocates looking beyond the opaque concept of AI, focusing on the concrete risks and biases of its underlying technology: machine-learning algorithms. The article discusses the biases that algorithms may produce through the input data, the testing of the algorithm and the decision model. Any transparency requirement for algorithms should result in explanations of these biases that are both understandable for the prospective recipients, and technically feasible for producers. Before asking how much transparency the law should require from algorithms, we should therefore consider if the explanation that programmers could offer is useful in specific legal contexts.","",""
67,"Yonghui Shang, Hoang Nguyen, X. Bui, Quang-Hieu Tran, H. Moayedi","A Novel Artificial Intelligence Approach to Predict Blast-Induced Ground Vibration in Open-Pit Mines Based on the Firefly Algorithm and Artificial Neural Network",2019,"","","","",181,"2022-07-13 09:32:36","","10.1007/s11053-019-09503-7","","",,,,,67,22.33,13,5,3,"","",""
51,"Xiaohang Wu, Yelin Huang, Zhenzhen Liu, Weiyi Lai, Erping Long, Kai Zhang, Jiewei Jiang, Duoru Lin, Kexin Chen, Tongyong Yu, Dongxuan Wu, Cong Li, Yanyi Chen, Minjie Zou, Chuan Chen, Yi Zhu, Chong Guo, Xiayin Zhang, Ruixin Wang, Yahan Yang, Yifan Xiang, Lijian Chen, Congxin Liu, J. Xiong, Z. Ge, Ding-ding Wang, Guihua Xu, Shao-lin Du, Chi Xiao, Jianghao Wu, Ke Zhu, Dan-yao Nie, Fan Xu, Jian Lv, Weirong Chen, Yizhi Liu, Haotian Lin","Universal artificial intelligence platform for collaborative management of cataracts",2019,"","","","",182,"2022-07-13 09:32:36","","10.1136/bjophthalmol-2019-314729","","",,,,,51,17.00,5,37,3,"Purpose To establish and validate a universal artificial intelligence (AI) platform for collaborative management of cataracts involving multilevel clinical scenarios and explored an AI-based medical referral pattern to improve collaborative efficiency and resource coverage. Methods The training and validation datasets were derived from the Chinese Medical Alliance for Artificial Intelligence, covering multilevel healthcare facilities and capture modes. The datasets were labelled using a three-step strategy: (1) capture mode recognition; (2) cataract diagnosis as a normal lens, cataract or a postoperative eye and (3) detection of referable cataracts with respect to aetiology and severity. Moreover, we integrated the cataract AI agent with a real-world multilevel referral pattern involving self-monitoring at home, primary healthcare and specialised hospital services. Results The universal AI platform and multilevel collaborative pattern showed robust diagnostic performance in three-step tasks: (1) capture mode recognition (area under the curve (AUC) 99.28%–99.71%), (2) cataract diagnosis (normal lens, cataract or postoperative eye with AUCs of 99.82%, 99.96% and 99.93% for mydriatic-slit lamp mode and AUCs >99% for other capture modes) and (3) detection of referable cataracts (AUCs >91% in all tests). In the real-world tertiary referral pattern, the agent suggested 30.3% of people be ‘referred’, substantially increasing the ophthalmologist-to-population service ratio by 10.2-fold compared with the traditional pattern. Conclusions The universal AI platform and multilevel collaborative pattern showed robust diagnostic performance and effective service for cataracts. The context of our AI-based medical referral pattern will be extended to other common disease conditions and resource-intensive situations.","",""
48,"M. Cukurova, C. Kent, R. Luckin","Artificial intelligence and multimodal data in the service of human decision-making: A case study in debate tutoring",2019,"","","","",183,"2022-07-13 09:32:36","","10.1111/BJET.12829","","",,,,,48,16.00,16,3,3,"The question: ""What is an appropriate role for AI?"" is the subject of much discussion and interest. Arguments about whether AI should be a human replacing technology or a human assisting technology frequently take centre stage. Education is no exception when it comes to questions about the role that AI should play, and as with many other professional areas, the exact role of AI in education is not easy to predict. Here, we argue that one potential role for AI in education is to provide opportunities for human intelligence augmentation, with AI supporting us in decision‐making processes, rather than replacing us through automation. To provide empirical evidence to support our argument, we present a case study in the context of debate tutoring, in which we use prediction and classification models to increase the transparency of the intuitive decision‐making processes of expert tutors for advanced reflections and feedback. Furthermore, we compare the accuracy of unimodal and multimodal classification models of expert human tutors' decisions about the social and emotional aspects of tutoring while evaluating trainees. Our results show that multimodal data leads to more accurate classification models in the context we studied. [ABSTRACT FROM AUTHOR]","",""
94,"F. Pedró, Miguel Subosa, A. Rivas, Paula Valverde","Artificial intelligence in education : challenges and opportunities for sustainable development",2019,"","","","",184,"2022-07-13 09:32:36","","","","",,,,,94,31.33,24,4,3,".........................................................................................................................................................................4 Executive Summary .......................................................................................................................................................5 Introduction .................................................................................................................................................................................... 7 Section I: Leveraging AI towards improving learning and equity ......................................................................................11 (1) AI to promote personalisation and better learning outcomes ...................................................................................................12 (2) Data analytics in Education Management Information Systems (EMIS) and the evolution to Learning Management Systems (LMS) ......................................................................................................15 Section II: Preparing learners to thrive in the future with AI ..............................................................................................17 (1) A new curriculum for a digital and AI-powered world ......................................................................................................................18 (2) Strengthening AI capacities through post-basic education and training ................................................................................. 22 Section III: AI in education challenges and policy implications ............................................................................25 First challenge: a comprehensive public policy on AI for sustainable development ....................................................................26 Second challenge: Ensuring inclusion and equity in AI in education .................................................................................................28 Third challenge: Preparing teachers for AI-powered education and preparing AI to understand education ......................28 Fourth challenge: Developing quality and inclusive data systems ....................................................................................................30 Fifth challenge: making research on AI in education significant .........................................................................................................31 Sixth challenge: ethics and transparency in data collection, use and dissemination ...................................................................32 CONCLUSIONS .............................................................................................................................................................34 ANNEX ..........................................................................................................................................................................37 AI definition and related concepts .................................................................................................................................................................37 REFERENCES .................................................................................................................................................................................41","",""
37,"M. Beil, Ingo Proft, Daniel van Heerden, S. Sviri, P. V. van Heerden","Ethical considerations about artificial intelligence for prognostication in intensive care",2019,"","","","",185,"2022-07-13 09:32:36","","10.1186/s40635-019-0286-6","","",,,,,37,12.33,7,5,3,"","",""
35,"Sonia K. Katyal","Private Accountability in an Age of Artificial Intelligence",2019,"","","","",186,"2022-07-13 09:32:36","","10.1017/9781108680844.004","","",,,,,35,11.67,35,1,3,"Author(s): Katyal, SK | Abstract: © 2019 American Statistical Association. All Rights Reserved. In this Article, I explore the impending conflict between the protection of civil rights and artificial intelligence (AI). While both areas of law have amassed rich and well-developed areas of scholarly work and doctrinal support, a growing body of scholars are interrogating the intersection between them. This Article argues that the issues surrounding algorithmic accountability demonstrate a deeper, more structural tension within a new generation of disputes regarding law and technology. As I argue, the true promise of AI does not lie in the information we reveal to one another, but rather in the questions it raises about the interaction of technology, property, and civil rights. For this reason, I argue that we are looking in the wrong place if we look only to the state to address issues of algorithmic accountability. Instead, given the state's reluctance to address the issue, we must turn to other ways to ensure more transparency and accountability that stem from private industry, rather than public regulation. The issue of algorithmic bias represents a crucial new world of civil rights concerns, one that is distinct in nature from the ones that preceded it. Since we are in a world where the activities of private corporations, rather than the state, are raising concerns about privacy, due process, and discrimination, we must focus on the role of private corporations in addressing the issue. Towards this end, I discuss a variety of tools to help eliminate the opacity of AI, including codes of conduct, impact statements, and whistleblower protection, which I argue carries the potential to encourage greater endogeneity in civil rights enforcement. Ultimately, by examining the relationship between private industry and civil rights, we can perhaps develop a new generation of forms of accountability in the process.","",""
32,"Jun-Ho Huh, Yeong-Seok Seo","Understanding Edge Computing: Engineering Evolution With Artificial Intelligence",2019,"","","","",187,"2022-07-13 09:32:36","","10.1109/ACCESS.2019.2945338","","",,,,,32,10.67,16,2,3,"The key to the explosion of the Internet of Things and the ability to collect, analyze, and provide big data in the cloud is edge computing, which is a new computing paradigm in which data is processed from edges. Edge Computing has been attracting attention as one of the top 10 strategic technology trends in the past two years and has innovative potential. It provides shorter response times, lower bandwidth costs, and more robust data safety and privacy protection than cloud computing. In particular, artificial intelligence technologies are rapidly incorporating edge computing. In this paper, we introduce the concepts, backgrounds, and pros and cons of edge computing, explain how it operates and its structure hierarchically with artificial intelligence concepts, list examples of its applications in various fields, and finally suggest some improvements and discuss the challenges of its application in three representative technological fields. We intend to clarify various analyses and opinions regarding edge computing and artificial intelligence.","",""
32,"Matt Taddy","The Technological Elements of Artificial Intelligence",2018,"","","","",188,"2022-07-13 09:32:36","","10.3386/W24301","","",,,,,32,8.00,32,1,4,"We have seen in the past decade a sharp increase in the extent that companies use data to optimize their businesses. Variously called the `Big Data' or `Data Science' revolution, this has been characterized by massive amounts of data, including unstructured and nontraditional data like text and images, and the use of fast and flexible Machine Learning (ML) algorithms in analysis. With recent improvements in Deep Neural Networks (DNNs) and related methods, application of high-performance ML algorithms has become more automatic and robust to different data scenarios. That has led to the rapid rise of an Artificial Intelligence (AI) that works by combining many ML algorithms together – each targeting a straightforward prediction task – to solve complex problems. We will define a framework for thinking about the ingredients of this new ML-driven AI. Having an understanding of the pieces that make up these systems and how they fit together is important for those who will be building businesses around this technology. Those studying the economics of AI can use these definitions to remove ambiguity from the conversation on AI's projected productivity impacts and data requirements. Finally, this framework should help clarify the role for AI in the practice of modern business analytics and economic measurement.","",""
157,"S. C. Rivera, Xiaoxuan Liu, A. Chan, A. Denniston, M. Calvert, H. Ashrafian, A. Beam, G. Collins, A. Darzi, J. Deeks, M. Elzarrad, Cyrus Espinoza, Andre Esteva, L. Faes, L. Ferrante di Ruffano, J. Fletcher, R. Golub, H. Harvey, C. Haug, Christopher Holmes, Adrian Jonas, P. Keane, Christopher J. Kelly, Aaron Y. Lee, Cecilia S Lee, Elaine Manna, J. Matcham, Melissa D. McCradden, D. Moher, Joao Monteiro, C. Mulrow, L. Oakden-Rayner, D. Paltoo, M. Panico, G. Price, Samuel d. Rowley, Richard Savage, Rupa Sarkar, S. Vollmer, C. Yau","Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI Extension",2020,"","","","",189,"2022-07-13 09:32:36","","10.1136/bmj.m3210","","",,,,,157,78.50,16,40,2,"Abstract The SPIRIT 2013 (The Standard Protocol Items: Recommendations for Interventional Trials) statement aims to improve the completeness of clinical trial protocol reporting, by providing evidence-based recommendations for the minimum set of items to be addressed. This guidance has been instrumental in promoting transparent evaluation of new interventions. More recently, there is a growing recognition that interventions involving artificial intelligence need to undergo rigorous, prospective evaluation to demonstrate their impact on health outcomes. The SPIRIT-AI extension is a new reporting guideline for clinical trials protocols evaluating interventions with an AI component. It was developed in parallel with its companion statement for trial reports: CONSORT-AI. Both guidelines were developed using a staged consensus process, involving a literature review and expert consultation to generate 26 candidate items, which were consulted on by an international multi-stakeholder group in a 2-stage Delphi survey (103 stakeholders), agreed on in a consensus meeting (31 stakeholders) and refined through a checklist pilot (34 participants). The SPIRIT-AI extension includes 15 new items, which were considered sufficiently important for clinical trial protocols of AI interventions. These new items should be routinely reported in addition to the core SPIRIT 2013 items. SPIRIT-AI recommends that investigators provide clear descriptions of the AI intervention, including instructions and skills required for use, the setting in which the AI intervention will be integrated, considerations around the handling of input and output data, the human-AI interaction and analysis of error cases. SPIRIT-AI will help promote transparency and completeness for clinical trial protocols for AI interventions. Its use will assist editors and peer-reviewers, as well as the general readership, to understand, interpret and critically appraise the design and risk of bias for a planned clinical trial.","",""
42,"Sherin M. Mathews","Explainable Artificial Intelligence Applications in NLP, Biomedical, and Malware Classification: A Literature Review",2019,"","","","",190,"2022-07-13 09:32:36","","10.1007/978-3-030-22868-2_90","","",,,,,42,14.00,42,1,3,"","",""
25,"S. Petersen, M. Abdulkareem, T. Leiner","Artificial Intelligence Will Transform Cardiac Imaging—Opportunities and Challenges",2019,"","","","",191,"2022-07-13 09:32:36","","10.3389/fcvm.2019.00133","","",,,,,25,8.33,8,3,3,"Artificial intelligence (AI) using machine learning techniques will change healthcare as we know it. While healthcare AI applications are currently trailing behind popular AI applications, such as personalized web-based advertising, the pace of research and deployment is picking up and about to become disruptive. Overcoming challenges such as patient and public support, transparency over the legal basis for healthcare data use, privacy preservation, technical challenges related to accessing large-scale data from healthcare systems not designed for Big Data analysis, and deployment of AI in routine clinical practice will be crucial. Cardiac imaging and imaging of other body parts is likely to be at the frontier for the development of applications as pattern recognition and machine learning are a significant strength of AI with practical links to image processing. Many opportunities in cardiac imaging exist where AI will impact patients, medical staff, hospitals, commissioners and thus, the entire healthcare system. This perspective article will outline our vision for AI in cardiac imaging with examples of potential applications, challenges and some lessons learnt in recent years.","",""
28,"M. Komorowski","Artificial intelligence in intensive care: are we there yet?",2019,"","","","",192,"2022-07-13 09:32:36","","10.1007/s00134-019-05662-6","","",,,,,28,9.33,28,1,3,"","",""
26,"E. Racine, W. Boehlen, M. Sample","Healthcare uses of artificial intelligence: Challenges and opportunities for growth",2019,"","","","",193,"2022-07-13 09:32:36","","10.1177/0840470419843831","","",,,,,26,8.67,9,3,3,"Forms of Artificial Intelligence (AI), like deep learning algorithms and neural networks, are being intensely explored for novel healthcare applications in areas such as imaging and diagnoses, risk analysis, lifestyle management and monitoring, health information management, and virtual health assistance. Expected benefits in these areas are wide-ranging and include increased speed in imaging, greater insight into predictive screening, and decreased healthcare costs and inefficiency. However, AI-based clinical tools also create a host of situations wherein commonly-held values and ethical principles may be challenged. In this short column, we highlight three potentially problematic aspects of AI use in healthcare: (1) dynamic information and consent, (2) transparency and ownership, and (3) privacy and discrimination. We discuss their impact on patient/client, clinician, and health institution values and suggest ways to tackle this impact. We propose that AI-related ethical challenges may represent an opportunity for growth in organizations.","",""
0,"","A Novel Approach to Adopt Explainable Artificial Intelligence in X-ray Image Classification",2022,"","","","",194,"2022-07-13 09:32:36","","10.33140/amlai.03.01.01","","",,,,,0,0.00,0,0,1,"Robust “Blackbox” algorithms such as Convolutional Neural Networks (CNNs) are known for making high prediction performance. However, the ability to explain and interpret these algorithms still require innovation in the understanding of influential and, more importantly, explainable features that directly or indirectly impact the performance of predictivity. In view of the above needs, this study proposes an interaction- based methodology – Influence Score (I-score) – to screen out the noisy and non-informative variables in the images hence it nourishes an environment with explainable and interpretable features that are directly associated to feature predictivity. We apply the proposed method on a real-world application in Pneumonia Chest X-ray Image data set and produced state- of-the-art results. We demonstrate how to apply the proposed approach for more general big data problems by improving the explain ability and interpretability without sacrificing the prediction performance. The contribution of this paper opens a novel angle that moves the community closer to the future pipelines of XAI problems.","",""
24,"Anastassia Lauterbach","Artificial intelligence and policy: quo vadis?",2019,"","","","",195,"2022-07-13 09:32:36","","10.1108/DPRG-09-2018-0054","","",,,,,24,8.00,24,1,3," Purpose This paper aims to inform policymakers about key artificial intelligence (AI) technologies, risks and trends in national AI strategies. It suggests a framework of social governance to ensure emergence of safe and beneficial AI.   Design/methodology/approach The paper is based on approximately 100 interviews with researchers, executives of traditional companies and startups and policymakers in seven countries. The interviews were carried out in January-August 2017.   Findings Policymakers still need to develop an informed, scientifically grounded and forward-looking view on what societies and businesses might expect from AI. There is lack of transparency on what key AI risks are and what might be regulatory approaches to handle them. There is no collaborative framework in place involving all important actors to decide on AI technology design principles and governance. Today's technology decisions will have long-term consequences on lives of billions of people and competitiveness of millions of businesses.   Research limitations/implications The research did not include a lot of insights from the emerging markets.   Practical implications Policymakers will understand the scope of most important AI concepts, risks and national strategies.   Social implications AI is progressing at a very fast rate, changing industries, businesses and approaches how companies learn, generate business insights, design products and communicate with their employees and customers. It has a big societal impact, as – if not designed with care – it can scale human bias, increase cybersecurity risk and lead to negative shifts in employment. Like no other invention, it can tighten control by the few over the many, spread false information and propaganda and therewith shape the perception of people, communities and enterprises.   Originality/value This paper is a compendium on the most important concepts of AI, bringing clarity into discussions around AI risks and the ways to mitigate them. The breadth of topics is valuable to policymakers, students, practitioners, general executives and board directors alike. ","",""
22,"Rushikesh S. Joshi, Alexander F. Haddad, Darryl Lau, C. Ames","Artificial Intelligence for Adult Spinal Deformity",2019,"","","","",196,"2022-07-13 09:32:36","","10.14245/ns.1938414.207","","",,,,,22,7.33,6,4,3,"Adult spinal deformity (ASD) is a complex disease that significantly affects the lives of many patients. Surgical correction has proven to be effective in achieving improvement of spinopelvic parameters as well as improving quality of life (QoL) for these patients. However, given the relatively high complication risk associated with ASD correction, it is of paramount importance to develop robust prognostic tools for predicting risk profile and outcomes. Historically, statistical models such as linear and logistic regression models were used to identify preoperative factors associated with postoperative outcomes. While these tools were useful for looking at simple associations, they represent generalizations across large populations, with little applicability to individual patients. More recently, predictive analytics utilizing artificial intelligence (AI) through machine learning for comprehensive processing of large amounts of data have become available for surgeons to implement. The use of these computational techniques has given surgeons the ability to leverage far more accurate and individualized predictive tools to better inform individual patients regarding predicted outcomes after ASD correction surgery. Applications range from predicting QoL measures to predicting the risk of major complications, hospital readmission, and reoperation rates. In addition, AI has been used to create a novel classification system for ASD patients, which will help surgeons identify distinct patient subpopulations with unique risk-benefit profiles. Overall, these tools will help surgeons tailor their clinical practice to address patients’ individual needs and create an opportunity for personalized medicine within spine surgery.","",""
21,"D. Ting, M. Ang, J. Mehta, D. Ting","Artificial intelligence-assisted telemedicine platform for cataract screening and management: a potential model of care for global eye health",2019,"","","","",197,"2022-07-13 09:32:36","","10.1136/bjophthalmol-2019-315025","","",,,,,21,7.00,5,4,3,"Artificial intelligence (AI) is the fourth industrial revolution.1 Deep learning is a robust machine learning technique that uses convolutional neural network to perform multilevel data abstraction without the need for manual feature engineering.2 In ophthalmology, many studies showed comparable, if not better, diagnostic performance in using AI to screen, diagnose, predict and monitor various eye conditions on fundus photographs and optical coherence tomography,3 4 including diabetic retinopathy (DR),5 age-related macular degeneration,6 glaucoma,7 retinopathy of prematurity (ROP).8   To date, many countries have reported well-established telemedicine programme to screen for DR and ROP,9–12 but limited for cataracts. Cataract is the leading cause of reversible blindness, affecting approximately 12.6 million (3.4–28.7 million) worldwide.13 14 The prevalence of cataract-related visual impairment also varies between high-income and low-income countries, with the latter having poorer access to tertiary care.13 In this issue, Wu et al 15 reported an AI-integrated telemedicine platform to screen and refer patients with cataract. This article consists of two parts: (1) the first part focusing on the AI system in detection of three tasks (capture mode, cataract diagnosis and referable cataract) and (2) the second part describing how these AI algorithms could be integrated in the telemedicine platform for real-world operational use. In this study, the referable cases were defined as: (1) grade 3 and grade 4 nuclear sclerotic …","",""
16,"K. Denecke, E. Gabarron, R. Grainger, S. Konstantinidis, A. Lau, O. Rivera-Romero, T. Miron-Shatz, M. Merolli","Artificial Intelligence for Participatory Health: Applications, Impact, and Future Implications",2019,"","","","",198,"2022-07-13 09:32:36","","10.1055/s-0039-1677902","","",,,,,16,5.33,2,8,3,"Summary Objective : Artificial intelligence (AI) provides people and professionals working in the field of participatory health informatics an opportunity to derive robust insights from a variety of online sources. The objective of this paper is to identify current state of the art and application areas of AI in the context of participatory health. Methods : A search was conducted across seven databases (PubMed, Embase, CINAHL, PsychInfo, ACM Digital Library, IEEExplore, and SCOPUS), covering articles published since 2013. Additionally, clinical trials involving AI in participatory health contexts registered at clinicaltrials.gov were collected and analyzed. Results : Twenty-two articles and 12 trials were selected for review. The most common application of AI in participatory health was the secondary analysis of social media data: self-reported data including patient experiences with healthcare facilities, reports of adverse drug reactions, safety and efficacy concerns about over-the-counter medications, and other perspectives on medications. Other application areas included determining which online forum threads required moderator assistance, identifying users who were likely to drop out from a forum, extracting terms used in an online forum to learn its vocabulary, highlighting contextual information that is missing from online questions and answers, and paraphrasing technical medical terms for consumers. Conclusions : While AI for supporting participatory health is still in its infancy, there are a number of important research priorities that should be considered for the advancement of the field. Further research evaluating the impact of AI in participatory health informatics on the psychosocial wellbeing of individuals would help in facilitating the wider acceptance of AI into the healthcare ecosystem.","",""
14,"C. Marsden, Trisha Meyer","Regulating disinformation with artificial intelligence:effects of disinformation initiatives on freedom of expression and media pluralism",2019,"","","","",199,"2022-07-13 09:32:36","","10.2861/003689","","",,,,,14,4.67,7,2,3,"This study examines the consequences of the increasingly prevalent use of artificial intelligence (AI) disinformation initiatives upon freedom of expression, pluralism and the functioning of a democratic polity.  The study examines the trade-offs in using automated technology to limit the spread of disinformation online. It presents options (from self-regulatory to legislative) to regulate automated content recognition (ACR) technologies in this context. Special attention is paid to the opportunities for the European Union as a whole to take the lead in setting the framework for designing these technologies in a way that enhances accountability and transparency and respects free speech. The present project reviews some of the key academic and policy ideas on technology and disinformation and highlights their relevance to European policy.  Chapter 1 introduces the background to the study and presents the definitions used. Chapter 2 scopes the policy boundaries of disinformation from economic, societal and technological perspectives, focusing on the media context, behavioural economics and technological regulation. Chapter 3 maps and evaluates existing regulatory and technological responses to disinformation. In Chapter 4, policy options are presented, paying particular attention to interactions between technological solutions, freedom of expression and media pluralism.","",""
0,"Jurgita Černevičienė, Audrius Kabašinskas","Review of Multi-Criteria Decision-Making Methods in Finance Using Explainable Artificial Intelligence",2022,"","","","",200,"2022-07-13 09:32:36","","10.3389/frai.2022.827584","","",,,,,0,0.00,0,2,1,"The influence of Artificial Intelligence is growing, as is the need to make it as explainable as possible. Explainability is one of the main obstacles that AI faces today on the way to more practical implementation. In practise, companies need to use models that balance interpretability and accuracy to make more effective decisions, especially in the field of finance. The main advantages of the multi-criteria decision-making principle (MCDM) in financial decision-making are the ability to structure complex evaluation tasks that allow for well-founded financial decisions, the application of quantitative and qualitative criteria in the analysis process, the possibility of transparency of evaluation and the introduction of improved, universal and practical academic methods to the financial decision-making process. This article presents a review and classification of multi-criteria decision-making methods that help to achieve the goal of forthcoming research: to create artificial intelligence-based methods that are explainable, transparent, and interpretable for most investment decision-makers.","",""
