Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
2,"Hamdah Alotaibi, Fawaz Alsolami, Rashid Mehmood","DNA Profiling: An Investigation of Six Machine Learning Algorithms for Estimating the Number of Contributors in DNA Mixtures",2021,"","","","",1,"2022-07-13 09:23:14","","10.14569/ijacsa.2021.0121115","","",,,,,2,2.00,1,3,1,"DNA (Deoxyribonucleic acid) profiling involves analysis of sequences of individual or mixed DNA profiles to identify persons these profiles belong to. DNA profiling is used in important applications such as for paternity tests, in forensic science for person identification on a crime scheme, etc. Finding the number of contributors in a DNA mixture is a major task in DNA profiling with challenges caused due to allele dropout, stutter, blobs, and noise. The existing methods for finding the number of unknowns in a DNA mixture suffer from issues including computational complexity and accuracy of estimating the number of unknowns. Machine learning has received attention recently in this area but with limited success. Many more efforts are needed for improving the robustness and accuracy of these methods. Our research aims to advance the state-of-the-art in this area. Specifically, in this paper, we investigate the performance of six machine learning algorithms -Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Logistic Regression (LR), Stochastic Gradient Descent (SGD), and Gaussian Naïve-Bayes (GNB) -applied to a publicly available dataset called PROVEDIt, containing mixtures with up to five contributors. We evaluate the algorithmic performance using confusion matrices and four performance metrics namely accuracy, F1-Score, Recall, and Precision. The results show that LR provides the highest Accuracy of 95% for mixtures with five contributors. Keywords—Machine learning; DNA profiling; DNA mixtures;","",""
0,"C. Calad, Fernando Gutiérrez, Paola Pastor, P. Sarma","Combining Machine Learning with Traditional Reservoir Physics for Predictive Modeling and Optimization of a Large Mature Waterflood Project in the Gulf of San Jorge Basin in Argentina",2020,"","","","",2,"2022-07-13 09:23:14","","10.2118/199048-ms","","",,,,,0,0.00,0,4,2,"  A novel technology that combines the benefits of speed of data sciences with the predictivity capabilities of traditional simulation is being applied to model two blocks of a large waterflood project in the Gulf of San Jorge basin in southern Argentina. The tool is being used to provide a prescription of injection water redistribution that optimizes production and reserves development and reduces injection cost.  The technology used is called DataPhysics* and combines the robustness of reservoir physics with the speed of data sciences techniques. The process solves a limited number of unknowns in a continuous scale making it several orders of magnitude faster than traditional numeric simulation. The reservoir model is created from raw (uninterpreted) data and is updated continuously allowing for close loop reservoir optimization in real time. Long term predictivity is enabled by the fact that the tool honors the reservoir physics.  At the time of writing this paper the recommendations of the predictive model have been implemented in the pilot sector of the field and early positive results have been observed.","",""
2,"A. Russo, Miguel A. Dur'an-Olivencia, I. Kevrekidis, S. Kalliadasis","Deep learning as closure for irreversible processes: A data-driven generalized Langevin equation",2019,"","","","",3,"2022-07-13 09:23:14","","","","",,,,,2,0.67,1,4,3,"The ultimate goal of physics is finding a unique equation capable of describing the evolution of any observable quantity in a self-consistent way. Within the field of statistical physics, such an equation is known as the generalized Langevin equation (GLE). Nevertheless, the formal and exact GLE is not particularly useful, since it depends on the complete history of the observable at hand, and on hidden degrees of freedom typically inaccessible from a theoretical point of view. In this work, we propose the use of deep neural networks as a new avenue for learning the intricacies of the unknowns mentioned above. By using machine learning to eliminate the unknowns from GLEs, our methodology outperforms previous approaches (in terms of efficiency and robustness) where general fitting functions were postulated. Finally, our work is tested against several prototypical examples, from a colloidal systems and particle chains immersed in a thermal bath, to climatology and financial models. In all cases, our methodology exhibits an excellent agreement with the actual dynamics of the observables under consideration.","",""
16,"Hong-Ming Yang, Xu-Yao Zhang, Fei Yin, Qing Yang, Cheng-Lin Liu","Convolutional Prototype Network for Open Set Recognition",2020,"","","","",4,"2022-07-13 09:23:14","","10.1109/TPAMI.2020.3045079","","",,,,,16,8.00,3,5,2,"Despite the success of convolutional neural network (CNN) in conventional closed-set recognition (CSR), it still lacks robustness for dealing with unknowns (those out of known classes) in open environment. To improve the robustness of CNN in open-set recognition (OSR) and meanwhile maintain its high accuracy in CSR, we propose an alternative deep framework called convolutional prototype network (CPN), which keeps CNN for representation learning but replaces the closed-world assumed softmax with an open-world oriented and human-like prototype model. To equip CPN with discriminative ability for classifying known samples, we design several discriminative losses for training. Moreover, to increase the robustness of CPN for unknowns, we interpret CPN from the perspective of generative model and further propose a generative loss, which is essentially maximizing the log-likelihood of known samples and serves as a latent regularization for discriminative learning. The combination of discriminative and generative losses makes CPN a hybrid model with advantages for both CSR and OSR. Under the designed losses, the CPN is trained end-to-end for learning the convolutional network and prototypes jointly. For application of CPN in OSR, we propose two rejection rules for detecting different types of unknowns. Experiments on several datasets demonstrate the efficiency and effectiveness of CPN for both CSR and OSR tasks.","",""
3,"A. Jiang, B. Jafarpour","History Matching under Uncertain Geologic Scenarios with Variational Autoencoders",2020,"","","","",5,"2022-07-13 09:23:14","","10.3997/2214-4609.202035194","","",,,,,3,1.50,2,2,2,"Summary Inference of high-resolution reservoir models from limited production data leads to ill-posed inverse problems. Parameterization methods are used to reduce the number of unknowns while maintaining the salient connectivity features of the reservoir model. Existing parameterization methods, such as principle component analysis (PCA), are not suitable for capturing complex non-Gaussian connectivity patterns that are exhibited in certain geological formation such as fluvial systems. Recent advances in machine learning have given rise to new approaches for dimensionality reduction that can be applied for parameterization of reservoir models in history matching. Many conventional parameterization techniques exhibit strong sensitivity to diversity in the geologic features (e.g., when multiple scenarios are used to account for prior uncertainty). One potential advantage of the new techniques is their ability to learn complex and diverse patterns, which leads to robustness against geologic uncertainty. We present variational autoencoders (VAEs) as a special form of convolutional neural networks for parametrization of history matching problems under multiple geologic scenarios. Autoencoders have achieved great performance in data compression by taking advantage of the power of convolutional neural networks for detecting local spatial patterns. We present VAEs as an effective parameterization approach for complex spatial models with non-Gaussian statistics. These methods project complex spatial models to a low-dimensional latent space, where a small number of latent variables with simple probability density functions (e.g., Gaussian) approximate the original models. The latent space variables are used to update the reservoir model during history matching. We evaluate the dimensionality reduction power of VAEs and use them with stochastic gradient-based inversion methods to perform history matching. We present history matching results when the training data is based on a single geologic scenario and when multiple geologic scenarios lead to very diverse features in the training data. Comparison between the performance of VAE and PCA shows that the former offers important advantages over PCA. The performance difference between the two methods become more significant when multiple geologic scenarios are present. We present several examples to demonstrate the implementation of VAE and its important properties in comparison to PCA.","",""
3,"A. Preece, Daniel Harborne, R. Raghavendra, Richard J. Tomsett, Dave Braines","Provisioning Robust and Interpretable AI/ML-Based Service Bundles",2018,"","","","",6,"2022-07-13 09:23:14","","10.1109/MILCOM.2018.8599838","","",,,,,3,0.75,1,5,4,"Coalition operations environments are characterised by the need to share intelligence, surveillance and reconnaissance services. Increasingly, such services are based on artificial intelligence (AI)and machine learning (ML)technologies. Two key issues in the exploitation of AI/ML services are robustness and interpretability. Employing a diverse portfolio of services can make a system robust to ‘unknown unknowns’. Interpretability - the need for services to offer explanation facilities to engender user trust - can be addressed by a variety of methods to generate either transparent or post hoc explanations according to users' requirements. This paper shows how a service-provisioning framework for coalition operations can be extended to address specific requirements for robustness and interpretability, allowing automatic selection of service bundles for intelligence, surveillance and reconnaissance tasks. The approach is demonstrated in a case study on traffic monitoring featuring a diverse set of AI/ML services based on deep neural networks and heuristic reasoning approaches.","",""
1,"Soha Mohamed, M. S. Fayed","Modelling of Received Signals in Molecular Communication Systems based machine learning: Comparison of azure machine learning and Python tools",2021,"","","","",7,"2022-07-13 09:23:14","","","","",,,,,1,1.00,1,2,1,"Molecular communication (MC) implemented on Nano networks has extremely attractive characteristics in terms of energy efficiency, dependability, and robustness. Even though, the impact of incredibly slow molecule diffusion and high variability environments remains unknown. Analysis and designs of communication systems usually rely on developing mathematical models that describe the communication channel. However, the underlying channel models are unknown in some systems, such as MC systems, where chemical signals are used to transfer information. In these cases, a new method to analyze and design is needed. In this paper, we concentrate on one critical aspect of the MC system, modelling MC received signal until time t , and demonstrate that using tools from ML makes it promising to train detectors that can be executed well without any information about the channel model. Machine learning (ML) is one of the intelligent methodologies that has shown promising results in the domain. This paper applies Azure Machine Learning (Azure ML) for flexible pavement maintenance regressions problems and solutions. For prediction, four parameters are used as inputs: the receiver radius, transmitter radius, distance between receiver and transmitter, and diffusion coefficient, while the output is mAP (mean average precision) of the received signal. Azure ML enables algorithms that can learn from data and experiences and accomplish tasks without having to be coded. In the established Azure ML, the regression algorithms such as, boost decision tree regression, Bayesian linear regression, neural network, and decision forest regression are selected. The best performance is chosen as an optimality criterion. Finally, a comparison that shows the potential benefits of Azure ML tool over programmed based tool (Python), used by developers on local PCs, is demonstrated.","",""
2,"Liang-Liang Wang, Jun-Jie Ding, Peichang Shi, Li Fu, Li Pan, Jiahao Tian, Dongsheng Cao, Hui Jiang, Xiao-Qin Ding","Ensemble machine learning to evaluate the in vivo acute oral toxicity and in vitro human acetylcholinesterase inhibitory activity of organophosphates.",2021,"","","","",8,"2022-07-13 09:23:14","","10.1007/s00204-021-03056-6","","",,,,,2,2.00,0,9,1,"","",""
3,"Willem Raes, Nicolas Knudde, Jorik De Bruycker, T. Dhaene, N. Stevens","Experimental Evaluation of Machine Learning Methods for Robust Received Signal Strength-Based Visible Light Positioning",2020,"","","","",9,"2022-07-13 09:23:14","","10.3390/s20216109","","",,,,,3,1.50,1,5,2,"In this work, the use of Machine Learning methods for robust Received Signal Strength (RSS)-based Visible Light Positioning (VLP) is experimentally evaluated. The performance of Multilayer Perceptron (MLP) models and Gaussian processes (GP) is investigated when using relative RSS input features. The experimental set-up for the RSS-based VLP technology uses light-emitting diodes (LEDs) transmitting intensity modulated light and a single photodiode (PD) as a receiver. The experiments focus on achieving robustness to cope with unknown received signal strength modifications over time. Therefore, several datasets were collected, where per dataset either the LEDs transmitting power is modified or the PD aperture is partly obfuscated by dust particles. Two relative RSS schemes are investigated. The first scheme uses the maximum received light intensity to normalize the received RSS vector, while the second approach obtains RSS ratios by combining all possible unique pairs of received intensities. The Machine Learning (ML) methods are compared to a relative multilateration implementation. It is demonstrated that the adopted MLP and GP models exhibit superior performance and higher robustness when compared to the multilateration strategies. Furthermore, when comparing the investigated ML models, the GP model is proven to be more robust than the MLP for the considered scenarios.","",""
0,"Maroun Touma, Shalisha Witherspoon, S. Witherspoon, Isabelle Crawford-Eng","A practical approach for applying Machine Learning in the detection and classification of network devices used in building management",2020,"","","","",10,"2022-07-13 09:23:14","","10.22541/au.160689781.19054555/v1","","",,,,,0,0.00,0,4,2,"With the increasing deployment of smart buildings and infrastructure, Supervisory Control and Data Acquisition (SCADA) devices and the underlying IT network have become essential elements for the proper operations of these highly complex systems. Of course, with the increase in automation and the proliferation of SCADA devices, a corresponding increase in surface area of attack on critical infrastructure has increased. Understanding device behaviors in terms of known and understood or potentially qualified activities versus unknown and potentially nefarious activities in near-real time is a key component of any security solution. In this paper, we investigate the challenges with building robust machine learning models to identify unknowns purely from network traffic both inside and outside firewalls, starting with missing or inconsistent labels across sites, feature engineering and learning, temporal dependencies and analysis, and training data quality (including small sample sizes) for both shallow and deep learning methods. To demonstrate these challenges and the capabilities we have developed, we focus on Building Automation and Control networks (BACnet) from a private commercial building system. Our results show that ”Model Zoo” built from binary classifiers based on each device or behavior combined with an ensemble classifier integrating information from all classifiers provides a reliable methodology to identify unknown devices as well as determining specific known devices when the device type is in the training set. The capability of the Model Zoo framework is shown to be directly linked to feature engineering and learning, and the dependency of the feature selection varies depending on both the binary and ensemble classifiers as well.","",""
1920,"A. Kurakin, Ian J. Goodfellow, Samy Bengio","Adversarial Machine Learning at Scale",2016,"","","","",11,"2022-07-13 09:23:14","","","","",,,,,1920,320.00,640,3,6,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.","",""
0,"Qingyi Gao","ADVERSARIAL LEARNING ON ROBUSTNESS AND GENERATIVE MODELS",2021,"","","","",12,"2022-07-13 09:23:14","","10.25394/PGS.15085227.V1","","",,,,,0,0.00,0,1,1,"In this dissertation, we study two important problems in the area of modern deep learning: adversarial robustness and adversarial generative model. In the first part, we study the generalization performance of deep neural networks (DNNs) in adversarial learning. Recent studies have shown that many machine learning models are vulnerable to adversarial attacks, but much remains unknown concerning its generalization error in this scenario. We focus on the $\ell_\infty$ adversarial attacks produced under the fast gradient sign method (FGSM). We establish a tight bound for the adversarial Rademacher complexity of DNNs based on both spectral norms and ranks of weight matrices. The spectral norm and rank constraints imply that this class of networks can be realized as a subset of the class of a shallow network composed with a low dimensional Lipschitz continuous function. This crucial observation leads to a bound that improves the dependence on the network width compared to previous works and achieves depth independence. We show that adversarial Rademacher complexity is always larger than its natural counterpart, but the effect of adversarial perturbations can be limited under our weight normalization framework. In the second part, we study deep generative models that receive great success in many fields. It is well-known that the complex data usually does not populate its ambient Euclidean space but resides in a lower-dimensional manifold instead. Thus, misspecifying the latent dimension in generative models will result in a mismatch of latent representations and poor generative qualities. To address these problems, we propose a novel framework called Latent Wasserstein GAN (LWGAN) to fuse the auto-encoder and WGAN such that the intrinsic dimension of data manifold can be adaptively learned by an informative latent distribution. In particular, we show that there exist an encoder network and a generator network in such a way that the intrinsic dimension of the learned encodes distribution is equal to the dimension of the data manifold. Theoretically, we prove the consistency of the estimation for the intrinsic dimension of the data manifold and derive a generalization error bound for LWGAN. Comprehensive empirical experiments verify our framework and show that LWGAN is able to identify the correct intrinsic dimension under several scenarios, and simultaneously generate high-quality synthetic data by samples from the learned latent distribution.","",""
0,"Rong Zhang, P. Chang","Robustness against adversary models on MNIST by Deep-Q Reinforcement Learning based Parallel-GANs",2021,"","","","",13,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,2,1,"This paper presents a method of enhancing robustness of classification machine learning model. Robustness of computer programming is an important topic, not only affects the security of user information but also stability and performance of program itself. That is, a technique aims to misclassify the machine learning model is called Adversarial Attack. We present an experiment to increase the robustness of machine learning models. The dataset MNIST, which includes hand-writing digits, is used as the experiment subject. Several techniques are applied such as Generative Adversarial Networks in Parallel form, Reinforcement Learning in Deep-Q formation, Dynamic sampling is used as prediction of unknown attack. Black-box is set to be experimental scenario. The experimental results show that the average robustness of the system under several attack conditions is as high as 90%.","",""
32,"Haipeng Yao, Danyang Fu, Peiying Zhang, Maozhen Li, Yunjie Liu","MSML: A Novel Multilevel Semi-Supervised Machine Learning Framework for Intrusion Detection System",2019,"","","","",14,"2022-07-13 09:23:14","","10.1109/JIOT.2018.2873125","","",,,,,32,10.67,6,5,3,"Intrusion detection technology has received increasing attention in recent years. Many researchers have proposed various intrusion detection systems using machine learning (ML) methods. However, there are two noteworthy factors affecting the robustness of the model. One is the severe imbalance of network traffic in different categories and the other is the nonidentical distribution between training set and test set in feature space. This paper presents a multilevel intrusion detection model framework named multilevel semi-supervised ML (MSML) to address these issues. The MSML framework includes four modules: 1) pure cluster extraction; 2) pattern discovery; 3) fine-grained classification (FC); and 4) model updating. In the pure cluster module, we introduce an concept of “pure cluster” and propose a hierarchical semi-supervised ${k}$ -means algorithm with an aim to find out all the pure clusters. In the pattern discovery module, we define the “unknown pattern” and apply cluster-based method aiming to find those unknown patterns. Then a test sample is sentenced to labeled known pattern or unlabeled unknown pattern. The FC module can achieves FC for those unknown pattern samples. The model updating module provides a mechanism for retraining. KDDCUP99 dataset is applied to evaluate MSML. Experimental results show that MSML is superior to other existing intrusion detection models in terms of overall accuracy, F1-score, and unknown pattern recognition capability.","",""
4,"J. Panda, H. Warrior","Evaluation of machine learning algorithms for predictive Reynolds stress transport modeling",2021,"","","","",15,"2022-07-13 09:23:14","","10.1007/s10409-022-09001-w","","",,,,,4,4.00,2,2,1,"","",""
1,"Yuntian Chen, Dongxiao Zhang","Integration of knowledge and data in machine learning",2022,"","","","",16,"2022-07-13 09:23:14","","","","",,,,,1,1.00,1,2,1,"Scientiﬁc research’s mandate is to comprehend and explore the world, as well as to improve it based on experience and knowledge. Knowledge embedding and knowledge discovery are two signif-icant methods of integrating knowledge and data. Through knowledge embedding, the barriers between knowledge and data can be eliminated, and machine learning models with physical common sense can be established. Meanwhile, humans’ understanding of the world is always limited, and knowledge discovery takes advantage of machine learning to extract new knowledge from observations. Knowledge discovery can not only assist researchers to better grasp the nature of physics, but it can also support them in conducting knowledge embedding research. A closed loop of knowledge generation and usage are formed by combining knowledge embedding with knowledge discovery, which can improve the robustness and accuracy of models and uncover previously unknown scientiﬁc principles. This study summarizes and ana-lyzes extant literature, as well as identiﬁes research gaps and future opportunities.","",""
0,"A. Studier-Fischer, S. Seidlitz, J. Sellner, B. Özdemir, M. Wiesenfarth, L. Ayala, J. Odenthal, S. Knödler, K. Kowalewski, C. Haney, I. Camplisson, M. Dietrich, Karsten Schmidt, G. A. Salg, H. Kenngott, T. Adler, N. Schreck, A. Kopp-Schneider, Klaus Maier-Hein, L. Maier-Hein, B. Müller-Stich, F. Nickel","Spectral organ fingerprints for machine learning-based intraoperative tissue classification with hyperspectral imaging in a porcine model",2022,"","","","",17,"2022-07-13 09:23:14","","10.1038/s41598-022-15040-w","","",,,,,0,0.00,0,22,1,"","",""
0,"Suleyman Emre Isik, Ali Eren Aytekin, Halil Vurus","A machine learning approach for abstraction and reasoning problems without large amounts of data",2022,"","","","",18,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,3,1,"Journal of Emerging Investigators • www.emerginginvestigators.org level abstraction-reasoning ability which makes it difficult for algorithms to handle volatile and hard-to-predict real-life problems. The problems caused by this task-based nature necessitated flexibility and robustness for certain broader subfields of AI, such as L5 self-driving, domestic robotics, or personal assistants; there is even increasing interest in generality itself (e.g., developmental robotics, artificial general intelligence) (2, 3). The first and most important step to take in order to offer an approach that is closer to human intelligence is to examine the concept of intelligence and to define it in the most useful way. Various definitions have been made for intelligence in the past. Legg and Hutter summarized the definitions made in the context of artificial intelligence research as follows: ""Intelligence measures a person's ability to achieve goals in a wide and varied environment (4)."" Two main characteristics are emphasized here: a task-goal focus and generalizability to a wide range of environments. Accordingly, while human intelligence can perform tasks with its high ability, these abilities can also be generalized for new tasks in new environments (skill acquisition). This feature is a mechanism that human nature has developed in line with evolutionary psychology to solve new unknown tasks and problems (5, 6). In the direction of the development of AI, many approaches have emerged to develop and evaluate AI models. One of them is the human observational approach that examines, judges, and scores the system’s inputs and outputs. This is a highly subjective, difficult, and expensive method to automate. White-box analysis, on the other hand, is inspecting the implementation of the system to determine its input-output response and score it (e.g., an algorithm that plays “Connect Four”) (7). Peer confrontation, for example, is having the system compete against either other AIs or humans. This is the preferred mode of evaluation for player-versus-player games, such as chess. The benchmarking approach, which is based on enabling the system through algorithms to produce outputs for a ""test set"" of inputs (or environments) for which the desired outcome is known (solvable by humans), is another of the most valuable approaches for the evaluation of artificial intelligence. In particular, it is reproducible (test set fixed), scalable (cheap to run the evaluation multiple times), easy to set up, and flexible enough to be applied to a wide variety of possible tasks (8). For this reason, benchmarking has been an important part of progress in artificial intelligence A machine learning approach for abstraction and reasoning problems without large amounts of data","",""
11,"B. Celik, J. Vanschoren","Adaptation Strategies for Automated Machine Learning on Evolving Data",2020,"","","","",19,"2022-07-13 09:23:14","","10.1109/TPAMI.2021.3062900","","",,,,,11,5.50,6,2,2,"Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust to changes in the underlying data. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on a variety of AutoML approaches for building machine learning pipelines, including Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.","",""
3,"Minsung Hong, R. Akerkar","Analytics and Evolving Landscape of Machine Learning for Emergency Response",2019,"","","","",20,"2022-07-13 09:23:14","","10.1007/978-3-030-15628-2_11","","",,,,,3,1.00,2,2,3,"","",""
2,"Jiafei Zhao, Rongkun Jiang, Xuetian Wang, Hongmin Gao","Robust CFAR Detection for Multiple Targets in K-Distributed Sea Clutter Based on Machine Learning",2019,"","","","",21,"2022-07-13 09:23:14","","10.3390/sym11121482","","",,,,,2,0.67,1,4,3,"For K-distributed sea clutter, a constant false alarm rate (CFAR) is crucial as a desired property for automatic target detection in an unknown and non-stationary background. In multiple-target scenarios, the target masking effect reduces the detection performance of CFAR detectors evidently. A machine learning based processor, associating the artificial neural network (ANN) and a clustering algorithm of density-based spatial clustering of applications with noise (DBSCAN), namely, DBSCAN-CFAR, is proposed herein to address this issue. ANN is trained with a symmetrical structure to estimate the shape parameter of background clutter, whereas DBSCAN is devoted to excluding interference targets and sea spikes as outliers in the leading and lagging windows that are symmetrical about the cell under test (CUT). Simulation results verified that the ANN-based method provides the optimal parameter estimation results in the range of 0.1 to 30, which facilitates the control of actual false alarm probability. The effectiveness and robustness of DBSCAN-CFAR are also confirmed by the comparisons of conventional CFAR processors in different clutter conditions, comprised of varying target numbers, shape parameters, and false alarm probabilities. Although the proposed ANN-based DBSCAN-CFAR processor incurs more elapsed time, it achieves superior CFAR performance without a prior knowledge on the number and distribution of interference targets.","",""
6,"Marie Kloenne, S. Niehaus, L. Lampe, A. Merola, J. Reinelt, I. Roeder, N. Scherf","Domain-specific cues improve robustness of deep learning-based segmentation of CT volumes",2019,"","","","",22,"2022-07-13 09:23:14","","10.1038/s41598-020-67544-y","","",,,,,6,2.00,1,7,3,"","",""
3,"A. Sadeghi, Gang Wang, Meng Ma, G. Giannakis","Learning while Respecting Privacy and Robustness to Distributional Uncertainties and Adversarial Data",2020,"","","","",23,"2022-07-13 09:23:14","","","","",,,,,3,1.50,1,4,2,"Data used to train machine learning models can be adversarial--maliciously constructed by adversaries to fool the model. Challenge also arises by privacy, confidentiality, or due to legal constraints when data are geographically gathered and stored across multiple learners, some of which may hold even an ""anonymized"" or unreliable dataset. In this context, the distributionally robust optimization framework is considered for training a parametric model, both in centralized and federated learning settings. The objective is to endow the trained model with robustness against adversarially manipulated input data, or, distributional uncertainties, such as mismatches between training and testing data distributions, or among datasets stored at different workers. To this aim, the data distribution is assumed unknown, and lies within a Wasserstein ball centered around the empirical data distribution. This robust learning task entails an infinite-dimensional optimization problem, which is challenging. Leveraging a strong duality result, a surrogate is obtained, for which three stochastic primal-dual algorithms are developed: i) stochastic proximal gradient descent with an $\epsilon$-accurate oracle, which invokes an oracle to solve the convex sub-problems; ii) stochastic proximal gradient descent-ascent, which approximates the solution of the convex sub-problems via a single gradient ascent step; and, iii) a distributionally robust federated learning algorithm, which solves the sub-problems locally at different workers where data are stored. Compared to the empirical risk minimization and federated learning methods, the proposed algorithms offer robustness with little computation overhead. Numerical tests using image datasets showcase the merits of the proposed algorithms under several existing adversarial attacks and distributional uncertainties.","",""
1,"Jiashuo Liu, Zheyan Shen, Peng Cui, Linjun Zhou, Kun Kuang, Bo Li, Yishi Lin","Invariant Adversarial Learning for Distributional Robustness",2020,"","","","",24,"2022-07-13 09:23:14","","","","",,,,,1,0.50,0,7,2,"Machine learning algorithms with empirical risk minimization are vulnerable to distributional shifts due to the greedy adoption of all the correlations found in training data. Recently, there are robust learning methods aiming at this problem by minimizing the worst-case risk over an uncertainty set. However, they equally treat all covariates to form the uncertainty sets regardless of the stability of their correlations with the target, resulting in the overwhelmingly large set and low confidence of the learner. In this paper, we propose the Invariant Adversarial Learning (IAL) algorithm that leverages heterogeneous data sources to construct a more practical uncertainty set and conduct robustness optimization, where covariates are differentiated according to the stability of their correlations with the target. We theoretically show that our method is tractable for stochastic gradient-based optimization and provide the performance guarantees for our method. Empirical studies on both simulation and real datasets validate the effectiveness of our method in terms of robust performance across unknown distributional shifts.","",""
111,"Heinrich Jiang, Ofir Nachum","Identifying and Correcting Label Bias in Machine Learning",2019,"","","","",25,"2022-07-13 09:23:14","","","","",,,,,111,37.00,56,2,3,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.","",""
6,"Manjari Pradhan, B. Bhattacharya, K. Chakrabarty, B. Bhattacharya","Predicting ${X}$ -Sensitivity of Circuit-Inputs on Test-Coverage: A Machine-Learning Approach",2019,"","","","",26,"2022-07-13 09:23:14","","10.1109/TCAD.2018.2878169","","",,,,,6,2.00,2,4,3,"Digital circuits are often prone to suffer from uncertain timing, inadequate sensor feedback, limited controllability of past states or inability of initializing memory-banks, and erroneous behavior of analog-to-digital converters, which may produce an unknown (<inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>) logic value at various circuit nodes. Additionally, many design bugs that are identified during the post-silicon validation phase manifest themselves as <inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>-values. The presence of such <inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>-sources on certain primary or secondary inputs of a logic circuit may cause loss of fault-coverage of a test set, which, in turn, may impact its reliability and robustness. In this paper, we provide a mechanism for predicting the sensitivity of <inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>-sources in terms of loss of fault-coverage, on the basis of learning only a few structural features of the circuit that are easy to extract from the netlist. We show that the <inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>-sources can be graded satisfactorily according to their sensitivity using support vector regression, thereby obviating the need for costly explicit simulation. Experimental results on several benchmark circuits demonstrate the efficacy, speed, and accuracy of prediction.","",""
0,"T. Ogunfunmi, R. Ramachandran, R. Togneri, B. Y. Smolenski, Visar Berisha","Guest Editorial: Algorithms and Architectures for Machine Learning Based Speech Processing",2019,"","","","",27,"2022-07-13 09:23:14","","10.1007/S00034-019-01161-7","","",,,,,0,0.00,0,5,3,"","",""
9,"Milena Nacchia, F. Fruggiero, A. Lambiase, K. Bruton","A Systematic Mapping of the Advancing Use of Machine Learning Techniques for Predictive Maintenance in the Manufacturing Sector",2021,"","","","",28,"2022-07-13 09:23:14","","10.3390/APP11062546","","",,,,,9,9.00,2,4,1,"The increasing availability of data, gathered by sensors and intelligent machines, is changing the way decisions are made in the manufacturing sector. In particular, based on predictive approach and facilitated by the nowadays growing capabilities of hardware, cloud-based solutions, and new learning approaches, maintenance can be scheduled—over cell engagement and resource monitoring—when required, for minimizing (or managing) unexpected equipment failures, improving uptime through less aggressive maintenance schedules, shortening unplanned downtime, reducing excess (direct and indirect) cost, reducing long-term damage to machines and processes, and improve safety plans. With access to increased levels of data (and over learning mechanisms), companies have the capability to conduct statistical tests using machine learning algorithms, in order to uncover root causes of problems previously unknown. This study analyses the maturity level and contributions of machine learning methods for predictive maintenance. An upward trend in publications for predictive maintenance using machine learning techniques was identified with the USA and China leading. A mapping study—steady set until early 2019 data—was employed as a formal and well-structured method to synthesize material and to report on pervasive areas of research. Type of equipment, sensors, and data are mapped to properly assist new researchers in positioning new research activities in the domain of smart maintenance. Hence, in this paper, we focus on data-driven methods for predictive maintenance (PdM) with a comprehensive survey on applications and methods until, for the sake of commenting on stable proposal, 2019 (early included). An equal repartition between evaluation and validation studies was identified, this being a symptom of an immature but growing research area. In addition, the type of contribution is mainly in the form of models and methodologies. Vibrational signal was marked as the most used data set for diagnosis in manufacturing machinery monitoring; furthermore, supervised learning is reported as the most used predictive approach (ensemble learning is growing fast). Neural networks, followed by random forests and support vector machines, were identified as the most applied methods encompassing 40% of publications, of which 67% related to deep neural network with long short-term memory predominance. Notwithstanding, there is no robust approach (no one reported optimal performance over different case tests) that works best for every problem. We finally conclude the research in this area is moving fast to gather a separate focused analysis over the last two years (whenever stable implementations will appear).","",""
6,"Senwei Liang, Shixiao W. Jiang, J. Harlim, Haizhao Yang","Solving PDEs on Unknown Manifolds with Machine Learning",2021,"","","","",29,"2022-07-13 09:23:14","","","","",,,,,6,6.00,2,4,1,"This paper proposes a mesh-free computational framework and machine learning theory for solving elliptic PDEs on unknown manifolds, identified with point clouds, based on diffusion maps (DM) and deep learning. The PDE solver is formulated as a supervised learning task to solve a least-squares regression problem that imposes an algebraic equation approximating a PDE (and boundary conditions if applicable). This algebraic equation involves a graph-Laplacian type matrix obtained via DM asymptotic expansion, which is a consistent estimator of second-order elliptic differential operators. The resulting numerical method is to solve a highly non-convex empirical risk minimization problem subjected to a solution from a hypothesis space of neural networks. In a well-posed elliptic PDE setting, when the hypothesis space consists of neural networks with either infinite width or depth, we show that the global minimizer of the empirical loss function is a consistent solution in the limit of large training data. When the hypothesis space is a twolayer neural network, we show that for a sufficiently large width, gradient descent can identify a global minimizer of the empirical loss function. Supporting numerical examples demonstrate the convergence of the solutions, ranging from simple manifolds with low and high co-dimensions, to rough surfaces with and without boundaries. We also show that the proposed NN solver can robustly generalize the PDE solution on new data points with generalization errors that are almost identical to the training errors, superseding a Nyström-based interpolation method. K eywords High-Dimensional PDEs ·Diffusion Maps ·Deep Neural Networks ·Convergence Analysis · Least-Squares Minimization · Manifolds · Point Clouds. ar X iv :2 10 6. 06 68 2v 2 [ m at h. N A ] 1 0 Ju n 20 22 A PREPRINT JUNE 13, 2022","",""
6,"H. Sharif, Rizwan Ahmed Khan","A Novel Machine Learning Based Framework for Detection of Autism Spectrum Disorder (ASD)",2019,"","","","",30,"2022-07-13 09:23:14","","10.1080/08839514.2021.2004655","","",,,,,6,2.00,3,2,3,"Computer vision and machine learning are the linchpin of field of automation. The medicine industry has adopted numerous methods to discover the root causes of many diseases in order to automate detection process. But, the biomarkers of Autism Spectrum Disorder (ASD) are still unknown, let alone automating its detection. Studies from the neuroscience domain highlighted the fact that corpus callosum and intracranial brain volume holds significant information for detection of ASD. Such results and studies are not tested and verified by scientists working in the domain of computer vision / machine learning. Thus, in this study we have proposed a machine learning based framework for automatic detection of ASD using features extracted from corpus callosum and intracranial brain volume from ABIDE dataset. Corpus callosum and intracranial brain volume data is obtained from T1-weighted MRI scans. Our proposed framework first calculates weights of features extracted from Corpus callosum and intracranial brain volume data. This step ensures to utilize discriminative capabilities of only those features that will help in robust recognition of ASD. Then, conventional machine learning algorithm (conventional refers to algorithms other than deep learning) is applied on features that are most significant in terms of discriminative capabilities for recognition of ASD. Finally, for benchmarking and to verify potential of deep learning on analyzing neuroimaging data i.e. T1-weighted MRI scans, we have done experiment with state of the art deep learning architecture i.e. VGG16 . We have used transfer learning approach to use already trained VGG16 model for detection of ASD. This is done to help readers understand benefits and bottlenecks of using deep learning approach for analyzing neuroimaging data which is difficult to record in large enough quantity for deep learning.","",""
6,"A. Soni, Dharamvir Dharmacharya, A. Pal, V. Srivastava, R. Shaw, Ankush Ghosh","Design of a Machine Learning-Based Self-driving Car",2021,"","","","",31,"2022-07-13 09:23:14","","10.1007/978-981-16-0598-7_11","","",,,,,6,6.00,1,6,1,"","",""
4,"Zheng Li, Ye Chen, Siyuan Chang, B. Rousseau, Haoxiang Luo","A one-dimensional flow model enhanced by machine learning for simulation of vocal fold vibration.",2021,"","","","",32,"2022-07-13 09:23:14","","10.1121/10.0003561","","",,,,,4,4.00,1,5,1,"A one-dimensional (1D) unsteady and viscous flow model that is derived from the momentum and mass conservation equations is described, and to enhance this physics-based model, a machine learning approach is used to determine the unknown modeling parameters. Specifically, an idealized larynx model is constructed and ten cases of three-dimensional (3D) fluid-structure interaction (FSI) simulations are performed. The flow data are then extracted to train the 1D flow model using a sparse identification approach for nonlinear dynamical systems. As a result of training, we obtain the analytical expressions for the entrance effect and pressure loss in the glottis, which are then incorporated in the flow model to conveniently handle different glottal shapes due to vocal fold vibration. We apply the enhanced 1D flow model in the FSI simulation of both idealized vocal fold geometries and subject-specific anatomical geometries reconstructed from the magnetic resonance imaging images of rabbits' larynges. The 1D flow model is evaluated in both of these setups and shown to have robust performance. Therefore, it provides a fast simulation tool that is superior to the previous 1D models.","",""
5,"Tao Chen, M. Ludkovski","A Machine Learning Approach to Adaptive Robust Utility Maximization and Hedging",2019,"","","","",33,"2022-07-13 09:23:14","","10.1137/20m1336023","","",,,,,5,1.67,3,2,3,"We investigate the adaptive robust control framework for portfolio optimization and loss-based hedging under drift and volatility uncertainty. Adaptive robust problems offer many advantages but require handling a double optimization problem (infimum over market measures, supremum over the control) at each instance. Moreover, the underlying Bellman equations are intrinsically multi-dimensional. We propose a novel machine learning approach that solves for the local saddle-point at a chosen set of inputs and then uses a nonparametric (Gaussian process) regression to obtain a functional representation of the value function. Our algorithm resembles control randomization and regression Monte Carlo techniques but also brings multiple innovations, including adaptive experimental design, separate surrogates for optimal control and the local worst-case measure, and computational speed-ups for the sup-inf optimization. Thanks to the new scheme we are able to consider settings that have been previously computationally intractable and provide several new financial insights about learning and optimal trading under unknown market parameters. In particular, we demonstrate the financial advantages of adaptive robust framework compared to adaptive and static robust alternatives.","",""
5,"D. Mccoy, W. Mgbara, Nir Horvitzc, W. Getz, A. Hubbard","Ensemble machine learning of factors influencing COVID-19 across US counties",2020,"","","","",34,"2022-07-13 09:23:14","","10.1038/s41598-021-90827-x","","",,,,,5,2.50,1,5,2,"","",""
5,"Francesco Regazzoni, D. Chapelle, P. Moireau","Combining data assimilation and machine learning to build data‐driven models for unknown long time dynamics—Applications in cardiovascular modeling",2021,"","","","",35,"2022-07-13 09:23:14","","10.1002/cnm.3471","","",,,,,5,5.00,2,3,1,"We propose a method to discover differential equations describing the long‐term dynamics of phenomena featuring a multiscale behavior in time, starting from measurements taken at the fast‐scale. Our methodology is based on a synergetic combination of data assimilation (DA), used to estimate the parameters associated with the known fast‐scale dynamics, and machine learning (ML), used to infer the laws underlying the slow‐scale dynamics. Specifically, by exploiting the scale separation between the fast and the slow dynamics, we propose a decoupling of time scales that allows to drastically lower the computational burden. Then, we propose a ML algorithm that learns a parametric mathematical model from a collection of time series coming from the phenomenon to be modeled. Moreover, we study the interpretability of the data‐driven models obtained within the black‐box learning framework proposed in this paper. In particular, we show that every model can be rewritten in infinitely many different equivalent ways, thus making intrinsically ill‐posed the problem of learning a parametric differential equation starting from time series. Hence, we propose a strategy that allows to select a unique representative model in each equivalence class, thus enhancing the interpretability of the results. We demonstrate the effectiveness and noise‐robustness of the proposed methods through several test cases, in which we reconstruct several differential models starting from time series generated through the models themselves. Finally, we show the results obtained for a test case in the cardiovascular modeling context, which sheds light on a promising field of application of the proposed methods.","",""
3,"Chao-Yu Guo, Yinghua Yang, Yi-Hau Chen","The Optimal Machine Learning-Based Missing Data Imputation for the Cox Proportional Hazard Model",2021,"","","","",36,"2022-07-13 09:23:14","","10.3389/fpubh.2021.680054","","",,,,,3,3.00,1,3,1,"An adequate imputation of missing data would significantly preserve the statistical power and avoid erroneous conclusions. In the era of big data, machine learning is a great tool to infer the missing values. The root means square error (RMSE) and the proportion of falsely classified entries (PFC) are two standard statistics to evaluate imputation accuracy. However, the Cox proportional hazards model using various types requires deliberate study, and the validity under different missing mechanisms is unknown. In this research, we propose supervised and unsupervised imputations and examine four machine learning-based imputation strategies. We conducted a simulation study under various scenarios with several parameters, such as sample size, missing rate, and different missing mechanisms. The results revealed the type-I errors according to different imputation techniques in the survival data. The simulation results show that the non-parametric “missForest” based on the unsupervised imputation is the only robust method without inflated type-I errors under all missing mechanisms. In contrast, other methods are not valid to test when the missing pattern is informative. Statistical analysis, which is improperly conducted, with missing data may lead to erroneous conclusions. This research provides a clear guideline for a valid survival analysis using the Cox proportional hazard model with machine learning-based imputations.","",""
3,"M. Ramezani, Pauline Mouches, E. Yoon, Deepthi Rajashekar, J. Ruskey, E. Leveille, Kristina Martens, M. Kibreab, Tracy Hammer, I. Kathol, Nadia Maarouf, J. Sarna, D. Martino, G. Pfeffer, Z. Gan-Or, N. Forkert, O. Monchi","Investigating the relationship between the SNCA gene and cognitive abilities in idiopathic Parkinson’s disease using machine learning",2021,"","","","",37,"2022-07-13 09:23:14","","10.1038/s41598-021-84316-4","","",,,,,3,3.00,0,17,1,"","",""
20,"Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae","Engineering problems in machine learning systems",2019,"","","","",38,"2022-07-13 09:23:14","","10.1007/s10994-020-05872-w","","",,,,,20,6.67,7,3,3,"","",""
2,"Joe Kwan, Fan Jiang, Le Hong","Applying machine learning methods to accelerate advanced process node yield ramp",2021,"","","","",39,"2022-07-13 09:23:14","","10.1117/12.2584008","","",,,,,2,2.00,1,3,1,"With the continuous growth in IC manufacturing complexity, developing new process nodes has become an ever increasing challenge. From the initial process node architectural explorations to initial design rule specifications to early RET development and “risk production” early NPI (New Product Introductions), critical decisions with far reaching performance and yield impact must be made. Applying innovative methods to enable early and broad engineered testing informs better architectural decisions and performance tradeoffs. Methods to identify, root-cause, categorize known yield detractors and to flag unknown potentially new risk patterns enable product yield risk mitigation and continuous learning. Accumulated learning from each step, each stage and each new product drives improved testing vehicles, better process optimization, and enhanced PDKs, all leading to more robust designs and ultimately higher performance and improved yield. In this paper, we describe innovative Machine Learning methods in DFM and DTCO Applications to improve test vehicle engineering, inform process development and accelerate process node yield ramp.","",""
1,"Linghua Meng, Huanjun Liu, S. Ustin, Xinle Zhang","Predicting Maize Yield at the Plot Scale of Different Fertilizer Systems by Multi-Source Data and Machine Learning Methods",2021,"","","","",40,"2022-07-13 09:23:14","","10.3390/rs13183760","","",,,,,1,1.00,0,4,1,"Timely and reliable maize yield prediction is essential for the agricultural supply chain and food security. Previous studies using either climate or satellite data or both to build empirical or statistical models have prevailed for decades. However, to what extent climate and satellite data can improve yield prediction is still unknown. In addition, fertilizer information may also improve crop yield prediction, especially in regions with different fertilizer systems, such as cover crop, mineral fertilizer, or compost. Machine learning (ML) has been widely and successfully applied in crop yield prediction. Here, we attempted to predict maize yield from 1994 to 2007 at the plot scale by integrating multi-source data, including monthly climate data, satellite data (i.e., vegetation indices (VIs)), fertilizer data, and soil data to explore the accuracy of different inputs to yield prediction. The results show that incorporating all of the datasets using random forests (RF) and AB (adaptive boosting) can achieve better performances in yield prediction (R2: 0.85~0.98). In addition, the combination of VIs, climate data, and soil data (VCS) can predict maize yield more effectively than other combinations (e.g., combinations of all data and combinations of VIs and soil data). Furthermore, we also found that including different fertilizer systems had different prediction accuracies. This paper aggregates data from multiple sources and distinguishes the effects of different fertilization scenarios on crop yield predictions. In addition, the effects of different data on crop yield were analyzed in this study. Our study provides a paradigm that can be used to improve yield predictions for other crops and is an important effort that combines multi-source remotely sensed and environmental data for maize yield prediction at the plot scale and develops timely and robust methods for maize yield prediction grown under different fertilizing systems.","",""
2,"Nick Fountain-Jones, Megan L. Smith, F. Austerlitz","Machine learning in molecular ecology",2021,"","","","",41,"2022-07-13 09:23:14","","10.1111/1755-0998.13532","","",,,,,2,2.00,1,3,1,"Advances in nextgeneration sequencing (NGS) platforms are allowing researchers to routinely collate large genomewide data sets to address a variety of ecological questions. However, with this big data comes big analytical challenges that are increasingly addressed using machine learning (for a review, see Schrider & Kern, 2018). Machine learning is a subfield of artificial intelligence and represents a conglomeration of methods where predictive accuracy is the primary goal (e.g., Belcaid & Toonen, 2015; Breiman, 2001; Elith et al., 2008; Lucas, 2020). Machine learning assumes that the datagenerating process is unknown and complex and finds the dominant patterns by learning the relationships between inputs and responses (Elith et al., 2008). Broadly, machine learning differs from other statistical approaches in two important ways. The first is that predictive performance drives model formulation rather than model selection or expert opinion, and the second is there is less emphasis on model selection ( Breiman, 2001; Lucas, 2020). For these reasons, machine learning has the reputation for being less interpretable and difficult to apply rigorously (Elith et al., 2008; Lucas, 2020; Molnar, 2018). However, in parallel with the revolution of sequencing techniques, there has also been a revolution in data science in terms of predictive performance and techniques to interpret machine learning models (FountainJones et al., 2019; Lucas, 2020; Molnar, 2018). There are now streamlined R and Python packages that make the robust use of algorithms from support vector machines (SVMs) to neural networks readily achievable (e.g., Abadi et al., 2015; Kuhn & Wickham, 2020, see Text Box 1 for some important machine learning terminology). Moreover, other statistical paradigms such as approximate Bayesian computation (ABC) are being applied sidebyside or within machine learning frameworks to enhance the utility of these approaches (e.g., Carlson, 2020; Raynal et al., 2019). The ability of machine learning algorithms to build powerful predictive models that capture complex nonlinear responses with minimal statistical assumptions has been harnessed by most molecular ecology subdisciplines for decades. For example, machine learning models were developed before the turn of the millennium to classify normal or cancerous tissue based on transcription profiles (Furey et al., 2000). Not long after gradient boosting models (GBMs) were developed (e.g., Hastie et al., 2009), researchers were applying the approach to classify population genetics models based on a suite of summary statistics such as Tajima's θπ (Lin et al., 2011). In addition, extensions of the popular random forest algorithm have been utilized in ecological genetics to untangle the drivers of climate adaptation (Fitzpatrick & Keller, 2015). Generally, however, advances in computer science and machine learning are slow to filter down to ecologists (Belcaid & Toonen, 2015; Elith & Hastie, 2008; FountainJones et al., 2019), partly through unfamiliarity with these types of approaches but also because of the rapid rate of advance in the data science field. This Special Issue aims to help expand the use of machine learning approaches and to help bring advances in data science to the toolkits of molecular ecologists. This issue comprises 17 papers grouped into four sections covering a diverse variety of molecular ecology subdisciplines. The first section covers how machine learning can be applied to make inferences about population demography. We further group these papers algorithmically with four papers utilizing random forest architecture and the remaining four using neural networks. The second section highlights how machine learning can detect signatures of selection across loci. The third section highlights how these methods can be applied to untangle the complex ecological drivers of genomic change (‘ecological genomics’) and species community dynamics. The last section explores how advances in machine learning can provide insights into species limits and contribute to biodiversity monitoring.","",""
1,"C. Knaak, M. Kröger, F. Schulze, P. Abels, A. Gillner","Deep Learning and Conventional Machine Learning for Image-Based in-Situ Fault Detection During Laser Welding: A Comparative Study",2021,"","","","",42,"2022-07-13 09:23:14","","10.20944/PREPRINTS202105.0272.V1","","",,,,,1,1.00,0,5,1,"An effective process monitoring strategy is a requirement for meeting the challenges posed by increasingly complex products and manufacturing processes. To address these needs, this study investigates a comprehensive scheme based on classical machine learning methods, deep learning algorithms, and feature extraction and selection techniques. In a first step, a novel deep learning architecture based on convolutional neural networks (CNN) and gated recurrent units (GRU) is introduced to predict the local weld quality based on mid-wave infrared (MWIR) and near-infrared (NIR) image data. The developed technology is used to discover critical welding defects including lack of fusion (false friends), sagging and lack of penetration, and geometric deviations of the weld seam. Additional work is conducted to investigate the significance of various geometrical, statistical, and spatio-temporal features extracted from the keyhole and weld pool regions. Furthermore, the performance of the proposed deep learning architecture is compared to that of classical supervised machine learning algorithms, such as multi-layer perceptron (MLP), logistic regression (LogReg), support vector machines (SVM), decision trees (DT), random forest (RF) and k-Nearest Neighbors (kNN). Optimal hyperparameters for each algorithm are determined by an extensive grid search. Ultimately, the three best classification models are combined into an ensemble classifier that yields the highest detection rates and achieves the most robust estimation of welding defects among all classifiers studied, which is validated on previously unknown welding trials.","",""
2,"Aamir S Ahanger, S. M. Khan, F. Masoodi","An Effective Intrusion Detection System using Supervised Machine Learning Techniques",2021,"","","","",43,"2022-07-13 09:23:14","","10.1109/ICCMC51019.2021.9418291","","",,,,,2,2.00,1,3,1,"With the increased use of Internet resources, cyber attackers are using novel ways to attack the services of network. Thus network security is becoming inevitable part of the network system. In order to detect such attacks efficiently and effectively, robust IDS (Intrusion Detection System) is needed. An IDS is a tool that analyzes each and every packet deeply to detects malicious activity by monitoring a network or a system. The main purpose of IDS is to identify unwanted or abnormal action and to inform the network administrator about such actions. Thus, IDS is important tool for the network administrator to prevent the network from both known and unknown attacks that make the network resources more vulnerable. Machine learning methods can be used to employ efficient intrusion detection system (IDS). In this research work four machine learning methods were used namely RF (Random Forest), DT (Decision Tree), MLP (Multilayer perceptron) and SVM (Support Vector Machine) for classification of the data. NSL-KDD dataset was used for training and testing these various machine learning models. Feature selections were used to eliminate the irrelevant and unwanted features from the dataset. Therefore feature selection reduces the dimensionality of the dataset which in turn reduces the computational complexity. The proposed model’s output was evaluated using three feature subsets, randomly selected from the NSL-KDD dataset. The proposed method has a classification accuracy of more than 99 percent.","",""
2,"E. Amiri Souri, A. Chenoweth, A. Cheung, S. Karagiannis, S. Tsoka","Cancer Grade Model: a multi-gene machine learning-based risk classification for improving prognosis in breast cancer",2021,"","","","",44,"2022-07-13 09:23:14","","10.1038/s41416-021-01455-1","","",,,,,2,2.00,0,5,1,"","",""
1,"Huda Ali Alatwi, C. Morisset","Adversarial Machine Learning In Network Intrusion Detection Domain: A Systematic Review",2021,"","","","",45,"2022-07-13 09:23:14","","","","",,,,,1,1.00,1,2,1,"Due to their massive success in various domains, deep learning techniques are increasingly used to design network intrusion detection solutions that detect and mitigate unknown and known attacks with high accuracy detection rates and minimal feature engineering. However, it has been found that deep learning models are vulnerable to data instances that can mislead the model to make incorrect classification decisions socalled (adversarial examples). Such vulnerability allows attackers to target NIDSs by adding small crafty perturbations to the malicious traffic to evade detection and disrupt the system’s critical functionalities. The problem of deep adversarial learning has been extensively studied in the computer vision domain; however, it is still an area of open research in network security applications. Therefore, this survey explores the researches that employ different aspects of adversarial machine learning in the area of network intrusion detection in order to provide directions for potential solutions. First, the surveyed studies are categorized based on their contribution to generating adversarial examples, evaluating the robustness of ML-based NIDs towards adversarial examples, and defending these models against such attacks. Second, we highlight the characteristics identified in the surveyed research. Furthermore, we discuss the applicability of the existing generic adversarial attacks for the NIDS domain, the feasibility of launching the proposed attacks in real-world scenarios, and the limitations of the existing mitigation solutions. Keywords—Network Intrusion Detection, Deep Neural Networks, Adversarial Examples, Adversarial Robustness, Adversarial Attacks, Evasion Attacks, Cyber Security, Machine Learning, Adversarial Machine Learning","",""
1,"T. Acharya, Ishan Khatri, A. Annamalai, M. Chouikha","Efficacy of Heterogeneous Ensemble Assisted Machine Learning Model for Binary and Multi-Class Network Intrusion Detection",2021,"","","","",46,"2022-07-13 09:23:14","","10.1109/I2CACIS52118.2021.9495864","","",,,,,1,1.00,0,4,1,"The exponential rise in internet technologies and allied applications encompass a significantly large number of networked devices have alarmed academia-industries to achieve more effective and robust security solutions. Undeniably, digitization has led to revolution globally; however, the security threats, breaches, and subsequent losses indicate the need for a robust cybersecurity solution. Unlike classical intrusion detection systems (IDS), network IDS (NIDS) has been becoming more challenging due to continuous changes in attack-patterns and anomaly behavior. As solution data-driven machine learning methods have exhibited better by learning over network traffic information and detecting anomalies; however, its generalization over a network with both known and unknown patterns remains questionable. Moreover, most of the classical approaches fail to address the key issues of class-imbalance, level-of-significance centric feature selection, normalization and over-fitting problems resulting in different performance by varied machine learning models. In this paper, a novel and robust heterogeneous ensemble machine learning model is developed to detect anomalies in NIDS. The proposed model first applies sub-sampling to alleviate the class-imbalance problem of NIDS datasets. Subsequently, performing normalization using the Min-Max algorithm, it mapped the input data in the range of 0 to 1, thus alleviating overfitting and convergence. The feature reduction is used to reduce the features; it retained the most suitable features without imposing computational overheads, often in meta-heuristic-based approaches. Finally, the proposed NIDS solution designed a Heterogeneous ensemble learning model with J48, k-NN, SVM, Bagging, AdaBoost, and RF algorithms as base-classifier to perform two-class as well as multi-class classification over feature-selected NSL-KDD, KDD99, and UNSW-NB-15 datasets. Performance assessment in terms of true-positive rate, false positive rate and AUC revealed that the proposed NIDS model exhibited better performance than the standalone classifiers and superior to other existing anomaly detection methods.","",""
1,"L. Ahmed, Y. A. M. Hamad","Machine Learning Techniques for Network-based Intrusion Detection System: A Survey Paper",2021,"","","","",47,"2022-07-13 09:23:14","","10.1109/NCCC49330.2021.9428827","","",,,,,1,1.00,1,2,1,"The rapid growth of Internet technologies and further dependence on online services, increase the demand for keeping these networks and data secure. The protection of online information is becoming even more vital to the national security and economic stability. Recently, network security has become one of the most concerning subjects in the current research and industry fields. Intrusion Detection Systems (IDSs) are considered as the backbone for network and data protection. Throughout time, different IDS approaches have been implemented to attain maximum detection accuracy. Machine learning IDS is one of the promising IDS techniques that have been created to detect known as well as unknown attacks. This paper investigates various machine learning techniques used to deploy Network-based Intrusion Detection System (NIDS). This survey could provide a more robust understanding of the existing techniques and assists intrigued researchers to identify research opportunities and investigate more in this direction.","",""
0,"Tony Hämmäinen, Julen Kahles","Clustering Unknown IoT Devices in a 5G Mobile Network Security Context via Machine Learning",2021,"","","","",48,"2022-07-13 09:23:14","","10.1109/wimob52687.2021.9606307","","",,,,,0,0.00,0,2,1,"We propose a novel machine-learning pipeline for clustering unknown IoT devices in an industrial 5G mobile-network setting. Organizing IoT devices as few homogeneous device groups improves the applicability of network-intrusion detection systems. More specifically, we develop feature engineering methods that transform IP-flows into device-level data points, define distance metrics between the data points, and apply the DBSCAN algorithm on them. Our experiments on a simulated IoT device network with varying levels of noise show that our proposed methodology outperforms alternative methods and is the only one producing a robust grouping of the IoT devices with noise present in the traffic data.","",""
0,"R. Oldroyd, M. Morris, M. Birkin","Predicting Food Safety Compliance for Informed Food Outlet Inspections: A Machine Learning Approach",2021,"","","","",49,"2022-07-13 09:23:14","","10.3390/ijerph182312635","","",,,,,0,0.00,0,3,1,"Consumer food environments have transformed dramatically in the last decade. Food outlet prevalence has increased, and people are eating food outside the home more than ever before. Despite these developments, national spending on food control has reduced. The National Audit Office report that only 14% of local authorities are up to date with food business inspections, exposing consumers to unknown levels of risk. Given the scarcity of local authority resources, this paper presents a data-driven approach to predict compliance for newly opened businesses and those awaiting repeat inspections. This work capitalizes on the theory that food outlet compliance is a function of its geographic context, namely the characteristics of the neighborhood within which it sits. We explore the utility of three machine learning approaches to predict non-compliant food outlets in England and Wales using openly accessible socio-demographic, business type, and urbanness features at the output area level. We find that the synthetic minority oversampling technique alongside a random forest algorithm with a 1:1 sampling strategy provides the best predictive power. Our final model retrieves and identifies 84% of total non-compliant outlets in a test set of 92,595 (sensitivity = 0.843, specificity = 0.745, precision = 0.274). The originality of this work lies in its unique and methodological approach which combines the use of machine learning with fine-grained neighborhood data to make robust predictions of compliance.","",""
0,"R. Mccarthy, Ananya Sen Gupta","Employing and Interpreting a Machine Learning Target-Cognizant Technique for Analysis of Unknown Signals in Multiple Reaction Monitoring",2021,"","","","",50,"2022-07-13 09:23:14","","10.1109/ACCESS.2021.3056955","","",,,,,0,0.00,0,2,1,"The aim of this interdisciplinary work is a robust signal processing and autonomous machine learning framework to associate well-known (target) as well as any potentially unknown (non-target) peaks present within gas chromatography-mass spectrometry (GC/MS/MS) raw instrument signal. Particularly, this work evaluates three machine learning algorithms abilities to autonomously associate raw signal peaks based on accuracy in training and testing. A target is a known congener that is expected to be present within the raw instrument signal and a non-target is an unknown or unexpected compound. Autonomously identifying target peaks within the GC/MS/MS and associating them with non-target peaks can help improve the analysis of collected samples. Association of peaks refers to classifying peaks as known congeners regardless if the peak is a target or non-target. Uncertainty of peaks fitted and discovered through raw instrument signals from GC/MS/MS data is assessed to create topographical illustrations of target annotated peaks among sample raw instrument signals collected across diverse locations in the Chicago area. The term “annotated peak” is used to assign peaks found at specific retention times as a known congener. Adaptive signal processing techniques are utilized to smooth data and correct baseline drifts as well as detect and separate coeluted (overlapped) peaks in the raw instrument signal to provide key feature extraction. 150 air samples are analyzed for individual polychlorinated biphenyls (PCB) with GC/MS/MS across Chicago, IL. 80% of the data is used for training classification of target PCBs and 20% of the data is evaluated to identify and associate consistently occurring non-target peaks with target PCBs. A random forest classifier is used to associate identified peaks to target PCB peaks. Geographical topographical representations of target PCBs in the raw instrument signal demonstrates how PCBs accumulate and degrade in certain locations.","",""
0,"M. Jessell, Jiateng Guo, Yunqiang Li, M. Lindsay, R. Scalzo, J. Giraud, G. Pirot, E. Cripps, V. Ogarko","Into the Noddyverse: A massive data store of 3D geological models for Machine Learning & inversion applications",2021,"","","","",51,"2022-07-13 09:23:14","","10.5194/essd-2021-304","","",,,,,0,0.00,0,9,1,"Abstract. Unlike some other well-known challenges such as facial recognition, where Machine Learning and Inversion algorithms are widely developed, the geosciences suffer from a lack of large, labelled datasets that can be used to validate or train robust Machine Learning and inversion schemes. Publicly available 3D geological models are far too restricted in both number and the range of geological scenarios to serve these purposes. With reference to inverting geophysical data this problem is further exacerbated as in most cases real geophysical observations result from unknown 3D geology, and synthetic test datasets are often not particularly geological, nor geologically diverse. To overcome these limitations, we have used the Noddy modelling platform to generate one million models, which represent the first publicly accessible massive training set for 3D geology and resulting gravity and magnetic datasets. This model suite can be used to train Machine Learning systems, and to provide comprehensive test suites for geophysical inversion. We describe the methodology for producing the model suite, and discuss the opportunities such a model suit affords, as well as its limitations, and how we can grow and access this resource. ","",""
0,"M. Rostami, O. Saarela, M. Escobar, D. Lana","Doubly Robust Estimation with Machine Learning Predictions",2021,"","","","",52,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,4,1,"The estimation of Average Treatment Effect (ATE) as a causal parameter is carried out in two steps, where in the first step, the treatment and outcome are modeled to incorporate the potential confounders, and in the second step, the predictions are inserted into the ATE estimators such as the Augmented Inverse Probability Weighting (AIPW) estimator. Due to the concerns regarding the nonlinear or unknown relationships between confounders and the treatment and outcome, there has been an interest in applying non-parametric methods such as Machine Learning (ML) algorithms instead. Farrell et al. (2018) proposed to use two separate Neural Networks (NNs) where there’s no regularization on the network’s parameters except the Stochastic Gradient Descent (SGD) in the NN’s optimization. Our simulations indicate that the AIPW estimator suffers extensively if no regularization is utilized. We propose the normalization of AIPW (referred to as nAIPW) which can be helpful in some scenarios. nAIPW, provably, has the same properties as AIPW, that is double-robustness and orthogonality (Chernozhukov et al., 2018). Further, if the first step algorithms converge fast enough, under regulatory conditions (Chernozhukov et al., 2018), nAIPW will be asymptotically normal. We also compare the performance of AIPW and nAIPW in terms of the bias and variance when small to moderate L1 regularization is imposed on the NNs.","",""
0,"L. Merte, M. K. Bisbo, I. Sokolović, M. Setvín, Benjamin Hagman, M. Shipilin, M. Schmid, U. Diebold, E. Lundgren, B. Hammer","Structure of an ultrathin oxide film solved by machine learning enhanced global optimization",2021,"","","","",53,"2022-07-13 09:23:14","","10.26434/chemrxiv-2021-mtbq2","","",,,,,0,0.00,0,10,1,"Determination of the atomic structure of solid surfaces is a challenge that has resisted solution despite advancements in experimental methods. Theory-based global optimization has the potential to revolutionize the field by providing reliable structure models as the basis for interpretation of experiments and for prediction of material properties. So far, however, the approach has been limited by the combinatorial complexity and computational expense of sufficiently accurate energy estimation for surfaces. We demonstrate how an evolutionary algorithm, utilizing machine learning for accelerated energy estimation and diverse population generation, can be used to solve an unknown surface structure—the (4 x 4) surface oxide on Pt3Sn(111)--based on limited experimental input. The algorithm is efficient and robust, and should be broadly applicable in surface studies, where it can replace manual, intuition based model generation.","",""
0,"C. Knaak, M. Kröger, F. Schulze, P. Abels, A. Gillner","Deep learning and conventional machine learning for image- based in-situ fault detection during laser welding: A compara- tive study",2021,"","","","",54,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,5,1,"An effective process monitoring strategy is a requirement for meeting the challenges posed by increasingly complex products and manufacturing processes. To address these needs, this study investigates a comprehensive scheme based on classical machine learning methods, deep learning algorithms, and feature extraction and selection techniques. In a first step, a novel deep learning architecture based on convolutional neural networks (CNN) and gated recurrent units (GRU) is introduced to predict the local weld quality based on mid-wave infrared (MWIR) and nearinfrared (NIR) image data. The developed technology is used to discover critical welding defects including lack of fusion (false friends), sagging and lack of penetration, and geometric deviations of the weld seam. Additional work is conducted to investigate the significance of various geometrical, statistical, and spatio-temporal features extracted from the keyhole and weld pool regions. Furthermore, the performance of the proposed deep learning architecture is compared to that of classical supervised machine learning algorithms, such as multi-layer perceptron (MLP), logistic regression (LogReg), support vector machines (SVM), decision trees (DT), random forest (RF) and k-Nearest Neighbors (kNN). Optimal hyperparameters for each algorithm are determined by an extensive grid search. Ultimately, the three best classification models are combined into an ensemble classifier that yields the highest detection rates and achieves the most robust estimation of welding defects among all classifiers studied, which is validated on previously unknown welding trials.","",""
0,"R. Pyke, Dattatreya Mellacheruvu, Charles W. Abbott, Steven Dea, E. Levy, Simo V. Zhang, Nikita Bedi, A. Colevas, Devayani P. Bhave, M. Chinnappa, G. Bartha, J. Lyle, J. West, M. Snyder, J. Sunwoo, Richard Chen, S. Boyle","Abstract 399: Pan-cancer survey of HLA loss of heterozygosity using a robustly validated NGS-based machine learning algorithm",2021,"","","","",55,"2022-07-13 09:23:14","","10.1158/1538-7445.AM2021-399","","",,,,,0,0.00,0,17,1,"HLA loss of heterozygosity (LOH) is increasingly being recognized as an important immune escape mechanism in response to checkpoint inhibitor therapy. HLA LOH reduces the repertoire of neoantigens displayed on the cell surface of cancer cells, limiting the efficacy of the immune system to detect and eliminate them. Though highly accurate HLA LOH detection algorithms are needed to allow clinical utility, the field lacks robust, allele-specific validation approaches. Moreover, algorithms of unknown sensitivity and specificity have led to significant discrepancies in the estimated occurrence of HLA LOH as an immune escape mechanism across tumor types. To address these challenges, we have developed a machine learning algorithm to detect HLA LOH (DASH - Deletion of Allele-Specific HLAs), established the accuracy of the algorithm with an allele-specific PCR validation strategy, investigated the frequencies of HLA LOH across 14 tumor types in a cohort of over 800 patients and observed allele-specific neoantigen expansion in response to immunotherapy. To build DASH, we profiled 279 patients on the ImmunoID NeXT Platform to create a training dataset. Our novel features, which account for allele-specific differences in exome probe capture and capitalize on our whole exome platform by including information about copy number alterations in the regions flanking the HLA genes, were used to train an XGBoost model. Orthogonal, allele-specific validation was required to accurately assess sensitivity and specificity for clinical utility. Thus, we profiled over 30 paired tumor-normal cell lines on the ImmunoID NeXT Platform® and identified cell lines with HLA LOH. Using in silico mixtures, we found 100% sensitivity and specificity for tumors with at least 36% tumor purity. Next, we designed a digital PCR (dPCR) assay using patient-specific, allele-specific primers that target a single HLA allele while avoiding all other HLA alleles and tested the limit of detection of the assay in the same cell lines. Then, we performed dPCR with patient-specific primers on 20 tumor and normal sample pairs and found 94% sensitivity. After establishing the high sensitivity and specificity of DASH, we profiled over 800 patients spanning 14 tumor types on the ImmunoID NeXT Platform. We found that over 25% of patients in the majority of tumor types had at least one HLA LOH event. Further, we observed that novel neoantigens that arose during checkpoint treatment were significantly more likely to bind to deleted HLA alleles as compared to the remaining HLA alleles in a head and neck carcinoma cohort treated with anti-PD-1 therapy, shedding light on the mechanism of immune escape in response to checkpoint inhibitors. In summary, we introduced an HLA LOH detection method, performed allele-specific validation, exposed widespread HLA across tumor types and observed the mechanism of immune escape in response to immunotherapy. Citation Format: Rachel Marty Pyke, Datta Mellacheruvu, Charles Abbott, Steven Dea, Eric Levy, Simo V. Zhang, Nikita Bedi, A. Dimitrios Colevas, Devayani Bhave, Manju Chinnappa, Gabor Bartha, John Lyle, John West, Michael Snyder, John Sunwoo, Richard Chen, Sean Michael Boyle. Pan-cancer survey of HLA loss of heterozygosity using a robustly validated NGS-based machine learning algorithm [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2021; 2021 Apr 10-15 and May 17-21. Philadelphia (PA): AACR; Cancer Res 2021;81(13_Suppl):Abstract nr 399.","",""
79,"Taesik Na, J. Ko, S. Mukhopadhyay","Cascade Adversarial Machine Learning Regularized with a Unified Embedding",2017,"","","","",56,"2022-07-13 09:23:14","","","","",,,,,79,15.80,26,3,5,"Injecting adversarial examples during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we first show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. We also propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario.","",""
8,"Kookjin Lee, Nathaniel Trask, P. Stinis","Machine learning structure preserving brackets for forecasting irreversible processes",2021,"","","","",57,"2022-07-13 09:23:14","","","","",,,,,8,8.00,3,3,1,"Forecasting of time-series data requires imposition of inductive biases to obtain predictive extrapolation, and recent works have imposed Hamiltonian/Lagrangian form to preserve structure for systems with reversible dynamics. In this work we present a novel parameterization of dissipative brackets from metriplectic dynamical systems appropriate for learning irreversible dynamics with unknown a priori model form. The process learns generalized Casimirs for energy and entropy guaranteed to be conserved and nondecreasing, respectively. Furthermore, for the case of added thermal noise, we guarantee exact preservation of a ﬂuctuation-dissipation theorem, ensuring thermodynamic consistency. We provide benchmarks for dissipative systems demonstrating learned dynamics are more robust and generalize better than either ""black-box"" or penalty-based approaches.","",""
8,"Mustafa Anil Koçak, David Ramirez, E. Erkip, D. Shasha","SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness",2017,"","","","",58,"2022-07-13 09:23:14","","10.1109/TPAMI.2019.2932415","","",,,,,8,1.60,2,4,5,"<italic>SafePredict</italic> is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, <inline-formula><tex-math notation=""LaTeX"">$1-\epsilon$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""kocak-ieq1-2932415.gif""/></alternatives></inline-formula>, by allowing refusals. Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm so that the error rate on non-refused predictions does not exceed <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq2-2932415.gif""/></alternatives></inline-formula>. The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor. When the base predictor happens not to exceed the target error rate <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq3-2932415.gif""/></alternatives></inline-formula>, SafePredict refuses only a finite number of times. When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee. Empirical results show that (i) SafePredict compares favorably with state-of-the-art confidence-based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals. Our software is included in the supplementary material, which can be found on the Computer Society Digital Library at <uri>http://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2932415</uri>.","",""
21,"Christopher Culley, S. Vijayakumar, Guido Zampieri, C. Angione","A mechanism-aware and multiomic machine-learning pipeline characterizes yeast cell growth",2020,"","","","",59,"2022-07-13 09:23:14","","10.1073/pnas.2002959117","","",,,,,21,10.50,5,4,2,"Significance Linking genotype and phenotype is a fundamental problem in biology, key to several biomedical and biotechnological applications. Cell growth is a central phenotypic trait, resulting from interactions between environment, gene regulation, and metabolism, yet its functional bases are still not completely understood. We propose and test a machine-learning approach that integrates large-scale gene expression profiles and mechanistic metabolic models, for characterizing cell growth and understanding its driving mechanisms in Saccharomyces cerevisiae. At its core, a custom-built multimodal learning method merges experimentally generated and model-generated data. We show that our approach can leverage the advantages of both machine learning and metabolic modeling, revealing unknown interactions between biological domains, incorporating mechanistic knowledge, and therefore overcoming black-box limitations of conventional data-driven approaches. Metabolic modeling and machine learning are key components in the emerging next generation of systems and synthetic biology tools, targeting the genotype–phenotype–environment relationship. Rather than being used in isolation, it is becoming clear that their value is maximized when they are combined. However, the potential of integrating these two frameworks for omic data augmentation and integration is largely unexplored. We propose, rigorously assess, and compare machine-learning–based data integration techniques, combining gene expression profiles with computationally generated metabolic flux data to predict yeast cell growth. To this end, we create strain-specific metabolic models for 1,143 Saccharomyces cerevisiae mutants and we test 27 machine-learning methods, incorporating state-of-the-art feature selection and multiview learning approaches. We propose a multiview neural network using fluxomic and transcriptomic data, showing that the former increases the predictive accuracy of the latter and reveals functional patterns that are not directly deducible from gene expression alone. We test the proposed neural network on a further 86 strains generated in a different experiment, therefore verifying its robustness to an additional independent dataset. Finally, we show that introducing mechanistic flux features improves the predictions also for knockout strains whose genes were not modeled in the metabolic reconstruction. Our results thus demonstrate that fusing experimental cues with in silico models, based on known biochemistry, can contribute with disjoint information toward biologically informed and interpretable machine learning. Overall, this study provides tools for understanding and manipulating complex phenotypes, increasing both the prediction accuracy and the extent of discernible mechanistic biological insights.","",""
20,"D. Grana, L. Azevedo, Mingliang Liu","A comparison of deep machine learning and Monte Carlo methods for facies classification from seismic data",2020,"","","","",60,"2022-07-13 09:23:14","","10.1190/geo2019-0405.1","","",,,,,20,10.00,7,3,2,"Among the large variety of mathematical and computational methods for estimating reservoir properties such as facies and petrophysical variables from geophysical data, deep machine-learning algorithms have gained significant popularity for their ability to obtain accurate solutions for geophysical inverse problems in which the physical models are partially unknown. Solutions of classification and inversion problems are generally not unique, and uncertainty quantification studies are required to quantify the uncertainty in the model predictions and determine the precision of the results. Probabilistic methods, such as Monte Carlo approaches, provide a reliable approach for capturing the variability of the set of possible models that match the measured data. Here, we focused on the classification of facies from seismic data and benchmarked the performance of three different algorithms: recurrent neural network, Monte Carlo acceptance/rejection sampling, and Markov chain Monte Carlo. We tested and validated these approaches at the well locations by comparing classification predictions to the reference facies profile. The accuracy of the classification results is defined as the mismatch between the predictions and the log facies profile. Our study found that when the training data set of the neural network is large enough and the prior information about the transition probabilities of the facies in the Monte Carlo approach is not informative, machine-learning methods lead to more accurate solutions; however, the uncertainty of the solution might be underestimated. When some prior knowledge of the facies model is available, for example, from nearby wells, Monte Carlo methods provide solutions with similar accuracy to the neural network and allow a more robust quantification of the uncertainty, of the solution.","",""
1,"Gi-Ung Kang, J. Ibal, Seungjun Lee, M. Jang, Yeong-Jun Park, Min-Chul Kim, Tae-Hyung Park, Min-Sueng Kim, Ryeong-Hui Kim, Jae-Ho Shin","Alteration of the Soil Microbiota in Ginseng Rusty Roots: Application of Machine Learning Algorithm to Explore Potential Biomarkers for Diagnostic and Predictive Analytics.",2021,"","","","",61,"2022-07-13 09:23:14","","10.1021/acs.jafc.1c01314","","",,,,,1,1.00,0,10,1,"Conceptualization to utilize microbial composition as a prediction tool has been widely applied in human cohorts, yet the potential capacity of soil microbiota as a diagnostic tool to predict plant phenotype remains unknown. Here, we collected 130 soil samples which are 54 healthy controls and 76 ginseng rusty roots (GRRs). Alpha diversities including Shannon, Simpson, Chao1, and phylogenetic diversity were significantly decreased in GRR (P < 0.05). Moreover, we identified 30 potential biomarkers. The optimized markers were obtained through fivefold cross-validation on a support vector machine and yielded a robust area under the curve of 0.856. Notably, evaluation of multi-index classification performance including accuracy, F1-score, and Kappa coefficient also showed robust discriminative capability (90.99%, 0.903, and 0.808). Taken together, our results suggest that the disease affects the microbial community and offers the potential ability of soil microbiota to identifying farms at the risk of GRR.","",""
4,"Himaghna Bhattacharjee, Nikolaos Anesiadis, D. Vlachos","Regularized machine learning on molecular graph model explains systematic error in DFT enthalpies",2021,"","","","",62,"2022-07-13 09:23:14","","10.1038/s41598-021-93854-w","","",,,,,4,4.00,1,3,1,"","",""
2,"Siddhant Agarwal, N. Tosi, P. Kessel, S. Padovan, D. Breuer, G. Montavon","Towards constraining Mars' thermal evolution using machine learning",2021,"","","","",63,"2022-07-13 09:23:14","","10.5194/EGUSPHERE-EGU21-4044","","",,,,,2,2.00,0,6,1,"<p>The thermal evolution of terrestrial planets depends strongly on several parameters and initial conditions that are poorly constrained. Often, direct or indirect observables from planetary missions such as elastic lithospheric thickness, crustal thickness and duration of volcanism are inverted to infer the unknown parameter values and initial conditions. The non-uniqueness and non-linearity of this inversion necessitates a probabilistic inversion framework. However, due to the expensive nature of forward dynamic simulations of thermal convection , Markov Chain Monte Carlo methods are rarely used. To address this shortcoming, some studies have recently shown the effectiveness of Mixture Density Networks (MDN) (Bishop 1995) in being able to approximate the posterior probability using only the dataset of simulations run prior to the inversion (Meier et al. 2007, de Wit et al. 2013, K&#228;ufl et al. 2016, Atkins et al. 2016).</p><p>Using MDNs, we systematically isolate the degree to which a parameter can be constrained using different &#8220;present-day&#8221; synthetic observables from 6130 simulations for a Mars-like planet. The dataset &#8211; generated using the mantle convection code GAIA (H&#252;ttig et al. 2013)- is the same as that used by Agarwal et al. (2020) for a surrogate modelling study.</p><p>The loss function used to optimize the MDN (log-likelihood) provides a single robust quantity that can be used to measure how well a parameter can be constrained. We test different numbers and combinations of observables (heat flux at the surface and core-mantle boundary, radial contraction, melt produced, elastic lithospheric thickness, and duration of volcanism) to constrain the following parameters: reference viscosity, activation energy and activation volume of the diffusion creep rheology, an enrichment factor for radiogenic elements in the crust, and initial mantle temperature. If all observables are available, reference viscosity can be constrained to within 32% of its entire range (10<sup>19</sup>&#8722;10<sup>22</sup> Pa s), crustal enrichment factor (1&#8722;50) to within 15%, activation energy (10<sup>5</sup>&#8722;5&#215;10<sup>5</sup> J mol-1 ) to within 80%, and initial mantle temperature (1600&#8722;1800K) to within 39%. The additional availability of the full present-day temperature profile or parts of it as an observable tightens the constraints further. The activation volume (4&#215;10<sup>-6</sup> &#8722;10&#215;10<sup>-6</sup>&#160; m<sup>3</sup> mol<sup>-1</sup>) cannot be constrained and requires research into new observables in space and time, as well as fields other than just temperature. Testing different levels of uncertainty (simulated using Gaussian noise) in the observables, we found that constraints on different parameters loosen at different rates, with initial temperature being the most sensitive. Finally, we present how the marginal MDN proposed by Bishop (1995) can be modified to model the joint probability for all parameters, so that&#160; the inter-parameter correlations and the associated degeneracy can be capture, thereby providing a more comprehensive picture of all the evolution scenarios that fit given observational constraints.</p>","",""
15,"Lin Li, X. Guo, N. Ansari","SmartLoc: Smart Wireless Indoor Localization Empowered by Machine Learning",2020,"","","","",64,"2022-07-13 09:23:14","","10.1109/TIE.2019.2931261","","",,,,,15,7.50,5,3,2,"Recently, machine learning (ML) has been widely adopted for fingerprint-based indoor localization because of its potency in delineating relationships between received signal strength (RSS) information and labels accurately. Existing ML-based indoor localization systems are less robust because they only adopt the output with the highest probability. This affects the final location estimate, hence compromising accuracy due to the severity of RSS fluctuations. Since different ML algorithms (MLAs) yield different performances, it is therefore intuitive to fuse predictions from multiple MLAs to improve the positioning performance in the presence of signal fluctuation. In this article, we propose SmartLoc, a smart wireless indoor localization framework to enhance indoor localization. In the offline phase, multiple MLAs are trained by utilizing an offline database. We further apply probability alignment to guarantee the predicted probabilities of each MLA at the same confidence level. In the online phase, given a testing RSS sample of a user at an unknown location, we extract the labels with probabilities greater than a certain threshold from each MLA to construct the space of candidate labels (SCL). The size of SCL can be adaptively determined by using our proposed dynamic size determination algorithm. Based on the SCL, we propose a probabilistic model to intelligently estimate the user's location by evaluating the label credibility simultaneously. A high label credibility indicates that the frequently occurred label is more likely to be true. Experimental results in a real changing environment verify the superiority of SmartLoc, outperforming the best among comparative methods by 10.8% in 75th percentile accuracy.","",""
0,"M. Panda, A. Azar","Hybrid Multi-Objective Grey Wolf Search Optimizer and Machine Learning Approach for Software Bug Prediction",2021,"","","","",65,"2022-07-13 09:23:14","","10.4018/978-1-7998-5788-4.CH013","","",,,,,0,0.00,0,2,1,"Software bugs (or malfunctions) pose a serious threat to software developers with many known and unknown bugs that may be vulnerable to computer systems, demanding new methods, analysis, and techniques for efficient bug detection and repair of new unseen programs at a later stage. This chapter uses evolutionary grey wolf (GW) search optimization as a feature selection technique to improve classifier efficiency. It is also envisaged that software error detection would consider the nature of the error when repairing it for remedial action instead of simply finding it either faulty or non-defective. To address this problem, the authors use bug severity multi-class classification to build an efficient and robust prediction model using multilayer perceptron (MLP), logistic regression (LR), and random forest (RF) for bug severity classification. Both tests are performed on two software error datasets, namely Ant 1.7 and Tomcat.","",""
15,"Yeounoh Chung, P. Haas, E. Upfal, Tim Kraska","Unknown Examples & Machine Learning Model Generalization",2018,"","","","",66,"2022-07-13 09:23:14","","","","",,,,,15,3.75,4,4,4,"Over the past decades, researchers and ML practitioners have come up with better and better ways to build, understand and improve the quality of ML models, but mostly under the key assumption that the training data is distributed identically to the testing data. In many real-world applications, however, some potential training examples are unknown to the modeler, due to sample selection bias or, more generally, covariate shift, i.e., a distribution shift between the training and deployment stage. The resulting discrepancy between training and testing distributions leads to poor generalization performance of the ML model and hence biased predictions. We provide novel algorithms that estimate the number and properties of these unknown training examples---unknown unknowns. This information can then be used to correct the training set, prior to seeing any test data. The key idea is to combine species-estimation techniques with data-driven methods for estimating the feature values for the unknown unknowns. Experiments on a variety of ML models and datasets indicate that taking the unknown examples into account can yield a more robust ML model that generalizes better.","",""
12,"Lixiang Hong, Jinjian Lin, Shuya Li, Fangping Wan, Hui Yang, Tao Jiang, Dan Zhao, Jianyang Zeng","A novel machine learning framework for automated biomedical relation extraction from large-scale literature repositories",2020,"","","","",67,"2022-07-13 09:23:14","","10.1038/s42256-020-0189-y","","",,,,,12,6.00,2,8,2,"","",""
9,"Ruben F. Kranenburg, J. Verduin, Y. Weesepoel, M. Alewijn, Marcel Heerschop, G. Koomen, P. Keizers, Frank Bakker, Fionn Wallace, Annette van Esch, Annemieke Hulsbergen, A. V. van Asten","Rapid and robust on‐scene detection of cocaine in street samples using a handheld near‐infrared spectrometer and machine learning algorithms",2020,"","","","",68,"2022-07-13 09:23:14","","10.1002/dta.2895","","",,,,,9,4.50,1,12,2,"Abstract On‐scene drug detection is an increasingly significant challenge due to the fast‐changing drug market as well as the risk of exposure to potent drug substances. Conventional colorimetric cocaine tests involve handling of the unknown material and are prone to false‐positive reactions on common pharmaceuticals used as cutting agents. This study demonstrates the novel application of 740–1070 nm small‐wavelength‐range near‐infrared (NIR) spectroscopy to confidently detect cocaine in case samples. Multistage machine learning algorithms are used to exploit the limited spectral features and predict not only the presence of cocaine but also the concentration and sample composition. A model based on more than 10,000 spectra from case samples yielded 97% true‐positive and 98% true‐negative results. The practical applicability is shown in more than 100 case samples not included in the model design. One of the most exciting aspects of this on‐scene approach is that the model can almost instantly adapt to changes in the illicit‐drug market by updating metadata with results from subsequent confirmatory laboratory analyses. These results demonstrate that advanced machine learning strategies applied on limited‐range NIR spectra from economic handheld sensors can be a valuable procedure for rapid on‐site detection of illicit substances by investigating officers. In addition to forensics, this interesting approach could be beneficial for screening and classification applications in the pharmaceutical, food‐safety, and environmental domains.","",""
4,"Chang Song, Zuoguan Wang, H. Li","Feedback Learning for Improving the Robustness of Neural Networks",2019,"","","","",69,"2022-07-13 09:23:14","","10.1109/ICMLA.2019.00124","","",,,,,4,1.33,1,3,3,"Recent research studies revealed that neural networks are vulnerable to adversarial attacks. State-of-the-art defensive techniques add various adversarial examples in training to improve models' adversarial robustness. However, these methods are not universal and can't defend unknown or non-adversarial evasion attacks. In this paper, we analyze the model robustness in the decision space. A feedback learning method is then proposed, to understand how well a model learns and to facilitate the retraining process of remedying the defects. The evaluations according to a set of distance-based criteria show that our method can significantly improve models' accuracy and robustness against different types of evasion attacks. Moreover, we observe the existence of inter-class inequality and propose to compensate for it by changing the proportions of examples generated in different classes.","",""
6,"Marcos Fabietti, M. Mahmud, Ahmad Lotfi","Machine Learning in Analysing Invasively Recorded Neuronal Signals: Available Open Access Data Sources",2020,"","","","",70,"2022-07-13 09:23:14","","10.1007/978-3-030-59277-6_14","","",,,,,6,3.00,2,3,2,"","",""
189,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Auto-sklearn: Efficient and Robust Automated Machine Learning",2019,"","","","",71,"2022-07-13 09:23:14","","10.1007/978-3-030-05318-5_6","","",,,,,189,63.00,32,6,3,"","",""
4,"R. Pattnaik, K. Sharma, K. Alabarta, D. Altamirano, M. Chakraborty, Aniruddha Kembhavi, M. Méndez, J. K. Orwat-Kapola","A Machine Learning Approach For Classifying Low-mass X-ray Binaries Based On Their Compact Object Nature",2020,"","","","",72,"2022-07-13 09:23:14","","10.1093/mnras/staa3899","","",,,,,4,2.00,1,8,2,"  Low Mass X-ray binaries (LMXBs) are binary systems where one of the components is either a black hole or a neutron star and the other is a less massive star. It is challenging to unambiguously determine whether a LMXB hosts a black hole or a neutron star. In the last few decades, multiple observational works have tried, with different levels of success, to address this problem. In this paper, we explore the use of machine learning to tackle this observational challenge. We train a random forest classifier to identify the type of compact object using the energy spectrum in the energy range 5-25 keV obtained from the Rossi X-ray Timing Explorer archive. We report an average accuracy of 87±13% in classifying the spectra of LMXB sources. We further use the trained model for predicting the classes for LMXB systems with unknown or ambiguous classification. With the ever-increasing volume of astronomical data in the X-ray domain from present and upcoming missions (e.g., SWIFT, XMM-Newton, XARM, ATHENA, NICER), such methods can be extremely useful for faster and robust classification of X-ray sources and can also be deployed as part of the data reduction pipeline.","",""
2,"Shahnaz TayebiHaghighi, Insoo Koo","Fault Diagnosis of Rotating Machine Using an Indirect Observer and Machine Learning",2020,"","","","",73,"2022-07-13 09:23:14","","10.1109/ICTC49870.2020.9289590","","",,,,,2,1.00,1,2,2,"Bearing is one of the important mechanical components to reduce friction in rotating machines. Early fault diagnosis in bearings is an important challenge to the prevention of full failure and avoiding disorder of the machine. In this paper, an indirect observer and machine learning technique are adopted for fault identification in bearing. To develop an indirect observer, in the first step, the autoregressive with uncertainty modeling technique is proposed to modeling the RMS (indirect) normal signal of bearing. After that, the robust (sliding fault detection) proportional multi integral with autoregressive external input modeling (ARPMI) observer was used to solve the unknown signal estimation in bearing. Besides, the support vector machine (SVM) technique for fault classification is proposed. The effectiveness of the proposed scheme is validated using Case Western Reverse University (CWRU) dataset. Experimental results show that, the proposed scheme improves the average performance for various rotational speed fault identification by about 10.5% and 13.5% compared with the proportional multi integral with autoregressive external input modeling (APMI) observer and proportional-integral with autoregressive external input modeling (API) observer, respectively.","",""
5,"S. Yaqubi, M. Homaeinezhad","Solving predictive control problem of fast‐varying multivariable systems by incorporating unknown active dynamics generated by real‐time adaptive learning machine",2020,"","","","",74,"2022-07-13 09:23:14","","10.1111/exsy.12567","","",,,,,5,2.50,3,2,2,"Not only adaptive predictive control of switched systems is a computationally intensive procedure, it also involves various challenges in addressing the problems of robust stabilization and precise tracking. This study proposes new strategies to deal with the aforementioned issues (namely safe and precise control alongside with reduction of computational burden). The first contribution of this work is reduction of conservatism for described class of systems. Control of switched systems with undetectable switching signals is often conducted in worst case switching configuration to ensure robustness, which potentially results in conservative design. The issue of conservativeness is intensified in multi input‐multi output (MIMO) dynamical systems due to increased dimensions. However, attaining a robust control scheme for all switching configurations while ensuring precise response is inherently paradoxical. To overcome this issue, this study proposes a new dual‐mode algorithm where control modes corresponding to safety and precision are activated at appropriate stages of system response. This is conducted based on incorporation of an adaptive fuzzy‐wavelet neural network identification scheme in predictive control of MIMO switched systems. However, as convergence of the adaptive algorithm to actual system is attained after a finite period of time, a safe‐mode control algorithm is proposed to maintain quality of transient response in convergence period. In other words, the proposed algorithm operates in safe and precise control modes to ensure robust stability in the convergence period and non‐conservative design in steady‐state. Second major contribution of the work is reduction of calculation burden based on incorporation of a suboptimal control algorithm. To this end, we propose a predictive control scheme based on a suboptimal gradient‐descent based controller, calculating feasible stabilizing inputs instead of optimal inputs. Effects of dynamical variations are incorporated in the model predictive control framework for increased compatibility with high‐speed switching dynamics. Then, based on incorporation of dual‐mode algorithm, precise steady‐state performance is attained while preventing notable perturbations in dynamical discontinuities at switching.","",""
1,"Sicong Ma, Pei-Lin Kang, C. Shang, Zhipan Liu","Chapter 19. Machine Learning for Heterogeneous Catalysis: Global Neural Network Potential from Construction to Applications",2020,"","","","",75,"2022-07-13 09:23:14","","10.1039/9781839160233-00488","","",,,,,1,0.50,0,4,2,"While the potential energy surface (PES) determines the physicochemical properties of matter, chemical system surfaces are often too complex to solve even with modern computing facilities. Heterogeneous catalysis, being widely utilized in industry, calls for new techniques and methods to resolve the active site structure and reaction intermediates at the atomic scale. In this chapter, we provide an overview of recent theoretical progress on large-scale atomistic simulation via the machine learning global neural network (G-NN) potential developed by our research group in recent years, focusing on methodology and representative applications in heterogeneous catalysis. The combination of global optimization and machine learning provides a convenient and automated way to generate the transferable and robust G-NN potential, which can be utilized to reveal new chemistry from unknown regions of the PES at an affordable computational cost. The predictive power of the G-NN potential is demonstrated in several examples, where the method is applied to explore the material crystal phases and the structure of supported catalysts, to follow surface structure evolution under high-pressure hydrogen and to determine the ternary oxide phase diagram. Limitations and future directions of the G-NN potential method are also discussed.","",""
1,"Iman Al Selaiti, Carlos Mata, L. Saputelli, D. Badmaev, Yara Alatrach, Erismar Rubio, R. Mohan, Daniel Quijada","Robust Data Driven Well Performance Optimization Assisted by Machine Learning Techniques for Natural Flowing and Gas-Lift Wells in Abu Dhabi",2020,"","","","",76,"2022-07-13 09:23:14","","10.2118/201696-ms","","",,,,,1,0.50,0,8,2,"  Asset management success is accomplished when the integrated production system is operating close to its intended potential. Continuous awareness of wells and facility conditions are key factor in the realization of designed capacity. In contrast, unknown status and conditions can severely limit production capacity.  The rise of instrumentation technologies over the last four decades have created new opportunities to understand well and reservoir behavior. However, despite of being proved as a cost-effective surveillance initiative, remote monitoring is still not adopted in over 60% of oil and gas fields around the world. Understanding the value of data through machine learning techniques is the basis for establishing a robust surveillance strategy.  The objective of this paper is to develop a data-driven approach, enabled by Artificial Intelligence (AI) methodologies including machine learning (ML), to find optimal operating envelope for gas-lift wells. The process involves building ML models for generating instantaneous predictions of multiphase flow rates and other quantities of interest, such as GOR, WCT, using real-time sensor data at the surface, historical performance, and sporadic test data.  Additionally, forecasting models were developed for generating short-term (30 days) forecast of cumulative oil, water, gas, and liquid production, multiphase flow rates, WCT, GOR, and reservoir pressure. Using time-series forecasting models, a sensitivity analysis was performed to generate short-term well response for a selected number of combinations of choke settings, and gas injection rates. Sensitivity analysis provides 2D maps of well response highlight an operating envelope, which are proposed to be combined with physical and operational constraints to arrive at optimal operating conditions, which may effortlessly add 2.5% net profit from optimum gas-lift alocation.  The results of this work show encouraging results, and demonstrate value that AI-enabled methodologies can provide in instrumented wells by enabling automated workflows for virtual metering, production allocation, short-term production forecasting, and deriving optimal operating conditions. The developed AI methodology has tremendous potential of integration in an end-to-end workflow of autonomous well control by utilizing available data to produce easy to update ML models, with little to no human intervention.","",""
1,"N. Thompson, J. Steck, E. Behrman","A non-algorithmic approach to “programming” quantum computers via machine learning",2020,"","","","",77,"2022-07-13 09:23:14","","10.1109/QCE49297.2020.00019","","",,,,,1,0.50,0,3,2,"Major obstacles remain to the implementation of macroscopic quantum computing: hardware problems of noise, decoherence, and scaling; software problems of error correction; and, most important, algorithm construction. Finding truly quantum algorithms is quite difficult, and many of these genuine quantum algorithms, like Shor's prime factoring or phase estimation, require extremely long circuit depth for any practical application, which necessitates error correction. In contrast, we show that machine learning can be used as a systematic method to construct algorithms, that is, to non-algorithmically “program” quantum computers. Quantum machine learning enables us to perform computations without breaking down an algorithm into its gate “building blocks”, eliminating that difficult step and potentially increasing efficiency by simplifying and reducing unnecessary complexity. In addition, our non-algorithmic machine learning approach is robust to both noise and to decoherence, which is ideal for running on inherently noisy NISQ devices which are limited in the number of qubits available for error correction. We demonstrate this using a fundamentally nonclassical calculation: experimentally estimating the entanglement of an unknown quantum state. Results from this have been successfully ported to the IBM hardware and trained using a hybrid reinforcement learning method.","",""
0,"Kelly Hirschbeck Smalenberger","Probabilistic Numerical Integration with Applications in Machine Learning",2020,"","","","",78,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,1,2,"We consider the approximation of unknown or intractable integrals using quadrature when the evaluation of the integrand is considered costly. This is a central problem in machine learning, including model averaging, (hyper-)parameter marginalization, and computing posterior predictive distributions. Recently Batch Bayesian Quadrature (BBQ) has combined the probabilistic integration techniques of Bayesian Quadrature with the parallelization techniques of Batch Bayesian Optimization, resulting in improved performance compared to Monte Carlo techniques, especially when parallelization is increased. While the selection of batches in BBQ mitigates costs of individual point selection, every point within every batch is nevertheless chosen serially, impeding the full potential of batch selection. We resolve this shortcoming. We developed a novel BBQ method which updates points within a batch without the costs of non-serial point selection. To implement this, we devise a dynamic domain decomposition. Combining these efficiently reduces uncertainty, lowers error estimates of the integrand, and results in more numerically robust integral estimates. Furthermore, we close an open question about the cessation criteria, which we establish and support using numerical methods. We present our findings within the context of the history of quadrature, show how our novel methods significantly improve the literature, and provide possibilities for future research. Department of Mathematics, UNC Charlotte, Charlotte, NC 28223","",""
0,"Kriti Srivastava, N. Shekokar","Design of Machine Learning and Rule Based Access Control System with Respect to Adaptability and Genuineness of the Requester",2020,"","","","",79,"2022-07-13 09:23:14","","10.4108/EAI.24-9-2020.166359","","",,,,,0,0.00,0,2,2,"INTRODUCTION: Access control system (ACS) plays a major role in data security. It becomes more challenging for the system to provide accurate ACS, if data is huge and data requesters are not fixed. This is very predominant in the era of big data where new data are adding to the system very frequently. The main issue here is to justify adaptability in ACS.    OBJECTIVE: The objective of this research is to have a comparative analysis of machine learning based access control methods with Rule based access control methods. Propose the most suitable method in detail.    METHODS: Role based access control methods are highly robust and works effectively under known scenarios. We need additional methods to handle unknown scenarios. A decision-making method is used to identify the certainty of the rules and Mamdani fuzzy model is used to evaluate the situation based on current environmental factors. For machine learning based access control method Random Forest is used.    RESULTS: Limitations of machine learning methods are discussed with respect to imbalanced data and bias in the algorithm. The proof of concept for rule-based access control method is tested for all the three modules involved in the framework. Certainty of the rules were accessed with the help of domain experts and accuracy of fuzzy rules were evaluated. Under critical conditions our framework was found to be accurate.    CONCLUSIONS: Machine learning systems are not suitable for access control if they suffer with imbalance data problem. Rule based system are consistent and highly adaptable to unknown situations. Rule based systems have evaluated the genuineness of the requester based on sensitivity of information, time, location, previous history and emergency parameters.","",""
0,"Ulrich J. Frey, M. Klein","Modelling Complex Investment Decisions for Renewables with Machine Learning",2018,"","","","",80,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,2,4,"The factors that drive the decision-making process behind private investments in renewables, e.g. solar  on roof tops are still somewhat unknown. We aim to develop a more comprehensive model with  potential factors from various backgrounds including social, economic and geographic drivers. We use  an existing data set of real investments in PV in Germany from 1991 to 2014. These 1.4 million  investment decisions are merged with other data sets with information on social, employment,  rural/urban characteristics, election results and other potential drivers. The variable of interest is the  installed capacity per county.  Since the interactions between these variables may be complex, non-linear and are basically not  known, we decided to use machine learning statistical methods. In order to increase the robustness of  results and to find out which algorithm performs best in terms of model quality, we used Generalized  Linear Models (GLM), random forests, gradient boosting and deep neural networks.  Model predictions are rather accurate: at the county level the adjusted R2 is 0.65 for GLM, 0.66 for  Random Forests, 0.68 for deep neural nets and 0.68 for gradient boosting. Agreement between  methods is only decent with deep neural nets calculating a much more balanced model in contrast to  gradient boosting. Concerning factor importance for investment decisions, the best two models  confirm that the amount of solar insolation received, the absolute number of population per county,  and the density and the distinction between urban and rural areas are most relevant","",""
12,"Samuel Ackerman, E. Farchi, O. Raz, Marcel Zalmanovici, Parijat Dube","Detection of data drift and outliers affecting machine learning model performance over time",2020,"","","","",81,"2022-07-13 09:23:14","","","","",,,,,12,6.00,2,5,2,"A trained ML model is deployed on another `test' dataset where target feature values (labels) are unknown. Drift is distribution change between the training and deployment data, which is concerning if model performance changes. For a cat/dog image classifier, for instance, drift during deployment could be rabbit images (new class) or cat/dog images with changed characteristics (change in distribution). We wish to detect these changes but can't measure accuracy without deployment data labels. We instead detect drift indirectly by nonparametrically testing the distribution of model prediction confidence for changes. This generalizes our method and sidesteps domain-specific feature representation.  We address important statistical issues, particularly Type-1 error control in sequential testing, using Change Point Models (CPMs; see Adams and Ross 2012). We also use nonparametric outlier methods to show the user suspicious observations for model diagnosis, since the before/after change confidence distributions overlap significantly. In experiments to demonstrate robustness, we train on a subset of MNIST digit classes, then insert drift (e.g., unseen digit class) in deployment data in various settings (gradual/sudden changes in the drift proportion). A novel loss function is introduced to compare the performance (detection delay, Type-1 and 2 errors) of a drift detector under different levels of drift class contamination.","",""
50,"A. Romagnoni, S. Jégou, K. Van Steen, G. Wainrib, J. Hugot, L. Peyrin-Biroulet, M. Chamaillard, J. Colombel, M. Cottone, M. D’Amato, R. D'Incà, J. Halfvarson, P. Henderson, A. Karban, N. Kennedy, M. Khan, M. Lémann, A. Levine, D. Massey, M. Milla, S. M. Ng, I. Oikonomou, H. Peeters, D. Proctor, J. Rahier, P. Rutgeerts, F. Seibold, L. Stronati, K. Taylor, L. Törkvist, Kullak Ublick, J. V. van Limbergen, A. van Gossum, M. Vatn, Hu Zhang, Wei Zhang, J. Andrews, P. Bampton, M. Barclay, T. Florin, R. Gearry, K. Krishnaprasad, I. Lawrance, G. Mahy, G. Montgomery, G. Radford-Smith, R. Roberts, L. Simms, K. Hanigan, A. Croft","Comparative performances of machine learning methods for classifying Crohn Disease patients using genome-wide genotyping data",2019,"","","","",82,"2022-07-13 09:23:14","","10.1038/s41598-019-46649-z","","",,,,,50,16.67,5,50,3,"","",""
2,"Jacques Janse Van Vuuren, Liqiong Tang, I. Al-Bahadly, K. Arif","A 3-Stage Machine Learning-Based Novel Object Grasping Methodology",2020,"","","","",83,"2022-07-13 09:23:14","","10.1109/ACCESS.2020.2987341","","",,,,,2,1.00,1,4,2,"The automatic grasping of objects previously unseen by a robotic system is a difficult task—of which there is currently no robust solution. The research presented in this article improves upon previous works that employ depth data and learning techniques to generate and select from a pool of hypothesised grasps by focusing on the pruning and selection process. In this work, a vision-based, sampling methodology that generates candidate grasps through a convolutional neural network is proposed. Each candidate grasp is assessed using scores derived from the candidate itself and other related input modalities—such as the centre of gravity of the object. The final selection is determined by a learning algorithm. To overcome human bias, objective measures of grasp performance are established that comprehensively measure the error introduced by the grasp trial itself. The proposed metrics are empirically demonstrated to quantify grasp quality, offer useful criteria for network training and provide better descriptive power than traditional measures of grasp outcome. Experimentation showed that the proposed methodology can generate a meaningful, final grasp within 1.3 seconds. Trials quantitatively demonstrate a small-object-in-isolation performance of 99%. For unknown objects, this equates to a 10% improvement relative to other similar methodologies. Testing also showed that grasp performance was improved by 5% when implementing the proposed metrics—compared to the baseline.","",""
7,"H. Singh, Y. Seol, E. Myshakin","Prediction of gas hydrate saturation using machine learning and optimal set of well-logs",2020,"","","","",84,"2022-07-13 09:23:14","","10.1007/s10596-020-10004-3","","",,,,,7,3.50,2,3,2,"","",""
6,"P. Nicholas, Gabriella Rossi, Ella Williams, Michael Bennett, T. Schork","Integrating real-time multi-resolution scanning and machine learning for Conformal Robotic 3D Printing in Architecture",2020,"","","","",85,"2022-07-13 09:23:14","","10.1177/1478077120948203","","",,,,,6,3.00,1,5,2,"Robotic 3D printing applications are rapidly growing in architecture, where they enable the introduction of new materials and bespoke geometries. However, current approaches remain limited to printing on top of a flat build bed. This limits robotic 3D printing’s impact as a sustainable technology: opportunities to customize or enhance existing elements, or to utilize complex material behaviour are missed. This paper addresses the potentials of conformal 3D printing and presents a novel and robust workflow for printing onto unknown and arbitrarily shaped 3D substrates. The workflow combines dual-resolution Robotic Scanning, Neural Network prediction and printing of PETG plastic. This integrated approach offers the advantage of responding directly to unknown geometries through automated performance design customization. This paper firstly contextualizes the work within the current state of the art of conformal printing. We then describe our methodology and the design experiment we have used to test it. We lastly describe the key findings, potentials and limitations of the work, as well as the next steps in this research.","",""
0,"A. Chakrabarty, K. Berntorp, S. D. Cairano","Learning-based Parameter-Adaptive Reference Governors /Author=Chakrabarty, Ankush; Berntorp, Karl; Di Cairano, Stefano /CreationDate=July 3, 2020 /Subject=Control, Machine Learning",2020,"","","","",86,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,3,2,"Reference governors (RGs) provide an effective method for ensuring safety via constraint enforcement in closedloop control systems. When the parameters of the underlying systems are unknown, but constant or slowly-varying, robust formulations of RGs that consider only the worst-case effect may be overly conservative and exhibit poor performance. This paper proposes a parameter-adaptive reference governor (PARG) architecture that is capable of generating safe trajectories in spite of parameter uncertainties without being as conservative as robust RGs. The proposed approach leverages on-line data to inform algorithms for robust parameter estimation. Subsequently, confidence bounds around parameter estimates are fed to supervised machine learners for approximating robust constraint admissible sets leveraged by the PARG. While initially, due to the absence of on-line data, the PARG may be as conservative as a robust RG, as more data is gathered and the confidence bounds become tighter, such conservativeness reduces, as demonstrated in a simulation example. American Control Conference (ACC) This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved. Copyright c © Mitsubishi Electric Research Laboratories, Inc., 2020 201 Broadway, Cambridge, Massachusetts 02139 Learning-based Parameter-Adaptive Reference Governors Ankush Chakrabarty†, Karl Berntorp, Stefano Di Cairano Abstract—Reference governors (RGs) provide an effective method for ensuring safety via constraint enforcement in closedloop control systems. When the parameters of the underlying systems are unknown, but constant or slowly-varying, robust formulations of RGs that consider only the worst-case effect may be overly conservative and exhibit poor performance. This paper proposes a parameter-adaptive reference governor (PARG) architecture that is capable of generating safe trajectories in spite of parameter uncertainties without being as conservative as robust RGs. The proposed approach leverages on-line data to inform algorithms for robust parameter estimation. Subsequently, confidence bounds around parameter estimates are fed to supervised machine learners for approximating robust constraint admissible sets leveraged by the PARG. While initially, due to the absence of on-line data, the PARG may be as conservative as a robust RG, as more data is gathered and the confidence bounds become tighter, such conservativeness reduces, as demonstrated in a simulation example.","",""
5,"Yakun Wang, Liangsheng Shi, Lin Lin, M. Holzman, Facundo Carmona, Qiuru Zhang","A robust data‐worth analysis framework for soil moisture flow by hybridizing sequential data assimilation and machine learning",2020,"","","","",87,"2022-07-13 09:23:14","","10.1002/vzj2.20026","","",,,,,5,2.50,1,6,2,"As the collection of soil moisture data is often costly, it is essential to implement data‐worth analysis in advance to obtain a cost‐effective data collection scheme. In previous data‐worth analysis, the model structural error is often neglected. In this paper, we propose a robust data‐worth analysis framework based on a hybrid data assimilation method. By constructing Gaussian process (GP) error model, this study attempts to alleviate biased data‐worth assessments caused by unknown model structural errors, and to excavate complementary values of multisource data without resorting to multiple governing equations. The results demonstrated that this proposed framework effectively identified and compensated for complex model structural errors. By training prior data, more accurate potential observations were obtained and data‐worth estimation accuracy was improved. The scenario diversity played a crucial role in establishing an effective GP training system. The integration of soil temperature into GP training unraveled new information and improved the data‐worth estimation. Instead of traditional evapotranspiration calculations, the direct inclusion of easy‐to‐obtain meteorological data into GP training yielded better data‐worth assessment.","",""
2,"S. Siltanen, Takanori Ide","Electrical Impedance Tomography, Enclosure Method and Machine Learning",2020,"","","","",88,"2022-07-13 09:23:14","","10.1109/MLSP49062.2020.9231717","","",,,,,2,1.00,1,2,2,"Electrical impedance tomography (EIT) is a non-destructive imaging method, where a physical body is probed with electric measurements at the boundary, and information about the internal conductivity is extracted from the data. The enclosure method of Ikehata [J. Inv. III-Posed Prob. 8(2000)] recovers the convex hull of an inclusion of unknown conductivity embedded in known background conductivity. Practical implementations of the enclosure method are based on least-squares (LS) fitting of lines to noise-robust values of the so-called indicator function. It is shown how a convolutional neural network instead of LS fitting improves the accuracy of the enclosure method significantly while retaining interpretability.","",""
28,"Jiuwen Cao, K. Zhang, Hongwei Yong, Xiaoping Lai, Badong Chen, Zhiping Lin","Extreme Learning Machine With Affine Transformation Inputs in an Activation Function",2019,"","","","",89,"2022-07-13 09:23:14","","10.1109/TNNLS.2018.2877468","","",,,,,28,9.33,5,6,3,"The extreme learning machine (ELM) has attracted much attention over the past decade due to its fast learning speed and convincing generalization performance. However, there still remains a practical issue to be approached when applying the ELM: the randomly generated hidden node parameters without tuning can lead to the hidden node outputs being nonuniformly distributed, thus giving rise to poor generalization performance. To address this deficiency, a novel activation function with an affine transformation (AT) on its input is introduced into the ELM, which leads to an improved ELM algorithm that is referred to as an AT-ELM in this paper. The scaling and translation parameters of the AT activation function are computed based on the maximum entropy principle in such a way that the hidden layer outputs approximately obey a uniform distribution. Application of the AT-ELM algorithm in nonlinear function regression shows its robustness to the range scaling of the network inputs. Experiments on nonlinear function regression, real-world data set classification, and benchmark image recognition demonstrate better performance for the AT-ELM compared with the original ELM, the regularized ELM, and the kernel ELM. Recognition results on benchmark image data sets also reveal that the AT-ELM outperforms several other state-of-the-art algorithms in general.","",""
0,"Shelby Heinecke","Resilient Structures and Robust Machine Learning Algorithms",2020,"","","","",90,"2022-07-13 09:23:14","","10.25417/UIC.13474959.V1","","",,,,,0,0.00,0,1,2,"Networks and learning algorithms are key themes in artificial intelligence (AI). Networks can be used to model the connections of a vast community of internet-of-things (IoT) devices, the internet, and distributed databases, among other important computing contexts for AI. Learning algorithms, which generate trained models, can be strategically designed to learn from a variety of data-rich networks, such as sensor networks, device networks, or even networks of humans working on crowdsourcing tasks. In real-world settings, noise generated from sources such as device inaccuracy or human error are unavoidable. Likewise, attacks in real-world networks, such as the injection of malware, are inevitable due to their ever-evolving sophistication. In this thesis, we study the intrusions of noise and malicious attacks on networks and learning from networks. We devise structural and algorithmic solutions for mitigating the effects of these unwanted intrusions. We first we consider viral propagation under the independent cascade model of infection spread on half-regular bipartite networks and characterize the most resilient structures. Then, we study learning and generalizing from a network of crowd workers, where crowd workers provide erroneous labels to unlabelled data at fixed, unknown error rates. In this setting, we develop a three-step probably approximately correct (PAC) algorithm that incorporates majority voting, pure-exploration bandits, and noisy-PAC learning and demonstrate our algorithm’s improvement over baseline approaches. Finally, we study learning from a network of participants, each with their own distribution on the unlabelled data in the presence of noise. We develop collaborative PAC algorithms robust to classification noise and prove sample complexity bounds. We also study the communication complexity of collaborative PAC learning, with and without classification noise, and develop communication efficient algorithms in both settings.","",""
36,"King Chung Ho, W. Speier, Haoyue Zhang, F. Scalzo, S. El-Saden, C. Arnold","A Machine Learning Approach for Classifying Ischemic Stroke Onset Time From Imaging",2019,"","","","",91,"2022-07-13 09:23:14","","10.1109/TMI.2019.2901445","","",,,,,36,12.00,6,6,3,"Current clinical practice relies on clinical history to determine the time since stroke (TSS) onset. Imaging-based determination of acute stroke onset time could provide critical information to clinicians in deciding stroke treatment options, such as thrombolysis. The patients with unknown or unwitnessed TSS are usually excluded from thrombolysis, even if their symptoms began within the therapeutic window. In this paper, we demonstrate a machine learning approach for TSS classification using routinely acquired imaging sequences. We develop imaging features from the magnetic resonance (MR) images and train machine learning models to classify the TSS. We also propose a deep-learning model to extract hidden representations for the MR perfusion-weighted images and demonstrate classification improvement by incorporating these additional deep features. The cross-validation results show that our best classifier achieved an area under the curve of 0.765, with a sensitivity of 0.788 and a negative predictive value of 0.609, outperforming existing methods. We show that the features generated by our deep-learning algorithm correlate with the MR imaging features, and validate the robustness of the model on imaging parameter variations (e.g., year of imaging). This paper advances magnetic resonance imaging analysis one-step-closer to an operational decision support tool for stroke treatment guidance.","",""
1,"Steve Göring, Rakesh Rao Ramachandra Rao, A. Raake","Prenc — Predict Number of Video Encoding Passes with Machine Learning",2020,"","","","",92,"2022-07-13 09:23:14","","10.1109/QoMEX48832.2020.9123108","","",,,,,1,0.50,0,3,2,"Video streaming providers spend huge amounts of processing time to get a quality-optimized encoding. While the quality-related impact may be known to the service provider, the impact on video quality is hard to assess, when no reference is available. Here, bitstream-based video quality models may be applicable, delivering estimates that include encoding-specific settings. Such models typically use several input parameters, e.g. bitrate, framerate, resolution, video codec, QP values and more. However, for a given bitstream, to determine which encoding parameters were selected, e.g., the number of encoding passes, is not a trivial task. This leads to our following research question: Given an unknown video bitstream, which encoding settings have been used? To tackle this reverse engineering problem, we introduce a system called prenc. Besides the use in video-quality estimation, such algorithms may also be used in other applications such as video forensics. We prove our concept by applying prenc to distinguish between one- and two-pass encoding. Starting from modeling the problem as a classification task, estimating bitstream-based features, we further describe a machine learning approach with feature selection to automatically predict the number of encoding passes for a given video bitstream. Our large-scale evaluation consists of 16 short movie type 4K videos that were segmented and encoded with different settings (resolutions, codecs, bitrates), so that we in total analyzed 131.976 DASH video segments. We further show that our system is robust, based on a 50% train and 50% validation approach without source video overlapping, where we get a classification performance of 65% F1 score. Moreover, we also describe the used bitstream-based features in detail, the feature pooling strategy and include other machine learning algorithms in our evaluation.","",""
0,"Farzin Piltan, Alexander E. Prosvirin, Jong-Myon Kim","Robot manipulator active fault-tolerant control using a machine learning-based automated robust hybrid observer",2020,"","","","",93,"2022-07-13 09:23:14","","10.3233/JIFS-189109","","",,,,,0,0.00,0,3,2,"Robotic manipulators represent a class of nonlinear and multiple-degrees-of-freedom robots that have pronounced coupling effects and can be used in various applications. The challenge of understanding complexity in a system’s dynamic behavior, coupling effects, and sources of uncertainty presents substantial challenges regarding fault estimation, detection, identification, and tolerant-control (FEDIT) in a robot manipulator. Thus, a proposed active fault-tolerant control algorithm, based on an adaptive modern sliding mode observer, is represented. Due to the effect of the system’s complexities and uncertainties for fault estimation, detection, and identification (FEDI), a sliding mode observer (SMO) is proposed. To address the sliding mode observer drawbacks for FEDI such as high-frequency oscillation (chattering) and fault estimation accuracy, the modern (T-S fuzzy higher order) technique is represented. In addition, the adaptive technique is applied to the modern sliding mode observer (MSMO) to self-tune the coefficients of the fault estimation observer to increase the reliability and robustness of decision-making for diagnosis of the fault. Next, the residual delivered by the adaptive MSMO (AMSMO) is split into windows, and each window is characterized by a numerical parameter. Finally, the machine learning technique known as a decision tree adaptively derives the threshold values that are used for problems of fault detection and fault identification in this work. Due to control of the effective fault, a surface automated new sliding mode controller (SANSMC) is presented in this work. To address the challenge of chattering and unlimited uncertainties (faults), the AMSMO is applied to the sliding mode controller (SMC). In addition, the surface-automated technique is used to fine-tune the surface coefficient to reduce the chattering and faults in the robot manipulator. The results show that the machine learning-based automated robust hybrid observer significantly improves the robustness, reliability, and accuracy of FEDIT in unknown conditions.","",""
28,"Yan Zhou, Murat Kantarcioglu, B. Xi","A survey of game theoretic approach for adversarial machine learning",2019,"","","","",94,"2022-07-13 09:23:14","","10.1002/widm.1259","","",,,,,28,9.33,9,3,3,"The field of machine learning is progressing at a faster pace than ever before. Many organizations leverage machine learning tools to extract useful information from a massive amount of data. In particular, machine learning finds its application in cybersecurity that begins to enter the age of automation. However, machine learning applications in cybersecurity face unique challenges other domains rarely do—attacks from active adversaries. Problems in areas such as intrusion detection, banking fraud detection, spam filtering, and malware detection have to face  challenges of adversarial attacks that modify data so that malicious instances would evade detection by the learning systems. The adversarial learning problem naturally resembles a game between the learning system and the adversary. In such a game, both players would attempt to play their best strategies against each other while maximizing their own payoffs. To solve the game, each player would search for an optimal strategy against the opponent based on the prediction of the opponent's strategy choice. The problem becomes even more complicated in settings where the learning system may have to deal with many adversaries of unknown types. Applying game‐theoretic approach, robust learning techniques have been developed to specifically address adversarial attacks and the preliminary results are promising. In this review, we summarize these results.","",""
14,"Hongfeng Li, Hong-Qin Zhao, Hong Li","Neural-Response-Based Extreme Learning Machine for Image Classification",2019,"","","","",95,"2022-07-13 09:23:14","","10.1109/TNNLS.2018.2845857","","",,,,,14,4.67,5,3,3,"This paper proposes a novel and simple multilayer feature learning method for image classification by employing the extreme learning machine (ELM). The proposed algorithm is composed of two stages: the multilayer ELM (ML-ELM) feature mapping stage and the ELM learning stage. The ML-ELM feature mapping stage is recursively built by alternating between feature map construction and maximum pooling operation. In particular, the input weights for constructing feature maps are randomly generated and hence need not be trained or tuned, which makes the algorithm highly efficient. Moreover, the maximum pooling operation enables the algorithm to be invariant to certain transformations. During the ELM learning stage, elastic-net regularization is proposed to learn the output weight. Elastic-net regularization helps to learn more compact and meaningful output weight. In addition, we preprocess the input data with the dense scale-invariant feature transform operation to improve both the robustness and invariance of the algorithm. To evaluate the effectiveness of the proposed method, several experiments are conducted on three challenging databases. Compared with the conventional deep learning methods and other related ones, the proposed method achieves the best classification results with high computational efficiency.","",""
16,"A. Moawad, A. Silge, T. Bocklitz, K. Fischer, P. Rösch, U. Roesler, M. Elschner, J. Popp, H. Neubauer","A Machine Learning-Based Raman Spectroscopic Assay for the Identification of Burkholderia mallei and Related Species",2019,"","","","",96,"2022-07-13 09:23:14","","10.3390/molecules24244516","","",,,,,16,5.33,2,9,3,"Burkholderia (B.) mallei, the causative agent of glanders, and B. pseudomallei, the causative agent of melioidosis in humans and animals, are genetically closely related. The high infectious potential of both organisms, their serological cross-reactivity, and similar clinical symptoms in human and animals make the differentiation from each other and other Burkholderia species challenging. The increased resistance against many antibiotics implies the need for fast and robust identification methods. The use of Raman microspectroscopy in microbial diagnostic has the potential for rapid and reliable identification. Single bacterial cells are directly probed and a broad range of phenotypic information is recorded, which is subsequently analyzed by machine learning methods. Burkholderia were handled under biosafety level 1 (BSL 1) conditions after heat inactivation. The clusters of the spectral phenotypes and the diagnostic relevance of the Burkholderia spp. were considered for an advanced hierarchical machine learning approach. The strain panel for training involved 12 B. mallei, 13 B. pseudomallei and 11 other Burkholderia spp. type strains. The combination of top- and sub-level classifier identified the mallei-complex with high sensitivities (>95%). The reliable identification of unknown B. mallei and B. pseudomallei strains highlighted the robustness of the machine learning-based Raman spectroscopic assay.","",""
2,"Alex Bauerle, Ángel Alexander Cabrera, Fred Hohman, Megan Maher, David Koski, Xavier Suau, Titus Barik, Dominik Moritz","Symphony: Composing Interactive Interfaces for Machine Learning",2022,"","","","",97,"2022-07-13 09:23:14","","10.1145/3491102.3502102","","",,,,,2,2.00,0,8,1,"Interfaces for machine learning (ML), information and visualizations about models or data, can help practitioners build robust and responsible ML systems. Despite their benefits, recent studies of ML teams and our interviews with practitioners (n=9) showed that ML interfaces have limited adoption in practice. While existing ML interfaces are effective for specific tasks, they are not designed to be reused, explored, and shared by multiple stakeholders in cross-functional teams. To enable analysis and communication between different ML practitioners, we designed and implemented Symphony, a framework for composing interactive ML interfaces with task-specific, data-driven components that can be used across platforms such as computational notebooks and web dashboards. We developed Symphony through participatory design sessions with 10 teams (n=31), and discuss our findings from deploying Symphony to 3 production ML projects at Apple. Symphony helped ML practitioners discover previously unknown issues like data duplicates and blind spots in models while enabling them to share insights with other stakeholders.","",""
1,"L. Merte, M. K. Bisbo, I. Sokolović, M. Setvín, Benjamin Hagman, M. Shipilin, M. Schmid, U. Diebold, E. Lundgren, B. Hammer","Structure of an Ultrathin Oxide on Pt3Sn(111) Solved by Machine Learning Enhanced Global Optimization.",2022,"","","","",98,"2022-07-13 09:23:14","","10.1002/anie.202204244","","",,,,,1,1.00,0,10,1,"Determination of the atomic structure of solid surfaces typically depends on comparison of measured properties with simulations based on hypothesized structural models. For simple structures, the models may be guessed, but for more complex structures there is a need for reliable theory-based search algorithms. So far, such methods have been limited by the combinatorial complexity and computational expense of sufficiently accurate energy estimation for surfaces. However, the introduction of machine learning methods has the potential to change this radically. Here, we demonstrate how an evolutionary algorithm, utilizing machine learning for accelerated energy estimation and diverse population generation, can be used to solve an unknown surface structure-the (4×4) surface oxide on Pt3Sn(111)-based on limited experimental input. The algorithm is efficient and robust, and should be broadly applicable in surface studies, where it can replace manual, intuition based model generation.","",""
1,"H. Rashidi, J. Pepper, T. Howard, K. Klein, L. May, S. Albahra, B. Phinney, M. Salemi, N. Tran","COMPARATIVE PERFORMANCE OF TWO AUTOMATED MACHINE LEARNING PLATFORMS FOR COVID-19 DETECTION BY MALDI-TOF-MS",2022,"","","","",99,"2022-07-13 09:23:14","","10.1101/2022.02.02.22270298","","",,,,,1,1.00,0,9,1,"The 2019 novel coronavirus infectious disease (COVID-19) pandemic has resulted in an unsustainable need for diagnostic tests. Currently, molecular tests are the accepted standard for the detection of SARS-CoV-2. Mass spectrometry (MS) enhanced by machine learning (ML) has recently been postulated to serve as a rapid, high-throughput, and low-cost alternative to molecular methods. Automated ML is a novel approach that could move mass spectrometry techniques beyond the confines of traditional laboratory settings. However, it remains unknown how different automated ML platforms perform for COVID-19 MS analysis. To this end, the goal of our study is to compare algorithms produced by two commercial automated ML platforms (Platforms A and B). Our study consisted of MS data derived from 361 subjects with molecular confirmation of COVID-19 status including SARS-CoV-2 variants. The top optimized ML model with respect to positive percent agreement (PPA) within Platforms A and B exhibited an accuracy of 94.9%, PPA of 100%, negative percent agreement (NPA) of 93%, and an accuracy of 91.8%, PPA of 100%, and NPA of 89%, respectively. These results illustrate the MS methods robustness against SARS-CoV-2 variants and highlight similarities and differences in automated ML platforms in producing optimal predictive algorithms for a given dataset.","",""
1,"M. Jessell, Jiateng Guo, Yunqiang Li, M. Lindsay, R. Scalzo, J. Giraud, G. Pirot, E. Cripps, V. Ogarko","Into the Noddyverse: a massive data store of 3D geological models for machine learning and inversion applications",2022,"","","","",100,"2022-07-13 09:23:14","","10.5194/essd-14-381-2022","","",,,,,1,1.00,0,9,1,"Abstract. Unlike some other well-known challenges such as facial recognition, where machine learning and inversion algorithms are widely developed, the geosciences suffer from a lack of large, labelled data sets that can be used to validate or train robust machine learning and inversion schemes. Publicly available 3D geological models are far too restricted in both number and the range of geological scenarios to serve these purposes. With reference to inverting geophysical data this problem is further exacerbated as in most cases real geophysical observations result from unknown 3D geology, and synthetic test data sets are often not particularly geological or geologically diverse. To overcome these limitations, we have used the Noddy modelling platform to generate 1 million models, which represent the first publicly accessible massive training set for 3D geology and resulting gravity and magnetic data sets (https://doi.org/10.5281/zenodo.4589883, Jessell, 2021). This model suite can be used to train machine learning systems and to provide comprehensive test suites for geophysical inversion. We describe the methodology for producing the model suite and discuss the opportunities such a model suite affords, as well as its limitations, and how we can grow and access this resource. ","",""
0,"E. Alladio, Brando Poggiali, Giulia Cosenza, E. Pilli","Multivariate statistical approach and machine learning for the evaluation of biogeographical ancestry inference in the forensic field",2022,"","","","",101,"2022-07-13 09:23:14","","10.1038/s41598-022-12903-0","","",,,,,0,0.00,0,4,1,"","",""
0,"Shizhen Jin, Zhaofeng Guo, Dongli Liu, Yanhua Yang","A Study on the Application of Distributed System Technology-Guided Machine Learning in Malware Detection",2022,"","","","",102,"2022-07-13 09:23:14","","10.1155/2022/4977898","","",,,,,0,0.00,0,4,1,"In recent years, with the development of information technology, the Internet has become an essential tool for human daily life. However, as the popularity and scale of the Internet continue to expand, malware has also emerged as an increasingly widespread trend, and its development has brought many negative impacts to the society. As the number of types of malware is getting enormous, the attacks are constantly updated, and at the same time, the spread is very fast, causing more and more damage to the network, the requirements and standards for malware detection are constantly rising. How to effectively detect malware is a research trend; in order to tackle the new needs and problems arising from the development of malware, this paper proposes to guide machine learning algorithms to implement malware detection in a distributed environment: firstly, each detection node in the distributed network performs anomaly detection on the captured software information and data, then performs feature analysis to discover unknown malware and obtain its samples, updates the new malware features to all feature detection nodes in the whole distributed network, and trains the random forest-based machine learning algorithm for malware classification and detection, thus completing the global response processing capability for malware. By building a distributed system framework, the global capture capability of malware detection is enhanced to robustly respond to the increasing and rapid spread of malware, and machine learning algorithms are integrated into it to achieve effective detection of malware. Extended experiments on the Ember 2017 and Ember 2018 databases show that our proposed approach achieves advanced performance and effectively addresses the problem of malware detection.","",""
0,"J. Steck, N. Thompson, E. Behrman","Programming Quantum Hardware via Levenberg Marquardt Machine Learning",2022,"","","","",103,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,3,1,"Significant challenges remain with the development of macroscopic quantum computing: hardware problems of noise, decoherence, and scaling; software problems of error correction; and, most important, algorithm construction. Finding truly quantum algorithms is quite difficult, and many quantum algorithms, like Shor’s prime factoring or phase estimation, require extremely long circuit depth for any practical application, necessitating error correction. Machine learning can be used as a systematic method to non-algorithmically “program” quantum computers. Quantum machine learning enables us to perform computations without breaking down an algorithm into its gate “building blocks”, eliminating that difficult step and potentially reducing unnecessary complexity. In addition, we have shown that our machine learning approach is robust to both noise and to decoherence, which is ideal for running on inherently noisy NISQ devices which are limited in the number of qubits available for error correction. We demonstrated this using a fundamentally non-classical calculation: experimentally estimating the entanglement of an unknown quantum state. Results from this have been successfully ported to the IBM hardware and trained using a powerful hybrid reinforcement learning technique which is a modified Levenberg-Marquardt (LM) method. The LM method is ideally suited to quantum machine learning as it only requires knowledge of the final measured output of the quantum computation, not intermediate quantum states which are generally not accessible. Since it processes all the learning data simultaneously, it also requires significantly fewer hits on the quantum hardware. Machine learning is demonstrated with results from simulations and runs on the IBM Qiskit hardware","",""
0,"Junzhong Xie, Xu-Yuan Zhou, Dong Luan, Hong Jiang","Machine Learning Force Field Aided Cluster Expansion Approach to Configurationally Disordered Materials: Critical Assessment of Training Set Selection and Size Convergence.",2022,"","","","",104,"2022-07-13 09:23:14","","10.1021/acs.jctc.2c00017","","",,,,,0,0.00,0,4,1,"Cluster expansion (CE) is a powerful theoretical tool to study the configuration-dependent properties of substitutionally disordered systems. Typically, a CE model is built by fitting a few tens or hundreds of target quantities calculated by first-principles approaches. To validate the reliability of the model, a convergence test of the cross-validation (CV) score to the training set size is commonly conducted to verify the sufficiency of the training data. However, such a test only confirms the convergence of the predictive capability of the CE model within the training set, and it is unknown whether the convergence of the CV score would lead to robust thermodynamic simulation results such as order-disorder phase transition temperature Tc. In this work, using carbon defective MoC1-x as a model system and aided by the machine-learning force field technique, a training data pool with about 13000 configurations has been efficiently obtained and used to generate different training sets of the same size randomly. By conducting parallel Monte Carlo simulations with the CE models trained with different randomly selected training sets, the uncertainty in calculated Tc can be evaluated at different training set sizes. It is found that the training set size that is sufficient for the CV score to converge still leads to a significant uncertainty in the predicted Tc and that the latter can be considerably reduced by enlarging the training set to that of a few thousand configurations. This work highlights the importance of using a large training set to build the optimal CE model that can achieve robust statistical modeling results and the facility provided by the machine-learning force field approach to efficiently produce adequate training data.","",""
0,"S. Mordensky, J. Lipor, J. DeAngelo, E. Burns, Cary R. Lindsey","Predicting Geothermal Favorability in the Western United States by Using Machine Learning: Addressing Challenges and Developing Solutions",2022,"","","","",105,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,5,1,"Previous moderate- and high-temperature geothermal resource assessments of the western United States utilized weight-of-evidence and logistic regression methods to estimate resource favorability, but these analyses relied upon some expert decisions. While expert decisions can add confidence to aspects of the modeling process by ensuring only reasonable models are employed, expert decisions also introduce human bias into assessments. This bias presents a source of error that may affect the performance of the models and resulting resource estimates. Our study aims to reduce expert input through robust data-driven analyses and better-suited data science techniques, with the goals of saving time, reducing bias, and improving predictive ability. We present six favorability maps for geothermal resources in the western United States created using two strategies applied to three modern machine learning algorithms (logistic regression, support-vector machines, and XGBoost). To provide a direct comparison to previous assessments, we use the same input data as the 2008 U.S. Geological Survey (USGS) conventional moderate- to high-temperature geothermal resource assessment. The six new favorability maps required far less expert decision-making, but broadly agree with the previous assessment. Despite the fact that the 2008 assessment results employed linear methods, the non-linear machine learning algorithms ( i.e., support-vector machines and XGBoost) produced greater agreement with the previous assessment than the linear machine learning algorithm ( i.e., logistic regression). It is not surprising that geothermal systems depend on non-linear combinations of features, and we postulate that the expert decisions during the 2008 assessment accounted for system non-linearities. Substantial challenges to applying machine learning algorithms to predict geothermal resource favorability include severe class imbalance ( i.e., there are very few known geothermal systems compared to the large area considered), and while there are known geothermal systems ( i.e., positive labels), all other sites have an unknown status ( i.e., they are unlabeled), instead of receiving a negative label ( i.e., the known/proven absence of a geothermal resource). We address both challenges through a custom undersampling strategy that can be used with any algorithm and then evaluated using F1 scores. for XGBoost: class weight, learning rate, number of estimators, and maximum depth of estimators. Class weight in XGBoost differs in exact implementation compared with logistic regression and SVMs, but this hyperparameter serves much the same purpose: a greater class weight places greater emphasis on accurately predicting positive labels ( i.e., known geothermal systems) than non-positive labels ( i.e., unknown resource potential). The other parameters are used to maximize prediction performance while also avoiding overfitting (Chen and Guestrin, 2016). We leave the other parameters of XGBoost at the default values found in Python’s XGBoost module as they pertain to the specifics of the optimization routine and have only a modest impact on performance (Chen and Guestrin, 2016).","",""
0,"Lane Fitzsimmons, Maya Dewan, J. Dexheimer","Diversity in Machine Learning: A Systematic Review of Text-Based Diagnostic Applications",2022,"","","","",106,"2022-07-13 09:23:14","","10.1055/s-0042-1749119","","",,,,,0,0.00,0,3,1,"OBJECTIVE  As the storage of clinical data has transitioned into electronic formats, medical informatics has become increasingly relevant in providing diagnostic aid. The purpose of this review is to evaluate machine learning models that use text data for diagnosis and to assess the diversity of the included study populations.   METHODS  We conducted a systematic literature review on three public databases. Two authors reviewed every abstract for inclusion. Articles were included if they used or developed machine learning algorithms to aid in diagnosis. Articles focusing on imaging informatics were excluded.   RESULTS  From 2,260 identified papers, we included 78. Of the machine learning models used, neural networks were relied upon most frequently (44.9%). Studies had a median population of 661.5 patients, and diseases and disorders of 10 different body systems were studied. Of the 35.9% (N = 28) of papers that included race data, 57.1% (N = 16) of study populations were majority White, 14.3% were majority Asian, and 7.1% were majority Black. In 75% (N = 21) of papers, White was the largest racial group represented. Of the papers included, 43.6% (N = 34) included the sex ratio of the patient population.   DISCUSSION  With the power to build robust algorithms supported by massive quantities of clinical data, machine learning is shaping the future of diagnostics. Limitations of the underlying data create potential biases, especially if patient demographics are unknown or not included in the training.   CONCLUSION  As the movement toward clinical reliance on machine learning accelerates, both recording demographic information and using diverse training sets should be emphasized. Extrapolating algorithms to demographics beyond the original study population leaves large gaps for potential biases.","",""
0,"Karthikeyan V, S. S","Hybrid machine learning classification scheme for speaker identification",2022,"","","","",107,"2022-07-13 09:23:14","","10.1111/1556-4029.15006","","",,,,,0,0.00,0,2,1,"Motivated by the requirement to prepare for the next generation of “Automatic Spokesperson Recognition” (ASR) system, this paper applied the fused spectral features with hybrid machine learning (ML) strategy to the speech communication field. This strategy involved the combined spectral features such as mel‐frequency cepstral coefficients (MFCCs), spectral kurtosis, spectral skewness, normalized pitch frequency (NPF), and formants. The characterization of suggested classification method could possibly serve in advanced speaker identification scenarios. Special attention was given to hybrid ML scheme capable of finding unknown speakers equipped with speaker id‐detecting classifier technique, known as “Random Forest‐Support Vector Machine” (RF‐SVM). The extracted speaker precise spectral attributes are applied to the hybrid RF‐SVM classifier to identify/verify the particular speaker. This work aims to construct an ensemble decision tree on a bounded area with minimal misclassification error using a hybrid ensemble RF‐SVM strategy. A series of standard, real‐time speaker databases, and noise conditions are functionally tested to validate its performance with other state‐of‐the‐art mechanisms. The proposed fusion method succeeds in the speaker identification task with a high identification rate (97% avg) and lower equal error rate (EER) (<2%), compared with the individual schemes for the recorded experimental dataset. The robustness of the classifier is validated using the standard ELSDSR, TIMIT, and NIST audio datasets. Experiments on ELSDSR, TIMIT, and NIST datasets show that the hybrid classifier produces 98%, 99%, and 94% accuracy, and EERs were 2%, 1%, and 2% respectively. The findings are then compared with well‐known other speaker recognition schemes and found to be superior.","",""
0,"G. M. Anand, Heitor C. Megale, Sean H. Murphy, Theresa Weis, Zuwan Lin, Yichun He, Xiao Wang, Jia Liu, S. Ramanathan","Machine learning directed organoid morphogenesis uncovers an excitable system driving human axial elongation",2022,"","","","",108,"2022-07-13 09:23:14","","10.1101/2022.05.10.491358","","",,,,,0,0.00,0,9,1,"The human embryo breaks symmetry to form the anterior-posterior axis of the body. As the embryo elongates along this axis, progenitors in the tailbud give rise to axial tissues that generate the spinal cord, skeleton, and musculature. The mechanisms underlying human axial elongation are unknown. While ethics necessitate in vitro studies, the variability of human organoid systems has hindered mechanistic insights. Here we developed a bioengineering and machine learning framework that optimizes symmetry breaking by tuning the spatial coupling between human pluripotent stem cell-derived organoids. This framework enabled the reproducible generation of hundreds of axially elongating organoids, each possessing a tailbud and an epithelial neural tube with a single lumen. We discovered that an excitable system composed of WNT and FGF signaling drives axial elongation through the induction of a signaling center in the form of neuromesodermal progenitor (NMP)-like cells. The ability of NMP-like cells to function as a signaling center and drive elongation is independent of their potency to generate mesodermal cell types. We further discovered that the instability of the underlying excitable system is suppressed by secreted WNT inhibitors of the secreted frizzled-related protein (SFRP) family. Absence of these inhibitors led to the formation of ectopic tailbuds and branches. Our results identify mechanisms governing stable human axial elongation to achieve robust morphogenesis.","",""
53,"Bethany M. Moore, Peipei Wang, P. Fan, Bryan J. Leong, Craig A. Schenck, J. P. Lloyd, Melissa D. Lehti-Shiu, R. Last, E. Pichersky, S. Shiu","Robust predictions of specialized metabolism genes through machine learning",2018,"","","","",109,"2022-07-13 09:23:14","","10.1073/pnas.1817074116","","",,,,,53,13.25,5,10,4,"Significance Specialized metabolites are critical for plant–environment interactions, e.g., attracting pollinators or defending against herbivores, and are important sources of plant-based pharmaceuticals. However, it is unclear what proportion of enzyme-encoding genes play a role in specialized metabolism (SM) as opposed to general metabolism (GM) in any plant species. This is because of the diversity of specialized metabolites and the considerable number of incompletely characterized pathways responsible for their production. In addition, SM gene ancestors frequently played roles in GM. We evaluate features distinguishing SM and GM genes and build a computational model that accurately predicts SM genes. Our predictions provide candidates for experimental studies, and our modeling approach can be applied to other species that produce medicinally or industrially useful compounds. Plant specialized metabolism (SM) enzymes produce lineage-specific metabolites with important ecological, evolutionary, and biotechnological implications. Using Arabidopsis thaliana as a model, we identified distinguishing characteristics of SM and GM (general metabolism, traditionally referred to as primary metabolism) genes through a detailed study of features including duplication pattern, sequence conservation, transcription, protein domain content, and gene network properties. Analysis of multiple sets of benchmark genes revealed that SM genes tend to be tandemly duplicated, coexpressed with their paralogs, narrowly expressed at lower levels, less conserved, and less well connected in gene networks relative to GM genes. Although the values of each of these features significantly differed between SM and GM genes, any single feature was ineffective at predicting SM from GM genes. Using machine learning methods to integrate all features, a prediction model was established with a true positive rate of 87% and a true negative rate of 71%. In addition, 86% of known SM genes not used to create the machine learning model were predicted. We also demonstrated that the model could be further improved when we distinguished between SM, GM, and junction genes responsible for reactions shared by SM and GM pathways, indicating that topological considerations may further improve the SM prediction model. Application of the prediction model led to the identification of 1,220 A. thaliana genes with previously unknown functions, each assigned a confidence measure called an SM score, providing a global estimate of SM gene content in a plant genome.","",""
0,"A. Barrett, M. Balme, J., Wright, M. Woods, S. Karachalios, M. Malinowski","CLASSIFYING PLANETARY SURFACES USING MACHINE LEARNING",2022,"","","","",110,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,7,1,"Introduction: Deep learning (e.g. [1]) convolutional neural networks were trained to discriminate meter scale variations in surface texture in satellite images of planetary surfaces [2]. Two versions of the model were trained, one classifying surface textures and aeolian bedforms in HiRISE images of Mars [3]. The second to discriminate between blocky ejecta and textures indicative of impact melt in LROCNAC images of the Moon [4]. An ever increasing volume of remote sensing data is being returned from the Moon and Mars, and while this has the potential to allow large scale mapping efforts at a greater spatial resolution than ever before, it is increasingly challenging for all relevant data to be surveyed in a reasonable amount of time. The networks classify surface textures based on morphological criteria rather than making determinations of perceived geological origin. Automating the production of a geological map is not yet possible and would be counter-productive, since the purpose of a mapping effort is as much to build understanding of the geological history of a site through discussion and exploration as it is to classify the surface. Rather our aim is to use machine learning to augment the human mapping workflow, and speed up the initial surveying needed for such an effort. A network performs “triage” on unmanageably large datasets, indicating to a human operator areas where predefined surface texture assemblages indicate that features of interest could be present. NOAH-H: The Novelty or Anomaly Hunter – HiRiSE (NOAH-H) [5] was developed as part of the ExoMars Rosalind Franklin [6] landing site selection process. It was trained on ~1500 example framelets, selected from across Arabia Terra. It performs semantic segmentation at a pixel scale, identifying surface textures as one of 14 ontological classes (fig 1). Seven surface classes define roughness types, six classes describe aeolian bedforms and the final class describes patches of boulders. The classification scheme is hierarchical. Care was taken to ensure that the class definitions were purely morphological, providing a robust descriptive level. These descriptive classes were then grouped into thematic categories such as bedrock, non-bedrock etc. to form the interpretive layer of the system. This ensures that the classification does not require contextual evidence which would be unknown to the network. While still providing the scientist the information needed for interpretation.","",""
0,"Alekh Agarwal, Tong Zhang","Minimax Regret Optimization for Robust Machine Learning under Distribution Shift",2022,"","","","",111,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,2,1,"In this paper, we consider learning scenarios where the learned model is evaluated under an unknown test distribution which potentially differs from the training distribution (i.e. distribution shift). The learner has access to a family of weight functions such that the test distribution is a reweighting of the training distribution under one of these functions, a setting typically studied under the name of Distributionally Robust Optimization (DRO). We consider the problem of deriving regret bounds in the classical learning theory setting, and require that the resulting regret bounds hold uniformly for all potential test distributions. We show that the DRO formulation does not guarantee uniformly small regret under distribution shift. We instead propose an alternative method called Minimax Regret Optimization (MRO), and show that under suitable conditions this method achieves uniformly low regret across all test distributions. We also adapt our technique to have stronger guarantees when the test distributions are heterogeneous in their similarity to the training data. Given the widespead optimization of worst case risks in current approaches to robust machine learning, we believe that MRO can be a strong alternative to address distribution shift scenarios.","",""
7,"Huanbo Sun, G. Martius","Machine Learning for Haptics: Inferring Multi-Contact Stimulation From Sparse Sensor Configuration",2019,"","","","",112,"2022-07-13 09:23:14","","10.3389/fnbot.2019.00051","","",,,,,7,2.33,4,2,3,"Robust haptic sensation systems are essential for obtaining dexterous robots. Currently, we have solutions for small surface areas, such as fingers, but affordable and robust techniques for covering large areas of an arbitrary 3D surface are still missing. Here, we introduce a general machine learning framework to infer multi-contact haptic forces on a 3D robot's limb surface from internal deformation measured by only a few physical sensors. The general idea of this framework is to predict first the whole surface deformation pattern from the sparsely placed sensors and then to infer number, locations, and force magnitudes of unknown contact points. We show how this can be done even if training data can only be obtained for single-contact points using transfer learning at the example of a modified limb of the Poppy robot. With only 10 strain-gauge sensors we obtain a high accuracy also for multiple-contact points. The method can be applied to arbitrarily shaped surfaces and physical sensor types, as long as training data can be obtained.","",""
7,"Lin Liu, R. Mukherjee, J. Robins","On Nearly Assumption-Free Tests of Nominal Confidence Interval Coverage for Causal Parameters Estimated by Machine Learning",2019,"","","","",113,"2022-07-13 09:23:14","","10.1214/20-sts786","","",,,,,7,2.33,2,3,3,"For many causal effect parameters of interest, doubly robust machine learning (DRML) estimators ψ^1 are the state-of-the-art, incorporating the good prediction performance of machine learning; the decreased bias of doubly robust estimators; and the analytic tractability and bias reduction of sample splitting with cross-fitting. Nonetheless, even in the absence of confounding by unmeasured factors, the nominal (1−α) Wald confidence interval ψ^1±zα/2s.e.ˆ[ψ^1] may still undercover even in large samples, because the bias of ψ^1 may be of the same or even larger order than its standard error of order n−1/2.  In this paper, we introduce essentially assumption-free tests that (i) can falsify the null hypothesis that the bias of ψ^1  is of smaller order than its standard error, (ii) can provide a upper confidence bound on the true coverage of the Wald interval, and (iii) are valid under the null under no smoothness/sparsity assumptions on the nuisance parameters. The tests, which we refer to as Assumption Free Empirical Coverage Tests (AFECTs), are based on a U-statistic that estimates part of the bias of ψ^1.  Our claims need to be tempered in several important ways. First no test, including ours, of the null hypothesis that the ratio of the bias to its standard error is smaller than some threshold δ  can be consistent [without additional assumptions (e.g., smoothness or sparsity) that may be incorrect]. Second, the above claims only apply to certain parameters in a particular class. For most of the others, our results are unavoidably less sharp. In particular, for these parameters, we cannot directly test whether the nominal Wald interval ψ^1±zα/2s.e.ˆ[ψ^1] undercovers. However, we can often test the validity of the smoothness and/or sparsity assumptions used by an analyst to justify a claim that the reported Wald interval’s actual coverage is no less than nominal. Third, in the main text, with the exception of the simulation study in Section 1, we assume we are in the semisupervised data setting (wherein there is a much larger dataset with information only on the covariates), allowing us to regard the covariance matrix of the covariates as known. In the simulation in Section 1, we consider the setting in which estimation of the covariance matrix is required. In the simulation, we used a data adaptive estimator which performs very well in our simulations, but the estimator’s theoretical sampling behavior remains unknown.","",""
15,"Suyun Liu, L. Vicente","The stochastic multi-gradient algorithm for multi-objective optimization and its application to supervised machine learning",2019,"","","","",114,"2022-07-13 09:23:14","","10.1007/S10479-021-04033-Z","","",,,,,15,5.00,8,2,3,"","",""
1,"Hyun Kwon, Sanghyun Lee","Textual Adversarial Training of Machine Learning Model for Resistance to Adversarial Examples",2022,"","","","",115,"2022-07-13 09:23:14","","10.1155/2022/4511510","","",,,,,1,1.00,1,2,1,"Deep neural networks provide good performance for image recognition, speech recognition, text recognition, and pattern recognition. However, such networks are vulnerable to attack by adversarial examples. Adversarial examples are created by adding a small amount of noise to an original sample in such a way that no problem is perceptible to humans, yet the sample will be incorrectly recognized by a model. Adversarial examples have been studied mainly in the context of images, but research has expanded to include the text domain. In the textual context, an adversarial example is a sample of text in which certain important words have been changed so that the sample will be misclassified by a model even though to humans it is the same as the original text in terms of meaning and grammar. In the text domain, there have been relatively few studies on defenses against adversarial examples compared with the number of studies on adversarial example attacks. In this paper, we propose an adversarial training method to defend against adversarial examples that target the latest text model, bidirectional encoder representations from transformers (BERT). In the proposed method, adversarial examples are generated using various parameters and then are applied in additional training of the target model to instill robustness against unknown adversarial examples. Experiments were conducted using five datasets (AG’s News, a movie review dataset, the IMDB Large Movie Review Dataset (IMDB), the Stanford Natural Language Inference (SNLI) corpus, and the Multi-Genre Natural Language Inference (MultiNLI) corpus), with TensorFlow as the machine learning library. According to the experimental results, the baseline model had an accuracy of 88.1% on the original sentences and an accuracy of 9.2% on the adversarial sentences, whereas the model that underwent the proposed training method maintained an average accuracy of 87.2% on the original sentences and had an average accuracy of 22.5% on the adversarial sentences.","",""
0,"Z. Liu, G. Shurin, L. Bian, David L. White, M. Shurin, A. Star","A Carbon Nanotube Sensor Array for the Label-Free Discrimination of Live and Dead Cells with Machine Learning.",2022,"","","","",116,"2022-07-13 09:23:14","","10.1021/acs.analchem.1c04661","","",,,,,0,0.00,0,6,1,"Developing robust cell recognition strategies is important in biochemical research, but the lack of well-defined target molecules creates a bottleneck in some applications. In this paper, a carbon nanotube sensor array was constructed for the label-free discrimination of live and dead mammalian cells. Three types of carbon nanotube field-effect transistors were fabricated, and different features were extracted from the transfer characteristic curves for model training with linear discriminant analysis (LDA) and support-vector machines (SVM). Live and dead cells were accurately classified in more than 90% of samples in each sensor group using LDA as the algorithm. The recursive feature elimination with cross-validation (RFECV) method was applied to handle the overfitting and optimize the model, and cells could be successfully classified with as few as four features and a higher validation accuracy (up to 97.9%) after model optimization. The RFECV method also revealed the crucial features in the classification, indicating the participation of different sensing mechanisms in the classification. Finally, the optimized LDA model was applied for the prediction of unknown samples with an accuracy of 87.5-93.8%, indicating that live and dead cell samples could be well-recognized with the constructed model.","",""
2,"P. Perdikaris, G. Karniadakis","Machine Learning of Space-Fractional Differential Equations | SIAM Journal on Scientific Computing | Vol. 41, No. 4 | Society for Industrial and Applied Mathematics",2019,"","","","",117,"2022-07-13 09:23:14","","","","",,,,,2,0.67,1,2,3,"Data-driven discovery of “hidden physics”—i.e., machine learning of differential equation models underlying observed data—has recently been approached by embedding the discovery problem into a Gaussian process regression of spatial data, treating and discovering unknown equation parameters as hyperparameters of a “physics informed” Gaussian process kernel. This kernel includes the parametrized differential operators applied to a prior covariance kernel. We extend this framework to the data-driven discovery of linear space-fractional differential equations. The methodology is compatible with a wide variety of space-fractional operators in Rd and stationary covariance kernels, including the Matérn class, and allows for optimizing the Matérn parameter during training. Since fractional derivatives are typically not given by closed-form analytic expressions, the main challenges to be addressed are a user-friendly, general way to set up fractionalorder derivatives of covariance kernels, together with feasible and robust numerical methods for such implementations. Making use of the simple Fourier-space representation of space-fractional derivatives in Rd, we provide a unified set of integral formulas for the resulting Gaussian process kernels. The shift property of the Fourier transform results in formulas involving d-dimensional integrals that can be efficiently treated using generalized Gauss–Laguerre quadrature. The implementation of fractional derivatives has several benefits. First, the method allows for discovering models involving fractional-order PDEs for systems characterized by heavy tails or anomalous diffusion, while bypassing the analytical difficulty of fractional calculus. Data sets exhibiting such features are of increasing prevalence in physical and financial domains. Second, a single fractionalorder archetype allows for a derivative term of arbitrary order to be learned, with the order itself being a parameter in the regression. As a result, even when used for discovering integer-order equations, the proposed method has several benefits compared to previous works on data-driven discovery of differential equations; the user is not required to assume a “dictionary” of derivatives of various orders and directly controls the parsimony of the models being discovered. We illustrate our method on several examples, including fractional-order interpolation of advection-diffusion and modeling relative stock performance in the S&P 500 with α-stable motion via a fractional diffusion equation.","",""
4,"T. Schmid","Deconstructing the Final Frontier of Artificial Intelligence: Five Theses for a Constructivist Machine Learning",2019,"","","","",118,"2022-07-13 09:23:14","","","","",,,,,4,1.33,4,1,3,"Ambiguity and diversity in human cognition can be regarded a final frontier in developing equivalent systems of artificial intelligence. Despite astonishing accomplishments, modern machine learning algorithms are still hardly more than adaptive systems. Deep neural networks, for example, represent complexity through complex connectivity but are not able to allow for abstraction and differentiation of interpretable knowledge, i.e., for key mechanisms of human cognition. Like support vector machines, random forests and other statistically motivated algorithms, they do neither reflect nor yield structures and strategies of human thinking. Therefore, we suggest to realign the use of existing machine learning tools with respect to the philosophical paradigm of constructivism, which currently is the key concept in human learning and professional teaching. Based on the idea that learning units like classifiers can be considered models with limited validity, we formulate five principles to guide a constructivist machine learning. We describe how to define such models and model limitations, how to relate them and how relationships allow to abstract and differentiate models. To this end, we propose the use of meta data for classifiers and other models. Moreover, we argue that such meta data-based machine learning results in a knowledge base that is both created by the means of automation and interpretable for humans. Over the last decade, it has become widely accepted to address computational systems intelligent. Not only journalists, but also scientists have adapted this habit in their publications. In fact, many classical engineering tasks like monitoring or regulating have profited from the employment of machine learning (Abellan-Nebot and Romero Subirón 2010; Mohanraj, Jayaraj, and Muraleedharan 2012). The same holds true for pattern recognition, most prominently in automated image and video analysis (Zafeiriou, Zhang, and Zhang 2015; Yang et al. 2011). And even though ultimate challenges like the infamous Turing test are left unsatisfied (You 2015), some exceptional results in specialized tasks like playing the game of go (Silver et al. 2016) make current learning machines look intelligent on a human level. Copyright held by the author(s). In A. Martin, K. Hinkelmann, A. Gerber, D. Lenat, F. van Harmelen, P. Clark (Eds.), Proceedings of the AAAI 2019 Spring Symposium on Combining Machine Learning with Knowledge Engineering (AAAI-MAKE 2019). Stanford University, Palo Alto, California, USA, March 25-27, 2019. A final frontier for learning systems, however, is the variety of alternative cognitive functions observable in a diverse set of individuals or from ambiguous stimuli (Kornmeier and Bach 2012). While philosophy has acknowledged and embraced the subjectivity and limitations of human cognition during the last decades (Prawat and Floden 1994), current learning systems regard cognition a complex, yet technical task to be solved. In particular, established algorithms do neither provide convincing answers to the challenges provided by an ambiguous environment; nor do they offer concepts that explicitly allow for contradictory judgements comparable to differences in social perception. The main reason for this shortcoming is that so far both algorithms and researchers have failed to incorporate a constructivist point of view. Constructivism implies not only cognition to be a highly individual phenomenon, but also humans to take an active role in their perception of the world – and that there is no such thing as a human-independent reality (Reich 2009). Yet algorithms and applications aiming to predict things other than laws of nature are implicitly founded on exactly this outdated asumption. In the following, we introduce axioms that allow machine learning to follow constructivist principles. Key features of this approach are the use of modern tools from empirical sciences, model-oriented learning, the ability to handle ambiguity, the ability to integrate supervised and unsupervised learning into a unified framework, the ability to create an individual knowledge base and the ability to abstract, differentiate or discard learned knowledge automatically. 1. The key component of cognitive functionality is a model. Since the introduction of artificial neural networks as a theoretical concept (McCulloch and Pitts 1943), many mathematicians and computer scientists have considered neurons the key component of learning systems. In education and psychology, however, cognitive functions are often seen as certain skills or abilities acquired and exposed by an individual human and described in terms like the concept of competence, which, e.g., is widely used in the modern European education system (Méhaut and Winch 2012). Functionalistic psychology explains cognitive functions of humans by the concept of mental models (Rouse and Morris 1986). Initially, mental models have been used to understand motor control, e.g., of hand movements (Veldhuyzen and Stassen 1977). In a more general sense, however, mental models are described as “hypothetical constructs” (Wickens 2000) that can be ordered hierarchically (Rasmussen 1979) and allow a human to make predictions about his physical and social environment (Oatley 1985). It has also been postulated that such models cannot be of static nature but rather underlay continuous modifications (Oatley 1985). Philosophers, too, consider models an important tool in human knowledge acquisition (Klaus 1967, p. 412) or even the only tool, respectively (Stachowiak 1973, p. 56). While varying and concurring theoretical definitions exist, most model concepts assume an image, an origin of the image and a relationship between them. This definition is, e.g., matched by the idea of mathematical modeling as proposed by Heinrich Hertz and others (Hertz 1894; Hamilton 1982). With the rise of robotics and artificial intelligence, engineers have adapted and extended this idea by postulating the concept of a cybernetic model, which involves a generalized subject and an object of the model (Rose 2009). Cybernetics, however, did neither reflect time-related aspects nor issues involved with individual model subjects. This matter was adressed by Herbert Stachowiak, who was influenced by cybernetics when developing his General Model Theory (Hof 2018). He postulated any model to be limited to specific subjects, specific temporal ranges and specific purposes (Stachowiak 1973, p. 133). Limitations, to this end, are considered a matter of fact rather than a matter of definition. Thus, such models circumvent ambiguity by viewing an otherwise ambiguous model with unknown validity limits as a number of models of limited validity. 2. Learning constitutes from constructing, reconstructing or deconstructing models. Modern education is dominated by the ideas of constructivism and constructivist learning (Fox 2001). At its heart, this approach is based on the assumption that humans acquire knowledge and competences actively and individually through processes called construction, reconstruction and deconstruction (Duffy and Jonassen 1992). Construction is associated with creation, innovation and production and implies searching for variations, combinations or transfers of knowledge (Reich 2004, p. 145). Analogously, reconstruction is associated with application, repetition or imitation and implies searching for order, patterns or models (Reich 2004, p. 145). Deconstruction is in the context of constructivism associated with reconsideration, doubt and modification and implies searching for omissions, additions and defective parts of acquired knowledge (Reich 2004, p. 145). Learning algorithms have been used for half a century to transform sample data into models in a mathematical sense, that is: into generalized mathematical relationships between image and origin. The two major approaches or objectives, known as supervised and unsupervised learning, either do or do not require a given target parameter. Artificial neural networks and their relatives are among the most popular and prominent algorithms for learning with a given target parameter (Singh, Thakur, and Sharma 2016), but statistically motivated approaches like support vector machines (Cristianini and Shawe-Taylor 2000) or random forests (Breiman 2001) are also widely used for supervised learning; a specialized field of supervised learning is reinforcement learning, which is popular in robotics (Kober and Peters 2012) and adaptive control (Lewis, Vrabie, and Vamvoudakis 2012). For unsupervised learning, too, biologically inspired approaches like self-organizing maps (Kohonen 2001) as well as statistically motivated approaches like k-means (Jain 2010) are employed. To some extent, machine learning parallels modern education concepts. A construction process in the constructivist sense may be matched by an unsupervised learning, i.e., identifying clusterings or dimensionality reduction, and can, e.g., be implemented with self-organizing maps, kmeans, autoencoders or feature clustering (Schmid 2018). A reconstruction process in the constructivist sense may be matched by a supervised learning, i.e., classification or regression tasks, and can, e.g., be implemented with artificial neural networks or random forests (Schmid 2018). Few researchers, however, have discussed a constructivist approach to machine learning (Drescher 1989; Quartz 1993), and even less how to design a deconstruction process. While domainspecific applications with manual re-engineering options exist (Herbst and Karagiannis 2000), to the best of our knowledge, there is currently only one working implementation of an algorithmic deconstruction process (Schmid 2018). 3. Deconstructing models computationally requires model-based meta data. In order to automate and implement a deconstruction process, successfully learned models must be held available for comparison or re-training. More over, possible matchings with n","",""
4,"Mohammed Ali Khan, A. Haque, V. S. B. Kurukuru","Machine Learning Based Islanding Detection for Grid Connected Photovoltaic System",2019,"","","","",119,"2022-07-13 09:23:14","","10.1109/ICPECA47973.2019.8975614","","",,,,,4,1.33,1,3,3,"This paper focus on developing a new islanding detection method with the help of machine learning and signal processing technique. The islanding detection method make sure that there is a proper remote monitoring of the grid integrated photovoltaic (PV) system. In case of grid fault or maintained of the grid, a proper informed signal is provided to various distributed generation (DG) networks so that they can disconnect with the grid and operate in isolated mode. A simulation of 1kW grid connected PV system is performed. The signal such as voltage, current and frequency are recorded at point of common coupling (PCC). The feature of recorded signals are extracted using wavelet transformation. The extracted features are used to form a islanding scenarios matrix. The matrix is further utilized to train a classifier using machine learning algorithm. From the result it can be observed that the trained classifier depicted 97.9% training accuracy with a training time of 16.9 sec which is better when compared with the literature. Further the trained classifier is subjected to test with an unknown islanding condition to observe the robustness of the classifier.","",""
3,"É. Chouzenoux, Henri G'erard, J. Pesquet","General risk measures for robust machine learning",2019,"","","","",120,"2022-07-13 09:23:14","","10.3934/fods.2019011","","",,,,,3,1.00,1,3,3,"A wide array of machine learning problems are formulated as the minimization of the expectation of a convex loss function on some parameter space. Since the probability distribution of the data of interest is usually unknown, it is is often estimated from training sets, which may lead to poor out-of-sample performance. In this work, we bring new insights in this problem by using the framework which has been developed in quantitative finance for risk measures. We show that the original min-max problem can be recast as a convex minimization problem under suitable assumptions. We discuss several important examples of robust formulations, in particular by defining ambiguity sets based on $\varphi$-divergences and the Wasserstein metric.We also propose an efficient algorithm for solving the corresponding convex optimization problems involving complex convex constraints. Through simulation examples, we demonstrate that this algorithm scales well on real data sets.","",""
3,"Kyemyung Park, T. Prüstel, Yong Lu, J. Tsang","Machine learning of stochastic gene network phenotypes",2019,"","","","",121,"2022-07-13 09:23:14","","10.1101/825943","","",,,,,3,1.00,1,4,3,"A recurrent challenge in biology is the development of predictive quantitative models because most molecular and cellular parameters have unknown values and realistic models are analytically intractable. While the dynamics of the system can be analyzed via computer simulations, substantial computational resources are often required given uncertain parameter values resulting in large numbers of parameter combinations, especially when realistic biological features are included. Simulation alone also often does not yield the kinds of intuitive insights from analytical solutions. Here we introduce a general framework combining stochastic/mechanistic simulation of reaction systems and machine learning of the simulation data to generate computationally efficient predictive models and interpretable parameter-phenotype maps. We applied our approach to investigate stochastic gene expression propagation in biological networks, which is a contemporary challenge in the quantitative modeling of single-cell heterogeneity. We found that accurate, predictive machine-learning models of stochastic simulation results can be constructed. Even in the simplest networks existing analytical schemes generated significantly less accurate predictions than our approach, which revealed interesting insights when applied to more complex circuits, including the extensive tunability of information propagation enabled by feedforward circuits and how even single negative feedbacks can utilize stochastic fluctuations to generate robust oscillations. Our approach is applicable beyond biology and opens up a new avenue for exploring complex dynamical systems.","",""
0,"","Runtime Verification for Critical Machine Learning Applications",2019,"","","","",122,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,0,3,"In the last decade, the application of Machine Learning (ML) has encountered an increasing interest in various applicative domains, especially for a wide panel of complex tasks (e.g. image-based pedestrian detection) classically performed by human operators (e.g. the driver). In ML, the objective is to synthesize an intended function (e.g., detect a pedestrian on an image) through a set of examples (images of road). The massive usage of such techniques has demonstrated its effectiveness way beyond other classical methods. Obviously, the designers of critical systems would like to benefit from the effectiveness of ML-based models mainly for complex image processing and model reduction. But, above effectiveness, the designers of critical systems must demonstrate that the obtained models are reasonably safe. Providing elements demonstrating the safety of a system is a classical issue addressed by various techniques tailored to the nature of the system, and covered by many safety standards (DO178C in aeronautics, ISO26262 in automative, IEC61508 in electronics, etc.). Nevertheless, the specificities of ML-based software disclose new safety threats that are not addressed by classical techniques. Despite very good results during training and testing of a ML software, it is not possible to provide sufficient guarantees that the training data set would be sufficient for expected real life situations, that during operational life the system may not face adversary situations (situation slightly different from training ones, but which lead to a complete different result of the ML, also called adversaries attacks), or that the distribution of situations may be different from the ones during training (distribution shift). All these threats are a major brake to ML deployment in safety critical applications. Most of works are focusing on the training data quality in order to increase robustness of the ML algorithm. However, to avoid overfitting, it is accepted that developing the dataset is limited, and a promising approach is to monitor the system at runtime, during operational life, in order to keep the system in a safe state, despite errors of the ML. A first approach inspired from fault tolerance and close to safety monitoring [3], is to adapt the simplex architecture to the monitoring of a neural controller, using a decision block able to detect an error and to switch from the neural controller to a high assurance controller (but less performant) [4]. Some works as [1] may be used to monitor the distance of input/ouput distribution during the exploitation vs the one observed during the learning phase and raise an alert when a “significant gap” is observed. Other works like [2], dedicated to Neural Networks, propose to collect neuron activation patterns during the learning phase and, thanks to an online monitoring, detect the occurrence of an unknown activation pattern that may indicate an erroneous prediction. All these works are ongoing, with very preliminary results, and actually no safety model is integrated in such proposals. We propose in this post-doc to specify, implement and verify a new runtime verification approach for ML, an adversarial runtime monitor. This approach is based on adversaries generated at runtime, and used to assess if the ML maybe fooled in an unsafe state. This might lead the monitor to detect if the ML is in a potential unsafe erroneous state, or in a potential erroneous state but safe. Once such a monitor would be designed, we also plan to use formal methods (verification) to prove the correctness of the monitor. This work will be applied to a case study, a ML software for drone collision avoidance studied and deployed in the context of the Delta project.","",""
1,"Patrick F. Riley, Samir V. Deshpande","Machine learning based spectral interpretation in chemical detection",2019,"","","","",123,"2022-07-13 09:23:14","","10.1117/12.2518929","","",,,,,1,0.33,1,2,3,"Increasingly the design of chemical detection alarm algorithms to alert Soldiers of danger grows more complex as new threats emerge. These algorithms need to be robust enough to prevent false alarms to interferents and sensitive enough to alarm to the incredibly small doses that could prove lethal. The design of these algorithms have left a plethora of data that can be leveraged and utilized in a variety of machine learning (ML) techniques. ML is a field of computer science that uses a set of programming and statistical techniques to enable computers to “learn” from input data without being explicitly programmed. Presented is an application of ML to change two independent fielded chemical detectors into an orthogonal system to improve detection algorithms. The approach models the data from an ion-mobility spectrometer (IMS) and a photoionization detector containing electrochemical sensors (PIDECS) to train a ML model (MLA). The semi-supervised MLA is trained using a supervised learning data set, composed of partially labeled data from the heterogeneous instruments, and then fine-tuned using an unsupervised learning algorithm. The MLA correctly identifies two chemical species with over-lapping IMS detection windows. ML can be utilized to improve the ability of currently fielded detectors or future devices to accurately label chemical unknowns given the parameters of detection. The techniques discussed here presents a starting point for improving current and future alarm algorithms with ML.","",""
1,"Andreas Wunsch, T. Liesch, S. Broda","Uncover Similarities of Groundwater Dynamics with Machine Learning based Hydrograph Clustering (Oral Talk IN43A-07)",2019,"","","","",124,"2022-07-13 09:23:14","","","","",,,,,1,0.33,0,3,3,"Understanding and characterizing groundwater system properties is of great importance to develop sustainable groundwater management strategies. For this purpose, groundwater hydrographs are a valuable source of knowledge, since they contain information about system properties (e.g. aquifer type), artificial (e.g. withdrawal/infiltration) and natural environmental factors (e.g. groundwater-streamflow interaction). Such factors interact and superimpose temporally and spatially, which makes determining the individual contributions a challenging task. However, understanding spatial dynamics patterns is a precious source of information for this purpose. Generally, in many regions, large amounts of groundwater data with high resolution in time and space are available but lack an adequate set of tools for analysis. Data driven models are possibly suited to fill in this gap. We developed a machine learning based ensemble-modelling approach to characterize and cluster groundwater hydrographs on regional scale according to their dynamics. We apply feature-based clustering to reduce data quality requirements and to improve exploitation of heterogeneous datasets. Such features describe hydrograph dynamics and serve as surrogates for clustering based on Self-Organizing-Maps and DS2L-SOM-enrichment for cluster determination. Ensemble modeling assures highly robust cluster results, even for real world observational networks undergoing changes. The test area of the method is the Upper Rhine Graben in Germany/France, using more than 1800 weekly sampled hydrographs in the period of 1986 to 2016. The majority shows lengths of almost 30 years, minimum length is six years. Results show that our approach is capable to identify homogeneous groups of hydrograph dynamics. The resulting clusters showed both known and unknown patterns, of which some correspond to certain environmental factors. However, we also discovered new patterns with unknown origin, which need further examination. Possible application of the found groundwater patterns could be for example regional groundwater forecasting by selecting and predicting representative group members. By adapting the describing features, this data-driven method is easily transferrable to other time-series-clustering frameworks.","",""
1,"Vincent Dangla, Christian Soize, Guilherme Cunha, M. Kassem, A. Mosson, B. V. D. Nieuwenhof","A probabilistic learning on manifolds as a new tool in machine learning and data science with applications in computational mechanics",2019,"","","","",125,"2022-07-13 09:23:14","","","","",,,,,1,0.33,0,6,3,"In Machine Learning (generally devoted to big-data case), the predictive learning (or the supervised learning) approach consists in identifying/learning a random mapping F: w↦ q = F(w), in which the parameters vector w (input) is modelled by a random vector W with known probability distribution Pw(dw) and where the vector of quantities of interest q (outputs) is the non-Gaussian random variable Q = F(W) = f(W,U) whose probability distribution is unknown, given an initial dataset (or training set) DN = {(wj,qj), j=1,…N} of N independent realizations of random vector (W,Q). The measurable mapping f is deterministic and U is a random vector whose probability distribution is known. The approach of probabilistic learning on manifold (recently introduced) will be presented, which allows for constructing a generator of an estimation of the joint probability distribution PW,Q(dw,dq; N) using only DN, which completely characterizes random mapping F. In this framework, novel computational statistical tools will be presented for the small-data challenge for which N is relatively small and consequently, is not sufficient large for constructing converged statistical estimates. In particular, we will present (1) the identification of the optimal independent component partition of the non-Gaussian random vector, (2) the learning from DN for which additional information is available based either on a nonparametric Bayesian approach or on Information Theory. Several applications will be presented such as, the identification of non-Gaussian random fields for random media, the Bayes inference with probabilistic learning, the robust design of an implant in a biological tissue at mesoscale, the nonparametric model-form uncertainties (i) in nonlinear solid dynamics applied a to MEMS and (ii) in nonlinear computational fluid dynamics applied to a Scramjet, nonconvex optimization under uncertainties.    [1] C. Soize, R. Ghanem, Data-driven probability concentration and sampling on manifold, Journal of Computational Physics, 321, 242-258 (2016).  [2] C. Soize, Optimal partition in terms of independent random vectors of any non-Gaussian vector defined by a set of realizations, SIAM/ASA Journal on Uncertainty Quantification, 5(1), 176-211 (2017).  [3] R. Ghanem, C. Soize, Probabilistic nonconvex constrained optimization with fixed number of function evaluations, International Journal for Numerical Methods in Engineering, 113(4), 719-741 (2018).  [4] C. Soize, Design optimization under uncertainties of a mesoscale implant in biological tissues using a probabilistic learning algorithm, Computational Mechanics, 62(3), 477-497 (2018).  [5] C. Soize, C. Farhat, Probabilistic learning for model-form uncertainties in nonlinear computational mechanics, International Journal for Numerical Methods in Engineering, Accepted 24 October 2018.  [6] C. Soize, R. Ghanem, C. Safta, X. Huan, Z.P. Vane, J. Oefelein, G. Lacaze, H.N. Najm, Enhancing model predictability for a scramjet using probabilistic learning on manifold, AIAA Journal, Accepted 13 September 2018.","",""
0,"E. Foca","Machine Learning Solutions for Process Control in Semiconductor Manufacturing",2019,"","","","",126,"2022-07-13 09:23:14","","10.1109/VLSI-TSA.2019.8804681","","",,,,,0,0.00,0,1,3,"Reliable and robust process control solutions are pivotal in the era of advanced technology nodes where the stochastic effects dominate the pattern characteristics. The key ingredients ofthe modern process control solutions are huge amount of data and smart data analytics algorithms. The state-of-the-art machine learning methods allow screening large amounts of data for statistically relevant, but so far unknown and unexpected correlations, hence providing new options for process optimizations.","",""
0,"Benjamin I Goodlich","Machine learning algorithms for the automatic detection and classification of physical activity in children with cerebral palsy who use mobility aids for ambulation",2019,"","","","",127,"2022-07-13 09:23:14","","10.25904/1912/2123","","",,,,,0,0.00,0,1,3,"Background: Literature related to objective measurement of habitual physical activity (PA) disproportionately over represents children with Cerebral Palsy (CP) who are ambulant. Consequently, it is unknown if methods used to examine PA, such as machine learning models built on accelerometer data, are able to accurately detect PA in children with CP who use mobility aids for ambulation. Objective: To develop and test machine learning models used for the automatic detection and classification of PA type in children with CP who use mobility aids for ambulation. Methods: Eleven children and adolescents with CP, age 11±3yrs (range 6-16yrs); six females; Gross Motor Function Classification System (GMFCS) III: n=5 and IV: n=6 participated. Participants completed six PA trials of increasing intensity while wearing an ActiGraph GT3X+ accelerometer on the wrist, hip and thigh. PA trials included: supine rest, seated colouring, seated ball throwing, overground walking with a mobility aid, wheelchair propulsion and riding on a modified tricycle. Decision Tree (DT), Support Vector Machine (SVM) and Random Forest (RF) classifiers were trained on 40 features in the vector magnitude of raw acceleration signal using 5s non-overlapping windows. Performance was evaluated using leave-one-subject-out cross validation. Comparisons of performance were subsequently made between all single placement models, all combinations of two placement models, and models trained on data from all three placements. Results: The best performing single-placement model was a RF classifier trained on wrist features, yielding an overall prediction accuracy of 79%. The best performing model built on a combination of two placements was a RF classifier trained on wrist and hip features, yielding an overall prediction accuracy of 92%. The combinations of multiple accelerometer placements were significantly more accurate than a single monitor alone. Models based on the combination of two placements were more accurate than those based on a combination of three placements; however, this difference was not significant. Limitations: The PA protocol consisted of structured activity trials performed in a controlled, clinical environment. Thus, the performance of the models under free living conditions require further investigation. The sample size used may limit the generalisability and robustness of the findings given the variability in movement patterns of the population of interest. Conclusions: Machine learning techniques afford robust and accurate classification of PA in children with CP who use mobility aids for ambulation (GMFCS III & IV) within a laboratory setting. This is significant, as it is the first study to develop methods for objectively measuring habitual PA in this population. Future research should investigate performance of the methods utilised in the current project in children engaged in free living conditions.","",""
0,"A. Vose, J. Balma, Damon Farnsworth, Kaylie Anderson, Y. Peterson","PharML.Bind: Pharmacologic Machine Learning for Protein-Ligand Interactions",2019,"","","","",128,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,5,3,"Is it feasible to create an analysis paradigm that can analyze and then accurately and quickly predict known drugs from experimental data? PharML.Bind is a machine learning toolkit which is able to accomplish this feat. Utilizing deep neural networks and big data, PharML.Bind correlates experimentally-derived drug affinities and protein-ligand X-ray structures to create novel predictions. The utility of PharML.Bind is in its application as a rapid, accurate, and robust prediction platform for discovery and personalized medicine. This paper demonstrates that graph neural networks (GNNs) can be trained to screen hundreds of thousands of compounds against thousands of targets in minutes, a vastly shorter time than previous approaches. This manuscript presents results from training and testing using the entirety of BindingDB after cleaning; this includes a test set with 19,708 X-ray structures and 247,633 drugs, leading to 2,708,151 unique protein-ligand pairings. PharML.Bind achieves a prodigious 98.3% accuracy on this test set in under 25 minutes. PharML.Bind is premised on the following key principles: 1) speed and a high enrichment factor per unit compute time, provided by high-quality training data combined with a novel GNN architecture and use of high-performance computing resources, 2) the ability to generalize to proteins and drugs outside of the training set, including those with unknown active sites, through the use of an active-site-agnostic GNN mapping, and 3) the ability to be easily integrated as a component of increasingly-complex prediction and analysis pipelines. PharML.Bind represents a timely and practical approach to leverage the power of machine learning to efficiently analyze and predict drug action on any practical scale and will provide utility in a variety of discovery and medical applications.","",""
0,"Sarah Laroui, E. Debreuve, X. Descombes, F. Villalba, F. Villiers, A. Vernay","Machine-learning assisted phenotyping: From fungal morphology to mode of action hypothesis",2019,"","","","",129,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,6,3,"Beyond growth inhibition, fungicides can also trigger specific morphological modifications visualized under transmitted light microscopy. These morphological changes result from the activity of a given compound via the inhibition of a molecular target, commonly named as its mode of action (MoA). We are hence able to classify different molecules into their respective MoA by observing their phenotypic signature, and even to detect new MoA with unknown phenotypic effect for further deconvolution. The aim of the presented work is to develop a robust method for automated recognition and classification of these phenotypic signatures in order to lead to a Mode of Action hypothesis. We compare two machine-learning methods (Random forest and Convolutional Neural Network) for direct processing of images generated on the grey mold Botrytis cinerea subjected to different antifungal molecules. © Bayer | Abteilung | Verfasser | Datum","",""
34,"A. Sargolzaei, C. Crane, Alireza Abbaspour, S. Noei","A Machine Learning Approach for Fault Detection in Vehicular Cyber-Physical Systems",2016,"","","","",130,"2022-07-13 09:23:14","","10.1109/ICMLA.2016.0112","","",,,,,34,5.67,9,4,6,"A network of vehicular cyber-physical systems (VCPSs) can use wireless communications to interact with each other and the surrounding environment to improve transportation safety, mobility, and sustainability. However, cloud-oriented architectures are vulnerable to cyber attacks, which may endanger passenger and pedestrian safety and privacy, and cause severe property damage. For instance, a hacker can use message falsification attack to affect functionality of a particular application in a platoon of VCPSs. In this paper, a neural network-based fault detection technique is applied to detect and track fault data injection attacks on the cooperative adaptive cruise control layer of a platoon of connected vehicles in real time. A decision support system was developed to reduce the probability and severity of any consequent accident. A case study with its design specifications is demonstrated in detail. The simulation results show that the proposed method can improve system reliability, robustness, and safety.","",""
6,"Changyuan Chen, M. Tello Ruiz, E. Lataire, G. Delefortrie, Marc Mansuy, Tianlong Mei, M. Vantorre","Ship Manoeuvring Model Parameter Identification Using Intelligent Machine Learning Method and the Beetle Antennae Search Algorithm",2019,"","","","",131,"2022-07-13 09:23:14","","10.1115/omae2019-95565","","",,,,,6,2.00,1,7,3,"  In order to identify more accurately and efficiently the unknown parameters of a ship motions model, a novel Nonlinear Least Squares Support Vector Machine (NLSSVM) algorithm, whose penalty factor and Radial Basis Function (RBF) kernel parameters are optimised by the Beetle Antennae Search algorithm (BAS), is proposed and investigated. Aiming at validating the accuracy and applicability of the proposed method, the method is employed to identify the linear and nonlinear parameters of the first-order nonlinear Nomoto model with training samples from numerical simulation and experimental data. Subsequently, the identified parameters are applied in predicting the ship motion. The predicted results illustrate that the new NLSSVM-BAS algorithm can be applied in identifying ship motion’s model, and the effectiveness is verified. Compared among traditional identification approaches with the proposed method, the results display that the accuracy is improved. Moreover, the robust and stability of the NLSSVM-BAS are verified by adding noise in the training sample data.","",""
3,"Noureldin Laban, B. Abdellatif, H. M. Ebeid, H. Shedeed, M. Tolba","Machine Learning for Enhancement Land Cover and Crop Types Classification",2018,"","","","",132,"2022-07-13 09:23:14","","10.1007/978-3-030-02357-7_4","","",,,,,3,0.75,1,5,4,"","",""
3,"Nicolas Känzig, Roland Meier, L. Gambazzi, Vincent Lenders, L. Vanbever","Machine Learninģ-based Detection of C&C Channels with a Focus on the Locked Shields Cyber Defense Exercise",2019,"","","","",133,"2022-07-13 09:23:14","","10.23919/CYCON.2019.8756814","","",,,,,3,1.00,1,5,3,"The diversity of applications and devices in enterprise networks combined with large traffic volumes make it inherently challenging to quickly identify malicious traffic. When incidents occur, emergency response teams often lose precious time in reverse-engineering the network topology and configuration before they can focus on malicious activities and digital forensics. In this paper, we present a system that quickly and reliably identifies Command and Control (C&C) channels without prior network knowledge. The key idea is to train a classifier using network traffic from attacks that happened in the past and use it to identify C&C connections in the current traffic of other networks. Specifically, we leverage the fact that - while benign traffic differs - malicious traffic bears similarities across networks (e.g., devices participating in a botnet act in a similar manner irrespective of their location). To ensure performance and scalability, we use a random forest classifier based on a set of computationally-efficient features tailored to the detection of C&C traffic. In order to prevent attackers from outwitting our classifier, we tune the model parameters to maximize robustness. We measure high resilience against possible attacks - e.g., attempts to camouflaging C&C flows as benign traffic - and packet loss during the inference. We have implemented our approach and we show its practicality on a real use case: Locked Shields, the world's largest cyber defense exercise. In Locked Shields, defenders have limited resources to protect a large, heterogeneous network against unknown attacks. Using recorded datasets (from 2017 and 2018) from a participating team, we show that our classifier is able to identify C&C channels with 99% precision and over 90% recall in near real time and with realistic resource requirements. If the team had used our system in 2018, it would have discovered 10 out of 12 C&C servers in the first hours of the exercise.","",""
1,"Y. Shiraishi, K. Chiba, A. Okada","SF3B1ness score: screening SF3B1 mutation status from over 60,000 transcriptomes based on a machine learning approach",2019,"","","","",134,"2022-07-13 09:23:14","","10.1101/572834","","",,,,,1,0.33,0,3,3,"In precision oncology, genomic evidence is used to determine the optimal treatment for each patient. However, identification of somatic mutations from genome sequencing data is often technically difficult and functional significance of somatic mutations is inconclusive in many cases. In this paper, to seek for an alternative approach, we tackle the problem of predicting functional mutations from transcriptome sequencing data. Focusing on SF3B1, a key splicing factor gene, we develop SF3B1ness score for classifying functional mutation status using a combination of Naive Bayes classifier and zero-inflated beta-binomial modeling (R package is available at (https://github.com/friend1WS/SF3B1ness). Using 8,992 TCGA exome and RNA sequencing data for evaluation, we show that the classifier based on SF3B1ness score is able to (1) attain very high precision (>93%) and sensitivity (>95%), (2) rescue several somatic mutations not identified by exome sequence analysis especially due to low variant allele frequencies, and (3) successfully measure functional importance for somatic mutation whose significance has been unknown. Furthermore, to demonstrate that the SF3B1ness score is highly robust and can be extensible to the cohorts outside training data, we performed a functional SF3B1 mutation screening on 51,577 additional transcriptome sequencing data. We have detected 135 samples with putative SF3B1 functional mutations including those that are rarely registered in the somatic mutation database (e.g., G664C, L747W, and R775G). Moreover, we could identify two cases with SF3B1 mutations from normal tissues, implying that SF3B1ness score can be used for detecting clonal hematopoiesis.","",""
1,"B. Luber, F. Sorribes-Palmer, G. Müller, L. Pietsch, K. Six","On-Board Wheel Profile Classification Based on Vehicle Dynamics - From Physical Effects to Machine Learning",2019,"","","","",135,"2022-07-13 09:23:14","","10.1007/978-3-030-38077-9_13","","",,,,,1,0.33,0,5,3,"","",""
57,"Alessandro Lumino, E. Polino, A. S. Rab, G. Milani, N. Spagnolo, N. Wiebe, F. Sciarrino","Experimental Phase Estimation Enhanced By Machine Learning",2017,"","","","",136,"2022-07-13 09:23:14","","10.1103/PhysRevApplied.10.044033","","",,,,,57,11.40,8,7,5,"Phase estimation protocols provide a fundamental benchmark for the field of quantum metrology. The latter represents one of the most relevant applications of quantum theory, potentially enabling the capability of measuring unknown physical parameters with improved precision over classical strategies. Within this context, most theoretical and experimental studies have focused on determining the fundamental bounds and how to achieve them in the asymptotic regime where a large number of resources is employed. However, in most applications it is necessary to achieve optimal precisions by performing only a limited number of measurements. To this end, machine learning techniques can be applied as a powerful optimization tool. Here, we implement experimentally single-photon adaptive phase estimation protocols enhanced by machine learning, showing the capability of reaching optimal precision after a small number of trials. In particular, we introduce a new approach for Bayesian estimation that exhibit best performances for very low number of photons N. Furthermore, we study the resilience to noise of the tested methods, showing that the optimized Bayesian approach is very robust in the presence of imperfections. Application of this methodology can be envisaged in the more general multiparameter case, that represents a paradigmatic scenario for several tasks including imaging or Hamiltonian learning.","",""
1,"A. Bartschat, Tim Unger, Tim Scherr, J. Stegmaier, R. Mikut, M. Reischl","Robustness of Deep Learning Architectures with Respect to Training Data Variation",2018,"","","","",137,"2022-07-13 09:23:14","","","","",,,,,1,0.25,0,6,4,"The use of deep neural networks (DNN) revolutionized machine learning tasks like image classification and segmentation [6]. However, DNNs are black box models developed with the aim to generalize a given training set and thus to process unknown data. Due to the high amount of model parameters, it is yet impossible to fully understand the decision making process. The identification of parameters with poor generalization is hard and manual improvement is impossible. Thus, robustness is subject of consideration.","",""
0,"Serkan Tokgoz, Erik Jonsson","Speaker ID on Apollo 11 corpus: A Study using different Machine Learning Models",2019,"","","","",138,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,2,3,"The main goal here is to match a voice sample from an unknown speaker to one of several labeled speaker models since speech is easily produced. For the feature extraction, Mel Frequency Cepstrum Coefficients will be used since it is one of the most common features used for speaker recognition. Before extracting the features, we will do pre-processing such as Voice Activity detection to ignore unvoiced parts of the speech. For classification and objective comparison, K-Nearest Neighborhood (KNN), Convolutional Neural Network (CNN) and I-vectors/PLDA results will be shared. The dataset used for the project is FEARLESS STEPS that consists of 10 hours of digitized recordings of the Apollo 11 Space Mission. These recordings were digitized by the Centre of Robust Speech Systems (CRSS) of The University of Texas at Dallas. It was typically used for speech activity detection, sentiment analysis and speaker recognition. In the research, there were a few challenges that were met using methods. Our main focus will be detecting speech parts of the speech signals and classifying the respective speakers in the given time frame. Keywords—MFCC, KNN, CNN, i-Vector, PLDA","",""
3,"Wei Huang, Jing Zhao, Guokuan Yu, P. Wong","Intelligent Vibration Control for Semiactive Suspension Systems Without Prior Knowledge of Dynamical Nonlinear Damper Behaviors Based on Improved Extreme Learning Machine",2020,"","","","",139,"2022-07-13 09:23:14","","10.1109/tmech.2020.3031840","","",,,,,3,1.50,1,4,2,"Aiming at enhancing the vehicle comfort and handling performances, this article concerns with the development of the vibration control method for the semiactive suspension systems installed with electrohydraulic dampers. To reject the external disturbances induced by the irregular road profile, sliding mode was intensively investigated for vibration control owing to its robust feature of insensitivity to the parametric uncertainty and external disturbances. However, the unknown nonlinearity of damper behaviors leads to model mismatch to cause the high-frequency switching of sliding-mode controllers. Consequently, the severe chattering phenomenon produces. Although saturated function is available to alleviate the chattering problem of sliding-mode control, there is always a tradeoff problem between tracking accuracy and chattering suppression. To solve this problem, this study provides a new intelligent robust control method for simultaneous improvements of tracking accuracy and chattering suppression. Given the computational efficiency, an improved extreme learning machine (ELM) is proposed to intelligently approximate and compensate the unmodeled dynamics with unknown nonlinearity to restrain the chattering problem, where a new adaptive learning law is designed in the premise of Lyapunov stability. To validate the effectiveness and efficiency of the proposed ELM-based robust control, a quarter-car test rig was set up for the hardware-in-the-loop test. Experimental results show that the proposed controller outperforms the sliding-mode controller with saturated function in depressing the sprung mass acceleration and tire deflection, showing its significance in both control performance enhancement and chattering elimination.","",""
5,"Badong Chen, Lei Xing, Haiquan Zhao, S. Du, J. Príncipe","Effects of Outliers on the Maximum Correntropy Estimation: A Robustness Analysis",2021,"","","","",140,"2022-07-13 09:23:14","","10.1109/TSMC.2019.2931403","","",,,,,5,5.00,1,5,1,"Recently, maximum correntropy criterion (MCC) has been widely and successfully used in robust signal processing and machine learning, in which the correntropy is maximized instead of minimizing the popular mean square error (MSE) to improve the robustness with respect to outliers or impulsive noises. A lot of efforts have been devoted to derive different adaptive algorithms under MCC, but to date, little insight has been gained as to how the MCC solution will be influenced by outliers. In this paper, we investigate this problem and our focus is mainly on the parameter estimation of a simple linear errors-in-variables (EIVs) model with scalar variables. Under some conditions, we derive an upper bound on the absolute value of the estimation error and show that the MCC solution can get very close to the true value of the unknown parameter even with arbitrarily large outliers in both the input and output variables. Illustrative examples are provided to verify and clarify the theory.","",""
1,"I. Haq, Z. Khan, Arshad Ahmad, B. Hayat, Asif Khan, Yeeun Lee, Ki-Il Kim","Evaluating and Enhancing the Robustness of Sustainable Neural Relationship Classifiers Using Query-Efficient Black-Box Adversarial Attacks",2021,"","","","",141,"2022-07-13 09:23:14","","10.3390/SU13115892","","",,,,,1,1.00,0,7,1,"Neural relation extraction (NRE) models are the backbone of various machine learning tasks, including knowledge base enrichment, information extraction, and document summarization. Despite the vast popularity of these models, their vulnerabilities remain unknown; this is of high concern given their growing use in security-sensitive applications such as question answering and machine translation in the aspects of sustainability. In this study, we demonstrate that NRE models are inherently vulnerable to adversarially crafted text that contains imperceptible modifications of the original but can mislead the target NRE model. Specifically, we propose a novel sustainable term frequency-inverse document frequency (TFIDF) based black-box adversarial attack to evaluate the robustness of state-of-the-art CNN, CGN, LSTM, and BERT-based models on two benchmark RE datasets. Compared with white-box adversarial attacks, black-box attacks impose further constraints on the query budget; thus, efficient black-box attacks remain an open problem. By applying TFIDF to the correctly classified sentences of each class label in the test set, the proposed query-efficient method achieves a reduction of up to 70% in the number of queries to the target model for identifying important text items. Based on these items, we design both character- and word-level perturbations to generate adversarial examples. The proposed attack successfully reduces the accuracy of six representative models from an average F1 score of 80% to below 20%. The generated adversarial examples were evaluated by humans and are considered semantically similar. Moreover, we discuss defense strategies that mitigate such attacks, and the potential countermeasures that could be deployed in order to improve sustainability of the proposed scheme.","",""
33,"Ved P. Kafle, Y. Fukushima, P. Martinez-Julia, T. Miyazawa","Consideration On Automation of 5G Network Slicing with Machine Learning",2018,"","","","",142,"2022-07-13 09:23:14","","10.23919/ITU-WT.2018.8597639","","",,,,,33,8.25,8,4,4,"Machine learning has the capability to provide simpler solutions to complex problems by analyzing a huge volume of data in a short time, learning for adapting its functionality to dynamically changing environments, and predicting near future events with reasonably good accuracy. The 5G communication networks are getting complex due to emergence of unprecedentedly huge number of new connected devices and new types of services. Moreover, the requirements of creating virtual network slices suitable to provide optimal services for diverse users and applications are posing challenges to the efficient management of network resources, processing information about a huge volume of traffic, staying robust against all potential security threats, and adaptively adjustment of network functionality for time-varying workload. In this paper, we introduce about the envisioned 5G network slicing and elaborate the necessity of automation of network functions for the design, construction, deployment, operation, control and management of network slices. We then revisit the machine learning techniques that can be applied for the automation of network functions. We also discuss the status of artificial intelligence and machine learning related activities being progressed in standards development organizations and industrial forums.","",""
37,"P. Athamanolap, V. Parekh, S. I. Fraley, Vatsal Agarwal, D. J. Shin, M. Jacobs, Tza-Huei Wang, Samuel Yang","Trainable High Resolution Melt Curve Machine Learning Classifier for Large-Scale Reliable Genotyping of Sequence Variants",2014,"","","","",143,"2022-07-13 09:23:14","","10.1371/journal.pone.0109094","","",,,,,37,4.63,5,8,8,"High resolution melt (HRM) is gaining considerable popularity as a simple and robust method for genotyping sequence variants. However, accurate genotyping of an unknown sample for which a large number of possible variants may exist will require an automated HRM curve identification method capable of comparing unknowns against a large cohort of known sequence variants. Herein, we describe a new method for automated HRM curve classification based on machine learning methods and learned tolerance for reaction condition deviations. We tested this method in silico through multiple cross-validations using curves generated from 9 different simulated experimental conditions to classify 92 known serotypes of Streptococcus pneumoniae and demonstrated over 99% accuracy with 8 training curves per serotype. In vitro verification of the algorithm was tested using sequence variants of a cancer-related gene and demonstrated 100% accuracy with 3 training curves per sequence variant. The machine learning algorithm enabled reliable, scalable, and automated HRM genotyping analysis with broad potential clinical and epidemiological applications.","",""
94,"Qiang Zhu, A. Samanta, Bingxi Li, R. Rudd, T. Frolov","Predicting phase behavior of grain boundaries with evolutionary search and machine learning",2017,"","","","",144,"2022-07-13 09:23:14","","10.1038/s41467-018-02937-2","","",,,,,94,18.80,19,5,5,"","",""
0,"Svitlana Volkova, Dustin L. Arendt, Emily Saldanha, M. Glenski, Ellyn Ayton, Joseph A. Cottam, Sinan G. Aksoy, Brett Jefferson, Karthnik Shrivaram","Explaining and predicting human behavior and social dynamics in simulated virtual worlds: reproducibility, generalizability, and robustness of causal discovery methods",2021,"","","","",145,"2022-07-13 09:23:14","","10.1007/s10588-021-09351-y","","",,,,,0,0.00,0,9,1,"","",""
0,"P. Rasouli, Ingrid Chieh Yu","Analyzing and Improving the Robustness of Tabular Classifiers using Counterfactual Explanations",2021,"","","","",146,"2022-07-13 09:23:14","","10.1109/ICMLA52953.2021.00209","","",,,,,0,0.00,0,2,1,"Recent studies have revealed that Machine Learning (ML) models are vulnerable to adversarial perturbations. Such perturbations can be intentionally or accidentally added to the original inputs, evading the classifier’s behavior to misclassify the crafted samples. A widely-used solution is to retrain the model using data points generated by various attack strategies. However, this creates a classifier robust to some particular evasions and can not defend unknown or universal perturbations. Counterfactual explanations are a specific class of post-hoc explanation methods that provide minimal modification to the input features in order to obtain a particular outcome from the model. In addition to the resemblance of counterfactual explanations to the universal perturbations, the possibility of generating instances from specific classes makes such approaches suitable for analyzing and improving the model’s robustness. Rather than explaining the model’s decisions in the deployment phase, we utilize the distance information obtained from counterfactuals and propose novel metrics to analyze the robustness of tabular classifiers. Further, we introduce a decision boundary modification approach using customized counterfactual data points to improve the robustness of the models without compromising their accuracy. Our framework addresses the robustness of black-box classifiers in the tabular setting, which is considered an under-explored research area. Through several experiments and evaluations, we demonstrate the efficacy of our approach in analyzing and improving the robustness of black-box tabular classifiers.","",""
101,"G. Lecu'e, M. Lerasle","Robust machine learning by median-of-means: Theory and practice",2017,"","","","",147,"2022-07-13 09:23:14","","10.1214/19-AOS1828","","",,,,,101,20.20,51,2,5,"We introduce new estimators for robust machine learning based on median-of-means (MOM) estimators of the mean of real valued random variables. These estimators achieve optimal rates of convergence under minimal assumptions on the dataset. The dataset may also have been corrupted by outliers on which no assumption is granted. We also analyze these new estimators with standard tools from robust statistics. In particular, we revisit the concept of breakdown point. We modify the original definition by studying the number of outliers that a dataset can contain without deteriorating the estimation properties of a given estimator. This new notion of breakdown number, that takes into account the statistical performances of the estimators, is non-asymptotic in nature and adapted for machine learning purposes. We proved that the breakdown number of our estimator is of the order of (number of observations)*(rate of convergence). For instance, the breakdown number of our estimators for the problem of estimation of a d-dimensional vector with a noise variance sigma^2 is sigma^2d and it becomes sigma^2 s log(d/s) when this vector has only s non-zero component. Beyond this breakdown point, we proved that the rate of convergence achieved by our estimator is (number of outliers) divided by (number of observation).  Besides these theoretical guarantees, the major improvement brought by these new estimators is that they are easily computable in practice. In fact, basically any algorithm used to approximate the standard Empirical Risk Minimizer (or its regularized versions) has a robust version approximating our estimators. As a proof of concept, we study many algorithms for the classical LASSO estimator. A byproduct of the MOM algorithms is a measure of depth of data that can be used to detect outliers.","",""
0,"Mohamed Abdelhack, Jiaming Zhang, Sandhya Tripathi, B. Fritz, M. Avidan, Yixin Chen, C. King","A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues",2021,"","","","",148,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,7,1,"Data quality is a common problem in machine learning, especially in high-stakes settings such as healthcare. Missing data affects accuracy, calibration, and feature attribution in complex patterns. Developers often train models on carefully curated datasets to minimize missing data bias; however, this reduces the usability of such models in production environments, such as real-time healthcare records. Making machine learning models robust to missing data is therefore crucial for practical application. While some classifiers naturally handle missing data, others, such as deep neural networks, are not designed for unknown values. We propose a novel neural network modification to mitigate the impacts of missing data. The approach is inspired by neuromodulation that is performed by biological neural networks. Our proposal replaces the fixed weights of a fully-connected layer with a function of an additional input (reliability score) at each input, mimicking the ability of cortex to upand down-weight inputs based on the presence of other data. The modulation function is jointly learned with the main task using a multi-layer perceptron. We tested our modulating fully connected layer on multiple classification, regression, and imputation problems, and it either improved performance or generated comparable performance to conventional neural network architectures concatenating reliability to the inputs. Models with modulating layers were more robust against degradation of data quality by introducing additional missingness at evaluation time. These results suggest that explicitly accounting for reduced information quality with a modulating fully connected layer can enable the deployment of artificial intelligence systems in real-time settings. ∗Author has since moved to the Krembil Center for Neuroinformatics, Toronto, ON, Canada Preprint. Under review. ar X iv :2 10 7. 08 57 4v 1 [ cs .L G ] 1 9 Ju l 2 02 1","",""
111,"T. Cordier, P. Esling, F. Lejzerowicz, J. Visco, Amine Ouadahi, Catarina I M Martins, T. Cedhagen, J. Pawlowski","Predicting the Ecological Quality Status of Marine Environments from eDNA Metabarcoding Data Using Supervised Machine Learning.",2017,"","","","",149,"2022-07-13 09:23:14","","10.1021/acs.est.7b01518","","",,,,,111,22.20,14,8,5,"Monitoring biodiversity is essential to assess the impacts of increasing anthropogenic activities in marine environments. Traditionally, marine biomonitoring involves the sorting and morphological identification of benthic macro-invertebrates, which is time-consuming and taxonomic-expertise demanding. High-throughput amplicon sequencing of environmental DNA (eDNA metabarcoding) represents a promising alternative for benthic monitoring. However, an important fraction of eDNA sequences remains unassigned or belong to taxa of unknown ecology, which prevent their use for assessing the ecological quality status. Here, we show that supervised machine learning (SML) can be used to build robust predictive models for benthic monitoring, regardless of the taxonomic assignment of eDNA sequences. We tested three SML approaches to assess the environmental impact of marine aquaculture using benthic foraminifera eDNA, a group of unicellular eukaryotes known to be good bioindicators, as features to infer macro-invertebrates based biotic indices. We found similar ecological status as obtained from macro-invertebrates inventories. We argue that SML approaches could overcome and even bypass the cost and time-demanding morpho-taxonomic approaches in future biomonitoring.","",""
12,"N. Khoa, M. M. Alamdari, T. Rakotoarivelo, Ali Anaissi, Yang Wang","Structural Health Monitoring Using Machine Learning Techniques and Domain Knowledge Based Features",2018,"","","","",150,"2022-07-13 09:23:14","","10.1007/978-3-319-90403-0_20","","",,,,,12,3.00,2,5,4,"","",""
5,"Stavros Pitoglou, Y. Koumpouros, Athanasios Anastasiou","Using Electronic Health Records and Machine Learning to Make Medical-Related Predictions from Non-Medical Data",2018,"","","","",151,"2022-07-13 09:23:14","","10.1109/ICMLDE.2018.00021","","",,,,,5,1.25,2,3,4,"Objectives: Administrative HIS (Hospital Information System) and EHR (Electronic Health Record) data are characterized by lower privacy sensitivity, thus easier portability and handling, as well as higher information quality. In this paper we test the hypothesis that the application of machine learning techniques on data of this nature can be used to address prediction/forecasting problems in the Health IT domain. The novelty of this approach consists in that medical data (test results, diagnoses, doctors’ notes etc.) are not included in the predictors’ dataset. Moreover, there is limited need for separation of patient cohorts based on specific health conditions. Methods: We experiment with the prediction of the probability of early readmission at the time of a patient’s discharge. We extract real HIS data and perform data processing techniques. We then apply a series of machine learning algorithms (Logistic Regression, Support Vector Machine, Gaussian Naïve Bayes, K-Nearest Neighbors and Deep Multilayer Neural Network) and measure the performance of the emergent models. Results: All applied methods performed well above random guessing, even with minimal hyper-parameter tuning. Conclusions: Given that the experiments provide evidence in favor of the underlying hypothesis, future experimentation on more fine-tuned (thus more robust) models could result in applications suited for productive environments.","",""
26,"Theja Tulabandhula, C. Rudin","Robust Optimization using Machine Learning for Uncertainty Sets",2014,"","","","",152,"2022-07-13 09:23:14","","","","",,,,,26,3.25,13,2,8,"Our goal is to build robust optimization problems for making decisions based on complex data from the past. In robust optimization (RO) generally, the goal is to create a policy for decision-making that is robust to our uncertainty about the future. In particular, we want our policy to best handle the the worst possible situation that could arise, out of an uncertainty set of possible situations. Classically, the uncertainty set is simply chosen by the user, or it might be estimated in overly simplistic ways with strong assumptions; whereas in this work, we learn the uncertainty set from data collected in the past. The past data are drawn randomly from an (unknown) possibly complicated high-dimensional distribution. We propose a new uncertainty set design and show how tools from statistical learning theory can be employed to provide probabilistic guarantees on the robustness of the policy.","",""
524,"S. Raschka","Python Machine Learning",2015,"","","","",153,"2022-07-13 09:23:14","","","","",,,,,524,74.86,524,1,7,"Unlock deeper insights into Machine Leaning with this vital guide to cutting-edge predictive analyticsAbout This BookLeverage Python's most powerful open-source libraries for deep learning, data wrangling, and data visualizationLearn effective strategies and best practices to improve and optimize machine learning systems and algorithmsAsk and answer tough questions of your data with robust statistical models, built for a range of datasetsWho This Book Is ForIf you want to find out how to use Python to start answering critical questions of your data, pick up Python Machine Learning whether you want to get started from scratch or want to extend your data science knowledge, this is an essential and unmissable resource.What You Will LearnExplore how to use different machine learning models to ask different questions of your dataLearn how to build neural networks using Keras and TheanoFind out how to write clean and elegant Python code that will optimize the strength of your algorithmsDiscover how to embed your machine learning model in a web application for increased accessibilityPredict continuous target outcomes using regression analysisUncover hidden patterns and structures in data with clusteringOrganize data using effective pre-processing techniquesGet to grips with sentiment analysis to delve deeper into textual and social media dataIn DetailMachine learning and predictive analytics are transforming the way businesses and other organizations operate. Being able to understand trends and patterns in complex data is critical to success, becoming one of the key strategies for unlocking growth in a challenging contemporary marketplace. Python can help you deliver key insights into your data its unique capabilities as a language let you build sophisticated algorithms and statistical models that can reveal new perspectives and answer key questions that are vital for success.Python Machine Learning gives you access to the world of predictive analytics and demonstrates why Python is one of the world's leading data science languages. If you want to ask better questions of data, or need to improve and extend the capabilities of your machine learning systems, this practical data science book is invaluable. Covering a wide range of powerful Python libraries, including scikit-learn, Theano, and Keras, and featuring guidance and tips on everything from sentiment analysis to neural networks, you'll soon be able to answer some of the most important questions facing you and your organization.Style and approachPython Machine Learning connects the fundamental theoretical principles behind machine learning to their practical application in a way that focuses you on asking and answering the right questions. It walks you through the key elements of Python and its powerful machine learning libraries, while demonstrating how to get to grips with a range of statistical models.","",""
0,"Miaomiao Gao, Xiao‐Zheng Jin, L. Ding","Robust Adaptive Fixed-time Trajectory Tracking Control of Manipulator based on Extreme Learning Machine",2020,"","","","",154,"2022-07-13 09:23:14","","10.23919/CCC50068.2020.9188544","","",,,,,0,0.00,0,3,2,"This paper mainly investigates the trajectory tracking control problems for manipulator systems with unknown dynamics and external disturbances. Firstly, an extreme learning machine (ELM) is adopt to compensate unknown dynamics of the manipulator. Then, an updating law is derived to ensure the convergence of ELM output wights. Besides, an indirect method is developed to avoid the potential singularity problem of the fixed-time sliding mode surface. Moreover, a robust adaptive controller is designed based on the outputs of ELM and sliding mode technique. By using the Lyapunov stability theory, the fixed-time convergence and stability of the adaptive control system can be guaranteed. Finally, simulation results is presented to show the efficiency of the proposed control structure with respect to different initial conditions.","",""
97,"T. Doster, A. Watnik","Machine learning approach to OAM beam demultiplexing via convolutional neural networks.",2017,"","","","",155,"2022-07-13 09:23:14","","10.1364/AO.56.003386","","",,,,,97,19.40,49,2,5,"Orbital angular momentum (OAM) beams allow for increased channel capacity in free-space optical communication. Conventionally, these OAM beams are multiplexed together at a transmitter and then propagated through the atmosphere to a receiver where, due to their orthogonality properties, they are demultiplexed. We propose a technique to demultiplex these OAM-carrying beams by capturing an image of the unique multiplexing intensity pattern and training a convolutional neural network (CNN) as a classifier. This CNN-based demultiplexing method allows for simplicity of operation as alignment is unnecessary, orthogonality constraints are loosened, and costly optical hardware is not required. We test our CNN-based technique against a traditional demultiplexing method, conjugate mode sorting, with various OAM mode sets and levels of simulated atmospheric turbulence in a laboratory setting. Furthermore, we examine our CNN-based technique with respect to added sensor noise, number of photon detections, number of pixels, unknown levels of turbulence, and training set size. Results show that the CNN-based demultiplexing method is able to demultiplex combinatorially multiplexed OAM modes from a fixed set with >99% accuracy for high levels of turbulence-well exceeding the conjugate mode demultiplexing method. We also show that this new method is robust to added sensor noise, number of photon detections, number of pixels, unknown levels of turbulence, and training set size.","",""
5,"Jie Zhang, Yanjiao Li, Wendong Xiao, Zhiqiang Zhang","Robust extreme learning machine for modeling with unknown noise",2020,"","","","",156,"2022-07-13 09:23:14","","10.1016/j.jfranklin.2020.06.027","","",,,,,5,2.50,1,4,2,"","",""
5,"W. Flynn, Sandeep Namburi, Carolyn Paisie, H. Reddi, Sheng Li, R. K. Murthy Karuturi, J. George","Pan-cancer machine learning predictors of primary site of origin and molecular subtype",2018,"","","","",157,"2022-07-13 09:23:14","","10.1101/333914","","",,,,,5,1.25,1,7,4,"Background It is estimated by the American Cancer Society that approximately 5% of all metastatic tumors have no defined primary site (tissue) of origin and are classified as cancers of unknown primary (CUPs). The current standard of care for CUP patients depends on immunohistochemistry (IHC) based approaches to identify the primary site. The addition of post-mortem evaluation to IHC based tests helps to reveal the identity of the primary site for only 25% of the CUPs, emphasizing the acute need for better methods of determination of the site of origin. CUP patients are therefore given generic chemotherapeutic agents resulting in poor prognosis. When the tissue of origin is known, patients can be given site specific therapy with significant improvement in clinical outcome. Similarly, identifying the primary site of origin of metastatic cancer is of great importance for designing treatment. Identification of the primary site of origin is an import first step but may not be sufficient information for optimal treatment of the patient. Recent studies, primarily from The Cancer Genome Atlas (TCGA) project, and others, have revealed molecular subtypes in several cancer types with distinct clinical outcome. The molecular subtype captures the fundamental mechanisms driving the cancer and provides information that is essential for the optimal treatment of a cancer. Thus, along with primary site of origin, molecular subtype of a tumor is emerging as a criterion for personalized medicine and patient entry into clinical trials. However, there is no comprehensive toolset available for precise identification of tissue of origin or molecular subtype for precision medicine and translational research. Methods and Findings We posited that metastatic tumors will harbor the gene expression profiles of the primary site of origin of the cancer. Therefore, we decided to learn the molecular characteristics of the primary tumors using the large number of cancer genome profiles available from the TCGA project. Our predictors were trained for 33 cancer types and for the 11 cancers where there are established molecular subtypes. We estimated the accuracy of several machine learning models using cross-validation methods. The extensive testing using independent test sets revealed that the predictors had a median sensitivity and specificity of 97.2% and 99.9% respectively without losing classification of any tumor. Subtype classifiers achieved median sensitivity of 87.7% and specificity of 94.5% via cross validation and presented median sensitivity of 79.6% and specificity of 94.6% in two external datasets of 1,999 total samples. Importantly, these external data shows that our classifiers can robustly predict the primary site of origin from external microarray data, metastatic cancer data, and patient-derived xenograft (PDX) data. Conclusion We have demonstrated the utility of gene expression profiles to solve the important clinical challenge of identifying the primary site of origin and the molecular subtype of cancers based on machine learning algorithms. We show, for the first time to our knowledge, that our pan-cancer classifiers can predict multiple cancers’ primary site of origin from metastatic samples. The predictors will be made available as open source software, freely available for academic non-commercial use.","",""
7,"G. Mastorakis","Human fall detection methodologies: from machine learning using acted data to fall modelling using myoskeletal simulation",2018,"","","","",158,"2022-07-13 09:23:14","","","","",,,,,7,1.75,7,1,4,"Human Fall Detection is a research area with interest from many disciplines and  aims to perform for many assisted-living monitoring applications to promptly identify  life-threatening situations. A fall occurs when a person is unable to maintain  balance due to a variety of issues; physical; mental or environmental. The accurate  detection of the fall is crucial as a missed detection can be fatal. Variability of human  physiological characteristics is currently unstudied as to the impact on a fall  detector's performance as young adults and elderly are expected to fall differently.  Another important issue is the scene occlusions. In the use of visual sensors, an  occluded fall is treated as a missed detection as the whereabouts of the person is  unknown when occluded. Finally, current studies are based on acted fall datasets  on which algorithms are trained. These dataset are unrepresentative of real fall  events and illustrate the events without occlusions or other scene in  uences.  Several fall detection algorithms were developed during the study aiming to achieve  accuracy in detection falls while fall-like actions such as lying down remain undetected.  Human fall datasets were used for training and testing purposes of A  machine learning algorithm using data from depth cameras which captured the  fall events from different views. A new pathway was introduced tackling the issues  of availability issues of data-driven machine learning approaches which was  achieved with the use of simulation data. The use of myoskeletal simulation was  then selected as a closer representation of the human body in terms of structure  and behaviour. With the use of a simulation model, a personalised estimation of  the fall event can be achieved as it is parametrised on a physical characteristic such as the height of the falling person. Alternative technologies such as accelerometers  have been used for fall detection to prove the validity of this approach on other  modalities. A study regarding the impact of occlusions for fall detection which  is one of the issues not properly investigated in current work is proposed and  examined. Synthetic occlusions were added to existing depth data from publicly  available datasets.  The research methodologies were evaluated using the most representative depth  video and accelerometer data from existing datasets, as well as YouTube videos  of real-fall events. The machine learning methodologies achieved good results on  similar body variability datasets. A discussion regarding the proof of concept of the  simulation-based approach for fall modelling is mentioned given the comparative  results against existing methodologies which achieves better than any existing  work evaluated against known datasets. The simulation approach is also evaluated  against occluded fall and non-fall event data, proving the further robustness of  the approach. This platform can be expanded to analyse any type of fall, or body  posture (e.g. elderly), without the use of humans to performs fall events.","",""
4,"Tharanga K Wijethunga, Jelena Stojaković, Michael A. Bellucci, Xingyu Chen, A. Myerson, B. Trout","General Method for the Identification of Crystal Faces Using Raman Spectroscopy Combined with Machine Learning and Application to the Epitaxial Growth of Acetaminophen.",2018,"","","","",159,"2022-07-13 09:23:14","","10.1021/acs.langmuir.8b01791","","",,,,,4,1.00,1,6,4,"Crystal morphology is one of the key crystallographic characteristics that governs the macroscopic properties of crystalline materials. The identification of crystal faces, or face indexing, is an important technique that is used to get information regarding a crystal's morphology. However, it is mainly limited to single crystal X-ray diffraction (SCXRD) and it is often not applicable to products of routine crystallizations becasue it requires high quality single crystals in a narrow size range. To overcome the limitations of the SCXRD method, we have developed a robust and convenient Raman face indexing method based on work by Moriyama et al. This method exploits small but detectable differences in Raman spectra of crystal faces caused by different orientations of the crystallographic axis relative to the direction and polarization of the excitation laser beam. The method requires the compilation of a Raman spectral library for each compound and must be built and validated by SCXRD face indexing. Once the spectral library is available for a compound, the identity of unknown crystal faces (from any crystal that is larger than laser beam) can be inferred by collecting and comparing the Raman spectra to spectra within the library. We have optimized this approach further by developing a machine-learning algorithm that identifies crystal faces by performing a statistical comparison of the spectra in the Raman library and the Raman spectra of the unknown crystal faces. Here, we report the development of the Raman face indexing method and apply it to three different epitaxial systems: Acetaminophen (APAP) grown as an overlayer crystal on d-mannitol (MAN), d-galactose (GAL), and xylitol (XYL) substrates. For each of these epitaxial systems, the crystals were grown under various experimental conditions and have a wide range of sizes and quality. Using the Raman face indexing method, we were able to perform high-throughput indexing of a large number of crystals from different crystallization conditions, which could not be achieved using SCXRD or other analytical techniques.","",""
19,"M. Hausknecht, Wen-Ke Li, M. Mauk, P. Stone","Machine Learning Capabilities of a Simulated Cerebellum",2017,"","","","",160,"2022-07-13 09:23:14","","10.1109/TNNLS.2015.2512838","","",,,,,19,3.80,5,4,5,"This paper describes the learning and control capabilities of a biologically constrained bottom-up model of the mammalian cerebellum. Results are presented from six tasks: 1) eyelid conditioning; 2) pendulum balancing; 3) proportional–integral–derivative control; 4) robot balancing; 5) pattern recognition; and 6) MNIST handwritten digit recognition. These tasks span several paradigms of machine learning, including supervised learning, reinforcement learning, control, and pattern recognition. Results over these six domains indicate that the cerebellar simulation is capable of robustly identifying static input patterns even when randomized across the sensory apparatus. This capability allows the simulated cerebellum to perform several different supervised learning and control tasks. On the other hand, both reinforcement learning and temporal pattern recognition prove problematic due to the delayed nature of error signals and the simulator’s inability to solve the credit assignment problem. These results are consistent with previous findings which hypothesize that in the human brain, the basal ganglia is responsible for reinforcement learning, while the cerebellum handles supervised learning.","",""
6,"R. Alata, J. Pauwels, M. Haelterman, S. Massar","Phase Noise Robustness of a Coherent Spatially Parallel Optical Reservoir",2020,"","","","",161,"2022-07-13 09:23:14","","10.1109/JSTQE.2019.2929181","","",,,,,6,3.00,2,4,2,"Reservoir computing is a machine learning algorithm particularly adapted to process time-dependent signals. It can be easily implemented experimentally with good performance. However experimental implementations are subject to noise, which degrades performance. We develop strategies to mitigate the effects of noise. The specific system on which we illustrate our approaches—currently under development in our laboratory—is a coherent linear Fabry–Perot resonator in which neurons are encoded as a grid of spots on the input mirror plane. In this system, changes in the length of the resonator are the major source of noise and can be modeled as phase noise. This can in principle be partially solved by active stabilisation, but it is interesting to find other strategies to counter the effect of phase noise. We show that a completely unknown phase can be tolerated with only a small degradation of performance by using appropriate training and a readout layer architecture in which the output weights depend on the noisy parameter (the phase). Furthermore, the phase can be estimated by the reservoir itself, leading to an architecture in which the reservoir has two outputs, one of which (the phase estimation) is used to control the other. Our approach should find applications in many experimental implementations of reservoir computing.","",""
1,"D. Majumdar, Anand Mahato","Comparison of SIFT & SURF Corner Detector as Features and other Machine Learning Techniques for Identification of Commonly used Leaves",2018,"","","","",162,"2022-07-13 09:23:14","","","","",,,,,1,0.25,1,2,4,"1Dean R&D, Prof. & Head, Dept. of M.Tech CSE, Nitte Meenakshi Institute of Technology Yelahanka, Bangalore-560 064, India 2 Student, M.Tech Sem IV, Department of M.Tech CSE, Nitte Meenakshi Institute of Technology Yelahanka, Bangalore-560 064, India ----------------------------------------------------------------------------***---------------------------------------------------------------------------Abstract— Scope of properly identifying the leaves very vast. The properly identifying medicinal leaves for various cures, identifying poisonous plants using leaves, determining the usage of the plant using detected leaves are some of the possible usages of leaf identification. The aim is to build a methodology using various feature extraction techniques to extract features, clustering algorithm to cluster the features and decision trees as a classifier. All these methodologies put together to form an effective method to efficiently recognize the unknown leaf image using trained model. Feature extraction techniques like SIFT and SURF which are robust and provide matching in spite of the change in intensity, size or rotation of the object in the images. Effective corner points are chosen from the image from which magnitude and orientation of surrounding are used to build descriptor that is the vector of feature for each corner points. For Clustering the data, various partitional, hierarchical, density based methods are used to cluster the data which cluster the data with respect to inter-connectivity, similarity, closeness, etc. The clusters data is used to build the decision tree like C4.5 and CART which uses entropy and Gini index as the splitting criteria. The unknown image features are used to traverse the decision tree of the closest cluster to yield the matching image output from the training set.","",""
0,"S. Priya, R. Annie Uthra","An Effective Concept Drift Detection Technique with Kernel Extreme Learning Machine for Email Spam Filtering",2020,"","","","",163,"2022-07-13 09:23:14","","10.1109/ICISS49785.2020.9316055","","",,,,,0,0.00,0,2,2,"The increase in the number of undesirable emails named spam has posed a major requirement to develop a highly dependent and robust antispam filters. This paper presents a novel email spam filtering technique with the capability of adapting with the dynamic environment. Concept drift detector attempts to determine the position of the concept drift in large data stream for replacing the baseline learner next to the modifications in the data distribution and therefore enhances accuracy. The proposed method detects the concept drift depending upon the computation of variation in the email content distribution using Statistical Test of Equal Proportions (STEPD) technique. The STEPD is a simpler commonly available model that identifies the concept drift with respect to a hypothesis test among two proportions. The SPEPD technique is used to determine the criteria of the concept drift for all unknown emails that assist the filtering technique in the recognition of the occurrence of the spam. In addition, the kernel extreme learning machine (KELM) based classification model is applied to classify the instances into two class labels namely spam and non-spam correspondingly. The experimental results of the STEPD-KELM model are tested against Enron dataset and the results are examined interms of distinct aspects. The experimental values indicated that the STEPD-KELM model has resulted to a maximum precision of 93.78%, recall of 96.54%, and accuracy of 95.33%.","",""
3,"M. Gohari, Mona Tahmasebi, A. Nozari","Application of machine learning for NonHolonomic mobile robot trajectory controlling",2014,"","","","",164,"2022-07-13 09:23:14","","10.1109/ICCKE.2014.6993354","","",,,,,3,0.38,1,3,8,"Mobile robots are very interested by researchers over the last few years because of their applications and physical characteristics. The workspace of mobile robots is not always ideal, but typically filled with disturbances (known or unknown) such as uneven surface terrain, natural friction, uncertainties, and parametric changes. In this study, a new approach namely active force control (AFC) scheme integrating artificial neural network (ANN) has been suggested to cope on the disturbances and thus improve the trajectory tracking characteristic of the system. Therefore, a two wheeled mobile robot has been simulated, and ANN technique is explicitly employed for the estimation of the inertia matrix that is needed in the inner feedback control loop of the AFC scheme. The robustness and efficiency of the identified control scheme are studied considering various forms of loading and operating conditions. For the purpose of benchmarking, the AFC scheme performance has been compared to PID controller.","",""
0,"J. Majumdar, Anand Mahato","Identification of Commonly used Medicinal Leaves using Machine Learning Techniques with SIFT Corner Detector as Features",2018,"","","","",165,"2022-07-13 09:23:14","","10.26438/IJCSE/V6I2.341346","","",,,,,0,0.00,0,2,4,"Medicinal leaves carry a huge value and importance in the medical field which can be directly used or medicines are made for medicinal purposes to cure patients. With the variety of leaves present, the proper identification of the leaves is very difficult without prior knowledge and experience. Computer Vision can bring the accurate identification of such leaves using the various feature extraction techniques using leaf images. The aim is to build a methodology using various feature extraction techniques to extract features, clustering algorithm to cluster the features and decision trees as a classifier. Feature extraction techniques like SIFT key descriptors which are robust and provide matching in spite of the change in intensity, size or rotation of the object in the images. Effective corner points are chosen from the image from which magnitude and orientation of surrounding are used to build descriptor that is the vector of feature for each corner points. For clustering the data, various partitional, hierarchical, density based methods are used to cluster the data which cluster the data with respect to inter-connectivity, similarity, closeness, etc. The clusters data is used to build the decision tree like C4.5 and CART which uses entropy and Gini index as the splitting criteria. All these methodologies put together to form an effective method to efficiently recognize the unknown leaf image using trained model. Keywords— SIFT Corner points, Chameleon Clustering, Decision Tree Classifier.","",""
0,"Ling Lan, Chia-Hao Liu, Qiang Du, S. Billinge","Robustness Test of the spacegroupMining Space Group Determination Model for atomic pair distribution functions",2022,"","","","",166,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,4,1,"Machine learning models based on convolutional neural network have been used for predicting space groups of crystal structures from their atomic pair distribution function (PDF). However, the PDFs used to train the model are calculated with respect to a fixed set of parameters, and the accuracy on PDFs generated with different choices of these parameters is unknown. In this paper, we report that the results of the top-1 accuracy and top-6 accuracy are robust when applied to PDFs of different choices of experimental parameters rmax, Qmax, and atomic displacement parameters.","",""
0,"Ling Lan, Chia-Hao Liu, Qiang Du, S. Billinge","Robustness test of the spacegroupMining model for determining space groups from atomic pair distribution function data",2022,"","","","",167,"2022-07-13 09:23:14","","10.1107/S1600576722002990","","",,,,,0,0.00,0,4,1,"Machine learning models based on convolutional neural networks have been used for predicting space groups of crystal structures from their atomic pair distribution function (PDF). However, the PDFs used to train the model are calculated using a fixed set of parameters that reflect specific experimental conditions, and the accuracy of the model when given PDFs generated with different choices of these parameters is unknown. In this work, the results of the top-1 accuracy and top-6 accuracy are robust when applied to PDFs of different choices of experimental parameters r  max, Q  max, Q  damp and atomic displacement parameters.","",""
0,"Samer Khamaiseh, Abdullah Al-Alaj, Mohammad Adnan, Hakam W. Alomari","The Robustness of Detecting Known and Unknown DDoS Saturation Attacks in SDN via the Integration of Supervised and Semi-Supervised Classifiers",2022,"","","","",168,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,4,1,"The design of existing machine-learning-based DoS detection systems in software-defined networking (SDN) suffers from two major problems. First, the proper time window for conducting network traffic analysis is unknown and has proven challenging to determine. Second, it is unable to detect unknown types of DoS saturation attacks. An unknown saturation attack is an attack that is not represented in the training data. In this paper, we evaluate three supervised classifiers for detecting a family of DDoS flooding attacks (UDP, TCP-SYN, IP-Spoofing, TCP-SARFU, and ICMP) and their combinations using different time windows. This work represents an extension of the runner-up best-paper award entitled ‘Detecting Saturation Attacks in SDN via Machine Learning’ published in the 2019 4th International Conference on Computing, Communications and Security (ICCCS). The results in this paper show that the trained supervised models fail in detecting unknown saturation attacks, and their overall detection performance decreases when the time window of the network traffic increases. Moreover, we investigate the performance of four semi-supervised classifiers in detecting unknown flooding attacks. The results indicate that semi-supervised classifiers outperform the supervised classifiers in the detection of unknown flooding attacks. Furthermore, to further increase the possibility of detecting the known and unknown flooding attacks, we propose an enhanced hybrid approach that combines two supervised and semi-supervised classifiers. The results demonstrate that the hybrid approach has outperformed individually supervised or semi-supervised classifiers in detecting the known and unknown flooding DoS attacks in SDN.","",""
7,"G. Biagetti, P. Crippa, L. Falaschetti, C. Turchetti","Machine learning regression based on particle bernstein polynomials for nonlinear system identification",2017,"","","","",169,"2022-07-13 09:23:14","","10.1109/MLSP.2017.8168148","","",,,,,7,1.40,2,4,5,"Polynomials have shown to be useful basis functions in the identification of nonlinear systems. However estimation of the unknown coefficients requires expensive algorithms, as for instance it occurs by applying an optimal least square approach. Bernstein polynomials have the property that the coefficients are the values of the function to be approximated at points in a fixed grid, thus avoiding a time-consuming training stage. This paper presents a novel machine learning approach to regression, based on new functions named particle-Bernstein polynomials, which is particularly suitable to solve multivariate regression problems. Several experimental results show the validity of the technique for the identification of nonlinear systems and the better performance achieved with respect to the standard techniques.","",""
5,"Abdullah Sheneamer, H. Hazazi, Swarup Roy, J. Kalita","Schemes for Labeling Semantic Code Clones using Machine Learning",2017,"","","","",170,"2022-07-13 09:23:14","","10.1109/ICMLA.2017.00-25","","",,,,,5,1.00,1,4,5,"Machine learning approaches built to identify code clones fail to perform well due to insufficient training samples and have been restricted only up to Type-III clones. A majority of the publicly available code clone corpora are incomplete in nature and lack labeled samples for semantic or Type-IV clones. We present here two schemes for labeling all types of clones including Type-IV clones. We restrict our study to Java code only. First, we use an unsupervised approach to label Type-IV clones and validate them using expert Java programmers. Next, we present a supervised scheme for labeling (or classifying) unknown samples based on labeled samples derived from our first scheme. We evaluate the performance of our schemes using six well-known Java code clone corpora and report on the quality of produced clones in terms of kappa agreement, mean error and accuracy scores. Results show that both schemes produce high quality code clones facilitating future use of machine learning in detecting clones of Type-IV.","",""
9,"A. El Bakri, M. Koumir, I. Boumhidi","Extreme learning machine-based non-linear observer for fault detection and isolation of wind turbine",2019,"","","","",171,"2022-07-13 09:23:14","","10.1080/1448837X.2019.1578044","","",,,,,9,3.00,3,3,3,"ABSTRACT This paper presents a robust fault detection and isolation (FDI) scheme for a variable speed wind turbine. The proposed scheme (extreme learning machine–state-dependent differential Riccati equation (ELM-SDDRE)) is an observer model-based approach, especially, a non-linear observer using SDDRE based on an improved model of the wind turbine by using the ELM. The standard SDDRE can be used for small model uncertainties. However, when the uncertainties are large, the SDDRE cannot detect and isolate the faults. The main objective of the ELM is the prediction of unknown nominal model dynamics to construct a new improved nominal model used by the observer for FDI. This makes the effect of uncertainties weak and consequently allows better faults detection. The faults considered in this paper are sensor faults in the rotating speeds of the rotor and generator outputs. The effectiveness of the proposed approach is illustrated through simulation.","",""
7,"Jiashuo Liu, Zheyan Shen, Peng Cui, Linjun Zhou, Kun Kuang, B. Li, Yishi Lin","Stable Adversarial Learning under Distributional Shifts",2020,"","","","",172,"2022-07-13 09:23:14","","","","",,,,,7,3.50,1,7,2,"Machine learning algorithms with empirical risk minimization are vulnerable under distributional shifts due to the greedy adoption of all the correlations found in training data. Recently, there are robust learning methods aiming at this problem by minimizing the worst-case risk over an uncertainty set. However, they equally treat all covariates to form the decision sets regardless of the stability of their correlations with the target, resulting in the overwhelmingly large set and low confidence of the learner. In this paper, we propose Stable Adversarial Learning (SAL) algorithm that leverages heterogeneous data sources to construct a more practical uncertainty set and conduct differentiated robustness optimization, where covariates are differentiated according to the stability of their correlations with the target. We theoretically show that our method is tractable for stochastic gradientbased optimization and provide the performance guarantees for our method. Empirical studies on both simulation and real datasets validate the effectiveness of our method in terms of uniformly good performance across unknown distributional shifts.","",""
35,"Masashi Sugiyama","Statistical Reinforcement Learning - Modern Machine Learning Approaches",2015,"","","","",173,"2022-07-13 09:23:14","","","","",,,,,35,5.00,35,1,7,"Reinforcement learning (RL) is a framework for decision making in unknown environments based on a large amount of data. Several practical RL applications for business intelligence, plant control, and game players have been successfully explored in recent years. Providing an accessible introduction to the field, this book covers model-based and model-free approaches, policy iteration, and policy search methods. It presents illustrative examples and state-of-the-art results, including dimensionality reduction in RL and risk-sensitive RLm. The book provides a bridge between RL and data mining and machine learning research.","",""
0,"Chuang Zhou, Yongle Lv, Jie Wu","Probabilistic Weighted Extreme Learning Machine for Robust Modeling",2019,"","","","",174,"2022-07-13 09:23:14","","10.1109/ICEICT.2019.8846409","","",,,,,0,0.00,0,3,3,"Radar performance prediction is becoming more and more important in Product-Life-Cycle-Management of radar. However, modeling the performance is difficult because of the strong nonlinearity from radar’s complex system as well as immeasurable effects like electromagnetic interference, instrument accuracy and so on. This paper proposes a probability weighted extreme learning machine to model the performance of radar under noise. First, a distributed extreme learning machine modeling is developed, upon which the probability distribution function (PDF) of multiple local models is estimated by the Parzen window method. This distribution function is further used as weights to integrate all local models to construct a global robust ELM model. The successful application of this robust probabilistic weighted ELM method to both artificial case and real life case demonstrates its great advantages in the modeling of an unknown system with various kinds of random noise.","",""
2,"C. Baron, Jie Zhang","Reliable On-Line Re-Optimization Control of a Fed-Batch Fermentation Process Using Bootstrap Aggregated Extreme Learning Machine",2017,"","","","",175,"2022-07-13 09:23:14","","10.1007/978-3-030-11292-9_14","","",,,,,2,0.40,1,2,5,"","",""
0,"F. Khan","Text independent offline hand writer recognition using machine learning",2017,"","","","",176,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,1,5,"Handwriting is a behavioural biometric that an individual learns and develops over time and automated writer identification systems can be developed by identifying these behavioural aspects of an individual’s writing style. These writer recognition systems greatly assist forensic experts by facilitating them with semi-automated tools that segment the text, narrow down the search, help with visualization and finally assist in the final identification of an unknown handwritten sample.    Handwriting, as a behavioural characteristic, has been a subject of interest for researchers for many decades and intensive research performed in this field has resulted in the development of multiple methods and algorithms. However, automated writer identification is still a challenging problem. Difficulties in segmenting text and the deviation of an individual from his or her unique writing style is the reason for ongoing research in this field.    This thesis aims to investigate the problems faced in automated writer identification and propose novel techniques of segmentation and classification that would contribute to the field of writer identification. This has led to four different contributions.    First a novel segmentation algorithm is proposed for segmenting sub-words within hand written Arabic words, the proposed method outperforms the previously used projection profile method. The second proposed method offers a segmentation free multi-scale Local Ternary Pattern Histogram for text independent writer identification. Local ternary patterns are applied at various scales to produce a predictor model for its respective scale while the high dimensionality problem of a multi-scale approach has been tackled with dimensionality reduction using SR-KDA. The third contribution tackles the problem of writer identification in noisy conditions. A robust offline text independent writer identification system is proposed using Bagged Discrete Cosine Transforms. The proposed system effectively utilizes discrete cosine transform for writer identification while avoiding problems of high dimensionality and memory limitations. Finally, in the fourth contribution a dissimilarity Gaussian mixture model is proposed for describing the contrast between different writers of a dataset. Furthermore, a weighted histogram approach is also proposed that penalizes bad prediction scores with a cost function to significantly enhance the identification rate.","",""
6,"Liu Bing, Shi Yuliang","Prediction of User's Purchase Intention Based on Machine Learning",2016,"","","","",177,"2022-07-13 09:23:14","","10.1109/ISCMI.2016.21","","",,,,,6,1.00,3,2,6,"In recent years, the use of machine learning methods to deal with the problem of user interest prediction has become a hot research direction in the field of electronic commerce. In the present stage, a naive Bayesian algorithm has the advantages of simple implementation and high classification efficiency. However, this method is too dependent on the distribution of samples in the sample space, and has the potential of instability. To this end, the decision tree method is introduced to deal with the problem of interest classification, and the innovative use of Localstorage technology in HTML5 to obtain the required the experimental data. Classification method uses the information entropy of the training data set to build the classification model, through the simple search of the classification model to complete the classification of unknown data items. Both theoretical analysis and experimental results show that the decision tree is used to deal with the problem of prediction of users' interests has obvious advantages in the efficiency and stability.","",""
5,"J. Xue, Feng Yan, Alma Riska, E. Smirni","Scheduling data analytics work with performance guarantees: queuing and machine learning models in synergy",2016,"","","","",178,"2022-07-13 09:23:14","","10.1007/s10586-016-0563-z","","",,,,,5,0.83,1,4,6,"","",""
15,"Luyu Ji, Junyong Wu, Yanzhen Zhou, Liangliang Hao","Using Trajectory Clusters to Define the Most Relevant Features for Transient Stability Prediction Based on Machine Learning Method",2016,"","","","",179,"2022-07-13 09:23:14","","10.3390/EN9110898","","",,,,,15,2.50,4,4,6,"To achieve rapid real-time transient stability prediction, a power system transient stability prediction method based on the extraction of the post-fault trajectory cluster features of generators is proposed. This approach is conducted using data-mining techniques and support vector machine (SVM) models. First, the post-fault rotor angles and generator terminal voltage magnitudes are considered as the input vectors. Second, we construct a high-confidence dataset by extracting the 27 trajectory cluster features obtained from the chosen databases. Then, by applying a filter–wrapper algorithm for feature selection, we obtain the final feature set composed of the eight most relevant features for transient stability prediction, called the global trajectory clusters feature subset (GTCFS), which are validated by receiver operating characteristic (ROC) analysis. Comprehensive simulations are conducted on a New England 39-bus system under various operating conditions, load levels and topologies, and the transient stability predicting capability of the SVM model based on the GTCFS is extensively tested. The experimental results show that the selected GTCFS features improve the prediction accuracy with high computational efficiency. The proposed method has distinct advantages for transient stability prediction when faced with incomplete Wide Area Measurement System (WAMS) information, unknown operating conditions and unknown topologies and significantly improves the robustness of the transient stability prediction system.","",""
21073,"Radford M. Neal","Pattern Recognition and Machine Learning",2007,"","","","",180,"2022-07-13 09:23:14","","10.1198/tech.2007.s518","","",,,,,21073,1404.87,21073,1,15,"the selection of symmetric factorial designs, that is, a design where all factors have the same number of levels. Chapter 3 focuses on selection of two-level factorial designs and discusses complementary design theory and related topics in the selection of designs. Chapter 4 covers the selection of three level designs followed by the general case of s-levels. Chapter 5 discusses estimation capacity, presenting the connections with complementary designs followed by the estimation capacity for two-level and s-level designs. Chapter 6 discusses and presents results for the construction of mixed-level designs. Giving many examples of the use of mixed two and four-level designs. The final unit of the book discusses designs where there are two-different groups of factors. Chapters 7 and 8 discuss factorial designs with restricted randomization. Focusing first on blocked designs for full factorials as well as blocked fractional factorial designs. Chapter 8 focuses on split-plot designs. The booked is concluded with a chapter on robust parameter designs. This book covers a broad range of topics for regular factorial designs and presents all of the material in very mathematical fashion. However, the authors do a wonderful job of keeping the statistical methodology at the forefront of the book and the mathematical detail is presented as the necessary tool to study these designs. The book will serve as a great text for an advanced graduate level course in design theory for students with the necessary mathematical background. The book will surely become an invaluable resource for researchers and graduate students doing research in the design of factorial experiments. In addition, practitioners will also find the book useful for the comprehensive collection of optimal designs presented at the end of many chapters. Overall, this is a very well written book and a necessary addition to the existing literature on the design of factorial experiments.","",""
0,"Xianglong Zeng, Chaoyang Wu, W. Ye","User-Definable Dynamic Hand Gesture Recognition Based on Doppler Radar and Few-Shot Learning",2021,"","","","",181,"2022-07-13 09:23:14","","10.1109/jsen.2021.3107943","","",,,,,0,0.00,0,3,1,"In recent years, the radar-based dynamic hand gesture recognition (DHGR) system has been widely used in the non-contact interaction with smart electronic devices because of its advantages of safety, privacy security and robustness to different illumination environments. In order to achieve high-precision gesture recognition, Machine Learning algorithms, especially the deep learning algorithms are generally chosen by researchers. However, most deep learning models are trained based on large gesture dataset and only accept certain predefined specified gestures as input, i.e., user-defined gestures are not allowed. In this work, a neural network model trained with meta-learning method is proposed to deal with the few-shot classification task and realize the user-definable DHGR. Instead of learning a mapping between the unknown input gesture and the fixed predefined classes, the proposed model is trained to learn to compare the input gesture with arbitrary gestures entered by users, and then decide which class of gesture is the most probable one. Without retraining, the model can allow the user to enter the self-defined gestures for one or a few times and then recognize these gestures next time. To the best of our knowledge, this is the first work to apply the meta few-shot learning to the DHGR problem in the true sense. In this model, an embedding module is used in the network to extract the features of the input micro-Doppler spectrograms, and a comparison module is used to execute the feature-level comparison. Finally, a weighting module is proposed to weigh the comparison results and make the prediction. The evaluation result shows that with only one support sample for each class (i.e., for the one-shot tasks), the proposed model achieves an accuracy of 91% for 5 gesture classes and 90% for 10 gesture classes. When using 3 support samples (i.e., for the 3-shot tasks), the accuracies for 5 and 10 gestures are further improved to 96% and 92%, respectively.","",""
0,"Jiashuo Liu, Zheyan Shen, Peng Cui, Linjun Zhou, Kun Kuang, B. Li","Distributionally Robust Learning with Stable Adversarial Training",2021,"","","","",182,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,6,1,"Machine learning algorithms with empirical risk minimization are vulnerable under distributional shifts due to the greedy adoption of all the correlations found in training data. There is an emerging literature on tackling this problem by minimizing the worst-case risk over an uncertainty set. However, existing methods mostly construct ambiguity sets by treating all variables equally regardless of the stability of their correlations with the target, resulting in the overwhelmingly-large uncertainty set and low confidence of the learner. In this paper, we propose a novel Stable Adversarial Learning (SAL) algorithm that leverages heterogeneous data sources to construct a more practical uncertainty set and conduct differentiated robustness optimization, where covariates are differentiated according to the stability of their correlations with the target. We theoretically show that our method is tractable for stochastic gradient-based optimization and provide the performance guarantees for our method. Empirical studies on both simulation and real datasets validate the effectiveness of our method in terms of uniformly good performance across unknown distributional shifts.","",""
19,"Guangyao Chen, Peixi Peng, Xiangqian Wang, Yonghong Tian","Adversarial Reciprocal Points Learning for Open Set Recognition",2021,"","","","",183,"2022-07-13 09:23:14","","10.1109/TPAMI.2021.3106743","","",,,,,19,19.00,5,4,1,"Open set recognition (OSR), aiming to simultaneously classify the seen classes and identify the unseen classes as unknown, is essential for reliable machine learning. The key challenge of OSR is how to reduce the empirical classification risk on the labeled known data and the open space risk on the potential unknown data simultaneously. To handle the challenge, we formulate the open space risk problem from the perspective of multi-class integration, and model the unexploited extra-class space with a novel concept Reciprocal Point. Follow this, a novel Adversarial Reciprocal Point Learning framework is proposed to minimize the overlap of known distribution and unknown distributions without loss of known classification accuracy. Specifically, each reciprocal point is learned by the extra-class space with the corresponding known category, and the confrontation among multiple known categories are employed to reduce the empirical classification risk. An adversarial margin constraint is proposed to reduce the open space risk by limiting the latent open space constructed by reciprocal points. Moreover, an instantiated adversarial enhancement method is designed to generate diverse and confusing training samples. Extensive experimental results on various benchmark datasets indicate that the proposed method is significantly superior to existing approaches and achieves state-of-the-art performance.","",""
3997,"S. Kotsiantis","Supervised Machine Learning: A Review of Classification Techniques",2007,"","","","",184,"2022-07-13 09:23:14","","","","",,,,,3997,266.47,3997,1,15,"The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single chapter cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.","",""
128,"S. Kiranyaz, T. Ince, M. Gabbouj","Multidimensional Particle Swarm Optimization for Machine Learning and Pattern Recognition",2013,"","","","",185,"2022-07-13 09:23:14","","10.1007/978-3-642-37846-1","","",,,,,128,14.22,43,3,9,"","",""
1,"Taouba Jouini, Zhiyong Sun","Distributed learning for optimal allocation of synchronous and converter-based generation",2021,"","","","",186,"2022-07-13 09:23:14","","10.1109/MED51440.2021.9480195","","",,,,,1,1.00,1,2,1,"Motivated by the penetration of converter-based generation into the electrical grid, we revisit the classical log-linear learning algorithm for optimal allocation of synchronous machines and converters for mixed power generation. The objective is to assign to each generator unit a type (either synchronous machine or DC/AC converter in closed-loop with droop control), while minimizing the steady state angle deviation relative to an optimum induced by unknown optimal configuration of synchronous and DC/AC converter-based generation. Additionally, we study the robustness of the learning algorithm against a uniform drop in the line susceptances and with respect to a well-defined feasibility region describing admissible power deviations. We show guaranteed probabilistic convergence to maximizers of the perturbed potential function with feasible power flows and demonstrate our theoretical findings via simulative examples of a power network with six generation units.","",""
7,"Yue Pan, Hongmei Liu, L. Metsch, D. Feaster","Factors Associated with HIV Testing Among Participants from Substance Use Disorder Treatment Programs in the US: A Machine Learning Approach",2017,"","","","",187,"2022-07-13 09:23:14","","10.1007/s10461-016-1628-y","","",,,,,7,1.40,2,4,5,"","",""
18,"Zhuangwei Shi, Han Zhang, Chengcai Jin, Xiongwen Quan, Yanbin Yin","A representation learning model based on variational inference and graph autoencoder for predicting lncRNA-disease associations",2021,"","","","",188,"2022-07-13 09:23:14","","10.1186/s12859-021-04073-z","","",,,,,18,18.00,4,5,1,"","",""
120,"Lei Zhang, D. Zhang","Evolutionary Cost-Sensitive Extreme Learning Machine",2015,"","","","",189,"2022-07-13 09:23:14","","10.1109/TNNLS.2016.2607757","","",,,,,120,17.14,60,2,7,"Conventional extreme learning machines (ELMs) solve a Moore–Penrose generalized inverse of hidden layer activated matrix and analytically determine the output weights to achieve generalized performance, by assuming the same loss from different types of misclassification. The assumption may not hold in cost-sensitive recognition tasks, such as face recognition-based access control system, where misclassifying a stranger as a family member may result in more serious disaster than misclassifying a family member as a stranger. Though recent cost-sensitive learning can reduce the total loss with a given cost matrix that quantifies how severe one type of mistake against another, in many realistic cases, the cost matrix is unknown to users. Motivated by these concerns, this paper proposes an evolutionary cost-sensitive ELM, with the following merits: 1) to the best of our knowledge, it is the first proposal of ELM in evolutionary cost-sensitive classification scenario; 2) it well addresses the open issue of how to define the cost matrix in cost-sensitive learning tasks; and 3) an evolutionary backtracking search algorithm is induced for adaptive cost matrix optimization. Experiments in a variety of cost-sensitive tasks well demonstrate the effectiveness of the proposed approaches, with about 5%–10% improvements.","",""
5,"S. Sriram, K. Simran, R. Vinayakumar, S. Akarsh, Kritik Soman","Towards Evaluating the Robustness of Deep Intrusion Detection Models in Adversarial Environment",2020,"","","","",190,"2022-07-13 09:23:14","","10.36227/techrxiv.12151749","","",,,,,5,2.50,1,5,2,"Network Intrusion Detection System (NIDS) is a method that is utilized to categorize network traffic as malicious or normal. Anomaly-based method and signature-based method are the traditional approaches used for network intrusion detection. The signature-based approach can only detect familiar attacks whereas the anomaly-based approach shows promising results in detecting new unknown attacks. Machine Learning (ML) based approaches have been studied in the past for anomaly-based NIDS. In recent years, the Deep Learning (DL) algorithms have been widely utilized for intrusion detection due to its capability to obtain optimal feature representation automatically. Even though DL based approaches improves the accuracy of the detection tremendously, they are prone to adversarial attacks. The attackers can trick the model to wrongly classify the adversarial samples into a particular target class. In this paper, the performance analysis of several ML and DL models are carried out for intrusion detection in both adversarial and non-adversarial environment. The models are trained on the NSLKDD dataset which contains a total of 148,517 data points. The robustness of several models against adversarial samples is studied.","",""
2,"H. G. Damavandi","Data analytics, interpretation and machine learning for environmental forensics using peak mapping methods",2016,"","","","",191,"2022-07-13 09:23:14","","10.17077/ETD.IX74OTR7","","",,,,,2,0.33,2,1,6,"In this work our driving motivation is to develop mathematically robust and computationally efficient algorithms that will help chemists towards their goal of pattern matching. Environmental chemistry today broadly faces difficult computational and interpretational challenges for vast and ever-increasing data repositories. A driving factor behind these challenges are little known intricate relationships between constituent analytes that constitute complex mixtures spanning a range of target and non-target compounds. While the end of goal of different environment applications are diverse, computationally speaking, many data interpretation bottlenecks arise from lack of efficient algorithms and robust mathematical frameworks to identify, cluster and interpret compound peaks. There is a compelling need for compound-cognizant quantitative interpretation that accounts for the full informational range of gas chromatographic (and mass spectrometric) datasets. Traditional target-oriented analysis focus only on the dominant compounds of the chemical mixture, and thus are agnostic of the contribution of unknown non-target analytes. On the other extreme, statistical methods prevalent in chemometric interpretation ignore compound identity altogether and consider only the multivariate data statistics, and thus are agnostic of intrinsic relationships between the well-known target and unknown target analytes. Thus, both schools of thought (target-based or statistical) in current-day chemical data analysis and interpretation fall short of quantifying the complex interaction between major and minor compound peaks in molecular mixtures commonly encountered in","",""
0,"P. Ai, Z. Deng, Y. Wang, C. Shen","Universal uncertainty estimation for nuclear detector signals with neural networks and ensemble learning",2021,"","","","",192,"2022-07-13 09:23:14","","10.1088/1748-0221/17/02/P02032","","",,,,,0,0.00,0,4,1,"Characterizing uncertainty is a common issue in nuclear measurement and has important implications for reliable physical discovery. Traditional methods are either insufficient to cope with the heterogeneous nature of uncertainty or inadequate to perform well with unknown mathematical models. In this paper, we propose using multi-layer convolutional neural networks for empirical uncertainty estimation and feature extraction of nuclear pulse signals. This method is based on deep learning, a recent development of machine learning techniques, which learns the desired mapping function from training data and generalizes to unseen test data. Furthermore, ensemble learning is utilized to estimate the uncertainty originating from trainable parameters of the network and to improve the robustness of the whole model. To evaluate the performance of the proposed method, simulation studies, in comparison with curve fitting, investigate extensive conditions and show its universal applicability. Finally, a case study is made using data from a NICA-MPD electromagnetic calorimeter module exposed to a test beam at DESY, Germany. The uncertainty estimation method successfully detected out-of-distribution samples and also achieved good accuracy in time and energy measurements.","",""
0,"Yi Dou","Robust Graph Learning for Misbehavior Detection",2022,"","","","",193,"2022-07-13 09:23:14","","10.1145/3488560.3502213","","",,,,,0,0.00,0,1,1,"Recent years have witnessed the thriving of online services like social media, e-commerce, and e-finance. Those services facilitate our daily lives while breeding malicious actors like fraudsters and spammers to promote misinformation, gain monetary rewards, or reap end users' privacy. Graph-based machine learning models have been playing a critical and irreplaceable role in modeling and detecting online misbehavior. With the observation that misbehaviors are different from massive regular behaviors, the graph models can leverage the relationship between data entities from a holistic view and reveal suspicious behaviors as anomalous nodes/edges/subgraphs on the graph. In this proposal, we investigate the graph-based misbehavior detection models from an adversarial perspective, considering the adversarial nature of malicious actors and real-world factors that impair graph models' robustness. We first introduce two published works enhancing the robustness of several graph-based misbehavior detectors using reinforcement learning. Then, we propose to explore: 1) the robustness of graph neural networks for misinformation detection on social media; and 2) the general robustness of graph neural networks towards unknown perturbations.","",""
0,"Hamdah Alotaibi, Fawaz Alsolami, Ehab A. Abozinadah, R. Mehmood","TAWSEEM: A Deep-Learning-Based Tool for Estimating the Number of Unknown Contributors in DNA Profiling",2022,"","","","",194,"2022-07-13 09:23:14","","10.3390/electronics11040548","","",,,,,0,0.00,0,4,1,"DNA profiling involves the analysis of sequences of an individual or mixed DNA profiles to identify the persons that these profiles belong to. A critically important application of DNA profiling is in forensic science to identify criminals by finding a match between their blood samples and the DNA profile found on the crime scene. Other applications include paternity tests, disaster victim identification, missing person investigations, and mapping genetic diseases. A crucial task in DNA profiling is the determination of the number of contributors in a DNA mixture profile, which is challenging due to issues that include allele dropout, stutter, blobs, and noise in DNA profiles; these issues negatively affect the estimation accuracy and the computational complexity. Machine-learning-based methods have been applied for estimating the number of unknowns; however, there is limited work in this area and many more efforts are required to develop robust models and their training on large and diverse datasets. In this paper, we propose and develop a software tool called TAWSEEM that employs a multilayer perceptron (MLP) neural network deep learning model for estimating the number of unknown contributors in DNA mixture profiles using PROVEDIt, the largest publicly available dataset. We investigate the performance of our developed deep learning model using four performance metrics, namely accuracy, F1-score, recall, and precision. The novelty of our tool is evident in the fact that it provides the highest accuracy (97%) compared to any existing work on the most diverse dataset (in terms of the profiles, loci, multiplexes, etc.). We also provide a detailed background on the DNA profiling and literature review, and a detailed account of the deep learning tool development and the performance investigation of the deep learning method.","",""
14,"Kevin Li, C. Gibson, D. Ho, Qi Zhou, J. Kim, O. Buhisi, D. Brown, M. Gerber","Assessment of machine learning algorithms in cloud computing frameworks",2013,"","","","",195,"2022-07-13 09:23:14","","10.1109/SIEDS.2013.6549501","","",,,,,14,1.56,2,8,9,"In the past decade, digitization of information has led to a data explosion in both volume and complexity. While traditional computing frameworks have failed to provide adequate computing power for the now common data-intensive computing tasks, cloud computing provides an effective alternative to enhance computing power. Machine learning algorithms are powerful analytical methods that allow machines to recognize patterns and facilitate human learning. However, the performance of individual machine learning algorithms within each cloud computing framework remains largely unknown. Furthermore, the lack of a robust selection methodology matching input data with effective machine learning algorithms limits the ability of practitioners to make effective use of cloud computing. This research compares various machine learning algorithms on the widely adopted Apache Mahout framework and the recently introduced GraphLab framework. Whereas previous work has examined the computational architectures of various cloud computing frameworks, this work focuses on a problem-based approach to architecture selection. The experimental results demonstrate that GraphLab generally outperforms Mahout with respect to runtime, scalability, and usability. However, Mahout outperforms GraphLab when the experiment focus shifts to error measurement.","",""
15,"D. Dittman, T. Khoshgoftaar, Randall Wald, A. Napolitano","Simplifying the Utilization of Machine Learning Techniques for Bioinformatics",2013,"","","","",196,"2022-07-13 09:23:14","","10.1109/ICMLA.2013.155","","",,,,,15,1.67,4,4,9,"The domain of bioinformatics has a number of challenges such as handling datasets which exhibit extreme levels of high dimensionality (large number of features per sample) and datasets which are particularly difficult to work with. These datasets contain many pieces of data (features) which are irrelevant and redundant to the problem being studied, which makes analysis quite difficult. However, techniques from the domain of machine learning and data mining are well suited to combating these difficulties. Techniques like feature selection (choosing an optimal subset of features for subsequent analysis by removing irrelevant or redundant features) and classifiers (used to build inductive models in order to classify unknown instances) can assist researchers in working with such difficult datasets. Unfortunately, many practitioners of bioinformatics do not have the machine learning knowledge to choose the correct techniques in order to achieve good classification results. If the choices could be simplified or predetermined then it would be easier to apply the techniques. This study is a comprehensive analysis of machine learning techniques on twenty-five bioinformatics datasets using six classifiers, and twenty-four feature rankers. We analyzed the factors at each of four feature subset sizes chosen for being large enough to be effective in creating inductive models but small enough to be of use for further research. Our results shows that Random Forest with 100 trees is the top performing classifier and that the choice of feature ranker is of little importance as long as feature selection occurs. Statistical analysis confirms our results. By choosing these parameters, machine learning techniques are more accessible to bioinformatics.","",""
9,"Wei Xiao, C. Belta, C. Cassandras","Feasibility-Guided Learning for Constrained Optimal Control Problems",2020,"","","","",197,"2022-07-13 09:23:14","","10.1109/CDC42340.2020.9303857","","",,,,,9,4.50,3,3,2,"Optimal control problems with constraints ensuring safety can be mapped onto a sequence of real time optimization problems through the use of Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs). One of the main challenges in these approaches is ensuring the feasibility of the resulting quadratic programs (QPs) if the system is affine in controls. In this paper, we improve the feasibility robustness (i.e., feasibility maintenance in the presence of time-varying and unknown unsafe sets) through the definition of a High Order CBF (HOCBF); this is achieved by a proposed feasibility-guided learning approach using machine learning techniques. The effectiveness of the proposed feasibility-guided learning approach is demonstrated on a robot control problem.","",""
6,"B. Lenz, Bernd Barak, Julia Mührwald, Carolin Leicht","Virtual Metrology in Semiconductor Manufacturing by Means of Predictive Machine Learning Models",2013,"","","","",198,"2022-07-13 09:23:14","","10.1109/ICMLA.2013.186","","",,,,,6,0.67,2,4,9,"Advanced Process Control (APC) is an important research area in Semiconductor Manufacturing (SM) to improve process stability crucial for product quality. In low-volume-high-mixture fabrication plants (fabs), Knowledge Discovery in Databases is extremely challenging due to complex technology mixtures and reduced availability of data for comparable process steps. High Density Plasma Chemical Vapor Deposition (HDP CVD) appears to be a process area in SM predestinated for application of Data Mining (DM). Enhancing physical metrology by predictive models leads to smart future fabs. Actual research focuses on Virtual Metrology (VM) using high sophisticated Machine Learning (ML) methods to model unknown functional interrelations and to predict the thickness of dielectric layers deposited onto a metallization layer of the manufactured wafers. Decision Trees (DT), Neural Networks (NN) and Support Vector Regression (SVR) have been investigated to maximize the accuracy of the regression. For data of various logistical granularities promising results have been achieved by implementing these statistical models.","",""
4,"Yilan Chen, Wei Huang, Lam M. Nguyen, Tsui-Wei Weng","On the Equivalence between Neural Network and Support Vector Machine",2021,"","","","",199,"2022-07-13 09:23:14","","","","",,,,,4,4.00,1,4,1,"Recent research shows that the dynamics of an inﬁnitely wide neural network (NN) trained by gradient descent can be characterized by Neural Tangent Kernel (NTK) [27]. Under the squared loss, the inﬁnite-width NN trained by gradient descent with an inﬁnitely small learning rate is equivalent to kernel regression with NTK [4]. However, the equivalence is only known for ridge regression currently [6], while the equivalence between NN and other kernel machines (KMs), e.g. support vector machine (SVM), remains unknown. Therefore, in this work, we propose to establish the equivalence between NN and SVM, and speciﬁcally, the inﬁnitely wide NN trained by soft margin loss and the standard soft margin SVM with NTK trained by subgradient descent. Our main theoretical results include establishing the equivalences between NNs and a broad family of (cid:96) 2 regularized KMs with ﬁnite-width bounds, which cannot be handled by prior work, and showing that every ﬁnite-width NN trained by such regularized loss functions is approximately a KM. Furthermore, we demonstrate our theory can enable three practical applications, including (i) non-vacuous generalization bound of NN via the corresponding KM; (ii) non-trivial robustness certiﬁcate for the inﬁnite-width NN (while existing robustness veriﬁcation methods would provide vacuous bounds); (iii) intrinsically more robust inﬁnite-width NNs than those from previous kernel regression. Our code for the experiments is available at","",""
0,"M. Polczynski","Application of Machine Learning Clustering Algorithms to Thematic Map Design",2013,"","","","",200,"2022-07-13 09:23:14","","","","",,,,,0,0.00,0,1,9,"The application of data mining and machine learning technology and especially machine learning clustering algorithms can aid in the design of thematic maps portraying structural characteristics of geographical distributions. However, a review of the literature indicates that such applications are relatively rare. In order to stimulate greater use of clustering algorithms for thematic map design, an overview of the k means and EM clustering algorithms is given and two illustrative examples of the application of these algorithms to thematic map design are offered. The first example provides a choropleth map of the United States with states classified by two characteristics: population density and population growth. The second example identifies geographical clusters of sites associated with the 17 th century frontier between the Polish-Lithuanian Commonwealth and the Ottoman Empire. An overview of further applications of machine learning to characterization of this frontier region is summarized, and an appendix provides additional detail on the k means clustering algorithm as applied in the first example provided here. Introduction Computer technology is having a major impact on both the design and production of maps. This is particularly true for thematic maps, which portray the structural characteristics of some particular geographical distribution not apparent in data presented in tabular form (Robinson 1975, 9-14). Coupled with the general availability of databases containing massive amounts of digital information, computer technology has made thematic maps an increasingly common element of media ranging from scholarly publications to daily news tabloids. Computer technology and the availability of massive databases have also enabled significant progress in the field of data mining and machine learning, which is also focused on finding and describing structural patterns in data (Witten, Frank and Hall 2011, 59). Data mining and machine learning are now widely applied in areas ranging from internet search engines to manufacturing quality improvement (Polczynski and Kochanski, 2010). Given that both thematic maps and data mining and machine learning are focused on communicating the structure of data in a form readily comprehendible by humans, there exists a natural affinity between these two fields. The purpose of this brief technical note is to help stimulate the expansion of the application of data mining and machine learning technologies to the design of thematic maps. This note contains the following sections: The research question frames the scope of this work, which is the application of machine learning clustering algorithms to the design of thematic maps illustrating structural characteristics of geographical distributions. A focused literature review traces selected highlights in the application of computers and especially machine learning to the design of thematic maps. A brief overview of two clustering methods is provided, and two illustrative examples of the application of clustering to map design are offered. Each example contains a brief description of the data used to design the map, the method of data analysis, and the result of applying clustering to the data presented as a thematic map. The conclusion describes how the examples answer the research question, and outlines a specific area for continued expansion of the application of data mining and machine learning to thematic map design. An appendix provides additional detail on one common clustering algorithm. Research Question The research question addressed here can be broken down into three levels: 1. Can data mining and machine learning technology be used as a general tool to aid in designing thematic maps? 2. Can machine learning clustering algorithms be used to discover structural characteristics of geographical distributions? 3. Does the Weka data mining and machine learning workbench provide an appropriate toolset for cartographers preparing thematic maps? Regarding the first question, one objective of thematic mapping is to portray the structural characteristics of some particular geographical distribution not apparent in data presented in tabular form. The objective of data mining is the extraction of implicit, previously unknown, and potentially useful information from data using machine learning algorithms designed to find and describe structural patterns in data. Thus, thematic maps and data mining share a common goal of converting databases into a format readily comprehendible by humans. Given the long history of the application of computer technology to thematic map design and the general availability of robust data mining and machine learning tools, it would seem that these common goals would result in widespread application of machine learning technology to thematic map design, yet examples of this are rare compared to the overall volume of research done on thematic mapping. This leads to the highest level research question posed here: Can machine learning technology be used as a general tool to aid in designing thematic maps? Clustering is one of the basic types of machine learning tools used for data mining. Clustering algorithms examine the attributes of a set of objects, and then separate the objects into different clusters with similar attributes. By analogy to thematic maps, the objects being clustered correspond to the particular geographical distribution being mapped, and object attributes correspond to the structural characteristics of the distribution. With specific reference to choropleth maps, the clusters formed correspond to the classes of the geographical distribution, and the process of assigning objects to clusters is termed classification 1 . This analogy leads to the next level of research question posed here: Can clustering algorithms be used to classify geographical distributions? Affirmative answers to the preceding questions are irrelevant if data mining and machine learning tools suitable for application to thematic map design are not readily-available to map designers. Weka is a workbench of data mining and machine learning algorithms developed at the University of Waikato 2 . In spite (or possibly because) of the fact that Weka is free and open-source, the tools are robust and are widely-used by the academic community. User manuals, training materials, and tutorials for Weka are readily available via the web. Given that Weka is readily available to map designers leads to the last question posed here: Does the Weka data mining and machine learning workbench provide an appropriate toolset for cartographers preparing thematic maps? Literature Review The literature on the use of computers for map design and production is quite extensive. For example, since 1974 the annual AutoCarto Symposium on Automated Cartography 3 has generated hundreds of publications in this area. Development of geographic information systems (GIS) has significantly added to this literature, with one review finding 319 relevant publications in the specific area of GIS-based multicriteria decision analysis (Malczewski 2006). A similar situation applies to data mining and machine learning technology, where thousands of articles and books have been published over the last decades. Given the abundance of literature in these areas, this review focuses exclusively on selected examples of work lying at the crossroads of thematic map design and data mining and machine learning. Key references to publications that can aid cartographers in applying data mining and machine learning algorithms and the Weka toolkit in particular are also included. Although not typically associated with machine learning, natural breaks (Jenks) classification provides an early example of an approach to choropleth map design which relies on iterative calculations that can only be practically implemented using a computer (Robinson and others 1984, 365; Dent 1985, 205). Dating back to 1967, natural breaks classification (Jenks 1967; Coulson 1987) bears a strong resemblance to k means clustering (MacQueen 1967), a commonly used machine learning algorithm which also dates back to this time period. The brief description of k means clustering provided later in this note reveals the similarities between k means clustering and natural breaks classification. One example of the application of technologies commonly associated with machine learning to thematic map design is ChoroWare, a software toolkit for choropleth map classification (Armstrong, Xiao, and Bennett 2003; Xiao and Armstrong 2006). The specific issue addressed by ChoroWare is that choropleth map design is typically constrained by multiple conflicting objectives, making selection of map class intervals a multiobjective problem. Choroware uses genetic algorithms to generate a set of nondominated solutions to the multiobjective choropleth class interval problem. Choroware is free open-source software available for download 4 , and runs under the Unix 1 As is commonly the case when fields of study with different origins intersect, a clash of terminology occurs. Machine learning clustering corresponds to choropleth map classification. The difficulty that arises is that in machine learning terminology, classification refers to a completely different type of data mining function. 2 Weka is available for download at http://www.cs.waikato.ac.nz/ml/weka/ 3 AutoCarto Symposium on Automated Cartography: http://www.cartogis.org/autocarto.php 4 Available at http://choroware.sourceforge.net/ operating system. Genetic algorithms comprise a set of techniques commonly used in machine learning for optimization and search problems (Coley 2003). Recent work by Andrienko and Andrienko views computational methods associated with data mining and machine learning as a compliment to graphical methods, with the objective being to gain additional knowledge about data which cannot be easily acquired directly from viewing and manipulatio","",""
